---
url: /daily/软件设计师/08_算法设计与分析.md
---

# 08\_算法设计与分析

## 算法基础

### 算法基础知识

算法的五个重要特性：

* 有穷性
* 确定性
* 可行性
* 输入：0或多个
* 输出：一个或多个

### 算法分析基础

算法复杂性包括两个方面，一个是算法效率的度量（时间复杂度），一个是算法运行所需要的计算机资源量的度量（空间复杂度），这也是评价算法优劣的重要依据。

#### 时间复杂度

一个程序的时间复杂度是指程序运行从开始到结束所需要的时间。通常分析时间复杂度的方法是从算法中选取一种对于所研究的问题来说是基本运算的操作，以该操作重复执行的次数作为算法的时间度量。一般来说，算法中原操作重复执行的次数是规模n的某个函数T（n）。由于许多情况下要精确计算T（n）是困难的，因此引入了渐进时间复杂度在数量上估计一个算法的执行时间。

我们通常使用"O（）"来表示时间复杂度，其定义如下：如果存在两个正常数c和m，对于所有的n，当n≥m时有f（n）≤cg（n），则有f（n）=O（g（n））也就是说，随着n的增大，f（n）渐进地不大于g（n）。例如，一个程序的实际执行时间为3n^3+2n^2+n,则T（n）=O（n^3）。T（n）和n^3的值随n的增长渐近地靠拢。常见的渐进时间复杂度有：

![image-20240414222317823](/assets/image-20240414222317823.AeBTNmBL.png)

#### 空间复杂度

一个程序的空间复杂度是指程序运行从开始到结束所需的存储量。它通常包括固定部分和可变部分两个部分。

在算法的分析与设计中，经常会发现时间复杂度和空间复杂度之间有着微妙的关系，经常可以相互转换，也就是可以利用空间来换时间，也可以用时间来换空间。

## 常用算法

## 参考资料

视频

希赛网

文字版

https://cloud.tencent.com/developer/article/2385825

https://cloud.tencent.com/developer/article/2387285

https://blog.csdn.net/chengsw1993/article/details/125714839

---

---
url: /数据库/03.PostgreSQL/1_PostgreSQL介绍.md
---
# 1\_PostgreSQL介绍

## 前言

养活国内大半资源数据库团队的PostgreSQL是什么，架构是怎么样的？有MySQL,为什么还要有PostgreSQL? 你是一个进程员，你维护了一个电商网站，网站后台需要存储千万级商品数据，你会选择使用什么存储这些商品数据？当然是MySQL,毕竟没有人比你更懂什么是MySQL。 但随着业务发展，产品经理提了各种需求。 比如，离用户最近的商品门店是哪个？MySQL对地理计算支持很弱，所以你选了Redis的Geohash。 又比如，要支持搜索商品，MySQL的全文搜索功能太弱，所以你又引入了Elasticsearch,也就是ES。 再比如，要做跨平台商品比价，不同电商平台的数据字段各不相同，MySQL比较结构太死板，于是你将它们组织成Jason文档，引入了更灵活的MongoDB来读写数据。 原本简单的单体架构就变成了MySQL加MongoDB加Redis加ES的复杂分布式系统。 每个中间件都要运维监控，本来就因为头发太少，每一根都有它的名字，再学下去，头发也跟着大把掉。有没有一个数据库能原生支持这些复杂需求？好办，没有什么是加一层中间层不能解决的，如果有，那就再加一层。而有这样一款数据库，能原生支持这些复杂需求，甚至被称为 “养活国内大半自研数据库团队” 的全能选手，它就是 PostgreSQL（简称 PG）。

## 一、PostgreSQL 是什么？—— 不止是关系型数据库

什么是PostgreSQL? PostgreSQL,简称PG,跟MySQL一样，是个用select、update这些SQL语句读写数据的关系型数据库，但在功能和扩展性方面，可以将MySQL按在地上反复摩擦。说是数据库天花板也不过分，它的能力早已超越传统关系型数据库的边界：

* **核心定位**：功能全面的 “全能数据库”，支持关系型数据、JSON 文档、地理位置、数组等多种数据类型，SQL 查询能力强大，还具备极高的扩展性。
* **行业价值**：国内众多自研数据库团队的底层基础，模块化设计让企业可按需定制功能，无需重复造轮子，因此被调侃 “养活国内大半自研数据库团队”。
* **核心优势**：完全开源可商用，稳定性强、数据一致性有保障，既能替代 MySQL 作为基础存储，也能在部分场景下平替 MongoDB、ES、InfluxDB 等中间件，简化系统架构。

## 二、核心技术概念拆解 —— 搞懂 PG 底层逻辑

要理解 PG 的强大，首先得吃透它的核心技术概念，这些是支撑其高性能、高可靠性的基石：

### 1. 数据页：数据存储的基本单位

PG 会将数据表以 “堆表文件” 的形式存储在磁盘上（文件以数字命名，对应数据表的 RelFileNode ID）。为了避免大文件读写效率低下，PG 把数据拆分成一个个 **8KB 大小的数据页**—— 读写数据时，只需操作相关的数据页，而非整个文件，大幅提升效率。

### 2. B-Link 树：优化查询的索引神器

数据页再多，没有高效索引也会变成 “大海捞针”。PG 采用 **B-Link 树** 作为核心索引结构，它是 B+ 树的改进版：

* 底层叶子节点记录数据行的地址，指向堆表中的实际数据；
* 非叶子节点之间增加了右指针，页分裂时无需返回父节点，可直接跳转到右节点查询，性能更优。
* 除了 B-Link 树，PG 还支持多种索引类型，这也是它能平替 ES 等搜索中间件的关键。

### 3. 进程架构：稳定高效的 “多进程模式”

PG 采用 “主进程 + 多后端进程 + 后台进程” 的架构，确保稳定性和并发处理能力：

* **Postmaster（主进程）**：数据库的 “总指挥”，负责监听客户端连接、Fork 新的后端进程，以及监控所有进程的健康状态。

* **后端进程（Postgres）**：每个客户端连接对应一个独立的后端进程，专门处理 SQL 请求，进程间完全隔离 —— 就算某个进程崩溃，也不会影响其他连接，稳定性拉满。

* 后台进程

  ：默默干活的 “工具人”，包括：

  * Check Pointer：定期将共享内存中的脏页（修改后未写入磁盘的数据）刷入磁盘；
  * Background Writer：异步写入脏页，减轻 Check Pointer 的 IO 压力；
  * WAL Writer：负责将 WAL 缓冲区的日志写入磁盘。

### 4. 共享内存：提升并发读写效率

如果每个后端进程都单独缓存数据页，会造成内存浪费且数据不一致。PG 设计了 **共享内存区域**，其中的 “共享缓冲区” 专门缓存数据页和索引页：

* 后端进程查询数据时，先查共享缓冲区，命中则直接返回；
* 未命中再从磁盘读取数据页，加载到共享缓冲区后再返回，大幅减少磁盘 IO。

### 5. WAL 日志机制：数据一致性的 “守护神”

共享内存中的数据还没写入磁盘时，数据库崩溃怎么办？PG 靠 **WAL（Write-Ahead Logging，预写日志）** 机制解决：

* 所有数据更新操作，会先写入 WAL 缓冲区，再修改共享内存中的数据页；
* 事务提交时，WAL 缓冲区的日志会被刷入磁盘；
* 数据库崩溃重启后，可通过 WAL 日志 “重做” 未持久化的操作，确保数据不丢失。
* 关键优势：WAL 日志是顺序写入，而数据页是随机写入，顺序写的性能是随机写的几十倍，兼顾效率与可靠性。

### 6. 数据访问层（Access Methods）：连接底层与上层的桥梁

后端进程不直接操作磁盘或共享内存，而是通过 **数据访问层** 调用接口：

* Buffer Manager：对接共享内存，管理数据页缓存；
* Storage Manager：对接磁盘，负责数据页的读写；
* 对外提供增删改查的标准化接口，供上层模块调用。

### 7. 模块化扩展：无限可能的 “插件生态”

PG 的核心模块（优化器、执行器、数据访问层）都设计为开放接口，支持通过插件扩展功能：

* 支持矢量搜索、图数据库、JSON 增强等多种定制化需求；
* 社区有大量成熟插件，几乎能满足所有业务场景，这也是它成为自研数据库底层基础的核心原因。

## 三、PostgreSQL 查询与更新流程 —— 完整链路拆解

搞懂了核心概念，再看 PG 如何处理 SQL 请求，整个流程就清晰了：

### 1. 连接建立

应用通过网络连接到 Postmaster 主进程，主进程 Fork 一个专属的后端进程，后续所有 SQL 操作都通过这个后端进程处理。

### 2. SQL 处理

* **解析器**：检查 SQL 语法正确性，生成语法树；
* **优化器**：根据索引情况、数据分布，选择最优执行计划（比如是否使用 B-Link 树索引）；
* **执行器**：按照执行计划，调用数据访问层（Access Methods）的接口函数。

### 3. 数据读写

* 查询操作（SELECT）

  ：

  1. 执行器通过 Buffer Manager 检查共享缓冲区，是否存在目标数据页；
  2. 存在则直接返回数据；不存在则通过 Storage Manager 从磁盘读取数据页，加载到共享缓冲区后返回。

* 更新操作（UPDATE/INSERT/DELETE）

  ：

  1. 先通过数据访问层找到目标数据页（共享缓冲区没有则从磁盘加载）；
  2. 将操作记录写入 WAL 缓冲区；
  3. 修改共享缓冲区中的数据页（生成脏页）；
  4. 事务提交时，WAL Writer 将 WAL 日志刷入磁盘；
  5. 脏页由 Background Writer 异步刷入磁盘，或 Check Pointer 定期刷入。

### 4. 结果返回

执行器将处理结果返回给应用，完成一次 SQL 交互。

## 四、有 MySQL 了，为什么还要用 PostgreSQL？

这是很多开发者的疑问，核心答案在于 **“全能性” 和 “扩展性”**：

* 功能覆盖更广：MySQL 对 JSON、地理计算、全文搜索的支持较弱，而 PG 原生支持这些功能，无需引入 Redis、ES、MongoDB 等中间件，简化架构；
* 扩展性更强：PG 的模块化设计支持二次开发，国内大厂（如阿里、腾讯）的自研数据库大多基于 PG 定制，而 MySQL 定制化难度较高；
* 数据一致性更优：WAL 机制、多进程架构让 PG 在高并发场景下的数据一致性更有保障，适合金融、电商等核心业务；
* 开源生态成熟：完全开源可商用，社区活跃，插件丰富，无商业绑定风险。

当然，MySQL 也有优势（比如轻量、部署简单、读写性能在简单场景下更优），但如果你的业务需要复杂数据类型、多场景支持，或未来有定制化需求，PG 无疑是更长远的选择。

## 五、总结

PostgreSQL 能成为 “数据库天花板”，核心在于它的 **架构设计（多进程 + 共享内存）、可靠机制（WAL 日志）、高效索引（B-Link 树）和无限扩展能力**。它不仅能替代 MySQL 作为基础存储，还能减少中间件依赖，让系统架构更简洁，这也是它能支撑国内大半自研数据库团队的根本原因。

如果你正在做数据库选型，或想深入理解数据库底层原理，PG 绝对值得深入学习 —— 它的设计思想，能让你对 “高性能、高可靠、高扩展” 的数据库有全新认知。

### 参考资料

<https://www.bilibili.com/video/BV1CkCQBoEyp>

[PostgreSQL：文档：18：PostgreSQL 18.0 文档 - PostgreSQL 数据库](https://postgres.ac.cn/docs/18/index.html)

[PostgreSQL JDBC 开发指导 - postgre - 博客园](https://www.cnblogs.com/pgsql/p/pgjdbc.html)

[为什么选择 Redrock Postgres？ | Redrock Postgres 文档](https://doc.rockdata.net/zh-cn/usage/why-redrock/)

---

---
url: /01.指南/20.相关/15.笔记技巧.md
---

# 笔记技巧

::: note 摘要

为了让读者阅读时，不处于大片黑白的世界里，我们需要掌握更多丰富的笔记表现力:smile\_cat:

::: right

2025-03-31 @Teek

:::

## 使用 emoji 表情

阅读大片大片的文字难免产生视觉疲劳，而使用 emoji 表情，不仅缓解精神的渐眠，也会胜过千言。

在 markdown 里，使用 `:表情:` 输入表情，如

```md
你好:smile:，我喜欢:dog:，我小时候经常拿:100:分哦~~~，欢迎来到我的博客:heart:，一起学习吧:muscle:
```

效果如下：

> 你好:smile:，我喜欢:dog:，我小时候经常拿:100:分哦~~~，欢迎来到我的博客:heart:，一起学习吧:muscle:

很多指令肯定是记不了的，我们可以也可以去特定的网站获取表情的格式。也可以 copy 一个表情过来，markdown 会自动解析表情。

分享一些 emoji 网站：

* [Markdown 所有支持的 emoji 列表](https://github.com/markdown-it/markdown-it-emoji/blob/master/lib/data/full.mjs)
* [emoji 表情备忘录](https://www.webfx.com/tools/emoji-cheat-sheet)：有很多表情的格式
* [emoji 表情](https://emojipedia.org/)：有很多表情可以 copy
* [gitmoji](https://github.com/carloscuesta/gitmoji) 通过 emoji 表达 git 的操作内容

> windows 系统下按 Win + . 快速打开表情选择框（不是右侧小键盘的 .）

## 外部链接

使用外部链接，文字会变色，并且可以点击跳转，格式如下：

```
[Teek 官网](https://vp.teek.top)
```

效果：

[Teek 官网](https://vp.teek.top)

## 文本高亮

使用 `<mark>` 标签或者 ` `` ` 让文本高亮。

`<mark>` 标签可用于文字的突出，如果是一段字符，则使用 ` `` ` 包裹起来。

```md
`Teek` 是一款 <mark>轻量 & 简洁高效 & 灵活配置</mark>的 VitePress 主题。
```

`Teek` 是一款 轻量 & 简洁高效 & 灵活配置的 VitePress 主题。

## 内置徽章

输入：

```md
#### 《沁园春·雪》 <Badge text="摘"/>

北国风光<Badge text="注释" type="warning"/>，千里冰封，万里雪飘。

> <Badge text="译文" type="danger" />: 北方的风光。
```

* text：显示的文本
* type：`info` | `tip` | `warning` | `danger`，默认是 `tip`

输出：

#### 《沁园春·雪》&#x20;

北国风光，千里冰封，万里雪飘。

> : 北方的风光。

## 外置徽章

如果想用更多的自定义徽章，可使用 [Shields](https://shields.io/)来生成

```md
![stars](https://img.shields.io/github/stars/Kele-Bingtang/vitepress-theme-teek)
![Teek badge](https://img.shields.io/npm/v/vitepress-theme-teek.svg?style=flat-square)
![kbt](https://img.shields.io/badge/teek-天客-green)
```

![Star](https://img.shields.io/github/stars/Kele-Bingtang/vitepress-theme-teek)

![NPM Download](https://img.shields.io/npm/v/vitepress-theme-teek.svg?style=flat-square)

![Teek](https://img.shields.io/badge/Teek-天客-green)

想了解更多 Shields 的使用，请访问 [Shields](https://shields.io/)。

## TODO 待办列表

输出：

* \[ ] 吃饭
* \[ ] 睡觉
* \[x] 打豆豆

输入：

```markdown
- [ ] 吃饭
- [ ] 睡觉
- [x] 打豆豆
```

确保 `[ ]` 里有一个空格。

::: tip
支持所有列表语法，如：`1.`、`-`、`+`、`*` 等。
:::

## 分享卡片列表

分享卡片列表容器，可用于 `友情链接`、`项目推荐`、`诗词展示` 等。

输入：

````yml
::: shareCard
```yaml
- name: George Chan
  desc: 让我给你讲讲他的传奇故事吧
  avatar: https://z3.ax1x.com/2021/09/30/4oKMVI.jpg
  link: https://cyc0819.top/
  bgColor: '#FFB6C1' # 可选，默认 var(--bodyBg)。颜色值有 # 号时请添加单引号
  textColor: '#621529' # 可选，默认 var(--textColor)

- name: butcher2000
  desc: 即使再小的帆，也能远航
  avatar: https://gcore.jsdelivr.net/gh/Kele-Bingtang/static/user/20211029181901.png
  link: https://blog.csdn.net/weixin_46827107
  bgColor: '#CBEAFA'
  textColor: '#6854A1'

- name: Evan's blog
  desc: 前端的小学生
  avatar: https://gcore.jsdelivr.net/gh/xugaoyi/image_store@master/blog/20200103123203.jpg
  link: https://xugaoyi.com/
  bgColor: '#B9D59C'
  textColor: '#3B551F'
```
:::
````

输出：

::: shareCard

```yaml
- name: George Chan
  desc: 让我给你讲讲他的传奇故事吧
  avatar: https://z3.ax1x.com/2021/09/30/4oKMVI.jpg
  link: https://cyc0819.top/
  bgColor: "#FFB6C1" # 可选，默认 var(--bodyBg)。颜色值有 # 号时请添加单引号
  textColor: "#621529" # 可选，默认 var(--textColor)

- name: butcher2000
  desc: 即使再小的帆，也能远航
  avatar: https://gcore.jsdelivr.net/gh/Kele-Bingtang/static/user/20211029181901.png
  link: https://blog.csdn.net/weixin_46827107
  bgColor: "#CBEAFA"
  textColor: "#6854A1"

- name: Evan's blog
  desc: 前端的小学生
  avatar: https://gcore.jsdelivr.net/gh/xugaoyi/image_store@master/blog/20200103123203.jpg
  link: https://xugaoyi.com/
  bgColor: "#B9D59C"
  textColor: "#3B551F"
```

:::

不指定颜色，默认为白色，如下演示：

````yml
::: shareCard
```yaml
- name: 《静夜思》
  desc: 床前明月光，疑是地上霜。举头望明月，低头思故乡。
  bgColor: '#395AE3'
  textColor: '#242A38'

- name: Teek
  desc: ✨一个轻量、简洁高效、灵活配置的 VitePress 主题
  link: https://github.com/Kele-Bingtang/vitepress-theme-teek
  bgColor: '#DFEEE7'
  textColor: '#2A3344'
```
:::
````

::: shareCard

```yaml
- name: 《静夜思》
  desc: 床前明月光，疑是地上霜。举头望明月，低头思故乡。

- name: Teek
  desc: ✨一个轻量、简洁高效、灵活配置的 VitePress 主题
  link: https://github.com/Kele-Bingtang/vitepress-theme-teek
  bgColor: "#DFEEE7"
  textColor: "#2A3344"
```

:::

## 图文卡片列表

图文卡片列表容器，可用于 `项目展示`、`产品展示` 等。

输入：

````yaml
::: imgCard
```yaml
- img: https://vp.teek.top/blog/bg1.webp
  link: https://vp.teek.top
  name: 标题
  desc: 描述内容描述内容描述内容描述内容
  author: Teek
  avatar: https://testingcf.jsdelivr.net/gh/Kele-Bingtang/static/user/avatar1.png
- img: https://vp.teek.top/blog/bg2.webp
  link: https://vp.teek.top
  name: 标题
  desc: 描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容
  author: Teek
  avatar: https://testingcf.jsdelivr.net/gh/Kele-Bingtang/static/user/avatar1.png
- img: https://vp.teek.top/blog/bg2.webp
  link: https://vp.teek.top
  name: 标题
  desc: 描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容
  author: Teek
  avatar: https://testingcf.jsdelivr.net/gh/Kele-Bingtang/static/user/avatar1.png
- img: https://vp.teek.top/blog/bg3.webp
  link: https://vp.teek.top
  name: 标题
  desc: 描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容
```
:::
````

输出：

::: imgCard

```yaml
- img: https://vp.teek.top/blog/bg1.webp
  link: https://vp.teek.top
  name: 标题
  desc: 描述内容描述内容描述内容描述内容
  author: Teek
  avatar: https://testingcf.jsdelivr.net/gh/Kele-Bingtang/static/user/avatar1.png
- img: https://vp.teek.top/blog/bg2.webp
  link: https://vp.teek.top
  name: 标题
  desc: 描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容
  author: Teek
  avatar: https://testingcf.jsdelivr.net/gh/Kele-Bingtang/static/user/avatar1.png
- img: https://vp.teek.top/blog/bg2.webp
  link: https://vp.teek.top
  name: 标题
  desc: 描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容
  author: Teek
  avatar: https://testingcf.jsdelivr.net/gh/Kele-Bingtang/static/user/avatar1.png
- img: https://vp.teek.top/blog/bg3.webp
  link: https://vp.teek.top
  name: 标题
  desc: 描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容
```

:::

## 导航卡片

导航卡片容器，可以用于制作 `导航站点`。

输入：

````yaml

::: navCard
```yaml
- name: 百度
  desc: 百度——全球最大的中文搜索引擎及最大的中文网站，全球领先的人工智能公司
  link: http://www.baidu.com/
  img: https://www.baidu.com/favicon.ico
  badge: 搜索引擎
- name: Google
  desc: 全球最大的搜索引擎公司
  link: http://www.google.com/
  img: https://ts1.cn.mm.bing.net/th/id/R-C.58c0f536ec073452434270fb559c3f8c?rik=SnOUNtUtPLX6ww&riu=http%3a%2f%2fwww.sz4a.cn%2fPublic%2fUploads%2fimage%2f20230303%2f1677839482835474.png&ehk=J1lqoeszPGEWzDOSZQ3JxzXsklfd0QzgrJu6ZVvESKk%3d&risl=&pid=ImgRaw&r=0
  badge: 搜索引擎
  badgeType: tip
```
:::
````

输出：

::: navCard

```yaml
- name: 百度
  desc: 百度——全球最大的中文搜索引擎及最大的中文网站，全球领先的人工智能公司
  link: http://www.baidu.com/
  img: https://www.baidu.com/favicon.ico
  badge: 搜索引擎
- name: Google
  desc: 全球最大的搜索引擎公司
  link: http://www.google.com/
  img: https://ts1.cn.mm.bing.net/th/id/R-C.58c0f536ec073452434270fb559c3f8c?rik=SnOUNtUtPLX6ww&riu=http%3a%2f%2fwww.sz4a.cn%2fPublic%2fUploads%2fimage%2f20230303%2f1677839482835474.png&ehk=J1lqoeszPGEWzDOSZQ3JxzXsklfd0QzgrJu6ZVvESKk%3d&risl=&pid=ImgRaw&r=0
  badge: 搜索引擎
  badgeType: tip
```

:::

## 自定义容器

自定义容器可以通过它们的类型、标题和内容来定义。

### 默认标题

输入：

```md
::: note
This is an note box.
:::

::: info
This is an info box.
:::

::: tip
This is a tip.
:::

::: warning
This is a warning.
:::

::: danger
This is a dangerous warning.
:::

::: details
This is a details block.
:::

::: center
Markdown 拓展
:::

::: tip 摘要
很久之前，我决定踏上的这条路，映照了我与未来的因果。
::: right
2021-11-13 @Teek
:::
```

输出：

::: note
This is an note box.
:::

::: info
This is an info box.
:::

::: tip
This is a tip.
:::

::: warning
This is a warning.
:::

::: danger
This is a dangerous warning.
:::

::: details

```ts
This is a details block.
```

:::

::: center
Markdown 拓展
:::

::: tip 摘要
很久之前，我决定踏上的这条路，映照了我与未来的因果。
::: right
2021-11-13 @Teek
:::

### 自定义标题

可以通过在容器的 `type` 之后附加文本来设置自定义标题。

输入：

````md
::: danger STOP
危险区域，请勿继续
:::

::: details 点我查看代码

```js
console.log("Hello, VitePress!");
```

:::
````

输出：

::: danger STOP
危险区域，请勿继续
:::

::: details 点我查看代码

```js
console.log("Hello, Teek!");
```

:::

## GitHub 风格的警报

VitePress 同样支持以标注的方式渲染 [GitHub 风格的警报](https://docs.github.com/en/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax#alerts)。

```md
> [!NOTE]
> 强调用户在快速浏览文档时也不应忽略的重要信息。

> [!TIP]
> 有助于用户更顺利达成目标的建议性信息。

> [!IMPORTANT]
> 对用户达成目标至关重要的信息。

> [!WARNING]
> 因为可能存在风险，所以需要用户立即关注的关键内容。

> [!CAUTION]
> 行为可能带来的负面影响。
```

> \[!NOTE]
> 强调用户在快速浏览文档时也不应忽略的重要信息。

> \[!TIP]
> 有助于用户更顺利达成目标的建议性信息。

> \[!IMPORTANT]
> 对用户达成目标至关重要的信息。

> \[!WARNING]
> 因为可能存在风险，所以需要用户立即关注的关键内容。

> \[!CAUTION]
> 行为可能带来的负面影响。

## 自定义锚点

要为标题指定自定义锚点而不是使用自动生成的锚点，请向标题添加后缀：

```
# 使用自定义锚点 {#my-anchor}
```

这允许将标题链接为 `#my-anchor`，而不是默认的 `#使用自定义锚点`。

## GitHub 风格的表格

输入：

```
| Tables        |      Are      |  Cool |
| ------------- | :-----------: | ----: |
| col 3 is      | right-aligned | $1600 |
| col 2 is      |   centered    |   $12 |
| zebra stripes |   are neat    |    $1 |
```

输出：

| Tables        |      Are      |   Cool |
| ------------- | :-----------: | -----: |
| col 3 is      | right-aligned | $1600 |
| col 2 is      |   centered    |   $12 |
| zebra stripes |   are neat    |    $1 |

## 目录表 (TOC)

输入：

```
[[toc]]
```

输出：

\[\[toc]]

可以使用 `markdown.toc` 选项配置 TOC 的呈现效果。

## 代码块中的语法高亮

VitePress 使用 [Shiki](https://github.com/shikijs/shiki) 在 Markdown 代码块中使用彩色文本实现语法高亮。Shiki 支持多种编程语言。需要做的就是将有效的语言别名附加到代码块的开头：

输入：

````
```js
export default {
  name: 'MyComponent',
  // ...
}
```
````

````
```html
<ul>
  <li v-for="todo in todos" :key="todo.id">
    {{ todo.text }}
  </li>
</ul>
```
````

输出：

```js
export default {
  name: "MyComponent",
  // ...
};
```

```html
<ul>
  <li v-for="todo in todos" :key="todo.id">{{ todo.text }}</li>
</ul>
```

在 Shiki 的代码仓库中，可以找到[合法的编程语言列表](https://shiki.style/languages)。

## 在代码块中实现行高亮

输入：

````
```js{4}
export default {
  data () {
    return {
      msg: 'Highlighted!'
    }
  }
}
```
````

输出：

```js{4}
export default {
  data () {
    return {
      msg: 'Highlighted!'
    }
  }
}
```

除了单行之外，还可以指定多个单行、多行，或两者均指定：

* 多行：例如 `{5-8}`、`{3-10}`、`{10-17}`
* 多个单行：例如 `{4,7,9}`
* 多行与单行：例如 `{4,7-13,16,23-27,40}`

输入：

````
```js{1,4,6-8}
export default { // Highlighted
  data () {
    return {
      msg: `Highlighted!
      This line isn't highlighted,
      but this and the next 2 are.`,
      motd: 'VitePress is awesome',
      lorem: 'ipsum'
    }
  }
}
```
````

输出：

```js{1,4,6-8}
export default { // Highlighted
  data () {
    return {
      msg: `Highlighted!
      This line isn't highlighted,
      but this and the next 2 are.`,
      motd: 'VitePress is awesome',
      lorem: 'ipsum',
    }
  }
}
```

也可以使用 `// [!code highlight]` 注释实现行高亮。

输入：

````
```js
export default {
  data () {
    return {
      msg: 'Highlighted!' // [!!code highlight]
    }
  }
}
```
````

输出：

```js
export default {
  data() {
    return {
      msg: "Highlighted!", // [!code highlight]
    };
  },
};
```

## 代码块中聚焦

在某一行上添加 `// [!code focus]` 注释将聚焦它并模糊代码的其他部分。

此外，可以使用 `// [!code focus:<lines>]` 定义要聚焦的行数。

输入：

````
```js
export default {
  data () {
    return {
      msg: 'Focused!' // [!!code focus]
    }
  }
}
```
````

输出：

```js
export default {
  data() {
    return {
      msg: "Focused!", // [!code focus]
    };
  },
};
```

## 代码块中的颜色差异

在某一行添加 `// [!code --]` 或 `// [!code ++]` 注释将会为该行创建 diff，同时保留代码块的颜色。

输入：

````
```js
export default {
  data () {
    return {
      msg: 'Removed' // [!!code --]
      msg: 'Added' // [!!code ++]
    }
  }
}
```
````

输出：

```js
export default {
  data () {
    return {
      msg: 'Removed' // [!code --]
      msg: 'Added' // [!code ++]
    }
  }
}
```

## 高亮“错误”和“警告”

在某一行添加 `// [!code warning]` 或 `// [!code error]` 注释将会为该行相应的着色。

输入：

````
```js
export default {
  data () {
    return {
      msg: 'Error', // [!!code error]
      msg: 'Warning' // [!!code warning]
    }
  }
}
```
````

输出：

```js
export default {
  data() {
    return {
      msg: "Error", // [!code error]
      msg: "Warning", // [!code warning]
    };
  },
};
```

## 行号

可以通过以下配置为每个代码块启用行号：

```js
export default {
  markdown: {
    lineNumbers: true,
  },
};
```

可以在代码块中添加 `:line-numbers` / `:no-line-numbers` 标记来覆盖在配置中的设置。

还可以通过在 `:line-numbers` 之后添加 `=` 来自定义起始行号，例如 `:line-numbers=2` 表示代码块中的行号从 2 开始。

输入：

````md
```ts {1}
// 默认禁用行号
const line2 = "This is line 2";
const line3 = "This is line 3";
```

```ts:line-numbers {1}
// 启用行号
const line2 = 'This is line 2'
const line3 = 'This is line 3'
```

```ts:line-numbers=2 {1}
// 行号已启用，并从 2 开始
const line3 = 'This is line 3'
const line4 = 'This is line 4'
```
````

输出：

```ts {1}
// 默认禁用行号
const line2 = "This is line 2";
const line3 = "This is line 3";
```

```ts:line-numbers {1}
// 启用行号
const line2 = 'This is line 2'
const line3 = 'This is line 3'
```

```ts:line-numbers=2 {1}
// 行号已启用，并从 2 开始
const line3 = 'This is line 3'
const line4 = 'This is line 4'
```

## 代码组

可以像这样对多个代码块进行分组：

输入：

````md
::: code-group

```js [config.js]
/**
 * @type {import('vitepress').UserConfig}
 */
const config = {
  // ...
};

export default config;
```

```ts [config.ts]
import type { UserConfig } from "vitepress";

const config: UserConfig = {
  // ...
};

export default config;
```

:::
````

输出：

::: code-group

```js [config.js]
/**
 * @type {import('vitepress').UserConfig}
 */
const config = {
  // ...
};

export default config;
```

```ts [config.ts]
import type { UserConfig } from "vitepress";

const config: UserConfig = {
  // ...
};

export default config;
```

:::

## 图片懒加载

通过在配置文件中将 `lazyLoading` 设置为 `true`，可以为通过 markdown 添加的每张图片启用懒加载。

```js
export default {
  markdown: {
    image: {
      // 默认禁用；设置为 true 可为所有图片启用懒加载。
      lazyLoading: true,
    },
  },
};
```

---

---
url: /@pages/tagsPage.md
---


---

---
url: /daily/软件设计师/11_标准化和软件知识产权基础知识.md
---

# 标准化和软件知识产权基础知识

## 知识产权基础知识

### 保护期限

![image.png](/assets/0541467bc4244d349ee1e44721a12f92.B1DvvzIQ.png)

### 知识产权人的确定

单位和个人的**著作权**归属情况：

![image.png](/assets/cdc4770237ec4b13bd0e321cf11e1bdb.BtMNSoTG.png)

单位和委托的区别在于，**当合同中未规定著作权的归属时，著作权默认归于单位，而委托创作中，著作权默认归属于创作方个人**，具体如下：

![image.png](/assets/8b508f9ce46b4dad964dfa450bb8ae15.DheI__Bs.png)

### 侵权判定

![img](/assets/20200819121710744.-0Li70HF.png)

![在这里插入图片描述](/assets/eab966c500624d4ca4da1d6cb89eb9f2.CKLHr5BG.png)

## 标准化基础知识

### 标准的分类

* 国际标准：ISO、IEC等国际标准化组织

* 国家标准：GB-中国、ANSI-美国、BS-英国、JIS-日本

* 区域标准：又称为地区标准，如PASC-太平洋地区标准会议、CEN-欧洲标准委员会、ASAC-亚洲标准咨询委员会、ARSO—非洲地区标准化组织

* 行业标准：GJB-中国军用标准、MIT-S-美国军用标准、IEEE-美国电气电子工程师协会

* 地方标准：国家的地方一级行政机构制订的标准

* 企业标准

* 项目规范

### 标准的编号

* 国际：标准代号+专业类号+顺序号+年代号。

* 我国标准代号及编号：
  * **强制性国家标准代号为GB，**
  * **推荐性国家标准代号为GB/T，**
  * **指导性国家标准代号为GB/Z，**
  * **国家实物标准代号GSB**。

* 行业标准代号：一般由汉语拼音大写字母组成，如汽车QC等。

* 地方标准代号：由大写汉语拼音DB加上省级行政区划代码的前两位组成。

* 企业标准代号：由“Q/”加上企业代号组成。

---

---
url: /daily/博客文档/其他博客文档/博客园美化.md
---

# 博客园美化

江北江南几度秋，梦里朱颜换。【宋代】周紫芝《卜算子·席上送王彦猷》

#### 博客侧边栏公告

看板娘

```html
<script src="https://cdn.jsdelivr.net/npm/live2d-widget@3.0.4/lib/L2Dwidget.min.js"></script>
<script type="text/javascript">
    L2Dwidget.init();
</script>
```

时钟

```html
<div>
<!-- 会动的小人时钟设置 -->
<embed allowscriptaccess="never" allownetworking="internal" invokeurls="false" src="https://files.cnblogs.com/files/mmzs/flashDate.swf"
pluginspage="https://files.cnblogs.com/files/mmzs/flashDate.swf" type="application/x-shockwave-flash" quality="high" autostart="0" wmode="transparent" width="220"
height="65" align="middle">
</div>
```

#### 页脚HTML

```html
<script type="text/javascript">
/* 鼠标特效 */
var a_idx = 0;
jQuery(document).ready(function($) {
    $("body").click(function(e) {
        var a = new Array("❤welcome to❤","❤博客园❤","❤辣條先生❤","❤富强❤","❤民主❤","❤文明❤","❤和谐❤","❤自由❤","❤平等❤","❤公正❤","❤法治❤","❤爱国❤","❤敬业❤","❤诚信❤","❤友善❤");
        var $i = $("<span></span>").text(a[a_idx]);
        a_idx = (a_idx + 1) % a.length;
        var x = e.pageX,
        y = e.pageY;
        $i.css({
            "z-index": 999999999999999999999999999999999999999999999999999999999999999999999,
            "top": y - 20,
            "left": x,
            "position": "absolute",
            "font-weight": "bold",
            "color": "rgb("+~~(255*Math.random())+","+~~(255*Math.random())+","+~~(255*Math.random())+")"
        });
        $("body").append($i);
        $i.animate({
            "top": y - 180,
            "opacity": 0
        },
        1500,
        function() {
            $i.remove();
        });
    });
});
</script>
```

#### 主题

一些组件：https://www.cnblogs.com/zouwangblog/p/10996446.html

https://www.yuque.com/awescnb/user

cute-cnblogs：https://www.cnblogs.com/miluluyo/p/11677303.html（使用）

Cnblogs-Theme-SimpleMemory：https://www.cnblogs.com/chenkeer/p/15193179.html

https://blog.csdn.net/weixin\_45765795/article/details/113928881

Silence：https://www.cnblogs.com/KOKODA/p/10522432.html

https://github.com/BNDong/Cnblogs-Theme-SimpleMemory

Sakura：https://www.cnblogs.com/xytpz/p/14417463.html

awescnb：https://www.cnblogs.com/alanjiang/p/16324307.html

Bili2.0：https://www.cnblogs.com/gshang/p/biliTheme.html

cute-cnblogs 版本一：

页首HTML

```html
<link rel='stylesheet' href='https://blog-static.cnblogs.com/files/elkyo/cute-cnblogs.css'>
<link rel='stylesheet' href='https://cdn.bootcss.com/animate.css/3.7.2/animate.min.css'>
<link rel="stylesheet" href="https://blog-static.cnblogs.com/files/elkyo/siyuan.css" />
<script src="https://cdn.bootcss.com/jquery/3.4.1/jquery.min.js"></script>
<script src="https://blog-static.cnblogs.com/files/elkyo/monitoring.js"></script>
<link rel="stylesheet" href="https://blog-static.cnblogs.com/files/elkyo/OwO.min.css" />
<script src="https://blog-static.cnblogs.com/files/elkyo/OwO.min.js"></script>
<script src="https://blog-static.cnblogs.com/files/elkyo/cute-cnblogs.js"></script>
<script src="https://sdk.jinrishici.com/v2/browser/jinrishici.js" charset="utf-8"></script>
<script src="https://blog-static.cnblogs.com/files/elkyo/three.min.js"></script>
<script src='https://blog-static.cnblogs.com/files/elkyo/star.js'></script>
<script>
    miluframe({
        /*个人链接地址*/
        Youself:'https://daneliya.github.io/',
        /*导航栏信息*/
        custom:[{
            name:'留言板',
            link:'https://www.cnblogs.com/xu-xiaolong/p/10875959.html',
            istarget:false
        },{
            name:'技能树',
            link:'https://daneliya.github.io/',
            istarget:true
        }],
        /*自己的友链页面后缀*/
        Friends_of_the:'p/15324545.html',
        /*自己的友链信息*/
        resume:{
            "name":"尘事如霜人如水",
            "link":"https://www.cnblogs.com/xu-xiaolong",
            /* "headurl":"https://images.cnblogs.com/cnblogs_com/elkyo/1558759/o_o_my.jpg"*/
            "headurl":"https://files.cnblogs.com/files/xu-xiaolong/AF266BD00FFD1B7CC84BA55EFC59CCF3.svg"
            "introduction":"大道至简，知易行难。"
        },
        /*友链信息*/
        unionbox:[{
            "name":"尘事如霜人如水",
            "introduction":"生活是没有标准答案的。",
            "url":"https://www.cnblogs.com/xu-xiaolong",
            /* "headurl":"https://images.cnblogs.com/cnblogs_com/elkyo/1558759/o_o_my.jpg"*/
            "headurl":"https://files.cnblogs.com/files/xu-xiaolong/AF266BD00FFD1B7CC84BA55EFC59CCF3.svg"
        },{
            "name":"尘事如霜人如水的技能树",
            "introduction":"大道至简，知易行难。",
            "url":"https://daneliya.github.io/",
            /* "headurl":"https://images.cnblogs.com/cnblogs_com/elkyo/1558759/o_o_my.jpg"*/
            "headurl":"https://files.cnblogs.com/files/xu-xiaolong/AF266BD00FFD1B7CC84BA55EFC59CCF3.svg"
        }],
        /*点击页面时候的弹出文本显示*/
        clicktext:new Array("ヾ(◍°∇°◍)ﾉﾞ加油哟~ ——尘事如霜人如水","生活是没有标准答案的。  ——尘事如霜人如水"),
        /*github链接*/
        githuburl:'https://github.com/daneliya'
    })
</script>
```

---

---
url: /Redis/Redis基础/3_布隆过滤器.md
---

# 布隆过滤器

## 概念

是一个很长的二进制向量，布隆过滤器主要**用于判断一个元素是否在一个集合中**。

## 理论基础

**Hash函数**的概念

在Java中的HashMap，HashSet存在hashcode()这个函数，哈希函数是可以将任意大小的输入数据转换成特定大小的输出数据的函数，转换后的数据称为**哈希值**。

哈希函数有以下特点：

* 如果根据同一个哈希函数得到的哈希值不同，那么这两个哈希值的原始输入值肯定不同。
* 如果根据同一个哈希函数得到的两个哈希值相等，两个哈希值的原始输入值有可能相等，有可能不相等。

参考资料

\[1]. https://zhuanlan.zhihu.com/p/348332384

---

---
url: /数据库/04.分布式数据库TIDB/2_部署迁移.md
---

# 部署迁移

> 注意，TiDB 仅支持 mac 和 Linux ，windows 可以使用虚拟机来安装。

最基础的 TiDB 测试集群通常由 2 个 TiDB 实例、3 个 TiKV 实例、3 个 PD 实例和可选的 TiFlash 实例构成。通过 TiUP Playground，可以快速搭建出上述的一套基础测试集群，各实例的作用：

* TiDB Server：负责SQL解析和请求处理
* PD Server：确保调度中心高可用
* TiKV Server：数据分片存储
* TiFlash：用于分析查询加速

安装迁移步骤如下：

## 一、安装 TiUP

系统环境

```
系统：OpenEuler-25.03
```

下载并安装 TiUP：

```sh
curl --proto '=https' --tlsv1.2 -sSf https://tiup-mirrors.pingcap.com/install.sh | sh
```

安装成功后会出现提示信息：

![tidb\_img4](/assets/tidb_img4.YokJdtAE.png)

记得执行一下 source 命令，让变量和设置在当前命令行中立即生效：

```Bash
source ${your_shell_profile}
```

## 二、启动并创建集群

执行启动命令时，每次重新执行命令会得到一个全新的集群，所以我们需要带上 --tag 参数来持久化数据：

```Bash
tiup playground --tag thumb
```

出现以下提示，即是启动成功了：

![tidb\_img5](/assets/tidb_img5.D6tRK7qZ.png)

根据提示访问 http://127.0.0.1:2379/dashboard，就能看到 TiDB 的面板了：

![image-20250615154530913](/assets/image-20250615154530913.Y3kLFSm6.png)

默认账号为root，密码为空；

同时部署的Grafana账号密码为（admin/admin）

![image-20250615154746136](/assets/image-20250615154746136.B-AUf9Rb.png)

这个面板可以用于监控数据库的 QPS、延迟、CPU、内存等核心数据、各个节点的运行状况、慢查询等信息，排查数据库问题的时候非常有用~

## 三、数据迁移

现在 TiDB 已经成功启动了，接下来就是将 MySQL 的数据迁移到 TiDB。TiDB 的迁移相关内容可以参考文档：[数据迁移概述](https://docs.pingcap.com/zh/tidb/stable/migration-overview/)。

在实际生产中，官方的建议是使用 TiDB DM（适合小数据量，例如 1 TB 以内）和 TiDB Lightning（大数据量）进行数据迁移。

💡 如果在生产环境中遇到数据迁移的需求，一定要有完善的流程和策略，举个例子：

1. 全量数据同步：使用 TiDB DM 工具进行初始数据迁移
2. 增量数据同步：配置 MySQL Binlog 到TiDB的实时同步
3. 双写验证阶段：应用同时写入 MySQL 和TiDB，比对数据一致性
4. 切换读流量：将读请求切换到 TiDB
5. 切换写流量：确认无问题后，将写请求切换到 TiDB
6. 下线旧MySQL：完成迁移后，逐步下线 MySQL 实例

不过项目的数据量远远达不到 TB 级别，同时只需要一次全量同步即可。而且上述的两种方式都比较麻烦，所以可以直接用 SQL 文件的方式迁移数据。

### 1、导出原始数据

首先将 MySQL 中的数据导出：

![tidb\_img7](/assets/tidb_img7.CcKbwOX5.png)

![tidb\_img8](/assets/tidb_img8.BT3HVi44.png)

### 2、创建 TiDB 数据库并导入

![tidb\_img9](/assets/tidb_img9.D5n31eSP.png)

手动创建数据库 后，执行刚刚导出的 SQL 文件：

![tidb\_img10](/assets/tidb_img10.Cs_Xjb-L.png)

这样，我们就完成了数据从 MySQL 到 TiDB 的迁移。由于 TiDB 兼容 MySQL 协议，所以不需要改动业务代码，需要更改 yml 文件中的 `spring.datasource.url` 即可，如果没有给 TiDB 设置密码等安全配置，记得要把 `spring.datasource.password` 留空。

### 3、更改链接配置

将 yml 文件中，数据库的配置修改为：

```YAML
spring:  
  datasource:  
    url: jdbc:mysql://localhost:4000/thumb_db  
    username: root  
    password:
```

### 4、测试

启动项目，访问点赞接口测试一下，TiDB 里已经新增了一条记录：

![tidb\_img11](/assets/tidb_img11.gM_pHdL8.png)

这样，迁移就完成了，可以自行测试一下迁移前后的性能对比。

## 四、问题

### 问题一：TiDB 部署之后，Windows 无法本地访问远程 TiDB

#### 方法一：

直接在启动命令之后加 `-host 0.0.0.0`，完整命令如下：

```
tiup playground --tag thumb --host 0.0.0.0
```

执行命令之后，看到下图就算成功了

![image-20250615154151204](/assets/image-20250615154151204.DYxnYOnW.png)

#### 方法二

1）修改配置

打开指定的配置文件 `/root/.tiup/data/thumb/tidb-0/tidb.toml`，找到 `[server]` 部分，并将 `host` 修改为 `0.0.0.0`：

```sh
[server]
host = "0.0.0.0"
port = 4000
```

如果没有 `[server]` 部分，可以直接添加上述内容。

2）重启TiDB服务

由于你使用的是 `tiup` 管理 TiDB，可以通过 `tiup` 命令重启服务：

```sh
tiup cluster restart <cluster-name>
```

如果不确定集群名称，可以先列出所有集群：

```sh
tiup cluster list
```

然后使用正确的集群名称重启服务。

3）验证监听状态

重启后，再次检查 TiDB 是否监听了 0.0.0.0:4000：

```sh
netstat -tuln | grep 4000
```

正确的输出应该是：

```sh
tcp        0      0 0.0.0.0:4000          0.0.0.0:*               LISTEN
```

---

---
url: /daily/软件设计师/04_操作系统知识.md
---

# 操作系统知识

## 一、操作系统概述

操作系统（Operating System，简称OS）是一种软件，用于管理计算机硬件和软件资源，提供给用户和应用程序一个简单、统一的接口，以方便用户和应用程序的操作和管理。

操作系统的背景和由来可以追溯到计算机发展的早期阶段。在 1950 年代，当时的计算机是非常庞大和昂贵的设备，主要用于科学计算和军事应用。为了充分利用这些计算机资源，需要解决如下问题：

1. 资源管理：计算机设备包括处理器、内存、磁盘、输入输出设备等，如何合理地分配、调度和管理这些资源，以提高计算机系统的利用率和效率。
2. 多道程序设计：为了充分利用计算机资源，需要让多个程序同时运行。但是，由于计算机资源有限，如何使多个程序同时运行且互不干扰成为了一个挑战。

为了解决上述问题，研究人员开始开发操作系统。20 世纪 50 年代末至 60 年代初，出现了一系列的操作系统，如GM-NAA I/O、FORTRAN Monitor System、IBM OS/360等。这些操作系统主要用于管理计算机硬件资源，提供了一些基本功能如进程管理、存储管理、文件管理等。

随着计算机技术的发展，操作系统变得越来越复杂，功能也越来越强大。现代操作系统不仅提供了更多的功能，如图形用户界面、网络通信、安全管理等，还支持多种硬件平台和应用程序。目前常见的操作系统有Windows、Mac OS、Linux等。

### :deciduous\_tree:操作系统的作用

| 主要作用           | 描述                                                       |
| :----------------- | :--------------------------------------------------------- |
| 资源管理           | 管理和分配计算机硬件资源，如处理器、内存、存储器和设备。   |
| 用户界面           | 提供与计算机系统交互的方式，如命令行界面和图形用户界面。   |
| 文件管理           | 管理计算机系统中的文件，包括创建、读写、复制和删除等操作。 |
| 进程管理           | 管理和调度计算机系统中的进程，实现多任务处理。             |
| 设备驱动程序       | 管理和控制计算机系统中的硬件设备，如打印机和键盘。         |
| 安全和保护         | 提供安全性和保护机制，保护计算机系统和用户的数据。         |
| 网络通信和连接     | 提供网络通信和连接功能，实现与外部网络的交互。             |
| 性能优化和调优     | 优化系统性能，提高计算机系统的响应速度和效率。             |
| 错误处理和故障恢复 | 处理错误和故障情况，保证系统的可靠性和稳定性。             |

### :deciduous\_tree:操作系统的特征

| 特征     | 描述                                                         |
| :------- | :----------------------------------------------------------- |
| 并发性   | 操作系统可以同时处理多个任务，使多个程序能够交替执行，提高计算机的效率和响应性 |
| 共享性   | 操作系统可以管理和控制计算机资源的共享，包括内存、处理器、硬盘等，使多个程序能够同时访问和使用这些资源 |
| 虚拟性   | 操作系统通过虚拟化技术，将物理资源抽象成虚拟资源，使多个程序能够共享和利用这些虚拟资源，提高资源的利用率 |
| 不确定性 | 操作系统需要处理各种不确定因素，如不可预测的用户输入、硬件故障、网络中断等，通过错误检测和恢复机制来应对这些不确定性的情况 |

### :deciduous\_tree:操作系统的功能

| 功能     | 描述                                                         |
| :------- | :----------------------------------------------------------- |
| 进程管理 | 控制和协调计算机系统中运行的进程，包括进程的创建、调度、同步和通信等。 |
| 存储管理 | 管理计算机系统中的存储器资源，包括内存分配、地址转换和内存保护等。 |
| 文件管理 | 对计算机系统中的文件进行组织、存储和访问，包括文件的创建、读写、删除和控制等。 |
| 设备管理 | 管理计算机系统中的输入/输出设备，包括设备的分配、控制和驱动等。 |
| 作业管理 | 控制和协调计算机系统中的作业，包括作业的调度、提交、分配和监控等。 |

### :deciduous\_tree:操作系统的分类

| 分类           | 描述                                                         |
| :------------- | :----------------------------------------------------------- |
| 批处理操作系统 | 处理批量作业，按照预定的顺序自动执行作业，并进行作业调度和资源管理。 |
| 分时操作系统   | 多个用户通过共享的终端或网络同时访问计算机系统，系统轮流分配给每个用户一小段时间来使用CPU。 |
| 实时操作系统   | 对要求即时响应的任务有较高的时间要求，能够快速响应外部事件，提供可预测的响应时间。 |
| 网络操作系统   | 提供网络资源共享和管理的操作系统，支持多台计算机互相通信和协作。 |
| 分布式操作系统 | 将多台物理分散的计算机组成一个互相关联的系统，在不同计算机之间进行任务分配、通信和资源共享。 |
| 微机操作系统   | 运行在个人计算机或工作站上的操作系统，例如Windows、macOS、Linux等。 |
| 嵌入式操作系统 | 运行在嵌入式系统中的操作系统，嵌入在各种设备中，如智能手机、汽车电子设备、家庭电器等。 |

### :deciduous\_tree:计算机启动的基本流程

计算机启动的基本流程为：

1. BIOS自检：开机后，计算机首先进行BIOS自检，检查硬件设备是否正常工作。BIOS（Basic Input/Output System）是一段固化在主板ROM芯片上的程序，用于初始化和检测计算机的硬件设备。
2. 启动引导：BIOS完成自检后，会从预设的启动设备（通常是硬盘）中读取引导扇区（Master Boot Record，MBR）中的引导程序。引导程序会加载操作系统的启动程序。
3. 操作系统加载：操作系统的启动程序会被引导程序加载到计算机的内存中。启动程序会进一步初始化硬件设备、建立内存空间映射以及其他必要的准备工作。
4. 用户登录：操作系统加载完成后，会显示用户登录界面。用户输入正确的用户名和密码后，操作系统会验证身份，并加载用户的配置文件和个人设置。
5. 桌面加载：登录成功后，操作系统会加载桌面环境或图形用户界面（GUI），提供用户操作和程序运行的接口。
6. 启动应用程序：在桌面加载后，用户可以通过启动菜单、快捷方式或命令行来启动需要的应用程序。操作系统会加载应用程序的相关文件和库文件，并分配系统资源给应用程序运行。

## 二、进程管理

### :deciduous\_tree:进程的组成和状态

操作系统中的进程是指正在运行的程序的实例。每个进程都有自己的地址空间、数据和代码。进程是操作系统进行资源分配和调度的基本单位。

每个进程都由操作系统维护，并被分配一个唯一的进程标识符（PID）。操作系统利用进程控制块（PCB）来记录和管理各个进程的状态信息，包括进程的优先级、执行状态、寄存器值、分配给进程的资源等。

操作系统通过调度算法来决定进程的运行顺序。调度算法可以根据进程的优先级、时间片轮转、最短作业优先等策略来进行选择。

进程可以通过进程间通信（IPC）来进行相互之间的数据交换和协作。常见的进程间通信机制包括管道、消息队列、共享内存和信号量等。

进程可以存在多个状态，包括运行态、就绪态、阻塞态等。进程可以通过系统调用来进行状态的转换和操作，比如创建新的进程、终止进程、挂起和恢复进程等。

操作系统中的进程管理功能包括进程的创建、终止、调度、同步和通信等。进程的管理和调度是操作系统的重要任务，它能够提高系统的资源利用率和系统的响应速度。

#### :ear\_of\_rice:概念

进程是计算机中正在运行的程序的实例。它是操作系统进行资源分配和管理的基本单位，包括代码、数据和执行状态等信息。

#### :ear\_of\_rice:组成

进程是计算机系统中执行中的一个独立单位，它是由程序、数据和程序控制块组成的。

1. 程序：进程包含了一个要执行的程序或指令集合，它以二进制形式存储在计算机的存储器中。程序定义了进程要执行的操作和计算步骤。
2. 数据：进程在执行过程中需要使用的数据存储在进程的数据区中。数据可以是输入数据、输出数据、中间计算结果等。
3. 程序控制块（PCB）：PCB是进程的管理数据结构，它包含了进程的各种属性和状态信息，如进程标识符、进程状态、程序计数器、寄存器值、优先级、资源分配情况等。PCB的作用是管理和控制进程的执行，系统通过操作PCB来切换和调度进程。

不同的操作系统和编程语言可能会有一些差异。此外，进程还可能包含其他组件，如堆栈区、文件描述符等，用于支持进程的执行和交互。

#### :ear\_of\_rice:状态

进程的三态图和五态图是用来描述进程状态的图形化表示方式。

进程的三态图是指包含三个状态的图，分别是**就绪态、运行态和阻塞态**。就绪态表示进程已经准备好运行，等待CPU分配时间片；运行态表示进程正在执行；阻塞态表示进程由于某些原因无法继续执行，需要等待某些事件的发生。

进程的五态图是在三态图的基础上增加了两个状态，分别是**创建态和终止态**。创建态表示进程正在被创建，尚未开始执行；终止态表示进程已经执行完毕，即将被销毁。创建态和终止态是进程的生命周期的两个重要阶段。

![img](/assets/e79366289353b4513f80bc900e250f9e.CBlqvzwZ.png)

### :deciduous\_tree:前趋图

进程的前趋图（Precedence Graph）是用于**表示和描述进程之间的依赖关系**的图形化工具。它描述了进程之间的先后顺序和执行的依赖关系。前趋图用于解决并发系统中进程调度和同步问题。

在前趋图中，每个进程表示为一个节点，节点之间的有向边表示进程之间的依赖关系。如果进程 A 必须在进程 B 之前执行，那么就会在 A 的节点到 B 的节点之间画一条有向边。这样，前趋图就能清楚地显示出进程之间的执行顺序和依赖关系。

通过观察前趋图，可以确定出一些关键路径和并发路径，从而做出合理的进程调度和同步策略。前趋图也可以用于识别潜在的死锁和资源竞争问题，帮助进行系统调优和性能优化。

进程的前趋图是一个有向图，用于表示和描述进程之间的依赖关系和执行顺序。它是并发系统中的重要工具，用于解决进程调度和同步问题。

![img](/assets/d0675fc48efd6f488d9a598354ae6dca.BnXA7O6L.png)

ABC可以并行执行，但是必须ABC都执行完后，才能执行D，这就确定了两点：任务间的并行、任务间的先后顺序。

### :deciduous\_tree:进程资源图

进程资源图是一种**描述进程所需资源和资源分配情况**的图形表示方法。它通常以矩形框表示进程，矩形框内部显示进程的标识符或名称，外部显示进程所需的各种资源。资源以箭头表示，箭头的方向表示资源的请求方向，箭头的粗细表示资源的数量或权重。进程资源图可以帮助我们直观地了解进程之间的资源需求和资源分配情况，有助于进行资源管理和优化，防止资源竞争和死锁等问题的发生。

![img](/assets/f42ce300a7459839797cf1fffb3453b6.WJQ5-B8L.png)

P代表进程，R代表资源，R方框中有几个圆球就表示有几个这种资源，在图中，R1指向P1，表示R1有一个资源已经分配给了P1，P1指向R2，表示P1还需要请求一个R2资源才能执行。

* 阻塞节点：某进程所请求的资源已经全部分配完毕，无法获取所需资源，该进程被阻塞了无法继续。如上图中P2。
* 非阻塞节点：某进程所请求的资源还有剩余，可以分配给该进程继续运行。如上图中P1、P3。当一个进程资源图中所有进程都是阻塞节点时，即陷入死锁状态。

![img](/assets/da2c00016fbc9ac567c7dd1435c1b1b0.BSp522Yi.png)

### :deciduous\_tree:同步与互斥

| 术语       | 描述                                                         |
| :--------- | :----------------------------------------------------------- |
| 互斥       | 某资源（即临界资源）在同一时间内只能由一个任务单独使用，使用时需要加锁，使用完后解锁才能被其他任务使用；如打印机。 |
| 同步       | 多个任务可以并发执行，只不过有速度上的差异，在一定情况下停下等待，不存在资源是否单独或共享的问题；如自行车和汽车。 |
| 临界资源   | 各进程间需要以互斥方式对其进行访问的资源。                   |
| 临界区     | 指进程中对临界资源实施操作的那段程序。本质是一段程序代码。   |
| 互斥信号量 | 对临界资源采用互斥访问，使用互斥信号量后其他进程无法访问，初值为1。 |
| 同步信号量 | 对共享资源的访问控制，初值一般是共享资源的数量。             |

### :deciduous\_tree:信号量

进程中的信号量是一种用于同步和互斥访问共享资源的机制。它是一个计数器，可以被多个进程共享，并且可以通过两个基本操作来操作它：P（等待）和V（释放）。

信号量的PV操作是信号量的两个基本操作，用于实现进程之间的同步和互斥。

P操作（等待操作）：

1. 如果信号量的值大于0，表示资源可用，进程可以继续执行，将信号量的值减1。
2. 如果信号量的值等于0，表示资源不可用，进程将被阻塞，直到资源可用。

V操作（释放操作）：

1. 将信号量的值加1。
2. 如果有其他进程因为等待资源而被阻塞，将其中一个被阻塞的进程唤醒。

利用P操作和V操作，可以实现对共享资源的互斥访问。例如，在进入临界区之前先执行P操作，退出临界区后执行V操作，这样可以确保在同一时间内仅有一个进程可以进入临界区。

此外，P操作和V操作还可用于实现进程之间的同步，例如在生产者消费者问题中，生产者在放置数据之前执行P操作，消费者在获取数据之前执行P操作，这样可以确保生产者和消费者之间的顺序执行。

![img](/assets/932c2752670085da1db5517e02e22259._PDnlV_Z.png)

### :deciduous\_tree:生产者和消费者

生产者消费者问题是一个经典的进程同步问题，描述了多个生产者和消费者共享一个有限缓冲区的情况。

在生产者消费者问题中，生产者负责将数据放入缓冲区，而消费者负责从缓冲区中取出数据。缓冲区有一定的容量，当缓冲区已满时，生产者必须等待，当缓冲区为空时，消费者必须等待。

为了实现生产者和消费者的同步，可以使用信号量或互斥锁来解决问题。以下是一种常见的实现方式：

1. 定义一个缓冲区，以及缓冲区的容量。
2. 定义一个互斥锁（mutex）来保护对缓冲区的访问。
3. 定义两个信号量：一个表示缓冲区的空槽数量，一个表示缓冲区的满槽数量。
4. 生产者进程执行以下步骤：
   * 等待空槽信号量，如果缓冲区已满则等待。
   * 获取互斥锁，保护对缓冲区的访问。
   * 将数据放入缓冲区。
   * 释放互斥锁。
   * 增加满槽信号量。
5. 消费者进程执行以下步骤：
   * 等待满槽信号量，如果缓冲区为空则等待。
   * 获取互斥锁，保护对缓冲区的访问。
   * 从缓冲区取出数据。
   * 释放互斥锁。
   * 增加空槽信号量。

通过使用互斥锁和信号量来控制生产者和消费者的访问，可以确保数据的正确性和同步。但需要注意的是，在实现过程中需要处理好各种边界条件，以避免死锁或竞争条件的发生。

![img](/assets/afb584a6f1f5d1d6d7b2466d093a7df2.B5CUB8l2.png)

![img](/assets/600aea0e6b9a4bd8b17f43ef19164412.fobi1ejT.png)

### :deciduous\_tree:死锁和线程

#### :ear\_of\_rice:死锁

死锁指的是在多进程/线程系统中，当两个或多个进程无法继续执行，因为它们所需要的资源被其他进程占用并且无法释放时产生的一种状态。换句话说，死锁是指两个或多个进程在互相等待对方所占有的资源而无法继续向前推进的情况。

死锁的发生是由于四个必要条件同时满足所导致的，即互斥条件、不可抢占条件、占有并等待条件和循环等待条件。当这四个条件同时满足时，系统进入死锁状态。

死锁的解决方法包括资源预分配策略、死锁检测与恢复、死锁预防和死锁避免等。这些方法旨在避免或解决死锁情况，使系统能够正常运行。

死锁产生的四个必要条件：

| 必要条件                       | 描述                                                         |
| :----------------------------- | :----------------------------------------------------------- |
| 互斥条件 (Mutual Exclusion)    | 资源只能被一个进程占用，其他进程必须等待该资源的释放         |
| 不可抢占条件 (Hold and Wait)   | 进程在持有至少一个资源的同时，又请求获取其他进程占有的资源   |
| 占有并等待条件 (Hold and Wait) | 进程在申请新的资源时，保持对已占有资源的占有，并等待获取新的资源 |
| 循环等待条件 (Circular Wait)   | 一组进程形成循环等待资源关系，每个进程都需要下一个进程所占有的资源才能继续执行，而这种资源关系存在循环的依赖关系 |

当上述四个条件同时满足时，系统就会进入死锁状态。

为了避免死锁的发生，我们需要采取适当的算法和策略来破坏这些条件之一，有下列方法：

| 解决措施 | 描述                                                         |
| :------- | :----------------------------------------------------------- |
| 死锁预防 | 通过限制并发进程对资源的请求，破坏死锁产生的四个条件之一，使系统在任何时刻都不满足死锁的条件。 |
| 死锁避免 | 使用银行家算法来避免死锁。预先计算出一条不会导致死锁的资源分配方法，只分配资源给能够保证系统安全性的进程，否则不分配资源，从而避免死锁的发生。 |
| 死锁检测 | 允许死锁的产生，但系统定时运行一个检测死锁的程序。如果检测到系统中发生死锁，则设法加以解除，可以通过资源剥夺或进程撤销来解除死锁。 |
| 死锁解除 | 在死锁发生后采取解除方法，例如强制剥夺资源，撤销进程等。这些方法会破坏死锁中的某个条件，以解除死锁。 |

死锁计算问题：系统内有n个进程，每个进程都需要R个资源，那么其发生死锁的最大资源数为**n**\*

**（R-1）**。其不发生死锁的最小资源数为**n\*（R-1）+1**。

![img](/assets/8e414d29d28f03f7fff9bf7367b1550c.Dm6bZKYR.png)

#### :ear\_of\_rice:线程

线程（Thread）是计算机中最小的执行单元，是进程内的一个独立执行流程。一个进程可以包含多个线程，而线程共享进程的资源，如内存空间、文件句柄等。每个线程有自己的程序计数器、栈和一些状态信息，但在同一个进程中的多个线程可以共享同一块内存空间。

相比于进程，线程更加轻量级，线程的切换和创建所需的开销较小，可以更高效地利用处理器的时间片。多线程可以同时进行多个任务，可以充分利用多核处理器的并行计算能力。

在多线程编程中，线程可以并发地执行，可以同时处理多个任务，提高了程序的并发性和效率。线程之间的通信可以通过共享内存或消息传递等机制进行。但线程之间的共享资源也可能引发同步问题，需要合理地使用锁、信号量等同步机制来保证数据的一致性和避免竞态条件。

线程是操作系统中的执行单位，可以将一个进程的任务划分为多个并发执行的线程，提高程序的并发性和效率。

### :deciduous\_tree:银行家算法

银行家算法是一种资源分配算法，主要用于避免死锁的发生。它最初由艾兹格·迪科斯彻发表，并被称为银行家算法。

该算法基于以下原则：

1. 每个进程在启动时，必须声明其最大需求的资源数量。
2. 在运行时，进程可以请求分配一定数量的资源。
3. 如果系统可以满足进程的资源请求，并且分配后不会导致死锁，则分配资源。
4. 如果系统不能立即满足进程的资源请求，进程进入等待状态，直到资源可用。
5. 当进程完成时，系统回收分配给该进程的资源。

银行家算法的核心是资源分配的安全性检查。在安全状态下，即使所有进程同时请求资源，也不会进入死锁状态。否则，系统会拒绝分配资源，以避免死锁的发生。

银行家算法的实现涉及到资源分配的数据结构和算法，可以通过银行家算法来进行资源的合理分配，从而提高系统的性能和可靠性。

![img](/assets/57efc5744795a7cbde9f0dc7e1235a55.CGAmBEYH.png)

## 三、存储管理

### :deciduous\_tree:页式存储

#### :ear\_of\_rice:概念

我们的程序往往大于内存的容量，因此在执行时，并不会一次性将所有内容都装入内存。相反，程序会被分为若干个固定大小的页，通常为4K。然后将这些页离散存储到内存中，而内存则按块划分。为了将程序中的页映射到内存中的块进行存储，我们使用页表。

在进程中，我们使用逻辑地址（虚地址）来表示地址，而在内存中，我们使用物理地址（实地址）来表示地址。

每个页被分为页号和页内地址。页号用于与块号相对应，表示存储的位置，页的大小可以表示页的数量。而页内地址表示存储的数据内容，它的大小可以表示数据的大小。通过页号和页内地址的组合，我们可以确定在内存中的具体地址。

当一个进程需要访问某个内存地址时，操作系统会首先查找进程的页表，确定该地址所对应的页。如果该页已经在内存中，操作系统直接将该页的内存地址返回给进程，进程可以直接访问。如果该页不在内存中，则发生缺页中断，操作系统会从磁盘中选择一个空闲的页面框，并将所需的页从磁盘读入到该页面框中，然后更新进程的页表，最后将页的内存地址返回给进程。

![img](/assets/4dae99ccd6fffdece672aecd2d6c0a23.B3hKXZM4.png)

#### :ear\_of\_rice:优缺点

以下是页式存储的优缺点表格展示：

| 优点                                                         | 缺点                                                     |
| :----------------------------------------------------------- | :------------------------------------------------------- |
| 能够实现程序的非连续存储                                     | 页表管理和地址转换开销较大                               |
| 提高了内存利用率                                             | 需要频繁进行页面调度和页表更新                           |
| 方便进行进程间的共享和保护                                   | 存储管理的复杂性增加                                     |
| 允许在不同的进程之间动态分配和释放内存                       | 可能导致内存碎片的产生                                   |
| 页面大小的选择可以灵活调整，以满足不同程序的需求             | 可能引起内存访问的局部性问题                             |
| 实现了虚拟内存的机制，使得程序能够运行于比实际内存大小更大的地址空间 | 对于小规模程序和低配置机器，页式存储可能过于复杂和开销大 |

![img](/assets/e0721aedbbe3ee4c400adb8ecfef7680.D2Jcpl4_.png)

#### :ear\_of\_rice:页面置换算法

页面置换算法（Page Replacement Algorithm）是操作系统中用于解决内存管理中页面置换问题的一种方法。当内存不足时，操作系统需要选择一个页面将其从内存中置换出来，为新的页面腾出空间。

有时候，进程空间分为100个页面，而系统内存只有10个物理块，无法全部满足分配，就需要将马上要执行的页面先分配进去，而后根据算法进行淘汰，使100个页面能够按执行顺序调入物理块中执行完。

缺页表示需要执行的页不在内存物理块中，需要从外部调入内存，会增加执行时间，因此，缺页数越多，系统效率越低。

常见的页面置换算法有以下几种：

| 页面置换算法           | 优点                                               | 缺点                                     |
| :--------------------- | :------------------------------------------------- | :--------------------------------------- |
| 最优算法 (OPT)         | 理论上的最佳效率，保证未来执行的页面都是即将访问的 | 无法实现，只用于比较其他算法的差距       |
| 先进先出算法 (FIFO)    | 算法简单，易于实现                                 | 可能产生抖动现象，即页数越多，缺页率越高 |
| 最近最少使用算法 (LRU) | 基于局部性原理，效率高，不会产生抖动现象           | 需要维护额外的数据结构，计算量较大       |

不同的页面置换算法有各自的优缺点，适用于不同的场景。操作系统需要根据具体的应用需求和性能要求选择合适的页面置换算法。

![img](/assets/6979309878b3c16bf04c0e8a56270e48.CZU54moa.png)

#### :ear\_of\_rice:快表

快表（Translation Lookaside Buffer，简称TLB）是存储管理中的一种高速缓存，用于加速虚拟地址到物理地址的转换过程。

在计算机系统中，使用虚拟内存来实现对内存的管理。虚拟内存将一个程序的地址空间划分为多个页（page），每个页的大小通常为4KB或者8KB。当程序访问某个虚拟地址时，需要将该虚拟地址转换为物理地址才能进行实际的内存访问。这个转换过程由操作系统中的页表（Page Table）来完成。

由于页表的大小可能很大，每次转换都需要访问内存，这样会导致比较高的延迟。为了加快地址转换的速度，引入了TLB。

TLB是一个高速缓存，存储了最近使用的一部分虚拟地址到物理地址的映射。当程序访问虚拟地址时，首先检查TLB是否已经缓存了对应的映射关系，如果缓存中存在，则直接使用缓存中的物理地址，从而避免了访问页表的开销。

如果TLB中不存在对应的映射关系，则需要访问页表来获取物理地址，并且将这个映射关系缓存到TLB中，以便将来的访问可以直接从TLB中获取物理地址。

TLB的大小有限，通常只能存储一部分的页表映射关系。当TLB已满并且需要存储新的映射关系时，需要进行替换。常用的替换算法包括最近最少使用（Least Recently Used，LRU）算法和随机替换算法。

TLB的存在可以大大加快地址转换的速度，提高程序的执行效率。然而，由于TLB是硬件资源，因此它的大小是有限的。如果程序的地址空间很大，TLB可能无法完全缓存所有的映射关系，这样会导致TLB的命中率降低，从而降低地址转换的速度。为了提高TLB的命中率，可以使用更高效的缓存替换算法，并且根据程序的访存模式进行优化。

快表是一块小容量的相联存储器，由快速存储器组成，按内容访问，速度快，并且可以从硬件上保证按内容并行查找，一般用来存放当前访问最频繁的少数活动页面的页号(可以看成是页表的频繁访问数据的副本)。

快表是将页表存于Cache中；慢表是将页表存于内存上。因此慢表需要访问两次内存才能取出数据，而快表是访问一次Cache和一次内存，因此更快。

![img](/assets/667a56ec188e46e6f27b626d44f6158a.w4ZnDH1U.png)

### :deciduous\_tree:段式存储和段页式存储

#### :ear\_of\_rice:段式存储

段式存储是操作系统中的一种存储管理技术，它将程序的逻辑地址空间划分为多个不同大小的段，每个段包含了一组相关的逻辑地址。段式存储的主要目的是提供更灵活的内存分配和管理方式，以满足不同程序的需求。

在段式存储中，每个段都有自己的基址和长度信息。逻辑地址由两部分组成：段号和段内偏移量。段号用于标识所在的段，而段内偏移量用于表示在该段内的偏移位置。

段式存储的优点是：

1. 灵活性：可以根据程序的需要划分不同大小的段，从而更好地适应各种程序的内存需求。
2. 安全性：通过段的保护机制，可以限制程序对其他段的访问，从而提高系统的安全性。
3. 共享性：可以实现段的共享，多个程序可以共享同一个段，减少内存占用。

然而，段式存储也存在一些问题：

1. 内碎片：由于段的大小不一致，可能会导致某些段内部存在未被充分利用的空间，从而产生内碎片。
2. 外碎片：由于段的分配和释放是离散进行的，可能会导致内存中存在大量不连续的空闲空间，从而产生外碎片。

![img](/assets/78a0b188f02e2f6c951d2e4b96a6bfe1.BoZOpomH.png)

![img](/assets/31038039cfd105eeb22860a761229aaa.CZjGYZke.png)

#### :ear\_of\_rice:段页式存储

段页式存储是一种结合了段式存储和页式存储的存储管理方式，主要用于操作系统的存储管理。

在段页式存储中，内存被划分为若干个大小不等的段（Segment），每个段是一个逻辑上相关的程序或数据单元。而每个段又被划分为若干个大小相等的页（Page），每个页的大小是固定的。段和页都有一个唯一的标识符（Segment ID和Page ID），用于访问和管理。

段页式存储通过段表（Segment Table）和页表（Page Table）来实现地址映射。段表中存储了每个段的基地址（Base Address）和段限长（Segment Length），而页表中存储了每个页的物理地址（Physical Address）。在进行地址转换时，首先根据段表找到对应的段的基地址，然后再根据页表找到对应的页的物理地址。

段页式存储的优点是可以更好地管理和保护程序和数据，同时也更灵活地分配和共享内存。它可以将整个程序或数据分为多个段，每个段都可以有不同的访问权限和保护级别。同时，页式存储可以将每个段分为多个页，实现了分页和虚拟内存的管理，可以更高效地利用内存空间。

![img](/assets/51a80b4ead7f95f48ccd62af3a9b856c.BGiLqhCW.png)

## 四、设备管理

设备管理是操作系统的一项重要功能，它负责管理计算机系统中的各种硬件设备，包括输入设备、输出设备和存储设备等。设备管理的主要任务包括设备的分配、控制和调度。

设备的分配是指将可用的设备资源分配给需要使用的进程或用户。在操作系统中，每个设备都有一个设备控制块（Device Control Block，DCB）来描述设备的状态和属性，包括设备类型、设备编号、设备状态等。当一个进程或用户需要使用某个设备时，系统会查找可用的设备并分配给它。

设备的控制是指对设备进行操作和控制。操作系统通过向设备发送控制命令来控制设备的操作。这些控制命令包括打开设备、关闭设备、读取数据、写入数据等。操作系统还会处理设备发生的中断和异常，以及设备的错误处理和恢复等。

设备的调度是指对设备的访问进行调度和管理。由于计算机系统中的设备资源是有限的，不同的进程或用户可能需要同时访问同一个设备。设备调度算法决定了进程或用户按照何种顺序访问设备，以保证设备的效率和公平性。一般来说，设备调度算法可以是先来先服务、最短作业优先、轮转调度等。

设备管理还包括设备驱动程序的开发和维护。设备驱动程序是操作系统中的一段代码，用于与硬件设备进行通信和交互。驱动程序将操作系统的请求转换为设备所能理解的命令，并将设备的响应传递给操作系统。

### :deciduous\_tree:设备分类

| 设备分类方式 | 描述                                           | 示例设备                         |
| :----------- | :--------------------------------------------- | :------------------------------- |
| 数据组织分类 | 块设备：以固定大小的块为单位进行数据传输的设备 | 硬盘，固态硬盘                   |
|              | 字符设备：以字符为单位进行数据传输的设备       | 键盘，鼠标，打印机               |
| 资源分配分类 | 独占设备：一次只能被一个程序使用的设备         | 硬盘，串口                       |
|              | 共享设备：可以被多个程序同时使用的设备         | 打印机                           |
|              | 虚拟设备：通过软件模拟出来的设备               | 虚拟机的虚拟硬盘，虚拟网络适配器 |
| 传输速率分类 | 低速设备：传输速率较慢的设备                   | 鼠标，键盘                       |
|              | 中速设备：传输速率适中的设备                   | 打印机                           |
|              | 高速设备：传输速率较快的设备                   | 硬盘，固态硬盘                   |

按数据组织分类将设备分为块设备和字符设备两类，块设备是以固定大小的数据块为单位进行数据传输，如硬盘驱动器；字符设备则以字符流为单位进行数据传输，如键盘和打印机。

从资源分配角度来分类设备，可以将设备分为独占设备、共享设备和虚拟设备。独占设备是一次只能被一个进程或用户占用的设备，如独占式打印机；共享设备可以被多个进程或用户同时使用，如网络打印机；虚拟设备是通过软件模拟而实现的设备，如虚拟磁盘。

数据传输速率分类将设备根据其数据传输速率的不同分为低速设备、中速设备和高速设备。低速设备的数据传输速率较慢，如串口设备；中速设备的数据传输速率适中，如打印机；高速设备的数据传输速率较快，如固态硬盘。

I/O软件层次结构：

![img](/assets/c04151e3b2b842a33f35684596b5c414.DiqmHtB8.png)

### :deciduous\_tree:输入输出

| 设备管理输入输出方式    | 特点                                                   | 适用场景                                 |
| :---------------------- | :----------------------------------------------------- | :--------------------------------------- |
| 程序控制（查询）方式    | CPU主动查询外设是否完成数据传输，效率低                | 低速设备、非实时性要求高的场景           |
| 程序中断方式            | 外设完成数据传输后，向CPU发送中断，效率相对较高        | 键盘等实时性较高的场景                   |
| DMA方式（直接主存存取） | 数据传输由DMA控制器完成，CPU仅需完成初始化操作，效率高 | 硬盘等高速设备                           |
| 通道                    | 通过专用通道控制设备输入输出，效率高，但复杂度高       | 大规模、高速设备                         |
| IO处理机                | 专用设备处理输入输出请求，减轻CPU负担，效率高          | 处理大量且需要复杂逻辑的设备输入输出请求 |

在一个总线周期结束后，CPU会响应DMA请求开始读取数据；CPU响应程序中断方式请求是在一条指令执行结束时；区分指令执行结束和总线周期结束。

注意：还有两种方式分别是通道和IO处理机，基本不考，了解即可；

### :deciduous\_tree:虚设备和SPOOLING技术

虚设备是指在计算机系统中，通过软件模拟或虚拟出来的设备，这些设备并不存在于硬件中，但在软件层面上可以通过编程等方式进行模拟使用。虚设备可以提供与实际设备相同的功能，比如虚拟磁盘、虚拟打印机等。

SPOOLING（Simultaneous Peripheral Operations On-line）技术是一种计算机输入/输出（I/O）管理技术，它的主要目的是提高计算机的效能和性能。具体来说，SPOOLING技术通过将多个I/O请求缓存到磁盘或内存中的输入/输出队列（spool）中，然后按照一定的顺序依次处理这些请求。这样可以避免了进程等待I/O操作完成而导致CPU空闲的情况，提高了系统的并发性和效率。

虚设备和SPOOLING技术通常是结合使用的。虚设备可以被用来模拟实际设备，将I/O请求写入到虚设备中的缓冲区，然后通过SPOOLING技术将这些请求按序处理。这样可以提高计算机系统的整体性能和吞吐量，同时也能增加系统的灵活性和可扩展性。

![img](/assets/6554a19e85a36168b2c88694daad211e.CsMkf8Lt.png)

### :deciduous\_tree:磁盘结构

磁盘有正反两个盘面，每个盘面有多个同心圆，每个同心圆是一个磁道，每个同心圆又被划分为多个扇区。数据存放在扇区中。读取数据时，磁头首先要寻找到对应的磁道，然后等待磁盘进行周期旋转，旋转到指定的扇区，才能读取到对应的数据。这会产生寻道时间和等待时间，即磁头移动到磁道所需的时间和等待读写的扇区转到磁头的下方所用的时间。

目前常用的磁盘调度算法有以下几种：

| 调度算法                 | 描述                                                         |
| :----------------------- | :----------------------------------------------------------- |
| 先来先服务 (FCFS)        | 根据进程请求访问磁盘的先后顺序进行调度                       |
| 最短寻道时间优先 (SSTF)  | 选取与当前磁头位置最近的磁道进行调度，使得每次的寻道时间最短。可能导致远处进程无法访问（饥饿现象） |
| 扫描算法 (SCAN)          | 又称“电梯算法”，磁头双向移动，选择离磁头当前位置最近的请求访问磁道，并且与磁头移动方向一致。磁头从里向外或从外向里一直移动完才掉头，类似电梯 |
| 单向扫描调度算法 (CSCAN) | 与SCAN不同的是，只做单向移动，即只能从里向外或从外向里       |

![img](/assets/96911c4168f5f0bef5abe8dac91488b0.x5CJ8eZp.png)

## 五、文件管理

操作系统中的文件管理是指操作系统对文件的创建、存储、删除和访问等操作的管理。文件是操作系统中的基本单位，用于存储和组织数据。

文件管理的主要任务包括以下几个方面：

1. 文件的创建和删除：操作系统提供了创建和删除文件的功能。创建文件时，需要指定文件的名称、大小和权限等信息。删除文件时，操作系统会释放文件所占用的存储空间。
2. 文件的存储和组织：文件在存储设备上占据一定的空间。操作系统负责将文件存储在适当的位置，并且维护文件的结构和组织。
3. 文件的访问和读写：操作系统提供了对文件的读取和写入操作。读取文件时，操作系统会将文件的内容从存储设备读取到内存中，供应用程序使用。写入文件时，操作系统会将应用程序的数据写入到文件中。
4. 文件的共享和保护：操作系统可以控制文件的共享和保护。共享可以使多个进程或用户同时访问同一个文件。保护可以限制对文件的访问权限，防止未经授权的访问。
5. 文件的备份和恢复：操作系统可以提供文件的备份和恢复功能，以保护文件数据的安全性。备份可以将文件的副本保存在其他存储设备上，以防止文件丢失。恢复可以将备份文件还原到原始位置，以恢复文件的完整性。

### :deciduous\_tree:索引文件

文件管理中的索引文件是一种特殊的文件，它记录着其他文件的位置和相关信息。索引文件的作用是加快文件的定位和访问速度。

在操作系统中，索引文件用于建立文件与存储设备上实际存储位置之间的映射关系。它记录每个文件在存储设备上的起始位置和长度等信息。通过索引文件，操作系统可以快速定位和访问文件，而不需要遍历整个存储设备。

索引文件有多种实现方式，包括：

1. 直接索引：每个文件都有一个索引项，其中记录了文件在存储设备上的起始位置和长度等信息。这种方式适用于小型文件系统。
2. 多级索引：使用多级索引可以解决索引项数量有限的问题。每个文件都有一个索引块，其中记录了多个索引项的位置。每个索引项又可以指向其他索引块，从而构成一个多级索引结构。
3. 哈希索引：通过计算文件的哈希值来确定索引项的位置。哈希索引可以快速定位文件，但是在文件删除和更新时需要重新计算哈希值，会带来一定的开销。

比如：系统中有13个索引节点，0-9为直接索引，即每个索引节点存放的是内容，假设每个物理盘大小为4KB，共可存4KB\*10=40KB数据；

10号索引节点为一级间接索引节点，大小为4KB，存放的并非直接数据，而是链接到直接物理盘

块的地址，假设每个地址占4B，则共有1024个地址，对应1024个物理盘，可1024\*4KB=4098KB数据。

二级索引节点类似，直接盘存放一级地址，一级地址再存放物理盘快地址，而后链接到存放数据的物理盘块，容量又扩大了一个数量级，为1024*1024*4KB数据。

计算机系统中采用的索引文件结构如下图所示：

![img](/assets/d8b09ea01f6607d46ef9f0a9b2e8135d.BL64EAOs.png)

![img](/assets/f68d65aeacbe8db60da1d5da933b6054.D9NiWv8A.png)

### :deciduous\_tree:树形文件

树形文件是一种常见的文件组织结构，它类似于文件系统中的目录树形结构。在操作系统中，树形文件由一系列文件和文件夹组成，形成一个层次结构。每个文件夹可以包含其他文件夹和文件，从而形成树状结构。

树形文件的示例如下所示：

* 根文件夹（Root Folder）
  * 文件夹1（Folder 1）
    * 子文件夹1.1（Subfolder 1.1）
      * 文件A（File A）
      * 文件B（File B）
    * 子文件夹1.2（Subfolder 1.2）
      * 文件C（File C）
  * 文件夹2（Folder 2）
    * 子文件夹2.1（Subfolder 2.1）
      * 文件D（File D）
    * 文件E（File E）

在这个示例中，根文件夹是整个树形文件的起始点，它下面包含了两个文件夹（文件夹1和文件夹2）。文件夹1下面又包含了两个子文件夹（子文件夹1.1和子文件夹1.2），以及一些文件（文件A和文件B）。类似地，文件夹2下面也包含了一个子文件夹（子文件夹2.1）和一个文件（文件E）。

在这个树形结构中，每个文件或目录都有一个唯一的路径来标识它们的位置。

1. 相对路径：相对路径是相对于当前工作目录的路径。它们不需要给出完整的路径，而是通过指定路径中的目录和文件名的相对位置来定位文件。例如，假设当前工作目录是`/home/user`，而我们要访问`/home/user/documents/file.txt`文件，我们可以使用相对路径`documents/file.txt`。
2. 绝对路径：绝对路径是从根目录开始的完整路径，用于准确定位文件或目录的位置。它们包含了文件或目录的完整路径，从根目录开始一直到目标文件或目录。例如，`/home/user/documents/file.txt`是一个绝对路径，它可以直接指向文件。
3. 全文件名：全文件名是文件的完整名称，包括文件名和扩展名。它不包括路径信息，只是文件的唯一标识。例如，`file.txt`就是一个全文件名。

需要注意的是，不同的操作系统在表示路径和文件名时可能有所不同。例如，在Windows系统中，路径使用反斜杠（\）而不是斜杠（/）来分隔目录和文件名。而在Linux和Mac OS X系统中使用斜杠来分隔。因此，对于相对路径、绝对路径和全文件名的表示也会有所差异。

![img](/assets/e21eb72b341c2d87ac6b0632d70dac0e.xqIdGr6l.png)

![img](/assets/85d19aa240a51125445b7a07afb9f4cc.DytdxEBQ.png)

### :deciduous\_tree:空间存储

| 管理方法   | 描述                                                         |
| :--------- | :----------------------------------------------------------- |
| 空闲区表法 | 将所有空闲空间整合成一张表，即空闲文件目录。                 |
| 空闲链表法 | 将所有空闲空间链接成一个链表，根据需要分配。                 |
| 成组链接法 | 既分组，每组内又链接成链表，是上述两种方法的综合。           |
| 位示图法   | 对每个物理空间用一位标识，为1则使用，为0则空闲，形成一张位示图。 |

这些方法用于管理磁盘上的空闲空间，以便在文件创建、写入和删除等操作时能够有效地分配和回收磁盘空间。具体使用哪种方法取决于文件系统的设计和需求。

![img](/assets/5e37e3f4e1a5483ca6d169b48dab4d6f.DpUpRvaF.png)

![img](/assets/98474bf00ce74fbb592f5f328976a582.BgqWk-Dp.png)

## 六、作业管理

操作系统的作业管理是指操作系统对于作业的调度、分配、控制和管理等一系列操作。作业是指用户提交给操作系统的一些任务或程序，作业管理是操作系统的一个核心功能。

作业管理的主要任务包括：

1. 作业调度：操作系统根据各种调度算法，决定哪个作业优先执行、如何分配计算资源等。调度算法可以按照优先级、先来先服务、时间片轮转等方式进行。
2. 作业分配：操作系统根据系统资源的使用情况和作业的需求，将作业分配给合适的处理器或计算机节点进行执行。作业分配可以按照负载均衡的原则进行，以确保系统资源的充分利用。
3. 作业控制：操作系统对作业的执行进行监控和控制，确保作业按照预期方式执行。包括对作业的启动、暂停、恢复、中止等操作。
4. 作业管理：操作系统对作业的状态、进度、资源占用等进行管理和记录，以便用户和系统管理员了解作业的情况。可以通过作业管理系统提供的接口进行作业信息的查询和操作。

### :deciduous\_tree:作业状态

作业：系统为完成一个用户的计算任务（或一次事务处理）所做的工作总和。

例如，对用户编写的源程序，需要经过编译、连接、装入以及执行等步骤得到结果，这其中的每一个步骤称为作业步。在操作系统中用来控制作业进入、执行和撤销的一组程序称为作业管理程序。

作业状态分为4种：提交、后备、执行和完成：

| 作业状态 | 描述                                                         |
| :------- | :----------------------------------------------------------- |
| 提交     | 作业被提交给计算机中心，通过输入设备送入计算机系统的过程状态。 |
| 后备     | 作业通过Spooling系统输入到计算机系统的后备存储器（磁盘）中，等待作业调度程序调度的状态。 |
| 执行     | 作业被作业调度程序选中，为其分配必要的资源并建立相应的进程后，进入执行状态。 |
| 完成     | 作业正常结束或异常终止时进入的状态。作业调度程序对该作业进行善后处理。 |

![img](/assets/1e80daa1265dc90e82ee084fb8c50819.7gJeFGag.png)

### :deciduous\_tree:作业调度算法

| 调度算法     | 描述                                                         |
| :----------- | :----------------------------------------------------------- |
| 先来先服务   | 按作业到达的先后进行调度，启动等待时间最长的作业。           |
| 短作业优先   | 以要求运行时间的长短进行调度，启动要求运行时间最短的作业。   |
| 响应比高优先 | 响应比高的作业优先启动。                                     |
| 优先级度算法 | 用户可以指定作业优先级，优先级高的作业先启动；也可以根据作业紧迫程度、IO 繁忙情况等由系统指定优先级。 |
| 均调度算法   | 根据系统的运行情况和作业的特性对作业进行分类，调度程序轮流选取不同类别的作业执行，力求均衡利用系统资源。 |

### :deciduous\_tree:用户界面

| 用户界面阶段       | 描述                                                         |
| :----------------- | :----------------------------------------------------------- |
| 控制面板式用户界面 | 在计算机发展早期，用户通过控制台开关、板键或穿孔纸带向计算机送入命令或数据，计算机通过指示灯和打印机输出运行情况或结果。 |
| 字符用户界面       | 基于字符的用户界面，用户通过键盘或其他输入设备输入字符，显示器或打印机输出字符。字符用户界面具有功能强大、灵活性好、屏幕开销少等优点，但操作步骤繁琐。 |
| 图形用户界面       | 随着多媒体技术发展，图形用户界面应运而生。用户可以使用字符、图形、图像和声音等进行交互，操作更加自然和方便。 |
| 新一代用户界面     | 新一代用户界面以用户为中心，通过自然、高效、高带宽、非精确、无地点限制等特征，以语音、自然语言、手势、头部跟踪、表情和视线跟踪等新的交互技术为用户提供更方便的输入方式。计算机通过多种感知通道理解用户意图，并以真实感的计算机仿真环境提供真实体验。 |

---

---
url: /Java/设计模式/04.行为型/16_strategy.md
---

# 行为型 - 策略(Strategy)

> 策略模式(strategy pattern): 定义了算法族, 分别封闭起来, 让它们之间可以互相替换, 此模式让算法的变化独立于使用算法的客户。@pdai

* 行为型 - 策略(Strategy)
  * [意图](#意图)
  * [类图](#类图)
  * [与状态模式的比较](#与状态模式的比较)
  * [实现](#实现)
  * [JDK](#jdk)

## [#](#意图) 意图

定义一系列算法，封装每个算法，并使它们可以互换。

策略模式可以让算法独立于使用它的客户端。

## [#](#类图) 类图

* Strategy 接口定义了一个算法族，它们都具有 behavior() 方法。
* Context 是使用到该算法族的类，其中的 doSomething() 方法会调用 behavior()，setStrategy(in Strategy) 方法可以动态地改变 strategy 对象，也就是说能动态地改变 Context 所使用的算法。

![img](/assets/1fc969e4-0e7c-441b-b53c-01950d2f2be5.nUAs9y5G.png)

## [#](#与状态模式的比较) 与状态模式的比较

状态模式的类图和策略模式类似，并且都是能够动态改变对象的行为。但是状态模式是通过状态转移来改变 Context 所组合的 State 对象，而策略模式是通过 Context 本身的决策来改变组合的 Strategy 对象。所谓的状态转移，是指 Context 在运行过程中由于一些条件发生改变而使得 State 对象发生改变，注意必须要是在运行过程中。

状态模式主要是用来解决状态转移的问题，当状态发生转移了，那么 Context 对象就会改变它的行为；而策略模式主要是用来封装一组可以互相替代的算法族，并且可以根据需要动态地去替换 Context 使用的算法。

## [#](#实现) 实现

设计一个鸭子，它可以动态地改变叫声。这里的算法族是鸭子的叫声行为。

```java
public interface QuackBehavior {
    void quack();
}
public class Quack implements QuackBehavior {
    @Override
    public void quack() {
        System.out.println("quack!");
    }
}
public class Squeak implements QuackBehavior{
    @Override
    public void quack() {
        System.out.println("squeak!");
    }
}
public class Duck {
    private QuackBehavior quackBehavior;

    public void performQuack() {
        if (quackBehavior != null) {
            quackBehavior.quack();
        }
    }

    public void setQuackBehavior(QuackBehavior quackBehavior) {
        this.quackBehavior = quackBehavior;
    }
}
public class Client {
    public static void main(String[] args) {
        Duck duck = new Duck();
        duck.setQuackBehavior(new Squeak());
        duck.performQuack();
        duck.setQuackBehavior(new Quack());
        duck.performQuack();
    }
}
squeak!
quack!
```

---

---
url: /01.指南/20.相关/02.插槽布局.md
---

# 插槽布局

## 插槽

Teek 提供了很多的插槽，能够被用来在页面的特定位置注入内容，下面这个例子展示了将一个组件注入到首页右侧卡片栏底部：

```ts
// .vitepress/theme/index.ts
import Teek from "vitepress-theme-teek";
import MyLayout from "./MyLayout.vue";
import "vitepress-theme-teek/index.css";

export default {
  extends: Teek,
  // 使用注入插槽的包装组件覆盖 Layout
  Layout: MyLayout,
};
```

```vue
<!-- .vitepress/theme/MyLayout.vue -->
<script setup>
import Teek from "vitepress-theme-teek";

const { Layout } = Teek;
</script>

<template>
  <Layout>
    <template #teek-home-info-after>自定义卡片栏</template>
  </Layout>
</template>
```

也可以使用 `h` 渲染函数。

```ts
// .vitepress/theme/index.ts
import Teek from "vitepress-theme-teek";
import "vitepress-theme-teek/index.css";
import { h } from "vue";
import MyComponent from "./components/MyComponent.vue";

export default {
  extends: Teek,
  Layout() {
    return h(Teek.Layout, null, {
      "teek-home-info-after": () => h(MyComponent),
    });
  },
};
```

Teek 主题的全部插槽如下：

## 首页插槽

当 `layout: 'home'` 在 frontmatter 中被启用时：

* `teek-home-before`：等于 VitePress 的 `home-hero-before` 插槽
* `teek-home-after`

当首页为文档风格时启用：

* `teek-home-features-before`&#x20;
* `teek-home-features-after` ：等于 VitePress 的 `home-features-after` 插槽

## Banner 插槽

* `teek-home-banner-before`
* `teek-home-banner-after`
* `teek-home-banner-content-before`
* `teek-home-banner-content-after`
* `teek-home-banner-feature-before`
* `teek-home-banner-feature-after`
* `teek-home-banner-name` ：覆盖 Banner 的文字，利用该插槽可以使用组件美化标题。可以接收 1 个参数，即配置项传过来的 `name`

如下是 Teek 当前实现的效果示例：

```vue
<script setup lang="ts">
import Teek from "vitepress-theme-teek";
</script>

<template>
  <Teek.Layout>
    <template #teek-home-banner-name="{ name }">
      <!-- 该 class 会激活 Teek 的内置样式 -->
      <h1 class="tk-banner-content__title" aria-label="首页横幅标题">
        {{ name }}
      </h1>
    </template>
  </Teek.Layout>
</template>
```

## 文章列表插槽

* `teek-home-post-before`
* `teek-home-post-after`
* `teek-home-post-list`&#x20;

Teek 默认实现了列表风格和卡片风格的文章列表，如果您需要定制自己的文章列表风格，可以通过 `teek-home-post-list` 插槽来覆盖 Teek 自带的文章列表，该插槽返回了当前的文章列表数量 `currentPosts`。

```vue
<script setup lang="ts">
import Teek from "vitepress-theme-teek";
</script>

<template>
  <Teek.Layout>
    <template #teek-home-post-list="{ currentPosts, transitionName }">
      <TransitionGroup tag="ul" :name="transitionName">
        <li v-for="post in currentPosts" :key="post.url">
          <span>文章地址：{{ post.url }}</span>
          <span>文章 frontmatter 数据：{{ post.frontmatter }}</span>
          <span>文章摘要：{{ post.excerpt }}</span>
          <span>文章作者：{{ post.author }}</span>
          <span>文章 frontmatter.title：{{ post.title }}</span>
          <span>文章 frontmatter.date：{{ post.date }}</span>
          <span>文章前 300 个文字：{{ post.capture }}</span>
        </li>
      </TransitionGroup>
    </template>
  </Teek.Layout>
</template>
```

v-bind 返回的 `transitionName` 为 post 配置项的 `transitionName`，具体可以查看 Teek 的列表风格和卡片风格的代码实现。

## 卡片栏插槽

* `teek-home-card-before`
* `teek-home-card`&#x20;
* `teek-home-card-after`
* `teek-home-card-my-before`
* `teek-home-card-my`&#x20;
* `teek-home-card-my-after`
* `teek-home-card-top-article-before`
* `teek-home-card-top-article`&#x20;
* `teek-home-card-top-article-after`
* `teek-home-card-category-before`
* `teek-home-card-category`&#x20;
* `teek-home-card-category-after`
* `teek-home-card-tag-before`
* `teek-home-card-tag`&#x20;
* `teek-home-card-tag-after`
* `teek-home-card-friend-link-before`
* `teek-home-card-friend-link`&#x20;
* `teek-home-card-friend-link-after`
* `teek-home-card-doc-analysis-before`
* `teek-home-card-doc-analysis`&#x20;
* `teek-home-card-doc-analysis-after`

移动端插槽：

* `teek-home-card-my-screen-before`&#x20;
* `teek-home-card-my-screen`&#x20;
* `teek-home-card-my-screen-after`&#x20;

不带 `-before` 和 `-after` 的插槽是直接覆盖卡片本身。

## 底部插槽

* `teek-footer-info-before`
* `teek-footer-info-after`：等于 VitePress 的 `layout-bottom` 插槽

## 文章页插槽

当 `layout: 'doc'` 在 frontmatter 中被启用时：

* `teek-article-analyze-before`：等于 VitePress 的 `doc-before` 插槽
* `teek-article-analyze-after`
* `teek-article-share-before`
* `teek-article-share-after`：等于 VitePress 的 `aside-outline-before` 插槽
* `teek-doc-after-appreciation-before`：等于 VitePress 的 `doc-after` 插槽
* `teek-doc-after-appreciation-after`：等于 Teek 的 `teek-comment-before` 插槽
* `teek-comment-before`
* `teek-comment-after`
* `teek-aside-bottom-appreciation-before`：等于 VitePress 的 `aside-bottom` 插槽
* `teek-aside-bottom-appreciation-after`
* `teek-doc-update-before`&#x20;
* `teek-doc-update-after`&#x20;

## 功能页插槽

当 `layout: 'page'` 在 frontmatter 中被启用时：

* `teek-page-top-before`：等于 VitePress 的 `page-top` 插槽
* `teek-page-top-after`

### 归档页插槽

* `teek-archives-top-before`
* `teek-archives-top-after`

### 目录页插槽

* `teek-catalogue-top-before`
* `teek-catalogue-top-after`

### 登录页插槽

* `teek-login-page` ：覆盖 Teek 的登录页，适用于自己实现一个登录页

### 风险链接提示页

* `teek-risk-link-page` ：覆盖 Teek 的风险链接提示页，适用于自己实现一个风险链接提示页

## 全局插槽

### 右下角按钮组插槽

* `teek-right-bottom-before`
* `teek-right-bottom-after`
* `teek-back-top` ：覆盖回到顶部组件，可以接收 4 个参数：
  1. show：是否显示，当处于顶部时为 false，往下滚动后为 true，可用于控制组件的显示
  2. progress：当前滚动条的进度
  3. icon：内置的默认图标
  4. scrollToTop：回到顶部方法

如：

```vue
<script setup lang="ts">
import Teek, { TkIcon } from "vitepress-theme-teek";
</script>

<template>
  <Teek.Layout>
    <template #teek-back-top="{ show, progress, scrollToTop }">
      <!-- 如下是 Teek 当前实现的部分内容，这些 class 会激活 Teek 的内置样式 -->
      <div
        v-show="show"
        title="回到顶部"
        @click="scrollToTop"
        class="tk-right-bottom-button__button back-top"
        :style="{ '--tk-progress': progress, cursor: 'pointer' }"
        role="button"
        aria-label="回到顶部"
        :aria-valuenow="progress"
        aria-valuemin="0"
        aria-valuemax="100"
      >
        <span class="content">{{ progress }}</span>
      </div>
    </template>
  </Teek.Layout>
</template>
```

* `teek-to-comment` ：覆盖滚动到评论区组件，可以接收 3 个参数：
  1. show：是否显示，当处于评论区域时为 false，离开评论区域为 true，可用于控制组件的显示
  2. icon：内置的默认图标
  3. scrollToComment：滚动到评论区方法

如下是 Teek 当前实现的效果示例：

```vue
<script setup lang="ts">
import Teek, { TkIcon } from "vitepress-theme-teek";
</script>

<template>
  <Teek.Layout>
    <template #teek-to-comment="{ show, icon, scrollToComment }">
      <!-- 如下是 Teek 当前实现的部分内容，这些 class 会激活 Teek 的内置样式 -->
      <div
        title="滚动到评论区"
        class="tk-right-bottom-button__button"
        @click="scrollToComment"
        role="button"
        aria-label="滚动到评论区"
      >
        <TkIcon :icon="icon" aria-hidden="true" />
      </div>
    </template>
  </Teek.Layout>
</template>
```

::: tip
只有当页面存在评论区时，该功能/插槽才会生效。
:::

### 其他插槽

* `teek-sidebar-trigger` ：覆盖侧边栏展开/折叠触发器组件，可以接收 2 个参数：
  1. active：点击触发器后为 `true`，`300s` 后为 `false`，目的是可以添加一个 `class` 或 `style` 来实现部分功能（过渡动画等）
  2. icon：icon：内置的默认图标
  3. toggleSideBar：点击触发器事件

如下是 Teek 当前实现的效果示例：

```vue
<script setup lang="ts">
import Teek, { TkIcon } from "vitepress-theme-teek";
</script>

<template>
  <Teek.Layout>
    <template #teek-sidebar-trigger="{ active, icon, toggleSideBar }">
      <!-- 如下是 Teek 当前实现的部分内容，这些 class 会激活 Teek 的内置样式 -->
      <div :class="{ active: active }" class="tk-sidebar-trigger is-active" @click="toggleSideBar">
        <div class="tk-right-bottom-button__button">
          <TkIcon :icon="icon" />
        </div>
      </div>
    </template>
  </Teek.Layout>
</template>
```

* `teek-loading` ：切换页面(路由)时加载 Loading 动画，切换结束关闭 Loading 动画插槽，可以接收 1 个参数：
  1. loading：开始切换页面(路由)时为 true，切换结束为 false

如下是 Teek 当前实现的效果示例：

```vue
<script setup lang="ts">
import Teek, { TkIcon } from "vitepress-theme-teek";
</script>

<template>
  <Teek.Layout>
    <template #teek-loading="{ loading }">
      <!-- 如下是 Teek 当前实现的部分内容，这些 class 会激活 Teek 的内置样式 -->
      <div class="tk-route-loading">
        <div v-show="loading" class="tk-route-loading__mask">
          <div class="tk-route-loading__loader">
            <div class="tk-route-loading__spinner" />
            <p>Teek 拼命加载中 ...</p>
          </div>
        </div>
      </div>
    </template>
  </Teek.Layout>
</template>
```

## 主题增强插槽

* `teek-theme-enhance-top`
* `teek-theme-enhance-bottom`

---

---
url: /daily/博客文档/VitPress/7_插件.md
---

# 插件

## 时间线

采用了 [@HanochMa/vitepress-markdown-timeline](https://github.com/HanochMa/vitepress-markdown-timeline) 的项目

Demo：https://hanochma.github.io/daily/2023-04

```sh
npm install vitepress-markdown-timeline
```

在 `config.mts` 中注册 markdown 解析插件

```js
import timeline from "vitepress-markdown-timeline"; 

export default {
  markdown: { 
    // 行号显示
    lineNumbers: true, 

    // 时间线  // [!code focus:4]
    config: (md) => { 
      md.use(timeline); 
    },
  }, 
}
```

在 `.vitepress/theme/index.ts` 中引入时间线样式

::: info 说明

如果你没有这个文件，就自己新建

:::

```js
// .vitepress/theme/index.ts
import DefaultTheme from 'vitepress/theme'

// 只需添加以下一行代码，引入时间线样式
import "vitepress-markdown-timeline/dist/theme/index.css";

export default {
  extends: DefaultTheme,
}
```

最后我们在markdown文件中，按格式使用即可

输入：

```markdown
::: timeline 2023-04-24
- 一个非常棒的开源项目 H5-Dooring 目前 star 3.1k
  - 开源地址 https://github.com/MrXujiang/h5-Dooring
  - 基本介绍 http://h5.dooring.cn/doc/zh/guide/
- 《深入浅出webpack》 http://webpack.wuhaolin.cn/
:::

::: timeline 2023-04-23
:::
```

## 谷歌分析

利用插件 [google-analytics](https://analytics.google.com/) ，来查看网站访问量

这里我们用 [@ZhongxuYang/vitepress-plugin-google-analytics](https://github.com/ZhongxuYang/vitepress-plugin-google-analytics) 的插件

```markdown
npm install vitepress-plugin-google-analytics
```

在 `.vitepress/theme/index.ts` 中引入

```js
// .vitepress/theme/index.ts
import DefaultTheme from "vitepress/theme"
import googleAnalytics from 'vitepress-plugin-google-analytics'  // [!code focus:1]

export default {
  extends: DefaultTheme,
  enhanceApp({app}) {
    googleAnalytics({  // [!code focus:3]
      id: 'G-******', //跟踪ID，在analytics.google.com注册即可
    }),
  },
}
```

## 图片缩放

Vuepress是可以直接安装插件 [medium-zoom](https://github.com/francoischalifour/medium-zoom) 的，非常好用

但是Vitepress直接用不了，在 [vitepress的issues中找到了方法#854](https://github.com/vuejs/vitepress/issues/854)

```sh
npm install medium-zoom
```

在 `.vitepress/theme/index.ts` 添加如下代码，并保存

```ts
// .vitepress/theme/index.ts
import DefaultTheme from 'vitepress/theme'

import mediumZoom from 'medium-zoom';
import { onMounted, watch, nextTick } from 'vue';
import { useRoute } from 'vitepress';

export default {
  extends: DefaultTheme,

  setup() {
    const route = useRoute();
    const initZoom = () => {
      // mediumZoom('[data-zoomable]', { background: 'var(--vp-c-bg)' }); // 默认
      mediumZoom('.main img', { background: 'var(--vp-c-bg)' }); // 不显式添加{data-zoomable}的情况下为所有图像启用此功能
    };
    onMounted(() => {
      initZoom();
    });
    watch(
      () => route.path,
      () => nextTick(() => initZoom())
    );
  },

}
```

点击图片后，还是能看到导航栏，加一个遮挡样式

在 `.vitepress/theme/style/var.css` 中加入如下代码，并保存

```css
/* .vitepress/theme/style/var.css */

.medium-zoom-overlay {
  z-index: 30;
}

.medium-zoom-image {
  z-index: 9999 !important;/* 给的值是21，但是实测盖不住，直接999 */
}
```

在 `.vitepress/theme/style/index.css` 中引入

```css
/* 图片缩放，遮挡导航栏 */
@import './var.css';
```

## 看板娘

第一次接触的人会比较懵，其实就是在右下角有个二次元的人物，类似电子宠物

这里使用 [@xinlei3166/vitepress-theme-website](https://github.com/xinlei3166/vitepress-theme-website) 的 [Live2D](https://www.live2d.com/zh-CHS/) 插件

```sh
npm install vitepress-theme-website
```

在 `.vitepress/theme/index.ts` 粘贴下面代码并保存

```js
// .vitepress/theme/index.ts
import DefaultTheme from 'vitepress/theme'

import { useLive2d } from 'vitepress-theme-website' // [!code focus:1]

export default {
  extends: DefaultTheme,

  setup() {

    // 看板娘 // [!code focus:20]
    useLive2d({
      enable: true,
      model: {
        url: 'https://raw.githubusercontent.com/iCharlesZ/vscode-live2d-models/master/model-library/hibiki/hibiki.model.json'
      },
      display: {
        position: 'right',
        width: '135px',
        height: '300px',
        xOffset: '35px',
        yOffset: '5px'
      },
      mobile: {
        show: true
      },
      react: {
        opacity: 0.8
      }
    })

  }
}
```

想要更换模型在 [@iCharlesZ](https://github.com/iCharlesZ/vscode-live2d-models#url) 这里找，替换 `model` 中的 `url` 链接即可

```js
useLive2d({
  model: {
  url: 'https://raw.githubusercontent.com/iCharlesZ/vscode-live2d-models/master/model-library/bilibili-22/index.json'  // [!code focus:1]
  }
})
```

## 浏览量

基本上使用的是 [不蒜子](http://busuanzi.ibruce.info/)，免费的且足够好用

```sh
npm install busuanzi.pure.js
```

```js
// .vitepress/theme/index.ts
import DefaultTheme from 'vitepress/theme'

import { inBrowser } from 'vitepress' // [!code focus:2]
import busuanzi from 'busuanzi.pure.js'

export default {
  extends: DefaultTheme,

  enhanceApp({ app , router }) { // [!code focus:7]
    if (inBrowser) {
      router.onAfterRouteChanged = () => {
        busuanzi.fetch()
      }
    }
  },
  
}
```

使用就很简单了，复制到页面中使用即可

::: tip 说明

本地开发出现数字即算成功，等你部署后会显示正确的数值

:::

```markdown
本站总访问量 <span id="busuanzi_value_site_pv" /> 次
本站访客数 <span id="busuanzi_value_site_uv" /> 人次
```

样式还可以根据自己需求选择封装

## ⭐自动侧边栏（插件vitepress-sidebar）

发现一款自动侧边栏，简单好用 [@jooy2/vitepress-sidebar](https://github.com/jooy2/vitepress-sidebar)

安装文档：https://vitepress-sidebar.jooy2.com/guide/getting-started

```sh
npm i -D vitepress-sidebar
```

在 `configs.mts` 中引入配置，可以根据 [作者api文档](https://vitepress-sidebar.jooy2.com/guide/api) 按需修改

```js
// .vitepress/configs.mts
import { generateSidebar } from 'vitepress-sidebar';  // [!code focus:5]

const vitepressSidebarOptions = {
  /* Options... */
};

export default defineConfig({
  themeConfig: {
    sidebar: generateSidebar({ // [!code focus:44]
      /*
       * For detailed instructions, see the links below:
       * https://vitepress-sidebar.jooy2.com/guide/api
       */
      documentRootPath: '/docs', //文档根目录
      // scanStartPath: null,
      // resolvePath: null,
      // useTitleFromFileHeading: true,
      // useTitleFromFrontmatter: true,
      // frontmatterTitleFieldName: 'title',
      // useFolderTitleFromIndexFile: false, //是否使用层级首页文件名做分级标题
      // useFolderLinkFromIndexFile: false, //是否链接至层级首页文件
      // hyphenToSpace: true,
      // underscoreToSpace: true,
      // capitalizeFirst: false,
      // capitalizeEachWords: false,
      collapsed: false, //折叠组关闭
      collapseDepth: 2, //折叠组2级菜单
      // sortMenusByName: false,
      // sortMenusByFrontmatterOrder: false,
      // sortMenusByFrontmatterDate: false,
      // sortMenusOrderByDescending: false,
      // sortMenusOrderNumericallyFromTitle: false,
      // sortMenusOrderNumericallyFromLink: false,
      // frontmatterOrderDefaultValue: 0,
      // manualSortFileNameByPriority: ['first.md', 'second', 'third.md'], //手动排序，文件夹不用带后缀
      removePrefixAfterOrdering: false, //删除前缀，必须与prefixSeparator一起使用
      prefixSeparator: '.', //删除前缀的符号
      // excludeFiles: ['first.md', 'secret.md'],
      // excludeFilesByFrontmatterFieldName: 'exclude',
      // excludeFolders: ['secret-folder'],
      // includeDotFiles: false,
      // includeRootIndexFile: false,
      // includeFolderIndexFile: false, //是否包含层级主页
      // includeEmptyFolder: false,
      // rootGroupText: 'Contents',
      // rootGroupLink: 'https://github.com/jooy2',
      // rootGroupCollapsed: false,
      // convertSameNameSubFileToGroupIndexPage: false,
      // folderLinkNotIncludesFileName: false,
      // keepMarkdownSyntaxFromTitle: false,
      // debugPrint: false,
    }),
  },
})
```

为了避免安装插件影响原项目，可以看下面的示例

stackblitz演示：https://stackblitz.com/edit/vite-y1rga7

> 等待生成后可查看，左侧是目录，右侧是页面
>
> 注意：插件在读取目录之后，你再修改文件名，需要重启才能生效

当位于一个页面时，隐藏另一个页面的菜单，可以参考[多侧边栏操作方法 | VitePress Sidebar](https://vitepress-sidebar.cdget.com/zhHans/advanced-usage/multiple-sidebars-how-to)

## 自动侧边栏（插件vite-plugin-vitepress-auto-nav）

> 项目地址：https://github.com/Xaviw/vite-plugin-vitepress-auto-nav
>
> 教程介绍：https://juejin.cn/post/7283060133646975012

安装

```sh
# 使用 ts 时推荐安装 vite，否则会有类型错误
npm i vite-plugin-vitepress-auto-nav vite -D
```

添加插件

```js
// .vitepress/config.ts
import AutoNav from "vite-plugin-vitepress-auto-nav";

export default defineConfig({
  vite: {
    plugins: [
      AutoNav({
        // 自定义配置
        pattern: ["**/!(README|TODO).md"], // 也可以在这里排除不展示的文件，例如不匹配 README 和 TODO 文件
        settings: {
           a: { hide: true }, // 不显示名称为 a 的文件夹或 md 文件
           b: { title: 'bb' }, // 名称为 b 的文件夹或文件在菜单中显示为 bb
           c/b: { sort : 3 }, // 通过路径精确匹配 c 文件夹下的 b 进行配置，排序时位于下标3的位置或最后
           c/b2: { useArticleTitle: false }, // 关闭使用文章一级标题作为文章名称
           d: { collapsed: true }, // 文件夹折叠配置
         },
         compareFn: (a, b) => {
           // 按最新提交时间(没有提交记录时为本地文件修改时间)升序排列
            return (b.options.lastCommitTime || b.options.modifyTime) - (a.options.lastCommitTime || a.options.modifyTime)
          },
          useArticleTitle: true // 全局开启使用文章一级标题作为文章名称
      }),
    ],
  },
});
```

## 自动导航和侧边栏（工具）

参考自[Notes](https://fang-kang.github.io/note/) 的VitePress 自动生成导航和侧边栏

优点：配置根目录后能自动生成导航和侧边栏

弊端：无法自定义导航栏名称

```js
/* .vitepress/navSidebarUtil.ts */
import { resolve, join, sep } from 'path'
import { readdirSync, statSync } from 'fs'
import { DefaultTheme } from 'vitepress'

interface SidebarGenerateConfig {
  /**
   * 需要遍历的目录. 默认:articles
   */
  dirName?: string
  /**
   * 忽略的文件名. 默认: index.md
   */
  ignoreFileName?: string
  /**
   * 忽略的文件夹名称. 默认: ['demo','asserts']
   */
  ignoreDirNames?: string[]
}

interface SideBarItem {
  text: string
  collapsible?: boolean
  collapsed?: boolean
  items?: SideBarItem[]
  link?: string
}

interface NavGenerateConfig {
  /**
   * 需要遍历的目录. 默认:articles
   */
  dirName?: string
  /**
   * 最大遍历层级. 默认:1
   */
  maxLevel?: number
}

/**
 * 判断是否为markdown文件
 * @param fileName 文件名
 * @returns 有返回值则表示是markdown文件,否则不是
 */
function isMarkdownFile(fileName: string) {
  return !!fileName.match(/.+\.md$/)
}

// 获取docs目录的完整名称(从根目录一直到docs目录)
const docsDirFullPath = join(__dirname, '../')
// 获取docs目录的完整长度
const docsDirFullPathLen = docsDirFullPath.length

/**
 * 获取dirOrFileFullName中第一个/docs/后的所有内容
 *  如:
 * /a-root/docs/test 则 获取到 /test
 * /a-root-docs/docs/test 则 获取到 /test
 * /a-root-docs/docs/docs/test 则 获取到 /docs/test
 * @param dirOrFileFullName 文件或者目录名
 * @returns
 */
function getDocsDirNameAfterStr(dirOrFileFullName: string) {
  // 使用docsDirFullPathLen采用字符串截取的方式，避免多层目录都叫docs的问题
  return `${sep}${dirOrFileFullName.substring(docsDirFullPathLen)}`
}

export function getSidebarData(sidebarGenerateConfig: SidebarGenerateConfig = {}) {
  const {
    dirName = 'articles',
    ignoreFileName = 'index.md',
    ignoreDirNames = ['demo', 'asserts'],
  } = sidebarGenerateConfig

  // 获取目录的绝对路径
  const dirFullPath = resolve(__dirname, `../${dirName}`)
  const allDirAndFileNameArr = readdirSync(dirFullPath)
  const obj = {}

  allDirAndFileNameArr.map(dirName => {
    let subDirFullName = join(dirFullPath, dirName)

    const property = getDocsDirNameAfterStr(subDirFullName).replace(/\\/g, '/') + '/'
    const arr = getSideBarItemTreeData(subDirFullName, 1, 3, ignoreFileName, ignoreDirNames)

    obj[property] = arr
  })

  return obj
}

function getSideBarItemTreeData(
  dirFullPath: string,
  level: number,
  maxLevel: number,
  ignoreFileName: string,
  ignoreDirNames: string[]
): SideBarItem[] {
  // 获取所有文件名和目录名
  const allDirAndFileNameArr = readdirSync(dirFullPath)
  const result: SideBarItem[] = []
  allDirAndFileNameArr.map((fileOrDirName: string, idx: number) => {
    const fileOrDirFullPath = join(dirFullPath, fileOrDirName)
    const stats = statSync(fileOrDirFullPath)
    if (stats.isDirectory()) {
      if (!ignoreDirNames.includes(fileOrDirName)) {
        const text = fileOrDirName.match(/^[0-9]{2}-.+/) ? fileOrDirName.substring(3) : fileOrDirName
        // 当前为文件夹
        const dirData: SideBarItem = {
          text,
          collapsed: false,
        }
        if (level !== maxLevel) {
          dirData.items = getSideBarItemTreeData(fileOrDirFullPath, level + 1, maxLevel, ignoreFileName, ignoreDirNames)
        }
        if (dirData.items) {
          dirData.collapsible = true
        }
        result.push(dirData)
      }
    } else if (isMarkdownFile(fileOrDirName) && ignoreFileName !== fileOrDirName) {
      // 当前为文件
      const matchResult = fileOrDirName.match(/(.+)\.md/)
      let text = matchResult ? matchResult[1] : fileOrDirName
      text = text.match(/^[0-9]{2}-.+/) ? text.substring(3) : text

      const fileData: SideBarItem = {
        text,
        link: getDocsDirNameAfterStr(fileOrDirFullPath).replace('.md', '').replace(/\\/g, '/'),
      }

      result.push(fileData)
    }
  })

  return result
}

export function getNavData(navGenerateConfig: NavGenerateConfig = {}) {
  const { dirName = 'articles', maxLevel = 2 } = navGenerateConfig
  const dirFullPath = resolve(__dirname, `../${dirName}`)
  const result = getNavDataArr(dirFullPath, 1, maxLevel)

  return result
}

/**
 * 获取顶部导航数据
 *
 * @param   {string}     dirFullPath  当前需要遍历的目录绝对路径
 * @param   {number}     level        当前层级
 * @param   {number[]}   maxLevel     允许遍历的最大层级
 * @return  {NavItem[]}               导航数据数组
 */
function getNavDataArr(dirFullPath: string, level: number, maxLevel: number): DefaultTheme.NavItem[] {
  // 获取所有文件名和目录名
  const allDirAndFileNameArr = readdirSync(dirFullPath)
  const result: DefaultTheme.NavItem[] = []

  allDirAndFileNameArr.map((fileOrDirName: string, idx: number) => {
    const fileOrDirFullPath = join(dirFullPath, fileOrDirName)
    const stats = statSync(fileOrDirFullPath)
    const link = getDocsDirNameAfterStr(fileOrDirFullPath).replace('.md', '').replace(/\\/g, '/')

    const text = fileOrDirName.match(/^[0-9]{2}-.+/) ? fileOrDirName.substring(3) : fileOrDirName

    if (stats.isDirectory()) {
      // 当前为文件夹
      const dirData: any = {
        text,
        link: `${link}/`,
      }

      if (level !== maxLevel) {
        const arr = getNavDataArr(fileOrDirFullPath, level + 1, maxLevel).filter(v => v.text !== 'index.md')
        if (arr.length > 0) {
          // @ts-ignore
          dirData.items = arr
          delete dirData.link
        }
      }

      dirData.activeMatch = link + '/'
      result.push(dirData)
    } else if (isMarkdownFile(fileOrDirName)) {
      // 当前为文件
      const fileData: DefaultTheme.NavItem = {
        text,
        link,
      }
      fileData.activeMatch = link + '/'
      result.push(fileData)
    }
  })

  return result
}
```

引入到配置中

```js
/* .vitepress/config.ts */
import { defineConfig } from 'vitepress'
import { getSidebarData, getNavData } from './navSidebarUtil'

export default defineConfig({
  // ...
  themeConfig: {
    nav: getNavData(),
    sidebar: getSidebarData(),
  },
})
```

其他侧边栏工具

https://blog.csdn.net/weixin\_46463785/article/details/128592038

https://docs.zhengxinonly.com （VitePress 系列教程：自动生成侧边栏 #7）

https://juejin.cn/post/7227358177489961018

## 代码组图标

使用的是 [@yuyinws/vitepress-plugin-group-icons](https://github.com/yuyinws/vitepress-plugin-group-icons)

参照教程安装：https://vpgi.vercel.app/

```sh
npm install vitepress-plugin-group-icons
```

然后在 `config.mts` 中配置

::: tip `groupIconMdPlugin` 报错？

请备份配置及文件后，重新安装VitePress

:::

```js
// .vitepress/config.mts
import { defineConfig } from 'vitepress'
import { groupIconMdPlugin, groupIconVitePlugin } from 'vitepress-plugin-group-icons'   // [!code focus:1]

export default defineConfig({

  markdown: {
    config(md) {   // [!code focus:3]
      md.use(groupIconMdPlugin) // 代码组图标
    },
  },

  vite: {   // [!code focus:5]
    plugins: [
      groupIconVitePlugin() // 代码组图标
    ],
  },

})
```

最后还需要再 `index.ts` 中引入样式

```js
// .vitepress/theme/index.ts
import DefaultTheme from 'vitepress/theme'

import 'virtual:group-icons.css' // 代码组样式  // [!code focus:1]

export default {
  extends: DefaultTheme,
}
```

使用时，请确保代码后有对应的文字触发

````markdown{2,6,10}
::: code-group
```sh [pnpm]
pnpm -v
```

```sh [yarn]
yarn -v
```

```sh [bun]
bun -v
```
:::
````

已经内置的常用图标有

```js
export const builtInIcons: Record<string, string> = {
  // package manager
  pnpm: 'logos:pnpm',
  npm: 'logos:npm-icon',
  yarn: 'logos:yarn',
  bun: 'logos:bun',
  // framework
  vue: 'logos:vue',
  svelte: 'logos:svelte-icon',
  angular: 'logos:angular-icon',
  react: 'logos:react',
  next: 'logos:nextjs-icon',
  nuxt: 'logos:nuxt-icon',
  solid: 'logos:solidjs-icon',
  // bundler
  rollup: 'logos:rollupjs',
  webpack: 'logos:webpack',
  vite: 'logos:vitejs',
  esbuild: 'logos:esbuild',
}
```

那么如何自定义呢，我们先在 [iconify](https://icon-sets.iconify.design/) 中找到中意的图标

::: tip 说明

* 本地图标格式：只能使用相对路径
* 远程图标格式：必须是 `logos:***`

:::

图标名复制后，可以在 `config.mts` 中配置

```js
// .vitepress/config.mts
import { defineConfig } from 'vitepress'
import { groupIconMdPlugin, groupIconVitePlugin, localIconLoader } from 'vitepress-plugin-group-icons' // [!code focus:1]

export default defineConfig({

  markdown: {
    config(md) {
      md.use(groupIconMdPlugin) //代码组图标
    },
  },

  vite: {
    plugins: [
      groupIconVitePlugin({  // [!code focus:8]
        customIcon: {
          ts: localIconLoader(import.meta.url, '../public/svg/typescript.svg'), //本地ts图标导入
          js: 'logos:javascript', //js图标
          md: 'logos:markdown', //markdown图标
          css: 'logos:css-3', //css图标
        },
      })
    ],
  },

})
```

## 禁用F12

使用的是 [@cellinlab/vitepress-protect-plugin](https://github.com/cellinlab/vitepress-protect-plugin/)

```
npm install vitepress-protect-plugin
```

然后在 `config.mts` 中配置，不用的功能不配置即可

```js
import { defineConfig } from "vitepress"
import vitepressProtectPlugin from "vitepress-protect-plugin" // [!code focus:1]

export default defineConfig({
  // other VitePress configs...
  vite: {
    plugins: [
      vitepressProtectPlugin({ // [!code focus:5]
        disableF12: true, // 禁用F12开发者模式
        disableCopy: true, // 禁用文本复制
        disableSelect: true, // 禁用文本选择
      }),
    ],
  },
})
```

## 切换路由进度条

当你切换页面，顶部会显示进度条，使用的是 [@Skyleen77/nprogress-v2](https://github.com/Skyleen77/nprogress-v2)

::: tip 说明

本方式由 [Aurorxa](https://github.com/Aurorxa) 提供推送 [#36](https://github.com/Yiov/vitepress-doc/pull/36)

:::

先安装 `nprogress-v2`

```sh
npm install nprogress-v2
```

然后再 `index.ts` 中配置，即可生效

```js
// .vitepress/theme/index.ts

import { NProgress } from 'nprogress-v2/dist/index.js' // 进度条组件 // [!code focus:2]
import 'nprogress-v2/dist/index.css' // 进度条样式

if (inBrowser) {
      NProgress.configure({ showSpinner: false }) // [!code focus:4]
      router.onBeforeRouteChange = () => {
        NProgress.start() // 开始进度条
      }
      router.onAfterRouteChanged = () => {
         busuanzi.fetch()
         NProgress.done() // 停止进度条 // [!code focus:1]
      }
}
```

## 评论

各种评论系统对比

|                    评论系统                     |                             说明                             |
| :---------------------------------------------: | :----------------------------------------------------------: |
| [Valine](https://valine.js.org/quickstart.html) | 不用登录账号即可评论，但容易产生垃圾评论，其次没有评论提醒通知 |
|        [Waline](https://waline.js.org/)         | 是Valine的升级版，登录后方可评论，有通知，但是需要自己部署服务端 |
|        [Twikoo](https://twikoo.js.org/)         | 不用登录账号即可评论，但容易产生垃圾评论，有通知，但是需要自己部署服务端 |
|        [Artalk](https://artalk.js.org/)         | 可设置是否启用登录账号后评论，有通知，但是需要自己部署服务端 |
|        [utteranc](https://utteranc.es/)         | GitHub登录后方可评论，评论数据在 GitHub issues 中，评论后有邮件通知，无需部署服务端，但UI样式一般，且移动端不显示头像 |
|       [gitalk](https://gitalk.github.io/)       | GitHub登录后方可评论，评论数据在 GitHub issues 中，评论后有邮件通知，无需部署服务端，UI样式一般，评论不支持点赞 |
|       ⭐[Giscus](https://giscus.app/zh-CN)       | GitHub登录后方可评论，评论数据在 GitHub Discussions 中，评论后有邮件通知，无需部署服务端，UI爱了 |

之前在 vuepress 中用过 Valine，效果并不是特别好

从上面对比来看，[Giscus](https://giscus.app/zh-CN) 最佳，因此决定使用 [Giscus](https://giscus.app/zh-CN)

::: details 关于 [@xinlei3166](https://github.com/xinlei3166/) 的 waline 插件

在使用看板娘发时候就已经装好了，直接引用就行了

```js
// .vitepress/theme/index.ts
import DefaultTheme from 'vitepress/theme'

import { useWaline } from 'vitepress-theme-website' // [!code focus:1]

export default {
  extends: DefaultTheme,

  setup() {

    useWaline({ // [!code focus:3]
      serverURL: 'https://you_url.com'
    }),

  }
}
```

记得 `serverURL` 换成自己的即可，但是作者的插件有个bug，暗黑模式下看不清字

所以要用 [waline](https://waline.js.org/) 话就 [参考官方的教程](https://waline.js.org/guide/get-started/)

:::

### 安装giscus

Giscus 是一个基于 GitHub Discussion 的评论系统，启用简便

进 Giscus App官网：https://github.com/apps/giscus

点击 `Install` 安装

选择 `Only select repositories`，再指定一个你想开启讨论的仓库

::: tip 注意

仓库必须是公开的，私有的不行

想单独放评论，新建一个也可

:::

安装完成后可以在个人头像-设置-应用 `Applications` 中看到

### 开启讨论

因为giscus会把评论数据都放到讨论 `discussions` 中

我们进入要开启讨论的仓库，点设置 - 勾选讨论 `Settings - discussions`

### 生成数据

进入官网：https://giscus.app/zh-CN

输入自己的仓库链接，满足条件会提示可用

下拉到 Discussion 分类推荐选 `General` ，懒加载评论也可以勾选下

关于讨论的类型分类如下，来自于 [Github的讨论文档](https://docs.github.com/zh/discussions/managing-discussions-for-your-community/managing-categories-for-discussions#about-categories-for-discussions)

|     类别      |    中文    |          说明          |
| :-----------: | :--------: | :--------------------: |
| Announcements |    公告    | 每次评论都会推送所有人 |
|    General    |    常规    |       开放式讨论       |
|     Ideas     |    想法    |       开放式讨论       |
|     Polls     |    投票    |      可投票与讨论      |
|      Q\&A      |    问答    |        问答形式        |
| Show and tell | 展示和说明 |       开放式讨论       |

下方就自动生成了你的关键数据

```js
<script src="https://giscus.app/client.js"
        data-repo="github repository" // [!code focus:4]
        data-repo-id="R_******"
        data-category="General"
        data-category-id="DIC_******"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="preferred_color_scheme"
        data-lang="zh-CN"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
```

其中 `data-repo` 、 `data-repo-id` 、 `data-category` 和 `data-category-id` 这4个是我们的关键数据

### 安装使用

有能力的可以用官方给的js数据封装

我这里用 [@T-miracle/vitepress-plugin-comment-with-giscus](https://github.com/T-miracle/vitepress-plugin-comment-with-giscus) 的插件

```sh
npm install vitepress-plugin-comment-with-giscus
```

在 `.vitepress/theme/index.ts` 中填入下面代码

并将我们之前获取的4个关键数据填入，其他保持默认保存

```js
// .vitepress/theme/index.ts
import DefaultTheme from 'vitepress/theme';
import giscusTalk from 'vitepress-plugin-comment-with-giscus'; // [!code focus:2]
import { useData, useRoute } from 'vitepress';

export default {
  extends: DefaultTheme,

  setup() {
    // Get frontmatter and route // [!code focus:22]
    const { frontmatter } = useData();
    const route = useRoute();
        
    // giscus配置
    giscusTalk({
      repo: 'your github repository', //仓库
      repoId: 'your repository id', //仓库ID
      category: 'Announcements', // 讨论分类
      categoryId: 'your category id', //讨论分类ID
      mapping: 'pathname',
      inputPosition: 'bottom',
      lang: 'zh-CN',
      }, 
      {
        frontmatter, route
      },
      //默认值为true，表示已启用，此参数可以忽略；
      //如果为false，则表示未启用
      //您可以使用“comment:true”序言在页面上单独启用它
      true
    );

}
```

安装完看下底部的效果吧

如果某一页不想启用，可以在当前页使用 `Frontmatter` 关闭

```markdown
---
comment: false
---
```

---

---
url: /10.配置/01.主题配置/40.插件配置.md
---

# 插件配置

## vitePlugins

内置 Vite 插件配置。

Teek 内置的 Vite 插件详细介绍请看 [Vite 插件](/guide/plugins)。

::: code-group

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  vitePlugins: {
    sidebar: true, // 是否启用 sidebar 插件
    sidebarOption: {}, // sidebar 插件配置项
    permalink: true, // 是否启用 permalink 插件
    permalinkOption: {}, // permalinks 插件配置项
    mdH1: true, // 是否启用 mdH1 插件
    catalogueOption: {}, // catalogues 插件配置项
    docAnalysis: true, // 是否启用 docAnalysis 插件
    docAnalysisOption: {}, // docAnalysis 插件配置项
    fileContentLoaderIgnore: [], // fileContentLoader 插件扫描 markdown 文档时，指定忽略路径，格式为 glob 表达式，如 **/test/**
    autoFrontmatter: true, // 是否启用 autoFrontmatter 插件
    // autoFrontmatter 插件配置项
    autoFrontmatterOption: {
      permalinkPrefix: "pages", // 自动生成 permalink 的固定前缀，如 pages、pages/demo，默认为 pages
      categories: true, // 是否自动生成 categories
      // ...
    },
  },
});
```

```ts [更多配置项]
import type { PermalinkOption } from "vitepress-plugin-permalink";
import type { SidebarOption } from "vitepress-plugin-sidebar-resolve";
import type { CatalogueOption } from "vitepress-plugin-catalogue";
import type { DocAnalysisOption } from "vitepress-plugin-doc-analysis";
import type { AutoFrontmatterOption } from "plugins/vitepress-plugin-auto-frontmatter";

interface Plugins {
  /**
   * 是否启用 sidebar 插件
   *
   * @default true
   */
  sidebar?: boolean;
  /**
   * sidebar 插件配置项
   */
  sidebarOption?: SidebarOption;
  /**
   * 是否启用 permalink 插件
   *
   * @default true
   */
  permalink?: boolean;
  /**
   * permalinks 插件配置项
   */
  permalinkOption?: PermalinkOption;
  /**
   * 是否启用 mdH1 插件
   *
   * @default true
   */
  mdH1?: boolean;
  /**
   * catalogues 插件配置项
   */
  catalogueOption?: CatalogueOption;
  /**
   * 是否启用 docAnalysis 插件
   *
   * @default true
   */
  docAnalysis?: boolean;
  /**
   * docAnalysis 插件配置项
   */
  docAnalysisOption?: DocAnalysisOption;
  /**
   * fileContentLoader 插件扫描 markdown 文档时，指定忽略路径，格式为 glob 表达式，如 test/**
   *
   * @default []
   */
  fileContentLoaderIgnore?: string[];
  /**
   * 是否启用 autoFrontmatter 插件
   *
   * @default false
   */
  autoFrontmatter?: boolean;
  /**
   * autoFrontmatter 插件配置项，并拓展出其他配置项
   *
   * permalinkPrefix 为自动生成 permalink 的固定前缀，如 pages、pages/demo，默认为 page。当禁用 permalink 插件后，不会自动生成 permalink
   * categories 为是否自动生成 categories
   *
   * @default '{ permalinkPrefix: "pages", categories: true }'
   */
  autoFrontmatterOption?: AutoFrontmatterOption & { permalinkPrefix?: string; categories?: boolean };
}
```

:::

## markdown

您可以对 Teek 内置的 Markdown 容器进行一些配置。

Teek 内置的 Markdown 插件详细介绍请看 [Markdown 拓展](/guide/markdown)。

### config

通过该 `config` 函数来加载更多的 `Markdown-it` 插件。

::: danger
请不要使用 VitePress 提供 `markdown.config` 函数来加载 `Markdown-it` 插件，因为 VitePress 方式会覆盖主题内置的 `Markdown-it` 插件。
:::

::: code-group

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";
import myMdPlugin from "my-md-plugin";

const teekConfig = defineTeekConfig({
  markdown: {
    config: md => {
      md.use(myMdPlugin);
    },
  },
});
```

```ts [更多配置项]
import type MarkdownIt from "markdown-it";

Markdown {
  /**
   * 注册更多 markdown 插件函数
   */
  config?: (md: MarkdownIt) => void;
}
```

:::

### container

Teek 内置的 Markdown 容器配置，配置项如下：

```ts
interface Markdown {
  /**
   * 内置 markdown 容器的 Label 配置
   */
  container?: {
    /**
     * 自定义容器标题
     */
    label?: {
      /**
       * note 容器的默认标题
       *
       * @default 'NOTE'
       */
      noteLabel?: string;
    };
    /**
     * 自定义 markdown 容器配置
     */
    config?: () => {
      /**
       * 容器类型
       */
      type: string;
      /**
       * 是否使用标题
       */
      useTitle?: boolean;
      /**
       * 默认标题
       */
      defaultTitle?: string;
      /**
       * 容器类名
       */
      className?: string;
    }[];
  };
}
```

#### 容器配置

Note 容器默认的标题是 `Note`，您可以通过修改其默认值，这在国际化环境下很有帮助：

```ts
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";
import myMdPlugin from "my-md-plugin";

const teekConfig = defineTeekConfig({
  markdown: {
    container: {
      label: {
        noteLabel: "笔记",
      },
    },
  },
});
```

#### 自定义容器

Teek 支持自定义 `Markdown` 容器配置。

通过 `markdown.container.config` 函数可以快速创建出类似于 Teek 内置的 `center` 和 `right` 容器或 VitePress 的 `info`、`tip`、`warning`、`danger` 容器。

先看例子：

```ts
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  markdown: {
    container: {
      config: () => [
        { type: "demo1", useTitle: true, defaultTitle: "demo1", className: "demo1-container" },
        { type: "demo2", useTitle: false, className: "demo2-container" },
      ],
    },
  },
});
```

示例中，我们创建了两个容器，其中第一个容器 `demo1` 通过 `useTitle: true` 来支持输入标题，如果不输入标题，则使用默认标题 `demo1`。

容器使用如下：

```markdown
::: demo1
测试 demo1 容器
:::

::: demo1 容器标题
测试 demo1 容器
:::

::: demo2
测试 demo2 容器
:::
```

生成的 HTML 结构如下：

```html

<div class="demo1 demo1-container">
  <p class="title demo1-title demo1-container-title">demo1</p>
  <p>测试 demo1 容器</p>
</div

<div class="demo1 demo1-container">
  <p class="title demo1-title demo1-container-title">容器标题</p>
  <p>测试 demo1 容器</p>
</div

<div class="demo2 demo2-container">
  <p>测试 demo2 容器</p>
</div

```

HTML 模板如下：

::: code-group

```html [开启标题]
<div class="${type} ${className}">
  <p class="title ${type}-title ${className}-title">${defaultTitle || 传入标题}</p>
  <p>${输入的内容}</p>
</div
```

```html [不开启标题]
<div class="${type} ${className}">
  <p>${输入的内容}</p>
</div
```

:::

虽然 Teek 按照 HTML 模板进行渲染，但是并没有提供 CSS 样式，您需要对 `class` 来自定义样式，如：

```scss
// .vitepress/theme/style/container.scss
.demo1-container {
  font-size: 16px;
  .title {
    font-size: 18px;
  }
}

.demo2-container {
  font-size: 12px;
}
```

然后引入样式文件：

```ts
// .vitepress/theme/index.ts
import Teek from "vitepress-theme-teek";
import "vitepress-theme-teek/index.css";
import "./style/container.scss";

export default {
  extends: Teek,
};
```

### demo

`markdown.demo` 用于配置 Demo 容器，如果您不了解什么是 Demo 容器，请看 [Demo 容器](/guide/markdown#demo-容器)。

::: code-group

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  markdown: {
    demo: {
      playgroundUrl: "", // Playground 链接
      playgroundMainFileName: "App.vue", // Playground 主文件名
      githubUrl: "", // Github 链接
      playgroundButtonTip: "在 Playground 中编辑", // 鼠标悬浮 Playground 按钮提示
      githubButtonTip: "在 Github 中编辑", // 鼠标悬浮 Github 按钮提示
      copyButtonTip: "复制代码", // 鼠标悬浮复制代码按钮提示
      collapseSourceButtonTip: "查看源代码", // 鼠标悬浮查看源代码按钮提示
      expandSourceButtonTip: "隐藏源代码", // 鼠标悬浮隐藏源代码按钮提示
    },
  },
});
```

```ts [更多配置项]
interface Markdown {
  /**
   * demo 插件配置
   */
  demo?: {
    /**
     * 是否禁用 demo 插件
     *
     * @default false
     */
    disabled?: boolean;
    /**
     * Playground 链接
     */
    playgroundUrl?: string;
    /**
     * Playground 主文件名
     *
     * @default 'App.vue'
     */
    playgroundMainFileName?: string;
    /**
     * Github 链接
     */
    githubUrl?: string;
    /**
     * 鼠标悬浮 Playground 按钮提示
     *
     * @default '在 Playground 中编辑'
     */
    playgroundButtonTip?: string;
    /**
     * 鼠标悬浮 Github 按钮提示
     *
     * @default '在 Github 中编辑'
     */
    githubButtonTip?: string;
    /**
     * 鼠标悬浮复制代码按钮提示
     *
     * @default '复制代码'
     */
    copyButtonTip?: string;
    /**
     * 鼠标悬浮复查看源代码按钮提示（代码块处于折叠状态）
     *
     * @default '查看源代码'
     */
    collapseSourceButtonTip?: string;
    /**
     * 鼠标悬浮复查看源代码按钮提示（代码块处于展开状态）
     *
     * @default '隐藏源代码'
     */
    expandSourceButtonTip?: string;
  };
}
```

:::

Demo 容器的按钮组默认有 4 个按钮：

* 查看/隐藏源代码
* 复制源代码
* 去 `Github` 编辑
* 去 `Playground` 编辑

如果您想自定义一些按钮，可以在 `.vitepress/theme/index.ts` 下使用如下插槽：

* `teek-demo-code-button-left`：按钮组最左边插槽
* `teek-demo-code-button-right`：按钮组最右边插槽

如：

```ts
// .vitepress/theme/index.ts
import Teek from "vitepress-theme-teek";
import "vitepress-theme-teek/index.css";
import { h } from "vue";
import MyButton from "./components/MyButton.vue";

export default {
  extends: Teek,
  Layout() {
    return h(Teek.Layout, null, {
      "teek-demo-code-button-left": () => h(MyButton),
    });
  },
};
```

---

---
url: /Java/数据结构/查找算法.md
---

# 查找算法

ASL (Aerage Search Length）

![img](/assets/image.BMw5v_YR.png)

### 顺序查找

顶序查找法的特点是：用所给关键字与线性表中各元素的关键字逐个比较，直到成功或失败。

顺序表、链表

![img](/assets/image.BMw5v_YR.png)

### 折半查找（二分查找法）

条件：要求待查找的列表必须是按关健字大小**有序排列的顺序表**。

将n个元素分成个数大致相同的两半，取R\[n/2]与k作比较。

(1)如果k=R\[n/2]，则找到k;

(2)如果k\<R\[n/2]，则只在数组R的左半部继续搜索k;

如果k>R\[n/2]，则只在数组R的右半部继续搜索k。

```c
int BSearch(int arr[], int low, int high, int key) {
    // low和high分别表示数组的起始位置和结束位置
    while (low <= high) {
        // 当low和high相等时，表示数组中没有找到key
        int wid = (low + high) / 2;
        // 将数组中的索引wid设置为搜索结果
        if (arr[wid] == key)
            return wid;
        // 如果wid大于key，表示搜索结果在数组中的右边
        else if (arr[wid] > key)
            // 将wid减去1，表示搜索结果在数组中的左边
            high = wid - 1;
        // 如果wid小于key，表示搜索结果在数组中的左边
        else
            // 将wid加上1，表示搜索结果在数组中的右边
            low = wid + 1;
    }
    // 如果low和high相等，表示没有找到key
    return -1;
}
```

```c
int Bsearch(int R[], int Rl, int left, int right, int k) {
    if (left > right)
        return -1; // 未找到k
    int mid = (left + right) / 2;
    if (R[mid] == k)
        return mid; // 我到k
    else if (R[mid] > k)
        return Bsearch(R, Rl, left, mid - 1, k); // 搜左半部
    else if (R[mid] < k)
        return Bsearch(R, Rl, mid + 1, right, k); // 搜右半部
}
```

### 分块查找

分块查找法要求将列表组织成以下索引顺序结构：

* 首先将列表分成若干个块 (子表)。一般情况下块的长度均勾，最后一块可以不满。每块中元素任意排列，即块内无序，但块与块之间有序。
* 构造一个索引表。其中每个索引项对应一个块并记录每块的起始位置，和每块中的最大关键字(或最小关键字)。索引表按关键字有序排列。

### 二叉排序树

#### 查找

二叉排序树(二叉查找树)，它是一种特殊结构的二叉树，其定义为：二叉树排序树或者是一棵空树，或者是具有如下性质的二叉树：

(1)若它的左子树非空，则左子树上所有结点的值均小于根结点的值；

(2)若它的右子树非空，则右子树上所有结点的值均大干根结点的值；

(3)它的左右子树也分别为二叉排序树。

由此定义可以得出二叉排序树的一个重要性质：中序遍历一个二叉排序树时可以得到一个递增有序序列。

```c
// 定义BTNode类型
typedef struct BTNode {
    int key;
    struct BTNode *left;
    struct BTNode *right;
} BTNode;

//查找二叉搜索树中是否存在指定的key
BTNode *BSTSearch(BTNode *p, int key) {
    //当p不为空时
    while (p!= NULL) {
        //如果key等于p的key，则返回p
        if (key == p->key) {
            return p;
        //如果key小于p的key，则p的左子树查找
        } else if (key < p->key) {
            p = p->left;
        //如果key大于p的key，则p的右子树查找
        } else {
            p = p->right;
        }
    }
    //如果查找不到，则返回NULL
    return NULL;
}
```

```c
// 定义BTNode类型
struct BTNode {
    int key;
    struct BTNode *lChild;
    struct BTNode *rChild;
};

// 查找给定关键字的节点
struct BTNode *BSTSearch(struct BTNode *p, int key) {
    // 如果没有找到，则返回空
    if (p == NULL) {
        return NULL;
    } else {
        // 如果找到，则返回查找到的节点
        if (p->key == key) {
            return p;
        } else if (key < p->key) {
            // 如果查找到的关键字小于当前节点的关键字，则查找当前节点的左子树
            return BSTSearch(p->lChild, key);
        } else {
            // 如果查找到的关键字大于当前节点的关键字，则查找当前节点的右子树
            return BSTSearch(p->rChild, key);
        }
    }
}
```

#### 插入

已知一个关键字值为Key的结点s，插入的方法：

1. 若二叉排序树是空树，则Key成为二叉排树的根；
2. 若二叉树排序树非空，则将key与二叉树排序树的根进行比较，如果Key的值等于根结点的值，则停止插入，如果Key的值小于根结点的值，则将Key插入左子树，如果Key的值大于根结点的值，则将key插入右子树。

```c
#include <stdio.h>
#include <stdlib.h>

// 定义一个BTNode类型的结构体
typedef struct BTNode {
    int key;
    struct BTNode *lChild;
    struct BTNode *rChild;
} BTNode;

// 定义一个BSTInsert函数，用于插入一个关键字key的节点
int BSTInsert(BTNode *sp, int key) {
    // 如果sp为空，则分配一个新的节点
    if (sp == NULL) {
        sp = (BTNode *) malloc(sizeof(BTNode));
        sp->lChild = sp->rChild = NULL;
        sp->key = key;
        return 1;
    }
    // 如果key和sp的key相等，则返回0
    if (key == sp->key) {
        return 0;
    } else if (key < sp->key) {
        // 如果key小于sp的key，则递归在sp的左子树中插入key
        return BSTInsert(sp->lChild, key);
    } else {
        // 如果key大于sp的key，则递归在sp的右子树中插入key
        return BSTInsert(sp->rChild, key);
    }
}

int main() {
    BTNode *root = NULL;
    BSTInsert(root, 5);
    BSTInsert(root, 3);
    BSTInsert(root, 7);
    BSTInsert(root, 1);
    BSTInsert(root, 4);
    BSTInsert(root, 6);
    BSTInsert(root, 8);

    return 0;
}
```

二叉排序树的生成方法：

假若给定一个元素序列，可以插入算法创建一棵二叉排序树。

将二叉排序树初始化为一棵空树，然后逐个读入元素，每读入一个元素，就建立一个新的结点插入到当前已

生成的二叉排序树中，即调用上述二叉排序树的插入算法将新结点插入。

```c
typedef struct Node {
    int key;
    struct Node *left;
    struct Node *right;
} BSTree;

BSTree CreateBST(BSTree *bst) {
    BSTree *hst = NULL;
    int key;
    scanf("%d", &key);
    while (key != 1000) {
        bst = InsertBST(bst, key);
        scanf("%d", &key);
    }
    return hst;
}
```

---

---
url: /daily/软件设计师/02_程序设计语言基础知识.md
---

# 程序设计语言基础知识

## 一、程序设计语言概述

### :palm\_tree:1.1 程序设计语言的基本概念

程序设计语言是为了书写计算机程序而人为设计的符号语言，用于对计算过程进行描述、组织和推导。

低级语言：机器语言（计算机硬件只能识别0和1的指令序列）、汇编语言。
高级语言：功能更强，抽象级别更高，与人们使用的自然语言比较接近。

各程序设计语言特点：

* Fortran语言（科学计算，执行效率高）
* Pascal语言（为教学而开发的，表达能力强，Delphi)
* C语言（指针操作能力强，高效）
* Lisp语言（函数式程序语言，符号处理，人工智能）
* C++语言（面向对象，高效）
* Java语言（面向对象，中间代码，跨平台）
* C#语言（面向对象，中间代码，.Net)
* Prolog语言（逻辑推理，简洁性，表达能力，数据库和专家系统）
* Python（可用于编写独立程序和快速脚本）

解释和编译都是将高级语言翻译成计算机硬件认可的机器语言加以执行。不同之处于编译程序生成独立的可执行文件，直接运行，运行时无法控制源程序，效率高。而解释程序不生成可执行文件，可以逐条解释执行，用于调试模式，可以控制源程序，因为还需要控制程序，因此执行速度慢，效率低。
程序设计语言组成：语法（一组规则）、语义（语法成分的含义）、语用（构成语言的各个记号和使用者的关系）。

```
例（2012年上半年）：51、编译和解释是实现高级程序设计语言翻译的两种基本形式。以下关于编译与解释的叙述中，正确的是。
A.在解释方式下，对源程序不进行词法分析和语法分析，直接进行语义分析
B.在解释方式下，无需进行语法、语法和语义分析，而是直接产生源程序的目标代码
C.在编译方式下，必须进行词法、语法和语义分析，然后再产生源程序的目标代码
D.在编译方式下，必须先形成源程序的中间代码，然后再产生与机器对应的目标代码
答案：C
解析：在编译方式下，先将源程序翻译为等价的目标程序，源程序的翻译和目标程序的运行是完全独立的两个阶段；而解释方式下，对源程序的翻译和运行是结合在一起进行的，并不生成目标代码。
编译过程基本上可以划分为词法分析、语法分析、语义分析、中间代码生成、代码优化和目标代码生成等几个阶段，其中，中间代码生成和代码优化不是必须的。在词法、语法、语义分析方面，编译方式和解释方式没有区别。
```

### :palm\_tree:1.2 程序设计语言的基本成分

数据成分：数据和数据类型

数据：常量、变量、全局量（存储空间在静态数据区分配）、局部量（存储空间在堆栈区分配）
数据类型：整型、字符型、双精度浮点型、单精度浮点型、布尔型等
运算成分：算数运算、逻辑运算、关系运算、位运算等
控制成分：顺序结构、条件结构、循环结构
传输成分：指明语言允许的数据传输方式，如赋值处理、数据的输入输出等
函数：main函数只有一个，是程序运行的起点。
传值调用：将实参的值传递给形参，形参的改变不会导致调用点所传的实参的值改变。实参可以是合法的变量、常量和表达式。
传址调用：即引用调用，将实参的地址传递给形参，即相当于实参存储单元的地址引用，因此其值改变的同时就改变了实参的值。实参不能为常量，只能是合法的变量和表达式。
因此，在编程时，要改变参数值，就传址，不改变参数值，就传值。

## 二、语言处理程序基础

### :palm\_tree:2.1 编译程序基本原理

#### :seedling:编译过程概念

编译程序的功能是把某高级语言书写的源程序翻译成与之等价的目标程序（汇编语言或机器语言）。

编译程序的执行过程可以分为以下几个步骤：

1. 词法分析（Lexical Analysis）：将输入的源代码分解成词法单元（tokens），例如标识符、关键字和常量等。
2. 语法分析（Parsing）：根据编程语言的语法规则，将词法单元组成的序列转化为抽象语法树（Abstract Syntax Tree，AST），并进行语法验证。
3. 语义分析（Semantic Analysis）：对抽象语法树进行语义检查，包括类型检查、作用域分析和语义错误检查等。
4. 中间代码生成（Intermediate Code Generation）：根据抽象语法树生成中间代码，如三地址码、字节码或虚拟机指令等。
5. 优化（Optimization）：对生成的中间代码进行优化，以提高程序的性能和效率，包括常量折叠、无用代码消除和循环优化等。
6. 目标代码生成（Code Generation）：将优化后的中间代码翻译成目标机器代码，可以是特定硬件平台的汇编语言或机器语言。
7. 目标代码优化（Code Optimization）：对生成的目标机器代码进行优化，以进一步提高程序的性能和效率。
8. 目标代码链接（Code Linking）：将生成的目标机器代码与库文件进行链接，生成可执行文件。
9. 可执行代码加载与执行（Code Loading and Execution）：将可执行文件加载到内存中，并执行程序。

编译程序工作过程分为6个阶段，如下图所示：

![img](/assets/b4d46de063d09bc021c87a39b0c4f006.BToHeLUo.png)

中间代码和目标代码生成：中间代码是根据语义分析产生的，需要经过优化链接，最终生成可执行的目标代码。引入中间代码的目的是进行与机器无关的代码优化处理。常用的中间代码有后缀式（逆波兰式）、三元式（三地址码）、四元式、树、图等形式。需要考虑三个问题（一是如何生成较短的目标代码；二是如何充分利用计算机中的寄存器，减少目标代码访问存储单元的次数；三是如何充分利用计算机指令系统的特点，以提高目标代码的质量）。

#### :seedling:表达式

![img](/assets/a46cbb91783b11d01b38569e4b038119.ikKGgp02.png)

前缀表达式：操作符位于操作数之前的表达式。例如，将中缀表达式 "2 + 3" 转换为前缀表达式可以得到 "+ 2 3"。

中缀表达式：操作符位于操作数之间的表达式。例如，"2 + 3" 就是一个中缀表达式。

后缀表达式：操作符位于操作数之后的表达式。例如，将中缀表达式 "2 + 3" 转换为后缀表达式可以得到 "2 3 +"。

在计算机中，通常使用后缀表达式进行数学计算，因为后缀表达式具有优先级，可以直接按照顺序进行计算，而无需考虑括号和优先级的问题。而前缀和中缀表达式则需要使用括号和优先级规则来确定计算顺序。

```
例（2013年下半年）：算术表达式a+(b-c) *d的后缀式是(22)（-、+、*表示算术的减、加、乘运算，字符的优先级和结合性遵循惯例）
(22)  A.c-d*a    B.abc-d*+    C.ab+c-d*    D.abcd-*+
解析：a+(b-c)*d → (a+((b-c)*d)) → (a+((bc)-*d)) → (a+((bc)-d)*) → (a((bc)-d)*)+ → abc-d*+ ，答案B。
```

### :palm\_tree:2.2 文法和语言的形式描述

#### :seedling:文法定义

计算机文法是用于描述计算机语言的一种形式化语法。计算机语言可以分为自然语言和形式语言两种类型，其中形式语言又可以分为上下文无关文法和上下文有关文法两种类型。

1. 自然语言：自然语言是人类日常交流所使用的语言，如英语、中文等。自然语言的语法结构较为复杂且灵活，不易用形式化的方式进行准确的描述和处理。
2. 形式语言：形式语言是为了满足计算机处理需要而设计的语言，一般使用符号、规则和语法来描述语言的结构和语义。形式语言分为上下文无关文法和上下文有关文法两种类型。
   * 上下文无关文法（CFG）：上下文无关文法是一种简单且常用的形式化语法，用于描述大多数编程语言的语法结构。它由终结符号、非终结符号、产生式和起始符号组成，可以描述语言中的句子结构和语义。
   * 上下文有关文法（CFL）：上下文有关文法是一种更复杂的形式化语法，可以描述具有上下文依赖关系的语言结构。上下文有关文法中的产生式的替换规则依赖于上下文环境，可以描述更复杂的语言特性。

计算机文法的定义和使用对于编译器设计、语言理解和程序分析等领域具有重要意义，它为计算机语言的编译、解析和语义分析提供了基础框架。

形式文法是一个有序四元组G= (V，T，S，P)  其中：

* V是非终结符集合，表示可以用来构造语言中各种句子的符号。
* T是终结符集合，表示语言中的基本符号或词汇。
* S是起始符号，是一个特殊的非终结符，表示语言中句子的起始位置。
* P是产生式规则集合，由形如A -> α的规则组成，其中A∈V，α∈(V∪T)\*，表示A可以被替换为α。

形式文法描述了一个语言的语法结构，它定义了哪些符号可以出现在句子中、符号的组合方式以及句子的结构。通过应用产生式规则，可以从起始符号开始生成语言中的句子。形式文法在自然语言处理、编译原理和人工智能等领域中被广泛应用。

#### :seedling:闭包

在编译程序中，正则闭包可以用于实现匹配和替换操作。编译器可以使用正则闭包来解析输入的源代码，将其转换为抽象语法树或其他中间表示形式。正则闭包还可以用于实现词法分析中的词法规则，如识别标识符、常量等。

正则闭包的原理是通过使用特殊的符号和操作来表示字符重复出现的模式。通常，正则表达式中的闭包操作符表示将一个或多个字符重复任意次数。例如，正则表达式a-z+表示匹配一个或多个小写字母。

编译程序可以使用正则闭包来构建有限自动机或正则表达式匹配器，用于识别和处理源代码中的模式。这些模式可以用于语法分析、语义分析和代码生成等编译过程中的不同阶段。

![img](/assets/87707edf4dc36e08e25049b68e99efb7.DoWYRmR4.png)

![img](/assets/dac69763da2633de0a0f15b3c571f261.Cs4Wn24l.png)

#### :seedling:文法类型

![img](/assets/720f6798b22682a03bd8ede99d4e775c.C9bNaCiy.png)

**程序设计语言中语法是上下文无关的，语义上下文有关**

### :palm\_tree:2.3 语法分析

#### :seedling:正规式

![img](/assets/809e3b6d149a5095fd7fc23b07a019ad.DmAb7wk1.png)

![img](/assets/eb2aeed59217fdf7741141d633cfe4ee.CKIIrnfk.png)

![img](/assets/8556191a97a2ddc885b81568d8fe15b0.Cy933RJk.png)

#### :seedling:有限自动机

有限自动机是一种计算模型，它可以接受一些输入，并根据预定的规则转移到不同的状态。

有限自动机可以分为确定性有限自动机（DFA）和非确定性有限自动机（NFA）两种。

* DFA是一种有限自动机，其在给定一个输入字符后，可以唯一确定其下一个状态。
* NFA是一种有限自动机，其在给定一个输入字符后，可能有多个下一个状态。

有限自动机可以根据输入字符的情况来判断其是确定的还是不确定的。若根据输入字符能得出唯一的后继状态，则是确定的；若根据输入字符能得出多个后继状态，则是不确定的。

![在这里插入图片描述](/assets/04fddafbd80e4dbf83af885c3cf093ed.DtxjONfI.png)

![img](/assets/c797c265e5866611bc0ad5f6c16f92af.Bvi6saof.png)

![img](/assets/aa227cc7a7d564a7491874c30b599eaa.DkOgDvGe.png)

#### :seedling:语法分析方法（考的少）

自上而下语法分析：最左推导，从左至右。给定文法G和源程序串r。从G的开始符号S出发，通过反复使用产生式对句型中的非终结符进行替换（推导），逐步推导出r。

* 递归下降思想：原理是利用函数之间的递归调用模拟语法树自上而下的构造过程，是一种自上而下的语法分析方法。

自下而上语法分析：最右推导，从右至左。从给定的输入串r开始，不断寻找子串与文法G中某个产生式P的候选式进行匹配，并用P的左部代替（归约）之，逐步归约到开始符号s。

* 移进-规约思想：设置一个栈，将输入符号逐个移进栈中，栈顶形成某产生式的右部时，就用左部去代替，称为归约。很明显，这个思想是通过右部来推导出左部，因此是自下而上语法分析的核心思想。

```
例（2016年上半年）：●移进-归约分析法是编译程序（或解释程序）对高级语言源程序进行语法分析的一种方法，属于（48）的语法分析方法。
A.自顶向下（或自上而下） B.自底向上（或自下而上） C.自左向右 D.自右向左
答案：B
```

参考资料

https://blog.csdn.net/chengsw1993/article/details/125043157

https://cloud.tencent.com/developer/article/2383590

---

---
url: /Java/架构设计/分布式/02.分布式搜索/1_搜索引擎Elasticsearch01.md
---

# 初识ElasticSearch

## 一、elasticsearch概述

### 1.1.了解ES

#### 1.1.1.elasticsearch的作用

elasticsearch是一款非常强大的开源搜索引擎，具备非常多强大功能，可以帮助我们从海量数据中快速找到需要的内容

例如：

* 在GitHub搜索代码

  ![image-20210720193623245](/assets/image-20210720193623245.CFYMTVgO.png)

* 在电商网站搜索商品

  ![image-20210720193633483](/assets/image-20210720193633483.BZ6DN1Gv.png)

* 在百度搜索答案

  ![image-20210720193641907](/assets/image-20210720193641907.BHDpDia6.png)

* 在打车软件搜索附近的车

  ![image-20210720193648044](/assets/image-20210720193648044.Bg3TZhwf.png)

#### 1.1.2.ELK技术栈

elasticsearch结合kibana、Logstash、Beats，也就是elastic stack（ELK）。被广泛应用在日志数据分析、实时监控等领域：

![image-20210720194008781](/assets/image-20210720194008781.B0YJ66Y2.png)

而elasticsearch是elastic stack的核心，负责存储、搜索、分析数据。

![image-20210720194230265](/assets/image-20210720194230265.BTRA6FQK.png)

#### 1.1.3.elasticsearch和lucene

elasticsearch底层是基于**lucene**来实现的。

**Lucene**是一个Java语言的搜索引擎类库，是Apache公司的顶级项目，由DougCutting于1999年研发。官网地址：https://lucene.apache.org/ 。

![image-20210720194547780](/assets/image-20210720194547780.DcxkF8Mu.png)

**elasticsearch**的发展历史：

* 2004年Shay Banon基于Lucene开发了Compass
* 2010年Shay Banon 重写了Compass，取名为Elasticsearch。

![image-20210720195001221](/assets/image-20210720195001221.wVbIzn8v.png)

#### 1.1.4.为什么不是其他搜索技术？

目前比较知名的搜索引擎技术排名：

![image-20210720195142535](/assets/image-20210720195142535.BxEQuWNY.png)

虽然在早期，Apache Solr是最主要的搜索引擎技术，但随着发展elasticsearch已经渐渐超越了Solr，独占鳌头：

![image-20210720195306484](/assets/image-20210720195306484.CfupRkEM.png)

#### 1.1.5.总结

什么是elasticsearch？

* 一个开源的分布式搜索引擎，可以用来实现搜索、日志统计、分析、系统监控等功能

什么是elastic stack（ELK）？

* 是以elasticsearch为核心的技术栈，包括beats、Logstash、kibana、elasticsearch

什么是Lucene？

* 是Apache的开源搜索引擎类库，提供了搜索引擎的核心API

### 1.2.倒排索引

倒排索引的概念是基于MySQL这样的正向索引而言的。

#### 1.2.1.正向索引

那么什么是正向索引呢？例如给下表（tb\_goods）中的id创建索引：

![image-20210720195531539](/assets/image-20210720195531539.BWte_zj3.png)

如果是根据id查询，那么直接走索引，查询速度非常快。

但如果是基于title做模糊查询，只能是逐行扫描数据，流程如下：

1）用户搜索数据，条件是title符合`"%手机%"`

2）逐行获取数据，比如id为1的数据

3）判断数据中的title是否符合用户搜索条件

4）如果符合则放入结果集，不符合则丢弃。回到步骤1

逐行扫描，也就是全表扫描，随着数据量增加，其查询效率也会越来越低。当数据量达到数百万时，就是一场灾难。

#### 1.2.2.倒排索引

倒排索引中有两个非常重要的概念：

* 文档（`Document`）：用来搜索的数据，其中的每一条数据就是一个文档。例如一个网页、一个商品信息
* 词条（`Term`）：对文档数据或用户搜索数据，利用某种算法分词，得到的具备含义的词语就是词条。例如：我是中国人，就可以分为：我、是、中国人、中国、国人这样的几个词条

**创建倒排索引**是对正向索引的一种特殊处理，流程如下：

* 将每一个文档的数据利用算法分词，得到一个个词条
* 创建表，每行数据包括词条、词条所在文档id、位置等信息
* 因为词条唯一性，可以给词条创建索引，例如hash表结构索引

如图：

![image-20210720200457207](/assets/image-20210720200457207.ah6sMLft.png)

倒排索引的**搜索流程**如下（以搜索"华为手机"为例）：

1）用户输入条件`"华为手机"`进行搜索。

2）对用户输入内容**分词**，得到词条：`华为`、`手机`。

3）拿着词条在倒排索引中查找，可以得到包含词条的文档id：1、2、3。

4）拿着文档id到正向索引中查找具体文档。

如图：

![image-20210720201115192](/assets/image-20210720201115192.DrAp0t_p.png)

虽然要先查询倒排索引，再查询倒排索引，但是无论是词条、还是文档id都建立了索引，查询速度非常快！无需全表扫描。

#### 1.2.3.正向和倒排

那么为什么一个叫做正向索引，一个叫做倒排索引呢？

* **正向索引**是最传统的，根据id索引的方式。但根据词条查询时，必须先逐条获取每个文档，然后判断文档中是否包含所需要的词条，是**根据文档找词条的过程**。

* 而**倒排索引**则相反，是先找到用户要搜索的词条，根据词条得到保护词条的文档的id，然后根据id获取文档。是**根据词条找文档的过程**。

是不是恰好反过来了？

那么两者方式的优缺点是什么呢？

**正向索引**：

* 优点：
  * 可以给多个字段创建索引
  * 根据索引字段搜索、排序速度非常快
* 缺点：
  * 根据非索引字段，或者索引字段中的部分词条查找时，只能全表扫描。

**倒排索引**：

* 优点：
  * 根据词条搜索、模糊搜索时，速度非常快
* 缺点：
  * 只能给词条创建索引，而不是字段
  * 无法根据字段做排序

### 1.3.es的一些概念

elasticsearch中有很多独有的概念，与mysql中略有差别，但也有相似之处。

#### 1.3.1.文档和字段

elasticsearch是面向\*\*文档（Document）\*\*存储的，可以是数据库中的一条商品数据，一个订单信息。文档数据会被序列化为json格式后存储在elasticsearch中：

![image-20210720202707797](/assets/image-20210720202707797.-MElXOWW.png)

而Json文档中往往包含很多的**字段（Field）**，类似于数据库中的列。

#### 1.3.2.索引和映射

**索引（Index）**，就是相同类型的文档的集合。

例如：

* 所有用户文档，就可以组织在一起，称为用户的索引；
* 所有商品的文档，可以组织在一起，称为商品的索引；
* 所有订单的文档，可以组织在一起，称为订单的索引；

![image-20210720203022172](/assets/image-20210720203022172.DcLkDIUn.png)

因此，我们可以把索引当做是数据库中的表。

数据库的表会有约束信息，用来定义表的结构、字段的名称、类型等信息。因此，索引库中就有**映射（mapping）**，是索引中文档的字段约束信息，类似表的结构约束。

#### 1.3.3.mysql与elasticsearch

我们统一的把mysql与elasticsearch的概念做一下对比：

| **MySQL** | **Elasticsearch** | **说明**                                                     |
| --------- | ----------------- | ------------------------------------------------------------ |
| Table     | Index             | 索引(index)，就是文档的集合，类似数据库的表(table)           |
| Row       | Document          | 文档（Document），就是一条条的数据，类似数据库中的行（Row），文档都是JSON格式 |
| Column    | Field             | 字段（Field），就是JSON文档中的字段，类似数据库中的列（Column） |
| Schema    | Mapping           | Mapping（映射）是索引中文档的约束，例如字段类型约束。类似数据库的表结构（Schema） |
| SQL       | DSL               | DSL是elasticsearch提供的JSON风格的请求语句，用来操作elasticsearch，实现CRUD |

是不是说，我们学习了elasticsearch就不再需要mysql了呢？

并不是如此，两者各自有自己的擅长支出：

* Mysql：擅长事务类型操作，可以确保数据的安全和一致性

* Elasticsearch：擅长海量数据的搜索、分析、计算

因此在企业中，往往是两者结合使用：

* 对安全性要求较高的写操作，使用mysql实现
* 对查询性能要求较高的搜索需求，使用elasticsearch实现
* 两者再基于某种方式，实现数据的同步，保证一致性

![image-20210720203534945](/assets/image-20210720203534945.DeNWJkhT.png)

### 1.4.分词器

分词器的作用是什么？

* 创建倒排索引时对文档分词
* 用户搜索时，对输入的内容分词

IK分词器有几种模式？

* ik\_smart：智能切分，粗粒度
* ik\_max\_word：最细切分，细粒度

IK分词器如何拓展词条？如何停用词条？

* 利用config目录的IkAnalyzer.cfg.xml文件添加拓展词典和停用词典
* 在词典中添加拓展词条或者停用词条

## 二、索引库操作

索引库就类似数据库表，mapping映射就类似表的结构。

我们要向es中存储数据，必须先创建“库”和“表”。

### 2.1.mapping映射属性

mapping是对索引库中文档的约束，常见的mapping属性包括：

* type：字段数据类型，常见的简单类型有：
  * 字符串：text（可分词的文本）、keyword（精确值，例如：品牌、国家、ip地址）
  * 数值：long、integer、short、byte、double、float、
  * 布尔：boolean
  * 日期：date
  * 对象：object
* index：是否创建索引，默认为true
* analyzer：使用哪种分词器
* properties：该字段的子字段

例如下面的json文档：

```json
{
    "age": 21,
    "weight": 52.1,
    "isMarried": false,
    "info": "我是小小黑",
    "email": "xxl@126.com",
    "score": [99.1, 99.5, 98.9],
    "name": {
        "firstName": "云",
        "lastName": "赵"
    }
}
```

对应的每个字段映射（mapping）：

* age：类型为 integer；参与搜索，因此需要index为true；无需分词器
* weight：类型为float；参与搜索，因此需要index为true；无需分词器
* isMarried：类型为boolean；参与搜索，因此需要index为true；无需分词器
* info：类型为字符串，需要分词，因此是text；参与搜索，因此需要index为true；分词器可以用ik\_smart
* email：类型为字符串，但是不需要分词，因此是keyword；不参与搜索，因此需要index为false；无需分词器
* score：虽然是数组，但是我们只看元素的类型，类型为float；参与搜索，因此需要index为true；无需分词器
* name：类型为object，需要定义多个子属性
  * name.firstName；类型为字符串，但是不需要分词，因此是keyword；参与搜索，因此需要index为true；无需分词器
  * name.lastName；类型为字符串，但是不需要分词，因此是keyword；参与搜索，因此需要index为true；无需分词器

### 2.2.索引库的CRUD

这里我们统一使用Kibana编写DSL的方式来演示。

#### 2.2.1.创建索引库和映射

##### 基本语法：

* 请求方式：PUT
* 请求路径：/索引库名，可以自定义
* 请求参数：mapping映射

格式：

```json
PUT /索引库名称
{
  "mappings": {
    "properties": {
      "字段名":{
        "type": "text",
        "analyzer": "ik_smart"
      },
      "字段名2":{
        "type": "keyword",
        "index": "false"
      },
      "字段名3":{
        "properties": {
          "子字段": {
            "type": "keyword"
          }
        }
      },
      // ...略
    }
  }
}
```

##### 示例：

```sh
PUT /xxl
{
  "mappings": {
    "properties": {
      "info":{
        "type": "text",
        "analyzer": "ik_smart"
      },
      "email":{
        "type": "keyword",
        "index": "falsae"
      },
      "name":{
        "properties": {
          "firstName": {
            "type": "keyword"
          }
        }
      },
      // ... 略
    }
  }
}
```

#### 2.2.2.查询索引库

**基本语法**：

* 请求方式：GET

* 请求路径：/索引库名

* 请求参数：无

**格式**：

```
GET /索引库名
```

**示例**：

![image-20210720211019329](/assets/image-20210720211019329.DeUYGGMJ.png)

#### 2.2.3.修改索引库

倒排索引结构虽然不复杂，但是一旦数据结构改变（比如改变了分词器），就需要重新创建倒排索引，这简直是灾难。因此索引库**一旦创建，无法修改mapping**。

虽然无法修改mapping中已有的字段，但是却允许添加新的字段到mapping中，因为不会对倒排索引产生影响。

**语法说明**：

```json
PUT /索引库名/_mapping
{
  "properties": {
    "新字段名":{
      "type": "integer"
    }
  }
}
```

**示例**：

![image-20210720212357390](/assets/image-20210720212357390.DJzoEpYg.png)

#### 2.2.4.删除索引库

**语法：**

* 请求方式：DELETE

* 请求路径：/索引库名

* 请求参数：无

**格式：**

```
DELETE /索引库名
```

在kibana中测试：

![image-20210720212123420](/assets/image-20210720212123420.BiSobFIg.png)

#### 2.2.5.总结

索引库操作有哪些？

* 创建索引库：PUT /索引库名
* 查询索引库：GET /索引库名
* 删除索引库：DELETE /索引库名
* 添加字段：PUT /索引库名/\_mapping

## 三、文档操作

### 3.1.新增文档

**语法：**

```json
POST /索引库名/_doc/文档id
{
    "字段1": "值1",
    "字段2": "值2",
    "字段3": {
        "子属性1": "值3",
        "子属性2": "值4"
    },
    // ...
}
```

**示例：**

```json
POST /xxl/_doc/1
{
    "info": "黑马程序员Java讲师",
    "email": "zy@itcast.cn",
    "name": {
        "firstName": "云",
        "lastName": "赵"
    }
}
```

**响应：**

![image-20210720212933362](/assets/image-20210720212933362.CqA7nThZ.png)

### 3.2.查询文档

根据rest风格，新增是post，查询应该是get，不过查询一般都需要条件，这里我们把文档id带上。

**语法：**

```json
GET /{索引库名称}/_doc/{id}
```

**通过kibana查看数据：**

```js
GET /xxl/_doc/1
```

**查看结果：**

![image-20210720213345003](/assets/image-20210720213345003.AyspffQg.png)

### 3.3.删除文档

删除使用DELETE请求，同样，需要根据id进行删除：

**语法：**

```js
DELETE /{索引库名}/_doc/id值
```

**示例：**

```json
# 根据id删除数据
DELETE /xxl/_doc/1
```

**结果：**

![image-20210720213634918](/assets/image-20210720213634918.qajwhAY6.png)

### 3.4.修改文档

修改有两种方式：

* 全量修改：直接覆盖原来的文档
* 增量修改：修改文档中的部分字段

#### 3.4.1.全量修改

全量修改是覆盖原来的文档，其本质是：

* 根据指定的id删除文档
* 新增一个相同id的文档

**注意**：如果根据id删除时，id不存在，第二步的新增也会执行，也就从修改变成了新增操作了。

**语法：**

```json
PUT /{索引库名}/_doc/文档id
{
    "字段1": "值1",
    "字段2": "值2",
    // ... 略
}

```

**示例：**

```json
PUT /xxl/_doc/1
{
    "info": "黑马程序员高级Java讲师",
    "email": "zy@itcast.cn",
    "name": {
        "firstName": "云",
        "lastName": "赵"
    }
}
```

#### 3.4.2.增量修改

增量修改是只修改指定id匹配的文档中的部分字段。

**语法：**

```json
POST /{索引库名}/_update/文档id
{
    "doc": {
         "字段名": "新的值",
    }
}
```

**示例：**

```json
POST /xxl/_update/1
{
  "doc": {
    "email": "ZhaoYun@itcast.cn"
  }
}
```

### 3.5.总结

文档操作有哪些？

* 创建文档：POST /{索引库名}/\_doc/文档id   { json文档 }
* 查询文档：GET /{索引库名}/\_doc/文档id
* 删除文档：DELETE /{索引库名}/\_doc/文档id
* 修改文档：
  * 全量修改：PUT /{索引库名}/\_doc/文档id { json文档 }
  * 增量修改：POST /{索引库名}/\_update/文档id { "doc": {字段}}

## 四、RestAPI

ES官方提供了各种不同语言的客户端，用来操作ES。这些客户端的本质就是组装DSL语句，通过http请求发送给ES。官方文档地址：https://www.elastic.co/guide/en/elasticsearch/client/index.html

其中的Java Rest Client又包括两种：

* Java Low Level Rest Client
* Java High Level Rest Client

![image-20210720214555863](/assets/image-20210720214555863.BNlp5F5h.png)

我们学习的是Java HighLevel Rest Client客户端API

### 4.0.导入Demo工程

#### 4.0.1.导入数据

首先导入课前资料提供的数据库数据：

![image-20210720220400297](/assets/image-20210720220400297.BguLKy5T.png)

数据结构如下：

```sql
CREATE TABLE `tb_hotel` (
  `id` bigint(20) NOT NULL COMMENT '酒店id',
  `name` varchar(255) NOT NULL COMMENT '酒店名称；例：7天酒店',
  `address` varchar(255) NOT NULL COMMENT '酒店地址；例：航头路',
  `price` int(10) NOT NULL COMMENT '酒店价格；例：329',
  `score` int(2) NOT NULL COMMENT '酒店评分；例：45，就是4.5分',
  `brand` varchar(32) NOT NULL COMMENT '酒店品牌；例：如家',
  `city` varchar(32) NOT NULL COMMENT '所在城市；例：上海',
  `star_name` varchar(16) DEFAULT NULL COMMENT '酒店星级，从低到高分别是：1星到5星，1钻到5钻',
  `business` varchar(255) DEFAULT NULL COMMENT '商圈；例：虹桥',
  `latitude` varchar(32) NOT NULL COMMENT '纬度；例：31.2497',
  `longitude` varchar(32) NOT NULL COMMENT '经度；例：120.3925',
  `pic` varchar(255) DEFAULT NULL COMMENT '酒店图片；例:/img/1.jpg',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
```

#### 4.0.2.导入项目

然后导入课前资料提供的项目:

![image-20210720220503411](/assets/image-20210720220503411.CJJD5-sq.png)

项目结构如图：

![image-20210720220647541](/assets/image-20210720220647541.D3CVNh1e.png)

#### 4.0.3.mapping映射分析

创建索引库，最关键的是mapping映射，而mapping映射要考虑的信息包括：

* 字段名
* 字段数据类型
* 是否参与搜索
* 是否需要分词
* 如果分词，分词器是什么？

其中：

* 字段名、字段数据类型，可以参考数据表结构的名称和类型
* 是否参与搜索要分析业务来判断，例如图片地址，就无需参与搜索
* 是否分词呢要看内容，内容如果是一个整体就无需分词，反之则要分词
* 分词器，我们可以统一使用ik\_max\_word

来看下酒店数据的索引库结构:

```json
PUT /hotel
{
  "mappings": {
    "properties": {
      "id": {
        "type": "keyword"
      },
      "name":{
        "type": "text",
        "analyzer": "ik_max_word",
        "copy_to": "all"
      },
      "address":{
        "type": "keyword",
        "index": false
      },
      "price":{
        "type": "integer"
      },
      "score":{
        "type": "integer"
      },
      "brand":{
        "type": "keyword",
        "copy_to": "all"
      },
      "city":{
        "type": "keyword",
        "copy_to": "all"
      },
      "starName":{
        "type": "keyword"
      },
      "business":{
        "type": "keyword"
      },
      "location":{
        "type": "geo_point"
      },
      "pic":{
        "type": "keyword",
        "index": false
      },
      "all":{
        "type": "text",
        "analyzer": "ik_max_word"
      }
    }
  }
}
```

几个特殊字段说明：

* location：地理坐标，里面包含精度、纬度
* all：一个组合字段，其目的是将多字段的值 利用copy\_to合并，提供给用户搜索

地理坐标说明：

![image-20210720222110126](/assets/image-20210720222110126.BhTiY0Vr.png)

copy\_to说明：

![image-20210720222221516](/assets/image-20210720222221516.cV5QckmP.png)

#### 4.0.4.初始化RestClient

在elasticsearch提供的API中，与elasticsearch一切交互都封装在一个名为RestHighLevelClient的类中，必须先完成这个对象的初始化，建立与elasticsearch的连接。

分为三步：

1）引入es的RestHighLevelClient依赖：

```xml
<dependency>
    <groupId>org.elasticsearch.client</groupId>
    <artifactId>elasticsearch-rest-high-level-client</artifactId>
</dependency>
```

2）因为SpringBoot默认的ES版本是7.6.2，所以我们需要覆盖默认的ES版本：

```xml
<properties>
    <java.version>1.8</java.version>
    <elasticsearch.version>7.12.1</elasticsearch.version>
</properties>
```

3）初始化RestHighLevelClient：

初始化的代码如下：

```java
RestHighLevelClient client = new RestHighLevelClient(RestClient.builder(
        HttpHost.create("http://192.168.150.101:9200")
));
```

这里为了单元测试方便，我们创建一个测试类HotelIndexTest，然后将初始化的代码编写在@BeforeEach方法中：

```java
package cn.itcast.hotel;

import org.apache.http.HttpHost;
import org.elasticsearch.client.RestHighLevelClient;
import org.junit.jupiter.api.AfterEach;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;

import java.io.IOException;

public class HotelIndexTest {
    private RestHighLevelClient client;

    @BeforeEach
    void setUp() {
        this.client = new RestHighLevelClient(RestClient.builder(
                HttpHost.create("http://192.168.150.101:9200")
        ));
    }

    @AfterEach
    void tearDown() throws IOException {
        this.client.close();
    }
}
```

### 4.1.创建索引库

#### 4.1.1.代码解读

创建索引库的API如下：

![image-20210720223049408](/assets/image-20210720223049408.BTa2gM4q.png)

代码分为三步：

* 1）创建Request对象。因为是创建索引库的操作，因此Request是CreateIndexRequest。
* 2）添加请求参数，其实就是DSL的JSON参数部分。因为json字符串很长，这里是定义了静态字符串常量MAPPING\_TEMPLATE，让代码看起来更加优雅。
* 3）发送请求，client.indices()方法的返回值是IndicesClient类型，封装了所有与索引库操作有关的方法。

#### 4.1.2.完整示例

在hotel-demo的cn.itcast.hotel.constants包下，创建一个类，定义mapping映射的JSON字符串常量：

```java
package cn.itcast.hotel.constants;

public class HotelConstants {
    public static final String MAPPING_TEMPLATE = "{\n" +
            "  \"mappings\": {\n" +
            "    \"properties\": {\n" +
            "      \"id\": {\n" +
            "        \"type\": \"keyword\"\n" +
            "      },\n" +
            "      \"name\":{\n" +
            "        \"type\": \"text\",\n" +
            "        \"analyzer\": \"ik_max_word\",\n" +
            "        \"copy_to\": \"all\"\n" +
            "      },\n" +
            "      \"address\":{\n" +
            "        \"type\": \"keyword\",\n" +
            "        \"index\": false\n" +
            "      },\n" +
            "      \"price\":{\n" +
            "        \"type\": \"integer\"\n" +
            "      },\n" +
            "      \"score\":{\n" +
            "        \"type\": \"integer\"\n" +
            "      },\n" +
            "      \"brand\":{\n" +
            "        \"type\": \"keyword\",\n" +
            "        \"copy_to\": \"all\"\n" +
            "      },\n" +
            "      \"city\":{\n" +
            "        \"type\": \"keyword\",\n" +
            "        \"copy_to\": \"all\"\n" +
            "      },\n" +
            "      \"starName\":{\n" +
            "        \"type\": \"keyword\"\n" +
            "      },\n" +
            "      \"business\":{\n" +
            "        \"type\": \"keyword\"\n" +
            "      },\n" +
            "      \"location\":{\n" +
            "        \"type\": \"geo_point\"\n" +
            "      },\n" +
            "      \"pic\":{\n" +
            "        \"type\": \"keyword\",\n" +
            "        \"index\": false\n" +
            "      },\n" +
            "      \"all\":{\n" +
            "        \"type\": \"text\",\n" +
            "        \"analyzer\": \"ik_max_word\"\n" +
            "      }\n" +
            "    }\n" +
            "  }\n" +
            "}";
}
```

在hotel-demo中的HotelIndexTest测试类中，编写单元测试，实现创建索引：

```java
@Test
void createHotelIndex() throws IOException {
    // 1.创建Request对象
    CreateIndexRequest request = new CreateIndexRequest("hotel");
    // 2.准备请求的参数：DSL语句
    request.source(MAPPING_TEMPLATE, XContentType.JSON);
    // 3.发送请求
    client.indices().create(request, RequestOptions.DEFAULT);
}
```

### 4.2.删除索引库

删除索引库的DSL语句非常简单：

```json
DELETE /hotel
```

与创建索引库相比：

* 请求方式从PUT变为DELTE
* 请求路径不变
* 无请求参数

所以代码的差异，注意体现在Request对象上。依然是三步走：

* 1）创建Request对象。这次是DeleteIndexRequest对象
* 2）准备参数。这里是无参
* 3）发送请求。改用delete方法

在hotel-demo中的HotelIndexTest测试类中，编写单元测试，实现删除索引：

```java
@Test
void testDeleteHotelIndex() throws IOException {
    // 1.创建Request对象
    DeleteIndexRequest request = new DeleteIndexRequest("hotel");
    // 2.发送请求
    client.indices().delete(request, RequestOptions.DEFAULT);
}
```

### 4.3.判断索引库是否存在

判断索引库是否存在，本质就是查询，对应的DSL是：

```json
GET /hotel
```

因此与删除的Java代码流程是类似的。依然是三步走：

* 1）创建Request对象。这次是GetIndexRequest对象
* 2）准备参数。这里是无参
* 3）发送请求。改用exists方法

```java
@Test
void testExistsHotelIndex() throws IOException {
    // 1.创建Request对象
    GetIndexRequest request = new GetIndexRequest("hotel");
    // 2.发送请求
    boolean exists = client.indices().exists(request, RequestOptions.DEFAULT);
    // 3.输出
    System.err.println(exists ? "索引库已经存在！" : "索引库不存在！");
}
```

### 4.4.总结

JavaRestClient操作elasticsearch的流程基本类似。核心是client.indices()方法来获取索引库的操作对象。

索引库操作的基本步骤：

* 初始化RestHighLevelClient
* 创建XxxIndexRequest。XXX是Create、Get、Delete
* 准备DSL（ Create时需要，其它是无参）
* 发送请求。调用RestHighLevelClient#indices().xxx()方法，xxx是create、exists、delete

## 五、RestClient操作文档

为了与索引库操作分离，我们再次参加一个测试类，做两件事情：

* 初始化RestHighLevelClient
* 我们的酒店数据在数据库，需要利用IHotelService去查询，所以注入这个接口

```java
package cn.itcast.hotel;

import cn.itcast.hotel.pojo.Hotel;
import cn.itcast.hotel.service.IHotelService;
import org.junit.jupiter.api.AfterEach;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.test.context.SpringBootTest;

import java.io.IOException;
import java.util.List;

@SpringBootTest
public class HotelDocumentTest {
    @Autowired
    private IHotelService hotelService;

    private RestHighLevelClient client;

    @BeforeEach
    void setUp() {
        this.client = new RestHighLevelClient(RestClient.builder(
                HttpHost.create("http://192.168.150.101:9200")
        ));
    }

    @AfterEach
    void tearDown() throws IOException {
        this.client.close();
    }
}

```

### 5.1.新增文档

我们要将数据库的酒店数据查询出来，写入elasticsearch中。

#### 5.1.1.索引库实体类

数据库查询后的结果是一个Hotel类型的对象。结构如下：

```java
@Data
@TableName("tb_hotel")
public class Hotel {
    @TableId(type = IdType.INPUT)
    private Long id;
    private String name;
    private String address;
    private Integer price;
    private Integer score;
    private String brand;
    private String city;
    private String starName;
    private String business;
    private String longitude;
    private String latitude;
    private String pic;
}
```

与我们的索引库结构存在差异：

* longitude和latitude需要合并为location

因此，我们需要定义一个新的类型，与索引库结构吻合：

```java
package cn.itcast.hotel.pojo;

import lombok.Data;
import lombok.NoArgsConstructor;

@Data
@NoArgsConstructor
public class HotelDoc {
    private Long id;
    private String name;
    private String address;
    private Integer price;
    private Integer score;
    private String brand;
    private String city;
    private String starName;
    private String business;
    private String location;
    private String pic;

    public HotelDoc(Hotel hotel) {
        this.id = hotel.getId();
        this.name = hotel.getName();
        this.address = hotel.getAddress();
        this.price = hotel.getPrice();
        this.score = hotel.getScore();
        this.brand = hotel.getBrand();
        this.city = hotel.getCity();
        this.starName = hotel.getStarName();
        this.business = hotel.getBusiness();
        this.location = hotel.getLatitude() + ", " + hotel.getLongitude();
        this.pic = hotel.getPic();
    }
}

```

#### 5.1.2.语法说明

新增文档的DSL语句如下：

```json
POST /{索引库名}/_doc/1
{
    "name": "Jack",
    "age": 21
}
```

对应的java代码如图：

![image-20210720230027240](/assets/image-20210720230027240.CVH6Magt.png)

可以看到与创建索引库类似，同样是三步走：

* 1）创建Request对象
* 2）准备请求参数，也就是DSL中的JSON文档
* 3）发送请求

变化的地方在于，这里直接使用client.xxx()的API，不再需要client.indices()了。

#### 5.1.3.完整代码

我们导入酒店数据，基本流程一致，但是需要考虑几点变化：

* 酒店数据来自于数据库，我们需要先查询出来，得到hotel对象
* hotel对象需要转为HotelDoc对象
* HotelDoc需要序列化为json格式

因此，代码整体步骤如下：

* 1）根据id查询酒店数据Hotel
* 2）将Hotel封装为HotelDoc
* 3）将HotelDoc序列化为JSON
* 4）创建IndexRequest，指定索引库名和id
* 5）准备请求参数，也就是JSON文档
* 6）发送请求

在hotel-demo的HotelDocumentTest测试类中，编写单元测试：

```java
@Test
void testAddDocument() throws IOException {
    // 1.根据id查询酒店数据
    Hotel hotel = hotelService.getById(61083L);
    // 2.转换为文档类型
    HotelDoc hotelDoc = new HotelDoc(hotel);
    // 3.将HotelDoc转json
    String json = JSON.toJSONString(hotelDoc);

    // 1.准备Request对象
    IndexRequest request = new IndexRequest("hotel").id(hotelDoc.getId().toString());
    // 2.准备Json文档
    request.source(json, XContentType.JSON);
    // 3.发送请求
    client.index(request, RequestOptions.DEFAULT);
}
```

### 5.2.查询文档

#### 5.2.1.语法说明

查询的DSL语句如下：

```json
GET /hotel/_doc/{id}
```

非常简单，因此代码大概分两步：

* 准备Request对象
* 发送请求

不过查询的目的是得到结果，解析为HotelDoc，因此难点是结果的解析。完整代码如下：

![image-20210720230811674](/assets/image-20210720230811674.QZ3o-zG0.png)

可以看到，结果是一个JSON，其中文档放在一个`_source`属性中，因此解析就是拿到`_source`，反序列化为Java对象即可。

与之前类似，也是三步走：

* 1）准备Request对象。这次是查询，所以是GetRequest
* 2）发送请求，得到结果。因为是查询，这里调用client.get()方法
* 3）解析结果，就是对JSON做反序列化

#### 5.2.2.完整代码

在hotel-demo的HotelDocumentTest测试类中，编写单元测试：

```java
@Test
void testGetDocumentById() throws IOException {
    // 1.准备Request
    GetRequest request = new GetRequest("hotel", "61082");
    // 2.发送请求，得到响应
    GetResponse response = client.get(request, RequestOptions.DEFAULT);
    // 3.解析响应结果
    String json = response.getSourceAsString();

    HotelDoc hotelDoc = JSON.parseObject(json, HotelDoc.class);
    System.out.println(hotelDoc);
}
```

### 5.3.删除文档

删除的DSL为是这样的：

```json
DELETE /hotel/_doc/{id}
```

与查询相比，仅仅是请求方式从DELETE变成GET，可以想象Java代码应该依然是三步走：

* 1）准备Request对象，因为是删除，这次是DeleteRequest对象。要指定索引库名和id
* 2）准备参数，无参
* 3）发送请求。因为是删除，所以是client.delete()方法

在hotel-demo的HotelDocumentTest测试类中，编写单元测试：

```java
@Test
void testDeleteDocument() throws IOException {
    // 1.准备Request
    DeleteRequest request = new DeleteRequest("hotel", "61083");
    // 2.发送请求
    client.delete(request, RequestOptions.DEFAULT);
}
```

### 5.4.修改文档

#### 5.4.1.语法说明

修改我们讲过两种方式：

* 全量修改：本质是先根据id删除，再新增
* 增量修改：修改文档中的指定字段值

在RestClient的API中，全量修改与新增的API完全一致，判断依据是ID：

* 如果新增时，ID已经存在，则修改
* 如果新增时，ID不存在，则新增

这里不再赘述，我们主要关注增量修改。

代码示例如图：

![image-20210720231040875](/assets/image-20210720231040875.C9QXEs-I.png)

与之前类似，也是三步走：

* 1）准备Request对象。这次是修改，所以是UpdateRequest
* 2）准备参数。也就是JSON文档，里面包含要修改的字段
* 3）更新文档。这里调用client.update()方法

#### 5.4.2.完整代码

在hotel-demo的HotelDocumentTest测试类中，编写单元测试：

```java
@Test
void testUpdateDocument() throws IOException {
    // 1.准备Request
    UpdateRequest request = new UpdateRequest("hotel", "61083");
    // 2.准备请求参数
    request.doc(
        "price", "952",
        "starName", "四钻"
    );
    // 3.发送请求
    client.update(request, RequestOptions.DEFAULT);
}
```

### 5.5.批量导入文档

案例需求：利用BulkRequest批量将数据库数据导入到索引库中。

步骤如下：

* 利用mybatis-plus查询酒店数据

* 将查询到的酒店数据（Hotel）转换为文档类型数据（HotelDoc）

* 利用JavaRestClient中的BulkRequest批处理，实现批量新增文档

#### 5.5.1.语法说明

批量处理BulkRequest，其本质就是将多个普通的CRUD请求组合在一起发送。

其中提供了一个add方法，用来添加其他请求：

![image-20210720232105943](/assets/image-20210720232105943.Dt9gw85O.png)

可以看到，能添加的请求包括：

* IndexRequest，也就是新增
* UpdateRequest，也就是修改
* DeleteRequest，也就是删除

因此Bulk中添加了多个IndexRequest，就是批量新增功能了。示例：

![image-20210720232431383](/assets/image-20210720232431383.DakfYLMb.png)

其实还是三步走：

* 1）创建Request对象。这里是BulkRequest
* 2）准备参数。批处理的参数，就是其它Request对象，这里就是多个IndexRequest
* 3）发起请求。这里是批处理，调用的方法为client.bulk()方法

我们在导入酒店数据时，将上述代码改造成for循环处理即可。

#### 5.5.2.完整代码

在hotel-demo的HotelDocumentTest测试类中，编写单元测试：

```java
@Test
void testBulkRequest() throws IOException {
    // 批量查询酒店数据
    List<Hotel> hotels = hotelService.list();

    // 1.创建Request
    BulkRequest request = new BulkRequest();
    // 2.准备参数，添加多个新增的Request
    for (Hotel hotel : hotels) {
        // 2.1.转换为文档类型HotelDoc
        HotelDoc hotelDoc = new HotelDoc(hotel);
        // 2.2.创建新增文档的Request对象
        request.add(new IndexRequest("hotel")
                    .id(hotelDoc.getId().toString())
                    .source(JSON.toJSONString(hotelDoc), XContentType.JSON));
    }
    // 3.发送请求
    client.bulk(request, RequestOptions.DEFAULT);
}
```

## 5.6.小结

文档操作的基本步骤：

* 初始化RestHighLevelClient
* 创建XxxRequest。XXX是Index、Get、Update、Delete、Bulk
* 准备参数（Index、Update、Bulk时需要）
* 发送请求。调用RestHighLevelClient#.xxx()方法，xxx是index、get、update、delete、bulk
* 解析结果（Get时需要）

---

---
url: /常用框架/SpringBoot/SpringBoot源码分析/初始化加载配置.md
---
# 初始化加载配置

https://blog.csdn.net/banyejiu/article/details/121670754

---

---
url: /Java/设计模式/02.创建型/1.md
---

# 创建型

---

---
url: /Java/架构设计/分布式/05.分布式任务调度/3_创建XXL-JOB任务调度.md
---

# 创建XXL-JOB任务调度

## 在任务管理中创建任务

![image-20250820101509244](/assets/image-20250820101509244.D2eBh8Uj.png)

## 在SpringBoot中创建定时任务

`myJobHandler`是刚才在可视化界面中，`JobHandler`我们自定义的名字

```java
import com.xxl.job.core.context.XxlJobHelper;
import com.xxl.job.core.handler.annotation.XxlJob;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Component;

@Slf4j
@Component
public class MyXxlJob {

    @XxlJob("myJobHandler")
    public void myJobHandler() throws Exception {
        XxlJobHelper.log("XXL-JOB, 这是我的第一个定时任务，开始执行了");
        log.info("XXL-JOB, 这是我的第一个定时任务，开始执行了");
    }

}
```

### 启动任务调度

![image-20250820101602288](/assets/image-20250820101602288.BaoKnaXa.png)

### 调度日志

点击左侧“调度日志”菜单，查看调度日志，此时我们停止项目运行，会线上调度失败。

![image-20250820101619016](/assets/image-20250820101619016.B8K3-Sn7.png)

---

---
url: /常用框架/EasyExcel/5_大量数据导出.md
---

# 大量数据导出

导出csv再转换成excel

导出csv
https://doc.hutool.cn/pages/CsvUtil/#%E7%94%9F%E6%88%90csv%E6%96%87%E4%BB%B6
https://blog.csdn.net/lzxlfly/article/details/107753891

csv转换成excel
https://blog.csdn.net/qq\_32046111/article/details/124879455
https://blog.csdn.net/Eiceblue/article/details/123845988

https://blog.csdn.net/weixin\_49456013/article/details/135001644

https://max.book118.com/html/2023/0510/8133104073005065.shtm

https://www.lsjlt.com/news/178303.html

java.lang.ClassNotFoundException: org.apache.poi.util.TempFileCreationStrategy

https://blog.csdn.net/weixin\_43949154/article/details/122071510

---

---
url: /StableDiffusion/Midjourney/4_单人及多人换脸.md
---

# 单人及多人换脸

---

---
url: /Redis/Redis源码/单线程执行逻辑.md
---
# 单线程执行逻辑

https://www.bilibili.com/video/BV1F5ZiYpEfy/

---

---
url: /daily/日常笔记/等保整改.md
---

# 等保整改

https://blog.csdn.net/bigwood99/article/details/125972053

https://blog.csdn.net/weixin\_42579598/article/details/114286578

https://zhidao.baidu.com/question/625587079762726572.html

Alibaba  Cloud Linux操作系统未开启登录失败处理功能，未开启连接超时自动退出策略

https://blog.csdn.net/hjxloveqsx/article/details/129004832

https://blog.csdn.net/kongzhian/article/details/119607201

https://blog.csdn.net/DaQiangZhuang/article/details/127849229

登录失败处理功能策略

```shell
# 编辑系统/etc/pam.d/system-auth 文件，在 auth 字段所在的那一部分添加如下pam_tally2.so模块的策略参数：
auth required pam_tally2.so  onerr=fail  deny=5  unlock_time=300 even_deny_root root_unlock_time=300
# 或者
auth required pam_tally2.so  onerr=fail  deny=5  unlock_time=300 no_magic_root
```

Centos8：pam包中pam\_tally2.so被删除，由pam\_faillock.so替代

[Centos8：pam包中pam\_tally2.so被删除，由pam\_faillock.so替代\_没有pam\_tally2.so-CSDN博客](https://blog.csdn.net/weixin_46156844/article/details/104675112)

操作超时退出功能策略

```shell
# 编辑/etc/profile系统文件，在文件后面添加：
export TMOUT=300       #表示无操作300秒后自动退出
# 使修改生效
source /etc/profile
```

设置用户密码规则（复杂密码策略）方法

https://blog.csdn.net/li\_c\_yang/article/details/129850082

限制IP登录Linux服务器的几种方式

https://blog.csdn.net/weixin\_45190065/article/details/128851505

linux下开启SSH，并且允许root用户远程登录,允许无密码登录

https://blog.csdn.net/weixin\_46156844/article/details/104675112

### Mysql

https://blog.csdn.net/xtaypyvi123456/article/details/125274821

开启登录失败和超时功能参数

```shell
mysql -u root -p

mysql> show variables like '%connection_control%';
Empty set (0.00 sec)

mysql> install plugin CONNECTION_CONTROL soname 'connection_control.so';
Query OK, 0 rows affected (0.16 sec)

mysql> install plugin CONNECTION_CONTROL_FAILED_LOGIN_ATTEMPTS soname 'connection_control.so';
Query OK, 0 rows affected (0.00 sec)

mysql> set global connection_control_failed_connections_threshold = 5;
Query OK, 0 rows affected (0.00 sec)

mysql> set global connection_control_min_connection_delay = 300000;
Query OK, 0 rows affected (0.00 sec)

mysql> exit
Bye
```

设置了wait\_timeout参数为86400，超时时间过长

```shell
mysql> show variables like '%timeout%';
mysql> set global wait_timeout=900;
```

密码复杂度策略设置

http://runxinzhi.com/mysqljs-p-14246130.html

validate\_password插件卸载：https://blog.csdn.net/kfepiza/article/details/126913185

```shell
# 安装 validate_password 插件，此插件可以验证密码强度，未达到规定强度的密码则不允许被设置
mysql> show variables like 'validate%';
Empty set (0.00 sec)

# 通过 INSTALL PLUGIN 命令可安装此插件
# 每个平台的文件名后缀都不同 对于 Unix 和类 Unix 系统，为.so，对于 Windows 为.dll
mysql> INSTALL PLUGIN validate_password SONAME 'validate_password.so';
Query OK, 0 rows affected, 1 warning (0.28 sec)

# 查看 validate_password 相关参数
mysql> show variables like 'validate%';
+--------------------------------------+--------+
| Variable_name                        | Value  |
+--------------------------------------+--------+
| validate_password_check_user_name    | ON     |
| validate_password_dictionary_file    |        |
| validate_password_length             | 8      |
| validate_password_mixed_case_count   | 1      |
| validate_password_number_count       | 1      |
| validate_password_policy             | MEDIUM |
| validate_password_special_char_count | 1      |
+--------------------------------------+--------+
7 rows in set (0.00 sec)

```

设置某个账号密码过期时间

```bash
# 通过 mysql.user 系统表查看数据库账号状态
mysql> select user,host,password_expired,password_lifetime,password_last_changed,account_locked from mysql.user;


# 单独设置该账号密码90天过期
ALTER USER 'ekdb_advanced_rw_1'@'%' PASSWORD EXPIRE INTERVAL 90 DAY;
ALTER USER 'ekdb_approval_rw_1'@'%' PASSWORD EXPIRE INTERVAL 90 DAY;
ALTER USER 'ekdb_auth_rw_1'@'%' PASSWORD EXPIRE INTERVAL 90 DAY;
ALTER USER 'ekdb_base_rw_1'@'%' PASSWORD EXPIRE INTERVAL 90 DAY;
ALTER USER 'ekdb_dormitory_rw_1'@'%' PASSWORD EXPIRE INTERVAL 90 DAY;
ALTER USER 'ekdb_exam_rw_1'@'%' PASSWORD EXPIRE INTERVAL 90 DAY;
ALTER USER 'ekdb_flow_rw_1'@'%' PASSWORD EXPIRE INTERVAL 90 DAY;
ALTER USER 'ekdb_information_rw_1'@'%' PASSWORD EXPIRE INTERVAL 90 DAY;
ALTER USER 'ekdb_probation_rw_1'@'%' PASSWORD EXPIRE INTERVAL 90 DAY;
ALTER USER 'ekdb_rotation_rw_1'@'%' PASSWORD EXPIRE INTERVAL 90 DAY;
ALTER USER 'ekdb_statistical_rw_1'@'%' PASSWORD EXPIRE INTERVAL 90 DAY;
ALTER USER 'ekdb_template_center_rw_1'@'%' PASSWORD EXPIRE INTERVAL 90 DAY;
ALTER USER 'ekdb_template_rw_1'@'%' PASSWORD EXPIRE INTERVAL 90 DAY;
ALTER USER 'ekdb_user_rw_1'@'%' PASSWORD EXPIRE INTERVAL 90 DAY;

```

### Mongo

navicat打开mongo命令行，mongo shell可执行文件路径尚未设置

https://blog.csdn.net/LiuLiYanSha/article/details/128106567

MangoDB数据库仅建立了r\*\*t用户，当前不存在多余的、过期的账户，但存在共享账户

https://baijiahao.baidu.com/s?id=1741105806917677143

https://blog.csdn.net/qq\_41428418/article/details/132175704

```
db.createUser({
  user: "test",
  pwd: "test",
  roles: ["dbOwner"]
})
```

MangoDB数据库未启用密码复杂度策略，未设置密码定期更换策略

https://mp.weixin.qq.com/s?src=11\&timestamp=1695180995\&ver=4785\&signature=vfu2NwmvnakhWxHttwxlRkBEE5j2ORZW0bJbvinvm2vCtEfjHN32J4zQScE3YZkMZAlfuoiZuLLqzoJed4zzzkbjpw8DQFBz2Z7fW1naPzfJ\*B5878qNU6KmkzC0UHol\&new=1

---

---
url: /daily/高等数学/06_中值定理.md
---

# 第六讲 中值定理

中值定理：考证明题，10'（①4‘+②6’）

十大定理（1-4为f(x)，5-9为f'(x)，10为$\int\_a^bf(x),dx$）

1. 定理1—有界与最值定理
2. 定理2—介值定理
3. 定理3—平均值定理
4. 定理4—零点定理
5. 定理5—费马定理
6. 定理6—罗尔定理
7. 定理7—拉格朗日中值定理
8. 定理8—柯西中值定理
9. 定理9—泰勒公式
10. 定理10—积分中值定理

## 一、涉及函数的中值定理

### 1、定理1（有界与最值定理）

$设f(x)在\[a,b]上连续，则m≤f(x)≤M，其中m，M分别为f(x)在\[a,b]上的最小值与最大值.$

### 2、定理2（介值定理）

$设f(x)在\[a,b]上连续，当m≤μ≤M时，存在ξ∈(a,b),使得f(ξ)=μ.$

### 3、定理3（平均值定理/离散的的平均值定理）

$当a\<x\_1\<x\_2<···\<x\_n\<b时，在\[x\_1，x\_n]内至少存在一点ξ，使f(ξ)=\frac{f(x\_1)+f(x\_2)+···+f(x\_n)}{n}.(算数平均值)$

> 注：
>
> 定理3是（离散的）平均值定理：$f(ξ)=\frac{1}{n}\sum\_{i=1}^nf(x\_i).(算数平均值)(n是个数)$
>
> 定理10是（连续的）平均值定理：$f(ξ)=\frac{1}{b-a}·\int\_a^bf(x),dx=\overline{f(x)}.(b-a是区间长度)$

### 4、定理4（零点定理）

$当f(a)·f(b)<0时，存在ξ∈（a,b）,使得f(ξ)=0.（端点异号，经过x轴，由介值定理推论）$

### 例题

1、$设f(x)在\[a,b]上连续，当a≤x\_1≤x\_2≤···≤x\_n≤b时，证明存在ξ∈\[x\_1,x\_n],使得f(ξ)=\frac{f(x\_1)+f(x\_2)+···+f(x\_n)}{n}$.

分析：$由最值定理m≤f(x)≤M，\ 所以m≤f(x\_1)≤M，m≤f(x\_2)≤M，···，m≤f(x\_n)≤M \ n·m≤f(x\_1)+f(x\_2)+···+f(x\_n)≤n·M \ m≤\frac{f(x\_1)+f(x\_2)+···+f(x\_n)}{n}≤M \ m≤ μ ≤M \ m≤ f(ξ) ≤M \ 所以 f(ξ)=\frac{f(x\_1)+f(x\_2)+···+f(x\_n)}{n}$

2、$设f(x)在\[a,b]上连续，当a≤x\_1≤x\_2≤···≤x\_n≤b时，证明存在ξ∈\[a,b],使得f(ξ)=\frac{f(x\_1)+f(x\_2)+···+f(x\_n)}{n}$.

## 二、设计导数（微分）的中值定理

### 5、定理5（费马定理）

$设f(x)满足在x\_0点处\left{
\begin{aligned}
① && 可导, \\
② && 取极值,  \\
\end{aligned}
\right.则f'(x\_0)=0.$

费马大定理：$x^n+y^n=z^n，(n>2)没有正整数解。$

证明：

证明导数在某一点等于几，用定义。$f'(x\_0)=\lim\limits\_{x-x\_0}\frac{f(x)-f(x\_0)}{x-x\_0} = 0$

![image-20240714220501204](/assets/image-20240714220501204.CTsH5SZ-.png)

### 6、定理6（罗尔定理）

$设f(x)满足\left{
\begin{aligned}
① && 在\[a,b]上连续, \\
② && 在(a,b)内可导，则存在ξ∈（a,b）,使得f'(ξ)=0,  \\
③ && f(a)=f(b). \\
\end{aligned}
\right.$

### 7、定理7（拉格朗日中值定理）

$设f(x)满足\left{
\begin{aligned}
① && 在\[a,b]上连续, \\
② && 在(a,b)内可导 \\
\end{aligned}
\right. $

$则存在ξ∈（a,b）,使得f(b)-f(a)=f'(ξ)(b-a)$

$或者写成f'(ξ)=\frac{f(b)-f(a)}{b-a}$

### 8、定理8（柯西中值定理）

$设f(x),g(x)满足\left{
\begin{aligned}
① && 在\[a,b]上连续, \\
② && 在(a,b)内可导，则存在ξ∈（a,b）, \\
③ && g'(x)≠0. \\
\end{aligned}
\right.$

$使得\frac{f(b)-f(a)}{g(b)-g(a)}=\frac{f'(ξ)}{g'(ξ)}$

### 9、定理9（泰勒公式）

### 10、定理10（积分中值定理/连续的平均值定理）

$设f(x)在\[a,b]上连续，证明存在ξ∈\[a,b],使得\int\_a^bf(x),dx=f(ξ)(b-a).$

证明：

![image-20240714003702362](/assets/image-20240714003702362.iCE0V29n.png)

$①最值原理 由于f(x)在\[a,b]上连续，则f(x)有最大值M和最小值m \ ⇩ \ m≤f(x)≤M \ ⇩ \ md(x)≤f(x)d(x)≤Md(x),d(x)>0 \ ⇩ \ \int\_a^bmd(x)≤\int\_a^bf(x),dx≤\int\_a^bMd(x) \ ⇩ \ m(b-a)≤\int\_a^bf(x),dx≤M(b-a) \ ⇩ \ ②介值定理 m≤\frac{\int\_a^bf(x),dx}{b-a}≤M \ ⇩ \ m≤μ≤M \ ⇩ \ 存在ξ∈\[a,b]，使f(ξ)=μ \ ⇩ \ \frac{\int\_a^bf(x),dx}{b-a} ⇨ f(ξ) ⇨ \int\_a^bf(x),dx=f(ξ)(b-a)$

---

---
url: /StableDiffusion/Midjourney/2_调整图片比例.md
---

# 调整图片比例

## 调整图片高、宽比的参数

**–aspect或–ar**

默认纵横比为 1:1。

\--aspect必须使用整数。使用 139:100 而不是 1.39:1。

**提示示例：**

```sh
/imagine prompt vibrant california poppies --ar 5:4 
```

## 不同版本最大纵横比比较

不同的Midjourney 版本模型具有不同的最大纵横比。

|      | 版本 5 | 版本 4c（默认） | 版本 4a 或 4b       | 版本 3     | 测试/测试  | 尼基       |
| ---- | ------ | --------------- | ------------------- | ---------- | ---------- | ---------- |
| 比率 | 任何\*  | 1:2 至 2:1      | 仅：1:1、2:3 或 3:2 | 5:2 至 2:5 | 3:2 至 2:3 | 1:2 至 2:1 |

## 常见的纵横比

\--aspect 1:1默认纵横比。

\--aspect 5:4常见的框架和打印比例。

\--aspect 3:2印刷摄影中常见。

\--aspect 7:4靠近高清电视屏幕和智能手机屏幕。

---

---
url: /常用框架/SpringBoot/SpringBoot服务整合/7_定时调度.md
---

# 定时调度

## 定时调度

在企业项目开发中，定时调度是一项重要的技术组成，利用定时调度可以帮助用户实现无人值守程序执行，在Spring中提供了简单的SpringTask调度执行任务，利用此组件可以实现间隔调度与CRON调度处理。

### 创建调度类

定义一个线程调度类。

```java
@Component
public class MySchedulerTask {

    @Scheduled(fixedRate = 2000)                // 采用间隔调度，每2秒执行一次
    public void runJobA() {                    // 定义一个要执行的任务
        System.out.println("【*** MyTaskA - 间隔调度 ***】"
                + new SimpleDateFormat("yyyy-MM-dd HH:mm:ss.SSS")
                .format(new Date()));
    }

    @Scheduled(cron = "* * * * * ?")            // 每秒调用一次
    public void runJobB() {
        System.err.println("【*** MyTaskB - 间隔调度 ***】"
                + new SimpleDateFormat("yyyy-MM-dd HH:mm:ss.SSS")
                .format(new Date()));
    }
}
```

### 创建自定义线程池

为了让多个任务并行执行，还需要建立一个定时调度池的配置类。

```java
@Configuration            // 定时调度的配置类一定要实现指定的父接口
public class SchedulerConfig implements SchedulingConfigurer {
    @Override
    public void configureTasks(ScheduledTaskRegistrar taskRegistrar) {        // 开启线程调度池
        taskRegistrar.setScheduler(Executors.newScheduledThreadPool(10));    // 10个线程池
    }
}
```

### 启动类开启调度

在程序启动类上追加定时任务配置注解。

```java
@SpringBootApplication   // 启动SpringBoot程序，而后自带子包扫描
@EnableScheduling    // 启用调度
public class SpringBootIntegrationSchedulerApplication {

    public static void main(String[] args) {
        SpringApplication.run(SpringBootIntegrationSchedulerApplication.class, args);
    }

}
```

本程序同时启动了两个定时调度，为了使两个线程调度之间不受影响，开辟了一个线程池，可以并行执行多个任务。

打印结果

```sh
【*** MyTaskB - 间隔调度 ***】2025-02-23 21:17:25.016
【*** MyTaskB - 间隔调度 ***】2025-02-23 21:17:26.025
【*** MyTaskA - 间隔调度 ***】2025-02-23 21:17:26.336
【*** MyTaskB - 间隔调度 ***】2025-02-23 21:17:27.011
【*** MyTaskB - 间隔调度 ***】2025-02-23 21:17:28.001
【*** MyTaskA - 间隔调度 ***】2025-02-23 21:17:28.332
【*** MyTaskB - 间隔调度 ***】2025-02-23 21:17:29.005
【*** MyTaskB - 间隔调度 ***】2025-02-23 21:17:30.003
【*** MyTaskA - 间隔调度 ***】2025-02-23 21:17:30.331
【*** MyTaskB - 间隔调度 ***】2025-02-23 21:17:31.004
【*** MyTaskB - 间隔调度 ***】2025-02-23 21:17:32.149
【*** MyTaskA - 间隔调度 ***】2025-02-23 21:17:32.325
【*** MyTaskB - 间隔调度 ***】2025-02-23 21:17:33.003
【*** MyTaskB - 间隔调度 ***】2025-02-23 21:17:34.004
【*** MyTaskA - 间隔调度 ***】2025-02-23 21:17:34.331
【*** MyTaskB - 间隔调度 ***】2025-02-23 21:17:35.007
【*** MyTaskB - 间隔调度 ***】2025-02-23 21:17:36.012
```

---

---
url: /daily/开发文档/抖音小程序开发.md
---

# 抖音小程序开发

> 抖音开放平台官网
>
> https://developer.open-douyin.com/

---

---
url: /常用框架/SpringBoot/SpringBoot程序开发/9_读取资源文件.md
---

# 读取资源文件

在实际的项目开发中，资源文件不可或缺，因为所有的提示文字信息都要在资源文件中进行定义，而且资源文件是实现国际化技术的主要手段。如果想在SpringBoot里面进行资源文件的配置，只需要做一些简单的application.yml配置即可，而且所有注入的资源文件都可以像最初的Spring处理那样，直接使用MessageSource进行读取。

## 一、资源文件配置

在src/main/resources源文件夹下创建一个i18n的子目录（包）。

建立src/main/resources/i18n/Messages.properties文件，文件内容定义如下：

```properties
welcome.url=www.xxl.cn
welcome.msg=欢迎{0}光临
```

## 二、项目文件配置

修改`application.yml`配置文件，追加资源文件配置

```yaml
spring:
  messages: # 定义配置文件，多个资源文件使用“,”分割
    basename: i18n/Messages
```

## 三、资源文件读取

在控制器中注入org.springframework.context.MessageSource接口对象，并且利用此对象实现资源文件读取。

```java
@Autowired
private MessageSource messageSource;

@GetMapping("/message")
public Object message() {
    Map<String, String> map = new HashMap<>();
    map.put("welcome.url", this.messageSource.getMessage("welcome.url", null, Locale.getDefault()));
    map.put("welcome.msg", this.messageSource.getMessage("welcome.msg", new Object[]{"卡皮巴拉"}, Locale.getDefault()));
    return map;
}
```

当程序中配置了资源文件之后，就可以通过MessageSource接口中提供的getMessage()方法进行资源的读取。

## 四、扩展：国际化开发

**提示：可以借用此机制实现国际化开发。**

当程序可以实现资源文件读取的时候，就意味着可以实现国际化开发处理了。MessageSource接口中的getMessage()方法里面需要接收一个Locale类的对象，此时就可以通过Locale类的设置来获取不同的资源文件。当然，也需要在项目中配置好不同语言的资源文件。例如，在src/main/resources/i18n目录中创建了`Messages_zh_CN.properties`和`Messages_en_US.properties`（注意baseName的名称相同）。

这样，当读取时可以采用不同的Locale对象实现指定语言的资源读取。例如，使用如下代码就可以实现`Messages_en_US.properties`资源文件的读取：

```java
map.put("welcome.msg", this.messageSource.getMessage("welcome.msg", new Object[]{"卡皮巴拉"}, new Locale("en", "US")));
```

需要注意的是，即使提供了不同语言的资源文件，在SpringBoot中也依然需要提供`Messages.properties`配置文件，否则将无法实现资源文件的读取。

完整示例：

Messages\_zh\_CN.properties

```properties
welcome.url=www.xxl.cn
welcome.msg=欢迎{0}光临
```

Messages\_en\_US.properties

```properties
welcome.url=www.xxl.cn
welcome.msg=welcome{0}
```

application.yml

```yaml
server:
  port: 80 # 设置运行服务所在端口
  servlet:
    context-path: /xxl # 定义ContextPath访问路径
spring:
  messages: # 定义配置文件，多个资源文件使用“,”分割
    basename: i18n/Messages, i18n/Messages_zh_CN, i18n/Messages_en_US
```

测试类

```java
@GetMapping("/i18n")
public Object i18n(String type) {
    Map<String, String> map = new HashMap<>();
    map.put("welcome.url", this.messageSource.getMessage("welcome.url", null, Locale.getDefault()));
    if (type != null && type.equals("en")) {
        map.put("welcome.msg", this.messageSource.getMessage("welcome.msg", new Object[]{"卡皮巴拉"}, new Locale("en", "US")));
        return map;
    }
    map.put("welcome.msg", this.messageSource.getMessage("welcome.msg", new Object[]{"卡皮巴拉"}, new Locale("zh", "CN")));
    return map;
}
```

访问：http://127.0.0.1/xxl/i18n

```json
{
    "welcome.msg": "欢迎卡皮巴拉光临",
    "welcome.url": "www.xxl.cn"
}
```

访问：http://127.0.0.1/xxl/i18n?type=en

```json
{
    "welcome.msg": "welcome卡皮巴拉",
    "welcome.url": "www.xxl.cn"
}
```

---

---
url: /Java/JVM性能调优/01.JVM概念/4_堆.md
---

# 堆

## 三种JVM

1. Sun公司的HotSpot。（java -version查看）
2. BEA的JRockit
3. IBM的J9VM

## 堆

Heap，一个JVM只有一个堆内存，堆内存的大小是可以调节的。

类加载器读取了类文件后，一般会把什么东西放到堆中？
类、方法、常量、变量、保存我们所有引用类型的真实对象。

堆内存中细分为三个区域：

* 新生区（伊甸园区）Young/New
* 养老区 old
* 永久区 Perm

![img](/assets/kuangstudycbc2c908-bf86-4263-9848-a63bfaa11fd7.9OFtfOnD.jpg)

### 新生区

新生区又叫做伊甸园区，包括：伊甸园区、幸存0区、幸存1区。

### 永久区

这个区域是**常驻内存**的。
用来存放JDK自身携带的Class对象、Interface元数据，存储的是Java运行时的一些环境或类信息~。
这个区域**不存在垃圾回收**。
关闭JVM虚拟机就会释放这个区域的内存。

什么情况下，在永久区就崩了？

* 一个启动类，加载了大量的第三方jar包。
* Tomcat部署了太多的应用。
* 大量动态生成的反射类；不断的被加载，直到内存满，就会出现OOM

### 永久代和元空间

什么是永久代和元空间？？
方法区是一种规范，不同的虚拟机厂商可以基于规范做出不同的实现，永久代和元空间就是出于不同jdk版本的实现。
方法区就像是一个接口，永久代与元空间分别是两个不同的实现类。
只不过永久代是这个接口最初的实现类，后来这个接口一直进行变更，直到最后彻底废弃这个实现类，由新实现类—元空间进行替代。

jdk1.8之前：
![img](/assets/kuangstudy590e2fb9-b6fe-465c-b9fe-c6281130c20d.hAx2L-qF.jpg)

jdk1.8以及之后：在堆内存中，逻辑上存在，物理上不存在（元空间使用的是本地内存）
![img](/assets/kuangstudy39072ea5-d640-4d2c-b80a-925e0780a0fc.DJKSU6F8.jpg)

### 常量池

1. 在jdk1.7之前，运行时常量池+字符串常量池是存放在方法区中，HotSpot VM对方法区的实现称为永久代。
   ![img](/assets/kuangstudyfa4dab7f-9a26-4298-b00c-a4b2d4afff7e.CuEhqo85.jpg)
2. 在jdk1.7中，字符串常量池从方法区移到堆中，运行时常量池保留在方法区中。
   ![img](/assets/kuangstudyfb7f3f21-0aec-4be9-b4ed-db5c61754645.CDMyqi2U.jpg)
3. jdk1.8之后，HotSpot移除永久代，使用元空间代替；此时字符串常量池保留在堆中，运行时常量池保留在方法区中，只是实现不一样了，JVM内存变成了直接内存。
   ![img](/assets/kuangstudydcc630f0-4406-4a14-9275-af78afa6ca73.BLwg8sts.jpg)

---

---
url: /Java/系统优化/性能优化/3_多级缓存.md
---

# 多级缓存

---

---
url: /Java/解决方案/多租户/多租户.md
---
# 多租户

SaaS 中的账户和租户概念：https://www.woshipm.com/pd/5184398.html

https://ysi13ckdb9.feishu.cn/wiki/Jkcbw8IqciIULzkvQcocuGKknQd

https://www.feishu.cn/community/article?id=7483079079776747521

---

---
url: /Java/Java开发技巧/03.高效编程/3_反射Reflection.md
---

# 反射Reflection

通过反射可以获取到任意对象的任意方法、属性，也可以构造出任意对象，可跳过private等修饰符设定的权限，在Spirng、MyBtais等框架中大量使用。

## 通过反射获取Class对象

创建Student对象，包含成员变量和构造方法，其中有一个构造方法是私有的

```java
public class Student {

    private String sid;
    private String sname;
    public Integer age;

    public Student() {
        System.out.println("调用无参构造方法创建了一个学生对象");
    }

    public Student(String sid) {
        this.sid = sid;
        System.out.println("调用带一个参数的构造方法创建了一个学生对象");
    }

    public Student(String sid, String sname) {
        this.sid = sid;
        this.sname = sname;
        System.out.println("调用带二个参数的构造方法创建了一个学生对象");
    }

    private Student(Integer age) {
        System.out.println("调用Student类私有的构造方法创建一个学生对象");
        this.age = age;
    }
}
```

通过反射获取Class对象有3种方法，如下所示：

```java
public static void main(String[] args) throws ClassNotFoundException {
    // 1.方法一 
    Class class1 = new Student().getClass(); 
    System.out.println(class1.getName()); 

    // 2.方法二 
    Class class2 = Student.class;
    System.out.println(class2.getName());

    // 3.方法三 Class.forname
    Class class3 = Class.forName("com.xk857.demo.Student");
    System.out.println(class3);

    // 4.比较3个对象，发现获取的是同一个对象
    System.out.println(class1 == class2);
    System.out.println(class1 == class3);
}
```

## 通过反射操作构造函数

| 功能                                   | 函数                                 |
| :------------------------------------- | :----------------------------------- |
| 获取所有公共的构造方法                 | getConstructors()                    |
| 获取所有构造方法（包含私有、默认的等） | getDeclaredConstructors()            |
| 获取单个“公共”的构造方法               | getConstructor(Type.class……)         |
| 获取单个构造方法（可以是私有的等）     | getDeclaredConstructor(Type.class……) |
| 调用构造方法                           | newInstance(param……)                 |
| 绕过访问修饰符权限                     | setAccessible(true)                  |

案例如下，可自行增加换行或其他输出，使其在控制台打印更加明显。

```java
@Test
public void test1() throws ClassNotFoundException, …… {
    Class class3 = Class.forName("com.xk857.demo.Student");

    // 1.获取所有公共的构造方法
    Constructor[] constructors = class3.getConstructors();
    for (Constructor constructor : constructors) {
        System.out.println(constructor);
    }

    // 2.所有构造方法（包含私有、默认的等）
    constructors = class3.getDeclaredConstructors();
    for (Constructor constructor : constructors) {
        System.out.println(constructor);
    }

    // 3.获取单个公共”的构造方法
    Constructor constructor1 = class3.getConstructor(String.class, String.class);
    System.out.println(constructor1+"\n");

    // 4.获取单个公共”的构造方法
    Constructor declaredConstructor = class3.getDeclaredConstructor(Integer.class);
    System.out.println(declaredConstructor+"\n");

    // 绕过访问修饰符权限，使private修饰的构造方法也能被调用
    declaredConstructor.setAccessible(true);
    // 通过newInstance创建对象
    Student student = (Student) declaredConstructor.newInstance(12);
}
```

## 通过反射操作变量

| 功能                                     | 函数                         |
| :--------------------------------------- | :--------------------------- |
| 获取所有公共属性                         | getFields()                  |
| 获取所有属性（包含私有、默认的等）       | getDeclaredFields()          |
| 获取单个"公共"字段                       | getField(字段名称)           |
| 获取单个字段（包含私有、默认的等）       | getDeclaredField(字段名称)   |
| 先通过反射获取对象，然后给对象设置属性值 | field.set(对象实例, 属性值); |
| 绕过访问修饰符权限                       | field.setAccessible(true)    |

案例如下，可自行增加换行或其他输出，使其在控制台打印更加明显。

```java
@Test
public void test2() throws ClassNotFoundException, ……{
    Class class3 = Class.forName("com.xk857.demo.Student");

    // 1.获取所有公有字段
    Field[] fields = class3.getFields();
    Arrays.stream(fields).forEach(System.out::println);

    // 2.获取所有字段（包含私有）
    fields = class3.getDeclaredFields();
    Arrays.stream(fields).forEach(System.out::println);

    // 3.获取单个字段
    Field field = class3.getField("age");
    System.out.println(field);

    // 4.获取单个字段（包含私有）
    field = class3.getDeclaredField("sname");

    // 5.获取到Student对象，通过反射设置private修饰的sname属性
    Student student = (Student) class3.getConstructor().newInstance();
    field.setAccessible(true);
    field.set(student, "张三");
    System.out.println(student.getSname());
}
```

## 通过反射操作成员方法

给Student添加两个hello方法，一个公有一个私有。

```java
public class Student {

    private String sid;
    private String sname;
    public Integer age;

    private void setSname(String sname) {
        this.sname = sname;
    }

    public void hello() {
        System.out.println("你好！我是" + this.sname);
    }

    private void hello(String name) {
        System.out.println(name + "你好！我是" + this.sname);
    }
}
```

| 功能                                                         | 函数                                            |
| :----------------------------------------------------------- | :---------------------------------------------- |
| 获取所有公有方法，包含了父类的方法也包含Object类的wait、notify等方法 | getMethods()                                    |
| 获取所有方法，包括私有但不包括继承                           | getDeclaredMethods()                            |
| 获取单个公共方法                                             | getMethod(方法名称)                             |
| 获取任意方法（不包含父类）                                   | getDeclaredMethod("方法名称", 参数类型.class……) |
| 执行方法                                                     | method.invoke(对象实例, 传递参数值……);          |

案例如下，可自行增加换行或其他输出，使其在控制台打印更加明显。

```java
@Test
public void test3() throws ClassNotFoundException, …… {
    Class class3 = Class.forName("com.xk857.demo.Student");
    Student student = (Student) class3.getConstructor().newInstance();

    // 1.获取所有公有方法，包含了父类的方法也包含Object类的wait、notify等方法
    Method[] methods = class3.getMethods();
    Arrays.stream(methods).forEach(System.out::println);
    System.out.println();

    // 2.获取所有方法，包括私有但不包括继承
    methods = class3.getDeclaredMethods();
    Arrays.stream(methods).forEach(System.out::println);

    // 3.获取单个公共方法，并执行
    Method method = class3.getMethod("hello");
    method.invoke(student);

    // 4.获取私有的方法并执行
    method = class3.getDeclaredMethod("hello", String.class);
    method.setAccessible(true);
    method.invoke(student, "张三");
}
```

---

---
url: /Java/Java开发技巧/03.高效编程/7_泛型的使用-方法抽取.md
---

# 泛型的使用-方法抽取

最近的需求中，需要在构建的树结构中增加每一级的层级level，之前使用的hutools TreeUtil.build 中需要数据库中有字段记录，才能赋值过去，但是增加字段后新增、编辑等逻辑需要改动。

```sql
# 对新增的level字段，赋值
UPDATE sys_management AS t1
JOIN (
    SELECT id, parent_id, @level := IF(parent_id = 0, 1, @level + 1) AS level
    FROM sys_management
    ORDER BY id
) AS t2 ON t1.id = t2.id
SET t1.level = t2.level;
```

所以编写了简单的构建方法并抽取成公共方法。

```java
/**
 * 
 * @param dataList
 * @param parentId
 * @param level
 * @return OrganizationTreeResult 为接收实体
 */
private static List<OrganizationTreeResult> buildTreeLevel(List<OrganizationTreeResult> dataList, int parentId, int level) {
    List<OrganizationTreeResult> treeList = new ArrayList<>();
    for (OrganizationTreeResult data : dataList) {
        if (data.getParentId() == parentId) {
            data.setLevel(level);
            data.setChildren(buildTreeLevel(dataList, data.getId(), level + 1));
            treeList.add(data);
        }
    }
    return treeList;
}
```

将上面方法中修改为公共方法，OrganizationTreeResult 修改为泛型，使用了泛型\<T>来代替OrganizationTreeResult，使得这个方法可以适用于不同类型的数据。

在泛型方法中，无法直接调用对象的方法或访问对象的属性，因为泛型类型是未知的。为了解决这个问题，可以使用一个接口TreeData来定义对象的通用方法，其中包括getParentId()、getId()、setLevel()和setChildren()，然后将对象实现该接口。修改后的代码如下所示：

```java
private static <T extends TreeData> List<T> buildTreeLevel(List<T> dataList, int parentId, int level) {
    List<T> treeList = new ArrayList<>();
    for (T data : dataList) {
        if (data.getParentId() == parentId) {
            data.setLevel(level);
            List<T> children = buildTreeLevel(dataList, data.getId(), level + 1);
            data.setChildren(children);
            treeList.add(data);
        }
    }
    return treeList;
}
```

需要将OrganizationTreeResult类实现TreeData接口，并在接口中定义这些方法。这样，就可以在泛型方法中调用这些通用方法了。

进一步改进`<T extends TreeData>`中的TreeData使用传参提供。

使用了反射来设置泛型对象的属性，以适应不同类型对象的属性命名。

```java
private static <T> List<T> buildTreeLevel(List<T> dataList, int parentId, int level, Class<T> dataType) {
    List<T> treeList = new ArrayList<>();
    for (T data : dataList) {
        try {
            Method getParentIdMethod = dataType.getMethod("getParentId");
            int dataParentId = (int) getParentIdMethod.invoke(data);
            if (dataParentId == parentId) {
                Method setLevelMethod = dataType.getMethod("setLevel", Integer.class);
                setLevelMethod.invoke(data, level);
                
                Method getIdMethod = dataType.getMethod("getId");
                int dataId = (int) getIdMethod.invoke(data);
                List<T> children = buildTreeLevel(dataList, dataId, level + 1, dataType);
                
                Method setChildrenMethod = dataType.getMethod("setChildren", Object.class);
                setChildrenMethod.invoke(data, children);
                
                treeList.add(data);
            }
        } catch (NoSuchMethodException | IllegalAccessException | InvocationTargetException e) {
            e.printStackTrace();
        }
    }
    return treeList;
}
```

在这个修改后的方法中，添加了一个额外的参数dataType，用于传递TreeData的类型。

通过使用反射，我们可以在运行时获取并调用对象的方法。

这样，就可以在调用方法时指定TreeData的具体类型，使方法适用于不同的数据类型。

```java
/**
 * 组装树结构（带层级）
 *
 * @param dataList
 * @param parentId
 * @param level
 * @param dataType
 * @return
 */
private static <T> List<T> buildTreeLevel(List<T> dataList, int parentId, int level, Class<T> dataType) {
    List<T> treeList = new ArrayList<>();
    for (T data : dataList) {
        try {
            Method getIdMethod = dataType.getMethod("getId");
            int dataId = (int) getIdMethod.invoke(data);
            if (hasChildren(dataList, dataId, dataType)) {
                Method setLevelMethod = dataType.getMethod("setLevel", Integer.class);
                setLevelMethod.invoke(data, level);

                List<T> children = buildTreeLevel(dataList, dataId, level + 1, dataType);
                Method setChildrenMethod = dataType.getMethod("setChildren", Object.class);
                setChildrenMethod.invoke(data, children);

                treeList.add(data);
            } else {
                Method setLevelMethod = dataType.getMethod("setLevel", Integer.class);
                setLevelMethod.invoke(data, level);

                Method setChildrenMethod = dataType.getMethod("setChildren", Object.class);
                List<T> emptyTreeList = new ArrayList<>();
                setChildrenMethod.invoke(data, emptyTreeList);
                treeList.add(data);
            }
        } catch (NoSuchMethodException | IllegalAccessException | InvocationTargetException e) {
            e.printStackTrace();
        }
    }
    return treeList;
}

/**
 * 是否存在下级
 *
 * @param dataList
 * @param parentId
 * @param dataType
 * @param <T>
 * @return
 */
private static <T> boolean hasChildren(List<T> dataList, int parentId, Class<T> dataType) {
    for (T data : dataList) {
        try {
            Method getParentIdMethod = dataType.getMethod("getParentId");
            int dataParentId = (int) getParentIdMethod.invoke(data);
            if (dataParentId == parentId) {
                return true;
            }
        } catch (NoSuchMethodException | IllegalAccessException | InvocationTargetException e) {
            e.printStackTrace();
        }
    }
    return false;
}
```

---

---
url: /Java/Java开发技巧/02.函数式编程/4_方法引用.md
---

# 方法引用

我们在使用lambda时，如果方法体中只有一个方法的调用的话（包括构造方法）,我们可以用方法引用进一步简化代码。

## 一、推荐用法

我们在使用lambda时不需要考虑什么时候用方法引用，用哪种方法引用，方法引用的格式是什么。我们只需要在写完lambda方法发现方法体只有一行代码，并且是方法的调用时使用快捷键尝试是否能够转换成方法引用即可。

当我们方法引用使用的多了慢慢的也可以直接写出方法引用。

## 二、基本格式

```
类名或者对象名::方法名
```

## 三、语法详解（了解）

### 3.1、引用类的静态方法

其实就是引用类的静态方法

#### 格式

```java
类名::方法名
```

#### 使用前提

如果我们在重写方法的时候，方法体中**只有一行代码**，并且这行代码是**调用了某个类的静态方法**，并且我们把要重写的**抽象方法中所有的参数都按照顺序传入了这个静态方法中**，这个时候我们就可以引用类的静态方法。

例如：

如下代码就可以用方法引用进行简化

```java
List<Author> authors = getAuthors();

Stream<Author> authorStream = authors.stream();

authorStream.map(author -> author.getAge())
    .map(age->String.valueOf(age));
```

注意，如果我们所重写的方法是没有参数的，调用的方法也是没有参数的也相当于符合以上规则。

优化后如下：

```java
List<Author> authors = getAuthors();

Stream<Author> authorStream = authors.stream();

authorStream.map(author -> author.getAge())
    .map(String::valueOf);
```

### 3.2、引用对象的实例方法

#### 格式

```java
对象名::方法名
```

#### 使用前提

如果我们在重写方法的时候，方法体中**只有一行代码**，并且这行代码是**调用了某个对象的成员方法**，并且我们把要重写的**抽象方法中所有的参数都按照顺序传入了这个成员方法中**，这个时候我们就可以引用对象的实例方法

例如：

```java
List<Author> authors = getAuthors();

Stream<Author> authorStream = authors.stream();
StringBuilder sb = new StringBuilder();
authorStream.map(author -> author.getName())
    .forEach(name->sb.append(name));
```

优化后：

```java
List<Author> authors = getAuthors();

Stream<Author> authorStream = authors.stream();
StringBuilder sb = new StringBuilder();
authorStream.map(author -> author.getName())
    .forEach(sb::append);
```

### 3.3、引用类的实例方法

#### 格式

```java
类名::方法名
```

#### 使用前提

如果我们在重写方法的时候，方法体中**只有一行代码**，并且这行代码是**调用了第一个参数的成员方法**，并且我们把要**重写的抽象方法中剩余的所有的参数都按照顺序传入了这个成员方法中**，这个时候我们就可以引用类的实例方法。

例如：

```java
interface UseString{
    String use(String str,int start,int length);
}

public static String subAuthorName(String str, UseString useString){
    int start = 0;
    int length = 1;
    return useString.use(str,start,length);
}
public static void main(String[] args) {

    subAuthorName("三更草堂", new UseString() {
        @Override
        public String use(String str, int start, int length) {
            return str.substring(start,length);
        }
    });

}
```

优化后如下：

```java
public static void main(String[] args) {

    subAuthorName("三更草堂", String::substring);

}
```

### 3.4、构造器引用

如果方法体中的一行代码是构造器的话就可以使用构造器引用。

#### 格式

```java
类名::new
```

#### 使用前提

如果我们在重写方法的时候，方法体中**只有一行代码**，并且这行代码是**调用了某个类的构造方法**，并且我们把**要重写的抽象方法中的所有的参数都按照顺序传入了这个构造方法中**，这个时候我们就可以引用构造器。

例如：

```java
List<Author> authors = getAuthors();
authors.stream()
    .map(author -> author.getName())
    .map(name->new StringBuilder(name))
    .map(sb->sb.append("-三更").toString())
    .forEach(str-> System.out.println(str));
```

优化后：

```java
List<Author> authors = getAuthors();
authors.stream()
    .map(author -> author.getName())
    .map(StringBuilder::new)
    .map(sb->sb.append("-三更").toString())
    .forEach(str-> System.out.println(str));
```

---

---
url: /Java/架构设计/分布式/01.分布式缓存/分布式缓存.md
---

# 分布式缓存

***

缓存是高并发系统不可或缺的技术，可以提高系统的性能和并发，因此是后台开发必学的知识点，也是面试重点。

## 知识点

* 什么是缓存？
* 本地缓存
  * Caffeine 库
* 多级缓存
* Redis 分布式缓存
  * 数据类型
  * 常用操作
  * Java 操作 Redis
    * Spring Boot Redis Template
    * Redisson
  * 主从模型搭建
  * 哨兵集群搭建
  * 日志持久化
  * Redis Stack
* 缓存（Redis）应用场景
  * 数据共享
  * 单点登录
  * 计数器
  * 限流
  * 点赞
  * 实时排行榜
  * 分布式锁
* 缓存常见问题
  * 缓存雪崩
  * 缓存击穿
  * 缓存穿透
  * 缓存更新一致性
* 相关技术：Memcached、Ehcache

## Redis笔记

Redis 详细笔记跳转至 [Redis专栏](/redis/quickstart.html)

---

---
url: /Java/架构设计/分布式/05.分布式任务调度/0_分布式任务调度框架概述.md
---

# 分布式任务调度框架概述

## 分布式任务调度框架概述

既然说是分布式任务调度框架，那么分布式调度框架和非分布式调度框架有什么区别呢？在分布式环境下，一个服务会部署到多个主机，也就是会有多个相同服务同时运行，那么使用非分布式调度框架会在每个主机上都运行一遍，导致资源浪费，而分布式任务调度框架则不会有这个问题。

什么是分布式任务调度框架？简单点来说就是定时任务，通过统一调度来执行，保障定时任务正常执行，但不会重复执行。

### 常见的分布式调度框架

* QuartZ：老牌框架，但是比较难用，通过调用API的的方式操作任务，需要持久化业务QuartzJobBean到底层数据表中，系统侵入性相当严重，任务多时性能受限。
* Elasticjob：优点是维护积极，虽然诞生时间久远，现在仍然在维护。可惜第三方中间件比较多，如果出现了问题，还需要去熟悉所依赖的中间件才能解决，使用和维护成本较高。项目地址：https://gitee.com/elasticjob/elastic-job
* **XXL-JOB**：轻量级分布式任务调度框架，解决了QuartZ框架的各项问题，也没有Elasticjob那样依赖其他中间件，使用简单，而且中文文档写的很详细。项目地址：https://www.xuxueli.com/xxl-job/
* SchedulerX：阿里云的产品，优点非常多，但是阿里云出品，非开源框架， 这是需要票票才能用到，比较费人民币。
* PowerJob：性能强劲，操作简单，但是相对于xxl-job时间较晚，用的企业不是很多，xxl-job是已经经过大量实践的，用起来比较稳定。

---

---
url: /Java/架构设计/分布式/04.分布式事务/0_分布式事务概念.md
---

# 分布式事务概念

### 🍊 CAP理论

在分布式环境下，我们无法同时保证数据一致性、可用性和分区容错性，只能选择其中两个。

分区容错性是必须要保证的，因为网络问题一定会发生。那么我们就需要让每个服务都有多个节点，这样就可以保证一个节点挂了之后，其他节点依然可以响应，这就是分区容错性。

但是一个服务有多个节点之后，一个节点挂了之后，其他节点如何保证数据一致性呢？这就要进行数据复制，确保每个节点上的数据都是最新的。这样就出现了一个问题，就是数据在复制过程中可能存在延迟，也就是说，我们无法保证每个节点上的数据都是同步的。这时候，我们就需要权衡一下，是选择保证数据的一致性，还是选择保证系统的可用性呢？

如果选择保证数据的一致性，那么当一个节点挂掉之后，其他节点会等待它恢复，以确保数据的一致性。这样的话，系统的可用性就会受到影响，因为用户可能会得到错误的响应或者超时。

如果选择保证系统的可用性，那么当一个节点挂掉之后，其他节点仍然可以响应请求，但是得到的数据可能不是最新的。这样的话，系统的数据一致性就会受到影响。

所以，我们只能在可用性和一致性之间权衡，选择其中一个。而这就是CAP定理告诉我们的。

想象一下，你是一家快递公司的老板，你要为公司的分布式系统设计一种方案来保证数据的一致性和可用性。快递公司有很多个仓库，每个仓库都存储着客户的快递信息（姓名、电话、地址、快递状态等等），这些信息需要在整个公司的分布式系统中进行共享。

首先，为了保证分区容错性，你会在每个仓库都设置一个主节点和多个从节点，当主节点出现故障时，从节点可以顶上来继续提供快递信息服务。这样，即使某个仓库的网络失效了，整个系统也不会因此瘫痪。

然而，当一个快递员送完货回到仓库，把快递信息更新到主节点上之后，这些更新的信息并不会立即同步到其他从节点上，因此就会出现数据一致性的问题。例如，你的客户A在网站上查询自己快递的状态，此时客户A的请求被分配到了一个从节点上处理，但是这个从节点上存储的快递信息还没有更新，因此客户A看到的信息可能不是最新的，这就是数据的最终一致性。

为了解决这个问题，你可以采取一种类似于“少数服从多数”的策略，即让每个从节点都去定期地跟主节点进行同步，而在同步的过程中，如果从节点和主节点上的数据不一致，从节点就会采用主节点上的数据。这样，虽然一段时间内客户看到的信息可能不是最新的，但是随着时间的推移，整个系统的数据最终还是会达到一致性的状态。

作为一个分布式系统的老板，你需要在数据一致性和可用性之间做出权衡，选择最适合公司应用场景的方案来保证系统的正常运行。这也是分布式系统设计中必须要考虑的一些问题，而CAP定理就是分布式系统设计的一个重要原则。

### 🍊 BASE理论

BASE理论是对CAP理论的延伸，其核心思想是在分布式系统中，即使无法做到强一致性，应用也可以采用适合的方式达到最终一致性。BASE理论分为三个方面，即基本可用、软状态和最终一致性。

基本可用是指分布式系统在出现故障的时候，允许损失部分可用性，即保证核心可用。比如在电商大促时，为了应对访问量激增，部分用户可能会被引导到降级页面，服务层也可能只提供降级服务。这就是损失部分可用性的体现。

软状态是指允许系统存在中间状态，而该中间状态不会影响系统整体可用性。比如在分布式存储中，一份数据至少会有三个副本，允许不同节点间副本同步的延时就是软状态的体现。MySQL Replication的异步复制也是一种体现。

最终一致性是指系统中的所有数据副本经过一定时间后，最终能够达到一致的状态。弱一致性和强一致性相反，最终一致性是弱一致性的一种特殊情况。在分布式系统中，达到强一致性很难，而最终一致性则是通过合适的策略达到数据一致性的一种方法。

在具体应用中，BASE理论的策略可以根据不同的业务场景制定。比如，在支付订单场景中，由于分布式本身就在数据一致性上面很难保证，从A服务到B服务的订单数据有可能造成数据不一致性。因此，此类场景会酌情考虑AP，不强制保证数据一致性，但保证数据最终一致性。

在实际应用中，BASE理论的应用场景很广泛。例如，Erueka 是 SpringCloud 系列用来做服务注册和发现的组件，它采用AP策略，保证了系统的可用性。而 Zookeeper 则保证了一致性和分区容错性，因此在 SpringCloud 中被抛弃。具体根据各自业务场景所需来制定相应的策略而选择适合的产品服务等。

分布式事务指的是事务的操作位于不同的节点上，需要保证事务的ACID特性。

举个例子，当电商网站在大促期间运营压力增大时，为保证核心功能仍能正常使用，可能会采取分流、降级等措施，这就是基本可用的体现；而当一份数据在分布式存储中至少需要有三个副本时，不同节点间副本同步的延时就是软状态的体现；最终一致性则是在数据一致性难以保证的情况下，最终数据还是能达到一致状态。在实际应用中，根据不同业务场景的需求来制定相应的策略，选择适合的产品服务等。比如在支付订单场景中，数据一致性难以保证，此时可以采用AP策略，不强制保证数据一致性，但保证最终一致性；而在下单场景中，如果库存和订单不在同一个节点上，就需要涉及分布式事务，保证事务的 AICD 特性。因此，BASE 理论提供了一种实际可行的思想来解决分布式系统中出现的数据一致性问题，而具体应用则需要根据业务场景灵活运用。

### 🍊 两阶段提交（2PC）

你可能会遇到这样一种情况：你需要在分布式系统中进行多个节点之间的事务处理，但是由于网络不稳定、节点故障等原因，可能会导致某些节点的事务执行失败。那么这时候该怎么办呢？这时候就可以使用两阶段提交（2PC）协议来保证分布式系统中的事务一致性。

首先，2PC协议包括两个阶段：准备阶段和提交阶段。在准备阶段，协调者向所有参与者发送准备请求，并询问参与者事务是否执行成功。如果所有参与者都可以正常执行事务，那么协调者就会发送提交请求，让所有参与者提交事务。否则，协调者会发送回滚请求，让所有参与者回滚事务。在提交阶段，所有参与者收到协调者的请求后，立即执行事务。如果事务执行成功，参与者将向协调者发送“准备就绪”响应；否则，参与者会发送“无法执行”响应。

2PC协议的优点在于可以保证分布式系统中的事务一致性。但是缺点也很明显。首先，所有参与者在等待其它参与者响应的时候都处于同步阻塞状态，无法进行其它操作，这就导致了同步阻塞的问题。其次，在2PC协议中，协调者在阶段二起到非常重要的作用，发生故障将会造成很大影响，这就是单点问题。还有，如果协调者只发送了部分提交消息，此时网络发生异常，那么只有部分参与者接收到提交消息，也就是说只有部分参与者提交了事务，这就导致了数据不一致的问题。

举个例子，假设有三个参与者A、B、C，需要执行一个事务。协调者向A、B、C发送准备命令，A响应成功，B响应失败，C没有响应。在第一阶段结束以后，协调者向A发送回滚命令，A执行回滚操作。这时候B和C还在等待协调者的指令，因为协调者没有超时机制，它会一直等待，导致资源被锁定，进而阻塞了其它操作的执行。

除了同步阻塞和单点故障外，2PC还存在数据不一致的问题。假设在第二阶段，协调者只向A和B发送了提交事务的指令，而C并没有收到，这将导致系统中的数据不一致。

在实际应用中，2PC通常会被替代或者改进，以解决上述问题。比如说，三阶段提交（3PC）就是2PC的改进，它能够解决2PC存在的同步阻塞问题。另外，某些应用可以采用基于消息的解决方案，来实现高可用性和数据一致性。

#### 🎉 XA事务

InnoDB是一个常用的MySQL存储引擎，它提供了对XA事务的支持，这使得分布式事务成为可能。分布式事务是指多个独立的事务资源参与到一个全局事务中，这些资源通常是关系型数据库，但也可以是其他类型的资源。全局事务要求所有参与事务的节点都要提交或回滚，这对于事务原有的ACID要求又有了提高。

XA事务由一个或多个资源管理器（Resource Managers）、一个事务管理器（Transaction Manager）和一个应用程序（Application Program）组成。资源管理器提供访问事务资源的方法，事务管理器协调参与全局事务的各个事务，并与参与全局事务的所有资源管理器进行通信，应用程序定义事务的边界，指定全局事务中的操作。

分布式事务使用两段式提交（two-phase commit）的方式。在第一阶段，所有参与全局事务的节点都开始准备（PREPARE），告诉事务管理器它们准备好提交了。在第二阶段，事务管理器告诉资源管理器执行ROLLBACK还是COMMIT。如果任何一个节点不能提交，则所有的节点都被告知需要回滚。分布式事务与本地事务不同之处在于需要多一次的PREPARE操作，待收到所有节点的同意信息后，再进行COMMIT或是ROLLBACK操作。

在MySQL数据库的分布式事务中，资源管理器就是MySQL数据库，事务管理器为连接MySQL服务器的客户端。最为常见的内部XA事务存在于binlog与InnoDB存储引擎之间。由于复制的需要，因此大多数数据库都开启了binlog功能。在事务提交时，先写二进制日志，再写InnoDB存储引擎的重做日志。这两个操作的要求是原子的，即二进制日志和重做日志必须同时写入。若二进制日志先写了，而在写入InnoDB存储引擎时发生了宕机，那么slave可能会接收到master传过去的二进制日志并执行，最终导致了主从不一致的情况。

举一个银行转账系统的例子，假设David想从他的账户向Maraih的账户转移10 000元。在这个场景中，银行的数据库充当了资源管理器的角色，客户端充当了事务管理器和应用程序的角色。这个全局事务涉及到两个独立的数据库节点，一般分别对应两个不同的银行。

在第一阶段（PREPARE），客户端开始向每个数据库节点发送准备提交的信号。每个节点都需要判断它们的操作是否可行，比如是否有足够的资金可用于转账。如果操作可行，节点会返回一个“同意”信号。否则，节点会返回一个“否决”信号。

在第二阶段，客户端向节点发送COMMIT或ROLLBACK命令。如果所有节点均返回了“同意”信号，那么客户端会发送COMMIT命令。如果有任何一个节点返回了“否决”信号，那么客户端会发送ROLLBACK命令。节点将执行对应的命令，并告知客户端它们的执行结果。

需要注意的是，在分布式事务中，PREPARE操作是非常重要的。如果有任何一个节点无法提交，那么所有节点都必须回滚。这就需要所有节点都能够判断一个操作是否可行，而不仅仅是局限于本地节点。因此，在分布式事务中，PREPARE操作必须是原子的，并且必须考虑到可能出现的网络故障和节点宕机等问题。

在MySQL的分布式事务中，二进制日志和InnoDB存储引擎的重做日志也必须满足原子性。因为这两个日志是用于实现主从复制和故障恢复的核心机制，任何一方的失败都可能导致数据的丢失或不一致。因此，在MySQL的分布式事务中，使用两段式提交来确保二进制日志和InnoDB存储引擎的重做日志是同时写入的。

### 🍊 三阶段提交（3PC）

三阶段提交（3PC）是一种分布式事务协议，相比于二阶段提交（2PC）多了一个阶段并引入了超时机制，从而减少了故障恢复时的复杂性。3PC包含了三个阶段，分别是准备阶段、预提交阶段和提交阶段。其中准备阶段不直接执行事务，而是先询问参与者是否有条件接受该事务，避免资源被无效地锁定。预提交阶段统一了各参与者的状态，使得在之后执行阶段有一个统一的标准。最后的提交阶段才真正执行事务。

例如，假如你是一位参与者，当你知道自己进入了预提交状态，就可以推断出其他参与者也都进入了预提交状态。这个过程使得参与者之间的状态得到统一，从而让协调者知道应该如何执行事务。

相比于2PC，3PC引入了预提交阶段，减少了故障恢复时的复杂性。2PC是同步阻塞的，协调者挂在提交请求还未发出去的时候会导致所有参与者都锁定资源并阻塞等待，严重影响性能。

以一个例子来说明，假设一个系统需要从A节点向B节点转移100元，C节点作为协调者节点。在使用2PC时，协调者节点会依次向A、B节点发送“准备提交”命令，如果所有节点都可以提交，则最后向所有节点发送“提交”命令。但是如果有任何一个节点出现问题，就会导致整个事务无法完成。

3PC的预提交阶段使得参与者之间的状态得到统一，减少了无效的锁定资源，从而避免了性能下降。另外，3PC也克服了2PC的无法处理协调者和参与者同时挂掉的问题，新协调者可以根据参与者当前的状态来执行相应的命令。3PC的缺点在于引入了一个新的阶段，导致性能会降低一些。而且大部分情况下资源都是可用的，这样每次明知可用执行还得询问一次，会浪费一些时间。

3PC是一种更加完善的分布式事务处理方法，可以在一定程度上解决2PC的问题，但也存在一些缺陷。了解这些方法可以让我们更好地处理分布式事务，保证数据一致性。

### 🍊 补偿事务（TCC）

补偿事务（TCC）是一种应用层面上的一致性保证机制，用于在分布式环境下保证事务的原子性。它通过将一个大事务拆成多个小事务，每个小事务都有自己的确认和撤销操作，以此保证整个事务的一致性。

举个例子：假设有一个在线购物网站，用户下单后需要先进行扣款，然后进行物流发货。使用TCC机制，可以将这个大事务拆成两个小事务，即扣款和发货，每个小事务拥有自己的确认和撤销操作。在Try阶段，扣款和发货的操作会预留资源，如果某个操作执行失败，则会在Cancel阶段将资源释放。在Confirm阶段，如果所有操作都执行成功，则可以提交整个事务，否则进行回滚操作。

TCC相对于2PC（两阶段提交）来说，实现和流程相对简单，但数据的一致性可能会稍差一些。

以转账为例，假设 Bob 想向 Smith 转账，我们可以将整个转账过程分为三个阶段：

在 Try 阶段，我们需要调用远程接口将 Smith 和 Bob 的钱冻结起来，即先检查转账是否可行并预留资源。 在 Confirm 阶段，我们执行远程调用的转账操作，并解冻已预留的资源。如果第二步执行成功，则转账成功；否则，我们需要执行 Cancel 阶段。 在 Cancel 阶段，我们需要调用远程冻结接口对应的解冻方法，释放预留资源并取消转账。

相比于 2PC，TCC 的实现比较简单，流程也相对清晰。但是，它的数据一致性不如 2PC。需要在实现时写很多补偿的代码，而在某些业务流程中，TCC 可能不太适合或难以处理。

总之，TCC机制是一种针对分布式环境下的一致性保障技术，可以将大事务拆成多个小事务，每个小事务都有自己的确认和撤销操作，以此来保证整个事务的原子性。

### 🍊 MQ事务消息

MQ事务消息是指在消息发送和本地事务执行中间加入了一次消息确认的过程，通过三个阶段的操作确保了消息和本地事务的同时成功或同时失败，并达到最终一致性的效果。其中，第一阶段是消息发送，第二阶段是本地事务执行，第三阶段是消息确认。这种方式需要在业务方法内提交两次请求，一次发送消息，一次确认消息。如果确认消息发送失败，则RocketMQ会定期扫描消息集群中的事务消息来确认，从而保证消息发送和本地事务同时成功或同时失败。

举个例子，假设我们要在一个电商平台上创建一个订单，订单信息需要发送到MQ，并在本地生成一笔订单记录。如果发送消息成功，但是本地事务执行失败，那么订单记录就会丢失，而如果本地事务执行成功，但是消息发送失败，MQ中也没有对应的订单记录。这时候，我们就可以使用MQ事务消息来解决这个问题，保证订单记录和MQ消息的同时成功或同时失败。在订单创建的业务方法中，我们需要分别完成订单记录的创建和消息的发送，并在业务方法执行结束之前提交消息确认请求。

MQ事务消息的优点在于实现了最终一致性，不需要依赖本地数据库事务，但是缺点也很明显，实现难度大，而且市面上许多主流MQ都不支持事务消息，比如RabbitMQ和Kafka。不过，一些第三方的MQ，如RocketMQ，支持事务消息的方式也是类似于采用的二阶段提交，这样就能够在一定程度上满足业务的需求。

假设小明在网上购物，他将商品添加到购物车后，点击“结算”按钮，页面跳转到支付界面，同时系统会发送一条消息到MQ中，告诉库存系统和订单系统小明购买了哪些商品，并将相应的库存锁定。

如果MQ不支持事务消息，每个系统将分别处理消息。库存系统会锁定库存，发现库存不足时抛出异常，并将消息撤回；订单系统会创建订单，如果由于某种原因创建订单失败，就不会发送撤回消息。这样就会出现库存系统和订单系统之间数据不一致的情况，例如库存被锁定，但是订单未生成。

如果MQ支持事务消息，小明在支付界面点击“确认支付”按钮后，MQ先发送一条Prepared消息给MQ客户端，并返回消息的地址。此时库存系统和订单系统暂时不做任何处理。小明的支付请求会被当做本地事务执行，如果支付成功，库存和订单数据就可以被提交；如果支付失败，数据就会回滚，库存也会被解锁。当库存和订单数据提交或回滚后，MQ客户端就会根据Prepared消息的地址来访问消息，并修改消息的状态。如果修改成功，消息就被提交，否则就会撤回消息并重新处理。

通过这个例子，我们可以看出MQ事务消息的优点在于实现了最终一致性，避免了数据不一致的情况。但是相对于普通消息来说，MQ事务消息的实现难度更大，需要发送方实现check接口来保证消息的正确性，而且市面上一些主流MQ如RabbitMQ和Kafka并不支持事务消息。

### 🍊 最大努力通知

最大努力通知，其实就是一种柔性事务的思想，适用于一些对时间不敏感的业务，例如短信通知。在这种情况下，我们可以尽力去完成事务的最终一致性，但是不能保证百分之百的成功，我们只能尽最大的努力去完成。

比如，我们在向用户发送短信通知时，但是由于网络原因或其他不可预知的情况导致发送失败，这时候我们可以使用最大努力通知的方式，多次尝试发送，直到达到一定的次数或者成功为止。如果还是无法成功，我们可以记录下来，并引入人工处理，或者直接舍弃。这样，我们可以最大限度地保证尽可能多的用户能够收到通知。

具体来说，我们可以将要发送的短信消息存储到本地消息表中，并定时调用对应的服务来进行发送。如果发送失败了，我们可以记录下来，并在后续的任务中再次尝试发送，直到发送成功为止。如果多次尝试仍然无法发送成功，我们可以选择人工介入或直接舍弃该消息。

这种方法的优点是实现简单，适用范围广，对于一些对时间不敏感的业务来说是一种非常好的选择。相比于强一致性事务和补偿性事务，最大努力通知更加灵活，可以在业务层面实现，对业务侵入性较小。

最大努力通知并不是唯一的解决方案，与之相对比的包括：2PC、3PC和TCC。2PC和3PC是一种强一致性事务，但是仍然存在数据不一致和阻塞等风险，而且只能用于数据库层面的事务处理。TCC是一种补偿性事务思想，可以在业务层面实现，但是对业务的侵入性较大，每个操作都需要实现对应的三个方法。

相比之下，本地消息、事务消息和最大努力通知都是最终一致性事务，适用于一些对时间不敏感的业务。其中，本地消息表会有后台任务定时去查看未完成的消息，然后去调用对应的服务，当一个消息多次调用都失败的时候可以记录下然后引入人工，或者直接舍弃。事务消息在半消息被commit后，如果订阅者一直不消费或消费不了，则会一直重试，直到进入死信队列。这些方案都强调了尽最大的努力去完成事务的最终一致性，而不是百分之百的保证成功，保证了业务的灵活性和可靠性。

### 参考资料

https://developer.aliyun.com/article/1408686

---

---
url: /Java/架构设计/分布式/04.分布式事务/2_分布式事务框架TX-LCN.md
---

# 分布式事务框架TX-LCN

分库分表环境下Seata无法正常工作，使用TX-LCN框架代替

参考资料：

\[1]. https://blog.csdn.net/qq\_42588990/article/details/121774173

---

---
url: /Java/架构设计/分布式/04.分布式事务/1_分布式事务Seata.md
---

# 1.分布式事务问题

## 1.1.本地事务

本地事务，也就是传统的**单机事务**。在传统数据库事务中，必须要满足四个原则：

![image-20210724165045186](/assets/image-20210724165045186.8HeThJHZ.png)

## 1.2.分布式事务

**分布式事务**，就是指不是在单个服务或单个数据库架构下，产生的事务，例如：

* 跨数据源的分布式事务
* 跨服务的分布式事务
* 综合情况

在数据库水平拆分、服务垂直拆分之后，一个业务操作通常要跨多个数据库、服务才能完成。例如电商行业中比较常见的下单付款案例，包括下面几个行为：

* 创建新订单
* 扣减商品库存
* 从用户账户余额扣除金额

完成上面的操作需要访问三个不同的微服务和三个不同的数据库。

![image-20210724165338958](/assets/image-20210724165338958.5b6YFle4.png)

订单的创建、库存的扣减、账户扣款在每一个服务和数据库内是一个本地事务，可以保证ACID原则。

但是当我们把三件事情看做一个"业务"，要满足保证“业务”的原子性，要么所有操作全部成功，要么全部失败，不允许出现部分成功部分失败的现象，这就是**分布式系统下的事务**了。

此时ACID难以满足，这是分布式事务要解决的问题

## 1.3.演示分布式事务问题

我们通过一个案例来演示分布式事务的问题：

1）**创建数据库，名为seata\_demo，然后导入课前资料提供的SQL文件：**

![image-20210724165634571](assets/image-20210724165634571.png)

2）**导入课前资料提供的微服务：**

![image-20210724165709994](/assets/image-20210724165709994.HcPFexaG.png)

微服务结构如下：

![image-20210724165729273](/assets/image-20210724165729273.B-b71Yld.png)

其中：

seata-demo：父工程，负责管理项目依赖

* account-service：账户服务，负责管理用户的资金账户。提供扣减余额的接口
* storage-service：库存服务，负责管理商品库存。提供扣减库存的接口
* order-service：订单服务，负责管理订单。创建订单时，需要调用account-service和storage-service

**3）启动nacos、所有微服务**

**4）测试下单功能，发出Post请求：**

请求如下：

```sh
curl --location --request POST 'http://localhost:8082/order?userId=user202103032042012&commodityCode=100202003032041&count=20&money=200'
```

如图：

![image-20210724170113404](/assets/image-20210724170113404.Q_2xJYHh.png)

测试发现，当库存不足时，如果余额已经扣减，并不会回滚，出现了分布式事务问题。

# 2.理论基础

解决分布式事务问题，需要一些分布式系统的基础知识作为理论指导。

## 2.1.CAP定理

1998年，加州大学的计算机科学家 Eric Brewer 提出，分布式系统有三个指标。

> * Consistency（一致性）
> * Availability（可用性）
> * Partition tolerance （分区容错性）

![image-20210724170517944](/assets/image-20210724170517944.DwjGoBSE.png)

它们的第一个字母分别是 C、A、P。

Eric Brewer 说，这三个指标不可能同时做到。这个结论就叫做 CAP 定理。

### 2.1.1.一致性

Consistency（一致性）：用户访问分布式系统中的任意节点，得到的数据必须一致。

比如现在包含两个节点，其中的初始数据是一致的：

![image-20210724170704694](/assets/image-20210724170704694.QBbI1A9p.png)

当我们修改其中一个节点的数据时，两者的数据产生了差异：

![image-20210724170735847](/assets/image-20210724170735847.CMl13nb8.png)

要想保住一致性，就必须实现node01 到 node02的数据 同步：

![image-20210724170834855](/assets/image-20210724170834855.COFOaZft.png)

### 2.1.2.可用性

Availability （可用性）：用户访问集群中的任意健康节点，必须能得到响应，而不是超时或拒绝。

如图，有三个节点的集群，访问任何一个都可以及时得到响应：

![image-20210724170932072](/assets/image-20210724170932072.C3rZPZ1F.png)

当有部分节点因为网络故障或其它原因无法访问时，代表节点不可用：

![image-20210724171007516](/assets/image-20210724171007516.ETfmeAKZ.png)

### 2.1.3.分区容错

**Partition（分区）**：因为网络故障或其它原因导致分布式系统中的部分节点与其它节点失去连接，形成独立分区。

![image-20210724171041210](/assets/image-20210724171041210.BTPDQAwJ.png)

**Tolerance（容错）**：在集群出现分区时，整个系统也要持续对外提供服务

### 2.1.4.矛盾

在分布式系统中，系统间的网络不能100%保证健康，一定会有故障的时候，而服务有必须对外保证服务。因此Partition Tolerance不可避免。

当节点接收到新的数据变更时，就会出现问题了：

![image-20210724171546472](/assets/image-20210724171546472.BTXO5gnJ.png)

如果此时要保证**一致性**，就必须等待网络恢复，完成数据同步后，整个集群才对外提供服务，服务处于阻塞状态，不可用。

如果此时要保证**可用性**，就不能等待网络恢复，那node01、node02与node03之间就会出现数据不一致。

也就是说，在P一定会出现的情况下，A和C之间只能实现一个。

## 2.2.BASE理论

BASE理论是对CAP的一种解决思路，包含三个思想：

* **Basically Available** **（基本可用）**：分布式系统在出现故障时，允许损失部分可用性，即保证核心可用。
* \*\*Soft State（软状态）：\*\*在一定时间内，允许出现中间状态，比如临时的不一致状态。
* **Eventually Consistent（最终一致性）**：虽然无法保证强一致性，但是在软状态结束后，最终达到数据一致。

## 2.3.解决分布式事务的思路

分布式事务最大的问题是各个子事务的一致性问题，因此可以借鉴CAP定理和BASE理论，有两种解决思路：

* AP模式：各子事务分别执行和提交，允许出现结果不一致，然后采用弥补措施恢复数据即可，实现最终一致。

* CP模式：各个子事务执行后互相等待，同时提交，同时回滚，达成强一致。但事务等待过程中，处于弱可用状态。

但不管是哪一种模式，都需要在子系统事务之间互相通讯，协调事务状态，也就是需要一个**事务协调者(TC)**：

![image-20210724172123567](/assets/image-20210724172123567.QXCIftFj.png)

这里的子系统事务，称为**分支事务**；有关联的各个分支事务在一起称为**全局事务**。

# 3.初识Seata

Seata是 2019 年 1 月份蚂蚁金服和阿里巴巴共同开源的分布式事务解决方案。致力于提供高性能和简单易用的分布式事务服务，为用户打造一站式的分布式解决方案。

官网地址：http://seata.io/，其中的文档、播客中提供了大量的使用说明、源码分析。

![image-20210724172225817](/assets/image-20210724172225817.DQI8DdG5.png)

## 3.1.Seata的架构

Seata事务管理中有三个重要的角色：

* **TC (Transaction Coordinator) -** \*\*事务协调者：\*\*维护全局和分支事务的状态，协调全局事务提交或回滚。

* **TM (Transaction Manager) -** \*\*事务管理器：\*\*定义全局事务的范围、开始全局事务、提交或回滚全局事务。

* **RM (Resource Manager) -** \*\*资源管理器：\*\*管理分支事务处理的资源，与TC交谈以注册分支事务和报告分支事务的状态，并驱动分支事务提交或回滚。

整体的架构如图：

![image-20210724172326452](/assets/image-20210724172326452.BKjU-fib.png)

Seata基于上述架构提供了四种不同的分布式事务解决方案：

* XA模式：强一致性分阶段事务模式，牺牲了一定的可用性，无业务侵入
* TCC模式：最终一致的分阶段事务模式，有业务侵入
* AT模式：最终一致的分阶段事务模式，无业务侵入，也是Seata的默认模式
* SAGA模式：长事务模式，有业务侵入

无论哪种方案，都离不开TC，也就是事务的协调者。

## 3.2.部署TC服务

参考课前资料提供的文档《 seata的部署和集成.md 》：

![image-20210724172549013](/assets/image-20210724172549013.0uSxUVu0.png)

## 3.3.微服务集成Seata

我们以order-service为例来演示。

### 3.3.1.引入依赖

首先，在order-service中引入依赖：

```xml
<!--seata-->
<dependency>
    <groupId>com.alibaba.cloud</groupId>
    <artifactId>spring-cloud-starter-alibaba-seata</artifactId>
    <exclusions>
        <!--版本较低，1.3.0，因此排除--> 
        <exclusion>
            <artifactId>seata-spring-boot-starter</artifactId>
            <groupId>io.seata</groupId>
        </exclusion>
    </exclusions>
</dependency>
<dependency>
    <groupId>io.seata</groupId>
    <artifactId>seata-spring-boot-starter</artifactId>
    <!--seata starter 采用1.4.2版本-->
    <version>${seata.version}</version>
</dependency>
```

### 3.3.2.配置TC地址

在order-service中的application.yml中，配置TC服务信息，通过注册中心nacos，结合服务名称获取TC地址：

```yaml
seata:
  registry: # TC服务注册中心的配置，微服务根据这些信息去注册中心获取tc服务地址
    type: nacos # 注册中心类型 nacos
    nacos:
      server-addr: 127.0.0.1:8848 # nacos地址
      namespace: "" # namespace，默认为空
      group: DEFAULT_GROUP # 分组，默认是DEFAULT_GROUP
      application: seata-tc-server # seata服务名称
      username: nacos
      password: nacos
  tx-service-group: seata-demo # 事务组名称
  service:
    vgroup-mapping: # 事务组与cluster的映射关系
      seata-demo: SH
```

微服务如何根据这些配置寻找TC的地址呢？

我们知道注册到Nacos中的微服务，确定一个具体实例需要四个信息：

* namespace：命名空间
* group：分组
* application：服务名
* cluster：集群名

以上四个信息，在刚才的yaml文件中都能找到：

![image-20210724173654258](/assets/image-20210724173654258.VuReJklG.png)

namespace为空，就是默认的public

结合起来，TC服务的信息就是：public@DEFAULT\_GROUP@seata-tc-server@SH，这样就能确定TC服务集群了。然后就可以去Nacos拉取对应的实例信息了。

### 3.3.3.其它服务

其它两个微服务也都参考order-service的步骤来做，完全一样。

# 4.动手实践

下面我们就一起学习下Seata中的四种不同的事务模式。

## 4.1.XA模式

XA 规范 是 X/Open 组织定义的分布式事务处理（DTP，Distributed Transaction Processing）标准，XA 规范 描述了全局的TM与局部的RM之间的接口，几乎所有主流的数据库都对 XA 规范 提供了支持。

### 4.1.1.两阶段提交

XA是规范，目前主流数据库都实现了这种规范，实现的原理都是基于两阶段提交。

正常情况：

![image-20210724174102768](/assets/image-20210724174102768.CsgEXR0Y.png)

异常情况：

![image-20210724174234987](/assets/image-20210724174234987.CVf75nw-.png)

一阶段：

* 事务协调者通知每个事物参与者执行本地事务
* 本地事务执行完成后报告事务执行状态给事务协调者，此时事务不提交，继续持有数据库锁

二阶段：

* 事务协调者基于一阶段的报告来判断下一步操作
  * 如果一阶段都成功，则通知所有事务参与者，提交事务
  * 如果一阶段任意一个参与者失败，则通知所有事务参与者回滚事务

### 4.1.2.Seata的XA模型

Seata对原始的XA模式做了简单的封装和改造，以适应自己的事务模型，基本架构如图：

![image-20210724174424070](/assets/image-20210724174424070.BduFtlbA.png)

RM一阶段的工作：

​	① 注册分支事务到TC

​	② 执行分支业务sql但不提交

​	③ 报告执行状态到TC

TC二阶段的工作：

* TC检测各分支事务执行状态

  a.如果都成功，通知所有RM提交事务

  b.如果有失败，通知所有RM回滚事务

RM二阶段的工作：

* 接收TC指令，提交或回滚事务

### 4.1.3.优缺点

XA模式的优点是什么？

* 事务的强一致性，满足ACID原则。
* 常用数据库都支持，实现简单，并且没有代码侵入

XA模式的缺点是什么？

* 因为一阶段需要锁定数据库资源，等待二阶段结束才释放，性能较差
* 依赖关系型数据库实现事务

### 4.1.4.实现XA模式

Seata的starter已经完成了XA模式的自动装配，实现非常简单，步骤如下：

1）修改application.yml文件（每个参与事务的微服务），开启XA模式：

```yaml
seata:
  data-source-proxy-mode: XA
```

2）给发起全局事务的入口方法添加@GlobalTransactional注解:

本例中是OrderServiceImpl中的create方法.

![image-20210724174859556](/assets/image-20210724174859556.ZNbImVSD.png)

3）重启服务并测试

重启order-service，再次测试，发现无论怎样，三个微服务都能成功回滚。

## 4.2.AT模式

AT模式同样是分阶段提交的事务模型，不过缺弥补了XA模型中资源锁定周期过长的缺陷。

### 4.2.1.Seata的AT模型

基本流程图：

![image-20210724175327511](/assets/image-20210724175327511.CAnADAQ2.png)

阶段一RM的工作：

* 注册分支事务
* 记录undo-log（数据快照）
* 执行业务sql并提交
* 报告事务状态

阶段二提交时RM的工作：

* 删除undo-log即可

阶段二回滚时RM的工作：

* 根据undo-log恢复数据到更新前

### 4.2.2.流程梳理

我们用一个真实的业务来梳理下AT模式的原理。

比如，现在又一个数据库表，记录用户余额：

| **id** | **money** |
| ------ | --------- |
| 1      | 100       |

其中一个分支业务要执行的SQL为：

```sql
update tb_account set money = money - 10 where id = 1
```

AT模式下，当前分支事务执行流程如下：

一阶段：

1）TM发起并注册全局事务到TC

2）TM调用分支事务

3）分支事务准备执行业务SQL

4）RM拦截业务SQL，根据where条件查询原始数据，形成快照。

```json
{
    "id": 1, "money": 100
}
```

5）RM执行业务SQL，提交本地事务，释放数据库锁。此时 `money = 90`

6）RM报告本地事务状态给TC

二阶段：

1）TM通知TC事务结束

2）TC检查分支事务状态

​	 a）如果都成功，则立即删除快照

​	 b）如果有分支事务失败，需要回滚。读取快照数据（`{"id": 1, "money": 100}`），将快照恢复到数据库。此时数据库再次恢复为100

流程图：

![image-20210724180722921](/assets/image-20210724180722921.CynjW32v.png)

### 4.2.3.AT与XA的区别

简述AT模式与XA模式最大的区别是什么？

* XA模式一阶段不提交事务，锁定资源；AT模式一阶段直接提交，不锁定资源。
* XA模式依赖数据库机制实现回滚；AT模式利用数据快照实现数据回滚。
* XA模式强一致；AT模式最终一致

### 4.2.4.脏写问题

在多线程并发访问AT模式的分布式事务时，有可能出现脏写问题，如图：

![image-20210724181541234](/assets/image-20210724181541234.DGpHG04y.png)

解决思路就是引入了全局锁的概念。在释放DB锁之前，先拿到全局锁。避免同一时刻有另外一个事务来操作当前数据。

![image-20210724181843029](/assets/image-20210724181843029.s55pLRew.png)

### 4.2.5.优缺点

AT模式的优点：

* 一阶段完成直接提交事务，释放数据库资源，性能比较好
* 利用全局锁实现读写隔离
* 没有代码侵入，框架自动完成回滚和提交

AT模式的缺点：

* 两阶段之间属于软状态，属于最终一致
* 框架的快照功能会影响性能，但比XA模式要好很多

### 4.2.6.实现AT模式

AT模式中的快照生成、回滚等动作都是由框架自动完成，没有任何代码侵入，因此实现非常简单。

只不过，AT模式需要一个表来记录全局锁、另一张表来记录数据快照undo\_log。

1）导入数据库表，记录全局锁

导入课前资料提供的Sql文件：seata-at.sql，其中lock\_table导入到TC服务关联的数据库，undo\_log表导入到微服务关联的数据库：

![image-20210724182217272](/assets/image-20210724182217272.hSNtLVlN.png)

2）修改application.yml文件，将事务模式修改为AT模式即可：

```yaml
seata:
  data-source-proxy-mode: AT # 默认就是AT
```

3）重启服务并测试

## 4.3.TCC模式

TCC模式与AT模式非常相似，每阶段都是独立事务，不同的是TCC通过人工编码来实现数据恢复。需要实现三个方法：

* Try：资源的检测和预留；

* Confirm：完成资源操作业务；要求 Try 成功 Confirm 一定要能成功。

* Cancel：预留资源释放，可以理解为try的反向操作。

### 4.3.1.流程分析

举例，一个扣减用户余额的业务。假设账户A原来余额是100，需要余额扣减30元。

* **阶段一（ Try ）**：检查余额是否充足，如果充足则冻结金额增加30元，可用余额扣除30

初识余额：

![image-20210724182424907](/assets/image-20210724182424907.qfivFLu6.png)

余额充足，可以冻结：

![image-20210724182457951](/assets/image-20210724182457951.DfheSRSd.png)

此时，总金额 = 冻结金额 + 可用金额，数量依然是100不变。事务直接提交无需等待其它事务。

* **阶段二（Confirm)**：假如要提交（Confirm），则冻结金额扣减30

确认可以提交，不过之前可用金额已经扣减过了，这里只要清除冻结金额就好了：

![image-20210724182706011](/assets/image-20210724182706011.DhB0isDN.png)

此时，总金额 = 冻结金额 + 可用金额 = 0 + 70  = 70元

* **阶段二(Canncel)**：如果要回滚（Cancel），则冻结金额扣减30，可用余额增加30

需要回滚，那么就要释放冻结金额，恢复可用金额：

![image-20210724182810734](/assets/image-20210724182810734.BHIc9cQ7.png)

### 4.3.2.Seata的TCC模型

Seata中的TCC模型依然延续之前的事务架构，如图：

![image-20210724182937713](/assets/image-20210724182937713.9orLLfK9.png)

### 4.3.3.优缺点

TCC模式的每个阶段是做什么的？

* Try：资源检查和预留
* Confirm：业务执行和提交
* Cancel：预留资源的释放

TCC的优点是什么？

* 一阶段完成直接提交事务，释放数据库资源，性能好
* 相比AT模型，无需生成快照，无需使用全局锁，性能最强
* 不依赖数据库事务，而是依赖补偿操作，可以用于非事务型数据库

TCC的缺点是什么？

* 有代码侵入，需要人为编写try、Confirm和Cancel接口，太麻烦
* 软状态，事务是最终一致
* 需要考虑Confirm和Cancel的失败情况，做好幂等处理

### 4.3.4.事务悬挂和空回滚

#### 1）空回滚

当某分支事务的try阶段**阻塞**时，可能导致全局事务超时而触发二阶段的cancel操作。在未执行try操作时先执行了cancel操作，这时cancel不能做回滚，就是**空回滚**。

如图：

![image-20210724183426891](/assets/image-20210724183426891.B014u-7r.png)

执行cancel操作时，应当判断try是否已经执行，如果尚未执行，则应该空回滚。

#### 2）业务悬挂

对于已经空回滚的业务，之前被阻塞的try操作恢复，继续执行try，就永远不可能confirm或cancel ，事务一直处于中间状态，这就是**业务悬挂**。

执行try操作时，应当判断cancel是否已经执行过了，如果已经执行，应当阻止空回滚后的try操作，避免悬挂

### 4.3.5.实现TCC模式

解决空回滚和业务悬挂问题，必须要记录当前事务状态，是在try、还是cancel？

#### 1）思路分析

这里我们定义一张表：

```sql
CREATE TABLE `account_freeze_tbl` (
  `xid` varchar(128) NOT NULL,
  `user_id` varchar(255) DEFAULT NULL COMMENT '用户id',
  `freeze_money` int(11) unsigned DEFAULT '0' COMMENT '冻结金额',
  `state` int(1) DEFAULT NULL COMMENT '事务状态，0:try，1:confirm，2:cancel',
  PRIMARY KEY (`xid`) USING BTREE
) ENGINE=InnoDB DEFAULT CHARSET=utf8 ROW_FORMAT=COMPACT;

```

其中：

* xid：是全局事务id
* freeze\_money：用来记录用户冻结金额
* state：用来记录事务状态

那此时，我们的业务开怎么做呢？

* Try业务：
  * 记录冻结金额和事务状态到account\_freeze表
  * 扣减account表可用金额
* Confirm业务
  * 根据xid删除account\_freeze表的冻结记录
* Cancel业务
  * 修改account\_freeze表，冻结金额为0，state为2
  * 修改account表，恢复可用金额
* 如何判断是否空回滚？
  * cancel业务中，根据xid查询account\_freeze，如果为null则说明try还没做，需要空回滚
* 如何避免业务悬挂？
  * try业务中，根据xid查询account\_freeze ，如果已经存在则证明Cancel已经执行，拒绝执行try业务

接下来，我们改造account-service，利用TCC实现余额扣减功能。

#### 2）声明TCC接口

TCC的Try、Confirm、Cancel方法都需要在接口中基于注解来声明，

我们在account-service项目中的`cn.itcast.account.service`包中新建一个接口，声明TCC三个接口：

```java
package cn.itcast.account.service;

import io.seata.rm.tcc.api.BusinessActionContext;
import io.seata.rm.tcc.api.BusinessActionContextParameter;
import io.seata.rm.tcc.api.LocalTCC;
import io.seata.rm.tcc.api.TwoPhaseBusinessAction;

@LocalTCC
public interface AccountTCCService {

    @TwoPhaseBusinessAction(name = "deduct", commitMethod = "confirm", rollbackMethod = "cancel")
    void deduct(@BusinessActionContextParameter(paramName = "userId") String userId,
                @BusinessActionContextParameter(paramName = "money")int money);

    boolean confirm(BusinessActionContext ctx);

    boolean cancel(BusinessActionContext ctx);
}
```

#### 3）编写实现类

在account-service服务中的`cn.itcast.account.service.impl`包下新建一个类，实现TCC业务：

```java
package cn.itcast.account.service.impl;

import cn.itcast.account.entity.AccountFreeze;
import cn.itcast.account.mapper.AccountFreezeMapper;
import cn.itcast.account.mapper.AccountMapper;
import cn.itcast.account.service.AccountTCCService;
import io.seata.core.context.RootContext;
import io.seata.rm.tcc.api.BusinessActionContext;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

@Service
@Slf4j
public class AccountTCCServiceImpl implements AccountTCCService {

    @Autowired
    private AccountMapper accountMapper;
    @Autowired
    private AccountFreezeMapper freezeMapper;

    @Override
    @Transactional
    public void deduct(String userId, int money) {
        // 0.获取事务id
        String xid = RootContext.getXID();
        // 1.扣减可用余额
        accountMapper.deduct(userId, money);
        // 2.记录冻结金额，事务状态
        AccountFreeze freeze = new AccountFreeze();
        freeze.setUserId(userId);
        freeze.setFreezeMoney(money);
        freeze.setState(AccountFreeze.State.TRY);
        freeze.setXid(xid);
        freezeMapper.insert(freeze);
    }

    @Override
    public boolean confirm(BusinessActionContext ctx) {
        // 1.获取事务id
        String xid = ctx.getXid();
        // 2.根据id删除冻结记录
        int count = freezeMapper.deleteById(xid);
        return count == 1;
    }

    @Override
    public boolean cancel(BusinessActionContext ctx) {
        // 0.查询冻结记录
        String xid = ctx.getXid();
        AccountFreeze freeze = freezeMapper.selectById(xid);

        // 1.恢复可用余额
        accountMapper.refund(freeze.getUserId(), freeze.getFreezeMoney());
        // 2.将冻结金额清零，状态改为CANCEL
        freeze.setFreezeMoney(0);
        freeze.setState(AccountFreeze.State.CANCEL);
        int count = freezeMapper.updateById(freeze);
        return count == 1;
    }
}
```

## 4.4.SAGA模式

Saga 模式是 Seata 即将开源的长事务解决方案，将由蚂蚁金服主要贡献。

其理论基础是Hector & Kenneth  在1987年发表的论文[Sagas](https://microservices.io/patterns/data/saga.html)。

Seata官网对于Saga的指南：https://seata.io/zh-cn/docs/user/saga.html

### 4.4.1.原理

在 Saga 模式下，分布式事务内有多个参与者，每一个参与者都是一个冲正补偿服务，需要用户根据业务场景实现其正向操作和逆向回滚操作。

分布式事务执行过程中，依次执行各参与者的正向操作，如果所有正向操作均执行成功，那么分布式事务提交。如果任何一个正向操作执行失败，那么分布式事务会去退回去执行前面各参与者的逆向回滚操作，回滚已提交的参与者，使分布式事务回到初始状态。

![image-20210724184846396](/assets/image-20210724184846396.DvoefU5D.png)

Saga也分为两个阶段：

* 一阶段：直接提交本地事务
* 二阶段：成功则什么都不做；失败则通过编写补偿业务来回滚

### 4.4.2.优缺点

优点：

* 事务参与者可以基于事件驱动实现异步调用，吞吐高
* 一阶段直接提交事务，无锁，性能好
* 不用编写TCC中的三个阶段，实现简单

缺点：

* 软状态持续时间不确定，时效性差
* 没有锁，没有事务隔离，会有脏写

## 4.5.四种模式对比

我们从以下几个方面来对比四种实现：

* 一致性：能否保证事务的一致性？强一致还是最终一致？
* 隔离性：事务之间的隔离性如何？
* 代码侵入：是否需要对业务代码改造？
* 性能：有无性能损耗？
* 场景：常见的业务场景

如图：

![image-20210724185021819](/assets/image-20210724185021819.DbOAMyzZ.png)

# 5.高可用

Seata的TC服务作为分布式事务核心，一定要保证集群的高可用性。

## 5.1.高可用架构模型

搭建TC服务集群非常简单，启动多个TC服务，注册到nacos即可。

但集群并不能确保100%安全，万一集群所在机房故障怎么办？所以如果要求较高，一般都会做异地多机房容灾。

比如一个TC集群在上海，另一个TC集群在杭州：

![image-20210724185240957](/assets/image-20210724185240957.C9lx7e5U.png)

微服务基于事务组（tx-service-group)与TC集群的映射关系，来查找当前应该使用哪个TC集群。当SH集群故障时，只需要将vgroup-mapping中的映射关系改成HZ。则所有微服务就会切换到HZ的TC集群了。

## 5.2.实现高可用

具体实现请参考课前资料提供的文档《seata的部署和集成.md》：

![image-20210724172549013](/assets/image-20210724172549013.0uSxUVu0.png)

第三章节：

![image-20210724185638729](/assets/image-20210724185638729.DKBFrwdQ.png)

---

---
url: /Java/架构设计/分布式/06.分布式监控/0_可观测性.md
---

# 分布式系统中的可观测性

## 一、基本概念

### 分布式监控（Monitoring）

在微服务架构下，一次用户调用会因为服务化拆分后，变成多个不同服务之间的相互调用，这也就需要对拆分后的每个服务都监控起来。

通过收集和分析各种指标数据来实现，例如CPU、内存、网络、磁盘等硬件指标，以及应用程序的请求、响应、日志等软件指标。通过监控这些指标，可以及时发现并解决系统的性能问题、故障和安全事件，保证系统的稳定运行和高效运作。

### 可观测性（Observability）

可观测性（Observability）指通过系统的外部输出推断其内部状态的能力。在软件开发中，可观测性是指通过日志、指标和追踪等数据，全面了解系统的运行状况，**以便及时发现和解决问题**。监控是针对系统健康状况的常规监护，而可观测则是对系统深层次内在逻辑的理解与解读，它通过获取更多元、更丰富的数据来提升系统管理和维护的效率与准确性。

可以简单将其理解为 “监控”，但它又比监控的概念更广更深，推荐阅读 [大厂文档](https://www.aliyun.com/getting-started/what-is/what-is-observability) 来进一步了解。

## 二、相关概念

### 维度与指标

维度（Dimension）是用‌来描述和分类数据的标‍签属性，比如用户 ID、应用 ID、模型‍名称等，关注 “是什么”。

指标（Metric）是用来量‌化的数值数据，比如‍请求次数、响应时间‍、Token 消耗‍量等，关注 “有多少”。

简单来说，维度是可以用来筛选的标签，指标是用来计算的数值。

举个例子：

* 维度：user\_id=12345, app\_id=67890, model\_name=deepseek-chat
* 指标：requests\_total=100, response\_time=1.5s, tokens\_used=2000

### 监控的数据分类

在实现可观测性时，我们需要关注多种不同类型的数据：

1）系统指‍标：包括 CPU ‌使用率、内存占用、‍磁盘 I/O、网络‍流量等基础设施层面的‍监控数据。

2）应用指‍标：涵盖接口响应时‌间、QPS（每秒查‍询率）、错误率、J‍VM 状态等应用层面的‍性能数据。HrGXUqFhjK7wNHkLfEF5cglTdzhFLxXIforC6kfYOak=

3）业务指‍标：针对我们平台的‌特定业务逻辑，比如‍ AI 模型调用次‍数、Token 消‍耗量、用户活跃度等。

4）调用链：在分布式系统中，一个请求可能经过多个服务组件。**Trace** 表示一个完整请求的调用链路，而 **Span** 则代表调用链中的一个操作单元。通过分析 Trace 和 Span，我们可以清晰地看到请求在系统中的流转过程，快速定位性能瓶颈。

![img](/assets/Ff74D8svT0dnLCTI.BRlsw4_K.webp)

### **百分位数**

在性能监控中，我们经常会看到 P50、P75、P90、P99 这些指标，它们被称为 **百分位数**。

* P50：中位数，表示 50% 的请求响应时间都在这个值以下
* P75：75% 的请求响应时间都在这个值以下
* P90：90% 的请求响应时间都在这个值以下
* P99：99% 的请求响应时间都在这个值以下

举个例子，如果一个接口‍的 P99 响应时间是 500ms，这意味着‌ 99% 的请求都能在 500ms 内完成，‍只有 1% 的请求可能超过这个时间。P99 ‍指标对于发现系统中的异常情况特别有用，因为它‍能反映出那些偶发的长尾延迟问题。

![img](/assets/V175yWyRWRI20B7K.A6zy1zBd.webp)

💡 注意，别把这玩意跟优先级 P0、P1、P2 搞混了。

## 三、怎么实现

要构建完善的可观测性体系，我们需要解决几个核心问题：

1）统计什么？

需要根据业务特点确定关键指标，既要覆盖 **系统层面** 的通用指标，也要包含 **业务特有** 的监控维度。

2）如何收集？

数据收集是‍可观测性的基础，可‌以通过代码埋点、探‍针技术、日志分析等‍多种方式实现。

3）如何存储？

监控数据通‍常量大且连续，需要‌选择合适的存储方案‍，比如时序数据库或‍专门的监控系统。

4）如何展示？

最终需要通‍过直观的图表和仪表‌板将数据呈现给用户‍，一般会实时监控（‍页面自动刷新）。

### 系统指标监控方式

1. 利用 AR‍MS 平台进行系统指标监控‌：这是一种开箱即用的方案，‍通过集成阿里云 ARMS ‍等监控平台，可以快速获得系‍统层面的全面监控能力。

2. 利用 Prometheus + Grafana 自定义业务指标监控：这是目前最主流的开源监控方案，提供了强大的自定义能力和丰富的生态支持。

   **💡** 其实大多数非企业级的项目，直接利用数据库和日志进行业务指标统计也足够了。毕竟多接入一个系统，就多一份成本。

3. Spring 自带的监控工具

   * 使用 Spring Boot Actuator：Spring Boot Actuator 是 Spring Boot 提供的一个监控和管理 Spring Boot 应用的工具。提供了丰富的端点，包括健康检查、性能指标、日志等，可以通过 HTTP 访问这些端点来获取应用的状态信息。

   * 使用 Spring Cloud Sleuth：Spring Cloud Sleuth 是 Spring Cloud 提供的一个分布式跟踪解决方案，它可以帮助我们跟踪微服务调用的路径、耗时等信息。通过集成 Sleuth，可以获取到微服务之间调用的链路信息，从而进行性能分析和故障排查。

   * 使用 Spring Cloud Metrics：Spring Cloud Metrics 是 Spring Cloud 提供的一个度量监控框架，它可以帮助我们获取微服务应用的度量指标，如 CPU 使用率、内存使用率、线程池信息等。通过收集和展示这些度量指标，可以更好地了解微服务应用的运行状态和性能瓶颈。

## 参考资料

如何监控微服务调用：https://blog.csdn.net/qq\_37756660/article/details/134860073

分布式架构的监控与指标：https://cloud.tencent.com/developer/article/2375812

可观测性与监控的区别：https://zhuanlan.zhihu.com/p/469580402

各种工具调用链对比：https://blog.csdn.net/u010191034/article/details/131474830

各种工具调用链对比：https://blog.51cto.com/u\_16213575/7699547

阿修罗监控AsuraMonitor：https://gitee.com/asuramonitor/monitor

---

---
url: /Java/系统优化/性能优化/4_分库分表实战.md
---

# 分库分表实战

### 一、需求分析

如果平台发展迅速，用户量激增，从数据库层面去思考，以考试作答系统为例，哪个表的数据会最大呢？

数据库设计：

1）app 应用表

显然不会，成百上千的应用已经多，但对数据库而已，这还是小量级

2）question 题目表

不太可能，一个应用一般最多也就几十个题目

3）scoring\_result 评分结果表

不太可能，一个应用对应不会有多少结果，比如 MBTI 也就 16 个。

4）user 表

有可能，如果用户达到几千万级，那么确实挺多了

5）user\_answer 用户答题记录表

一个用户可以对同个应用多次答题，也可以在多个应用多次答题，理论上如果用户量足够大，那么这个表肯定是最先遇到瓶颈的。

除了清理数据外，常见的一种优化方案是分库分表。

### 二、分库分表概念

先简单了解下分库分表的场景。

随着用户量的激增和时间的堆砌，存在数据库里面的数据越来越多，此时的数据库就会产生瓶颈，出现资源报警、查询慢等场景。

首先单机数据库所能承载的连接数、I/O 及网络的吞吐等都是有限的，所以当并发量上来了之后，数据库就渐渐顶不住了。

而且如果单表的数据量过大，查询的性能也会下降。因为数据越多底层存储的 B+ 树就越高，树越高则查询 I/O 的次数就越多，那么性能也就越差。

分库和分表怎么区分呢？

把以前存在 **一个数据库** 实例里的数据拆分成多个数据库实例，部署在不同的服务器中，这是分库。

把以前存在 **一张表** 里面的数据拆分成多张表，这是分表。

一般而言：

* 分表：是为了解决由于单张表数据量多大，而导致查询慢的问题。大致三、四千万行数据就得拆分，不过具体还是得看每一行的数据量大小，有些字段都很小的可能支持更多行数，有些字段大的可能一千万就顶不住了。
* 分库：是为了解决服务器资源受单机限制，顶不住高并发访问的问题，把请求分配到多台服务器上，降低服务器压力。

比如电商网站的使用人数不断增加， 用户数不断增加，订单数也日益增长，此时就应该把用户库和订单库拆开来，这样就能降低数据库的压力，且对业务而言数据分的也更清晰，并且理论上订单数会远大于用户数，还可以针对订单库单一升配。

由于电商网站品类不断增加，在促销活动的作用下订单取得爆炸式增长，如果所有订单仅存储在一张表中，这张表得有多大？

因此此时就需要根据订单表进行分表，可以按时间维度，比如 order\_202401、order\_202402 来拆分，如果每天的订单量很大，则可以通过 order\_20240101、order\_20240102 这样拆分。

### 三、技术选型

#### 1、通用选型思路

在公司内如果进行技术选型，一般有以下几个考察点：

1）场景适配，考察选择的框架或组件所提供的功能是否符合当前的需求。

2）团队能力，考察当前团队是否有能力使用和运维选择的三方框架和组件。比如团队没人会 c++ ，你选个 c++ 开发的组件，这可能不太合适，后续遇到问题一脸懵逼，学习成本大。

3）技术栈匹配度，考察引入的组件是否有很多附带的依赖，比如引入 rpc 框架，可能需要配套引入注册中心、配置中心等等，需要确认目前项目是否已经拥有这部分能力，评估成本。

4）社区与生态，选择的开源组件社区是否活跃，资料是否丰富，不要如果遇到个小众的不活跃的社区，出了 bug 可能都没人修，且需要观察生态，比如我们用spring 生态就很好，基本上 java 需要的能力例如 orm 支持、大数据支持等等都有，如果选择生态不好的，后续要进行一些扩展这部分的成本也很大。

主要是以上四点，最终就是考虑成本和收益再做决定。

#### 2、分库分表开源组件选型

常见的分库分表开源组件有：ShardingSphere、MyCat、Cobar 等。

官方文档：https://github.com/apache/shardingsphere

![img](/assets/1716363082235-b7d9a47d-3668-42d9-834a-28db1d4365c7.DojzL6xS.png)

官方文档：https://github.com/MyCATApache/Mycat21671423090896154626\_0.40376985789512365

![img](/assets/1716363112942-0b5c0e09-f124-4268-ab6d-316cab86f0b4.DDyuIQf3.png)

官方文档：https://github.com/alibaba/cobar

![img](/assets/1716363136634-fdcf4c68-269f-4ca5-80ee-002aac42b019.Dki6Zyre.png)

看下 star 数其实就有个选择预期了，Sharding-JDBC 相比而言功能更丰富，还支持读写分离、数据脱敏、分布式事务等等。

并且 ShardingSphere 不仅支持嵌入式的 Sharding-JDBC，还支持 Sharding-Proxy（独立代理服务）和Sharding-Sidecar（服务网格模式）。

再者 ShardingSphere 非常活跃，社区庞大且资料丰富，项目迭代也非常快，毕竟是 apache 项目。

因此本项目选择 ShardingSphere 内的 Sharding-JDBC。

### 四、Sharding-JDBC 原理

Sharding-JDBC 核心原理其实很简单，可以用几个字总结：**改写SQL**

比如我们想根据 appId 来将对应的用户答题记录表进行分表。

将 appId % 2 等于 0 的应用的所有用户的答题记录都划分到 user\_answer\_0，等于 1 的应用的所有用户的答题记录都划分到 user\_answer\_1。

按照我们正常的想法处理逻辑就是：

```java
if（appId % 2 == 0）{
	userAnswer0Service.save(userAnswer);
} else {
	userAnswer1Service.save(userAnswer);
}
```

而用了 Sharding-JDBC 后，我们只要写好配置，Sharding-JDBC 就会根据配置，执行我们上面的逻辑，在业务代码上我们就可以透明化分库分表的存在，减少了很多重复逻辑！

它会解析 SQL ，根据我们指定的 **分片键**，按照我们设置的逻辑来计算得到对应的路由分片（数据库或者表），最终改写 SQL 后进行 SQL 的执行。

### 五、方案设计

分库分表的核心是确定按照什么维度（或者字段）进行拆分，一般会选择唯一的、业务合理的、能够均匀分配的字段。

个人建议：你在哪个字段加索引，就用哪个字段分表，核心在于用户的查询，一定要根据业务的实际情况来。尽量避免出现跨表和跨库查询。

对于本项目，user\_answer 有个天然的拆分字段即 appId，不同应用的用户答题记录没有关联，因此我们可以根据 appId 拆解 user\_answer 表。

实现流程比较简单：

1. 新建 user\_answer\_0 和 user\_answer\_1，作为 user\_answer 表的分表
2. 引入 Sharding-JDBC
3. 配置文件中设置分表逻辑

### 六、后端开发

1）新建表，直接复制 user\_answer 的 DDL 表结构，改个名称即可。

```sql
create table if not exists user_answer_xxx
(
  id              bigint auto_increment primary key,
  appId           bigint                             not null comment '应用 id',
  appType         tinyint  default 0                 not null comment '应用类型（0-得分类，1-角色测评类）',
  choiceJson      text                               not null comment '用户答案',
  resultId        bigint                             null comment '评分结果 id',
  resultName      varchar(128)                       null comment '结果名称，如物流师',
  resultDesc      text                               null comment '结果描述',
  resultPicture   varchar(1024)                      null comment '结果图标',
  resultScore     int                                null comment '得分',
  scoringStrategy tinyint  default 0                 not null comment '评分策略（0-自定义，1-AI）',
  userId          bigint                             not null comment '用户 id',
  createTime      datetime default CURRENT_TIMESTAMP not null comment '创建时间',
  updateTime      datetime default CURRENT_TIMESTAMP not null on update CURRENT_TIMESTAMP comment '更新时间',
  isDelete        tinyint  default 0                 not null comment '是否删除',
  index idx_appId (appId),
  index idx_userId (userId)
) comment '用户答题记录' collate = utf8mb4_unicode_ci;
```

2）maven 引入 Sharding-JDBC 依赖：

```xml
<dependency>
  <groupId>org.apache.shardingsphere</groupId>
  <artifactId>shardingsphere-jdbc-core-spring-boot-starter</artifactId>
  <version>5.2.0</version>
</dependency>
```

仅需在 application.yml 配置一下参数：

```yaml
spring:
shardingsphere:
#数据源配置
datasource:
# 多数据源以逗号隔开即可
names: yudada
yudada:
type: com.zaxxer.hikari.HikariDataSource
driver-class-name: com.mysql.cj.jdbc.Driver
jdbc-url: jdbc:mysql://localhost:3306/yudada?allowPublicKeyRetrieval=true&useSSL=false&autoReconnect=true&characterEncoding=utf8
username: root
password: 123456
# 规则配置
rules:
sharding:
# 分片算法配置
sharding-algorithms:
# 自定义分片规则名
answer-table-inline:
## inline 类型是简单的配置文件里面就能写的类型，其他还有自定义类等等
type: INLINE
props:
algorithm-expression: user_answer_$->{appId % 2}
tables:
user_answer:
actual-data-nodes: yudada.user_answer_$->{0..1}
# 分表策略
table-strategy:
standard:
sharding-column: appId
sharding-algorithm-name: answer-table-inline
```

配置解析：

1）需要将数据源挪至 shardingsphere 下

2）定于数据源的名字和 url 等配置

3）自定义分片规则，即 answer-table-inline，分片算法为 user\_answer\_$->{appId % 2} ，这个含义就是根据 appId % 2 的结果拼接表名，改写 SQL

4）设置对应的表使用分片规则，即 tables:user\_answer:table-strategy，指定分片键为 appId，分片的规则是 answer-table-inline

### 七、验证测试

新建表和配置后，直接使用单元测试即可测试结果：

```java
@SpringBootTest
public class UserAnswerShardingTest {

    @Resource
    private UserAnswerService userAnswerService;

    @Test
    void test() {

        UserAnswer userAnswer1 = new UserAnswer();

        userAnswer1.setAppId(1L);
        userAnswer1.setUserId(1L);
        userAnswer1.setChoices("1");
        userAnswerService.save(userAnswer1);

        UserAnswer userAnswer2 = new UserAnswer();
        userAnswer2.setAppId(2L);
        userAnswer2.setUserId(1L);
        userAnswer2.setChoices("2");
        userAnswerService.save(userAnswer2);

        UserAnswer userAnswerOne = userAnswerService.getOne(Wrappers.lambdaQuery(UserAnswer.class).eq(UserAnswer::getAppId, 1L));
        System.out.println(JSONUtil.toJsonStr(userAnswerOne));

        UserAnswer userAnswerTwo = userAnswerService.getOne(Wrappers.lambdaQuery(UserAnswer.class).eq(UserAnswer::getAppId, 2L));
        System.out.println(JSONUtil.toJsonStr(userAnswerTwo));
    }
}
```

观察数据库中两张表的数据：

![img](/assets/1716364651709-7842f03f-c011-4ef4-8b53-0a3ddb8bbb70.CUjnMA-0.png)

![img](/assets/1716364661663-500c9371-df87-4188-a73e-1d3aeb13bdbd.CV4E8U4n.png)1671423090896154626\_0.9453560921366477

注意，分表后，一定不能更新分表字段！

如果有报错，可以把 appId 的更新设置为空来解决：

```java
try {
    UserAnswer userAnswerWithResult = scoringStrategyExecutor.doScore(choices, app);
    userAnswerWithResult.setId(newUserAnswerId);
    userAnswerWithResult.setAppId(null);
    userAnswerService.updateById(userAnswerWithResult);
} catch (Exception e) {
    e.printStackTrace();
    throw new BusinessException(ErrorCode.OPERATION_ERROR, "评分错误");
}
```

---

---
url: /@pages/categoriesPage.md
---


---

---
url: /Java/JVM性能调优/02.JVM性能监控与调优/4_分析GC日志.md
---

# 分析GC日志

## 分析GC日志

### GC分类

![在这里插入图片描述](/assets/094109fecd8f4d5fb7d2af75666753d7.7lNW3c37.png)

针对HotSpot VM的实现，它里面的GC按照回收区域又分为两大种类型：一种是部分收集（Partial GC），一种是整堆收集（Full GC）

* 部分收集（Partial GC）：不是完整收集整个Java堆的垃圾收集。其中又分为：
  * 新生代收集（Minor GC / Young GC）：只是新生代（Eden / S0, S1）的垃圾收集
  * 老年代收集（Major GC / Old GC）：只是老年代的垃圾收集。目前，只有CMS GC会有单独收集老年代的行为。注意，很多时候Major GC会和Full GC混淆使用，需要具体分辨是老年代回收还是整堆回收。
  * 混合收集（Mixed GC）：收集整个新生代以及部分老年代的垃圾收集。目前，只有G1 GC会有这种行为。
* 整堆收集（Full GC）：收集整个java堆和方法区的垃圾收集。

### GC日志分类

#### MinorGC

MinorGC（或young GC或YGC）日志：

```sh
[GC (Allocation Failure) [PSYoungGen: 31744K->2192K (36864K) ] 31744K->2200K (121856K), 0.0139308 secs] [Times: user=0.05 sys=0.01, real=0.01 secs]
# [GC (Allocation Failure) [新生代: Young GC 前新生代内存占用->Young GC 后新生代内存占用 (新生代总大小) ] Young GC 前 JVM 堆内存占用->Young GC 后 JVM 堆内存使用 (JVM 堆总大小), Young GC 耗时] [Times: user=Young GC 用户耗时 sys=Young GC 系统耗时, real=Young GC 实际耗时]
```

#### FullGC

```sh
[Full GC (Metadata GC Threshold) [PSYoungGen: 5104K->0K (132096K) ] [Par01dGen: 416K->5453K (50176K) ]5520K->5453K (182272K), [Metaspace: 20637K->20637K (1067008K) ], 0.0245883 secs] [Times: user=0.06 sys=0.00, real=0.02 secs]
# [Full GC (Metadata GC Threshold) [新生代: GC 前新生代内存占用->GC 后新生代内存占用 (新生代总大小) ] [老年代: GC 前老年代内存占用->GC 后老年代内存占用 (老年代总大小) ]GC 前堆内存占用->GC 后堆内存占用 (JVM 堆总大小), [元空间:  GC 前元空间内存占用->GC 后元空间内存占用 (元空间总大小) ], GC 耗时] [Times: user=GC 用户耗时 sys=GC 系统耗时 , real=GC 实际耗时]
```

### GC日志结构剖析

#### **垃圾收集器**

* Serial收集器：新生代显示 "\[DefNew"，即 Default New Generation
* ParNew收集器：新生代显示 "\[ParNew"，即 Parallel New Generation
* Parallel Scavenge收集器：新生代显示"\[PSYoungGen"，JDK1.7使用的即PSYoungGen
* Parallel Old收集器：老年代显示"\[ParoldGen"
* G1收集器：显示”garbage-first heap“

#### **GC原因**

* Allocation Failure：表明本次引起GC的原因是因为新生代中没有足够的区域存放需要分配的数据
* Metadata GCThreshold：Metaspace区不够用了
* FErgonomics：JVM自适应调整导致的GC
* System：调用了System.gc()方法

#### **GC日志格式规律**

GC日志格式的规律一般都是：GC前内存占用-＞GC后内存占用（该区域内存总大小）

```sh
[PSYoungGen: 5986K->696K (8704K) ] 5986K->704K (9216K)
```

* 中括号内：GC回收前年轻代堆大小，回收后大小，（年轻代堆总大小）
* 括号外：GC回收前年轻代和老年代大小，回收后大小，（年轻代和老年代总大小）

注意：Minor GC堆内存总容量 = 9/10 年轻代 + 老年代。原因是Survivor区只计算from部分，而JVM默认年轻代中Eden区和Survivor区的比例关系，Eden:S0:S1=8:1:1。

#### **GC时间**

GC日志中有三个时间：user，sys和real

* user：进程执行用户态代码（核心之外）所使用的时间。这是执行此进程所使用的实际CPU 时间，其他进程和此进程阻塞的时间并不包括在内。在垃圾收集的情况下，表示GC线程执行所使用的 CPU 总时间。
* sys：进程在内核态消耗的 CPU 时间，即在内核执行系统调用或等待系统事件所使用的CPU 时间
* real：程序从开始到结束所用的时钟时间。这个时间包括其他进程使用的时间片和进程阻塞的时间（比如等待 I/O 完成）。对于并行gc，这个数字应该接近（用户时间＋系统时间）除以垃圾收集器使用的线程数。

由于多核的原因，一般的GC事件中，real time是小于sys time＋user time的，因为一般是多个线程并发的去做GC，所以real time是要小于sys＋user time的。如果real＞sys＋user的话，则你的应用可能存在下列问题：IO负载非常重或CPU不够用。

### GC日志分析工具

#### GCEasy

GCEasy是一款在线的GC日志分析器，可以通过GC日志分析进行内存泄露检测、GC暂停原因分析、JVM配置建议优化等功能，大多数功能是免费的。

官网地址：https://gceasy.io/

#### GCViewer

GCViewer是一款离线的GC日志分析器，用于可视化Java VM选项 -verbose:gc 和 .NET生成的数据 -Xloggc:、、\<file>。还可以计算与垃圾回收相关的性能指标（吞吐量、累积的暂停、最长的暂停等）。当通过更改世代大小或设置初始堆大小来调整特定应用程序的垃圾回收时，此功能非常有用。

源码下载：https://github.com/chewiebug/GCViewer

运行版本下载：https://github.com/chewiebug/GCViewer/wiki/Changelog

#### GChisto

* 官网上没有下载的地方，需要自己从SVN上拉下来编译
* 不过这个工具似乎没怎么维护了，存在不少bug

#### HPjmeter

* 工具很强大，但是只能打开由以下参数生成的GC log，-verbose:gc  -Xloggc:gc.log。添加其他参数生成的gc.log无法打开
* HPjmeter集成了以前的HPjtune功能，可以分析在HP机器上产生的垃圾回收日志文件

## 参考资料

https://blog.csdn.net/qq\_43468008/article/details/129774175

https://www.yuque.com/u21195183/jvm/ukmb3k

https://juejin.cn/post/6844903669251440653

---

---
url: /daily/面试专栏/微服务相关/5_服务容灾.md
---

# 服务容灾

---

---
url: /daily/面试专栏/微服务相关/6_服务网关.md
---

# 服务网关

---

---
url: /daily/高等数学/01_高等数学预备知识.md
---

# 高等数学预备知识

## 一、函数的概念与特性

### 1、函数

### 2、反函数

### 3、复合函数

### 4、函数的四种特性及重要结论

## 二、函数的图像

直角坐标系下的图像

1、常见图像

2、图像变换

极坐标系下的图像

1、用描点法画常见图像

（1）心形线

（2）玫瑰线

（3）阿基米德螺线

（4）伯努利双纽线

2、用直角系观点画极坐标系下图像

## 三、常用基础知识

### 1、数列

#### （1）**等差数列**

首项为a1，公差为d（d≠0）的数列：
$$
a\_1,a\_1+d,a\_1+2d,···,a\_1+(n-1)d,···
$$
①通项公式：
$$
a\_n=a\_1+（n-1）d
$$
②前n项的和：
$$
S\_n=\frac{n}{2}\[2a\_1+(n-1)d]=\frac{n}{2}\[a\_1+a\_n]
\如：1+2+···+n=\frac{n(1+n)}{2}
$$

#### （2）**等比数列**:star:

首项为a1，公比为r（r≠0）的数列：
$$
a\_1,a\_1r,a\_1r2,···,a\_1r^{n-1},···
$$
①通项公式：
$$
a\_n=a\_1r^{n-1}
$$
②前n项的和：
$$
前n项的和S\_n=\left{
\begin{aligned}
na\_1, && r=1\\
\\
\frac{a\_1(1-r^n)}{1-r}, && r≠1 && 老教材里：\frac{首项(1-公比^n)}{1-公比}
\end{aligned}
\right.
$$

②常用
$$
1+r+r^2+···+r^{n-1}=\frac{1-r^2}{1-r}(r≠1)

\若|r|<1⇒\sum\_{n=1}^{\infty}r^{n-1}=\lim\_{x\to\infty}\frac{1-r^n}{1-r}=\frac{1}{1-r}
\故\sum\_{n=1}^{\infty}r^{n-1}=\frac{1}{1-r}，|r|<1
$$

#### （3）一些常见数列的前n项和

$$
①\sum\_{k=1}^{n}k=1+2+3+···+n=\frac{n(n+1)}{2}\\
\②\sum\_{k=1}^{n}k^2=1^2+2^2+3^2+···+n^2=\frac{n(n+1)(2n+1)}{6}\\
\③\sum\_{k=1}^{n}\frac{1}{k(k+1)}=\frac{1}{1×2}+\frac{1}{2×3}+\frac{1}\\{3×4}+···+\frac{1}{n(n+1)}
\\=\frac{1}{1}-\frac{1}{2}+\frac{1}{2}-\frac{1}{3}+\frac{1}{3}-\frac{1}{4}+···+\frac{1}{n}-\frac{1}{n+1}
\\=1-\frac{1}{n+1}=\frac{n}{n+1}
\即：\frac{1}{k(k+1)}=\frac{1}{k}-\frac{1}{k+1}（数三爱考）
$$

例题：

如：${a\_n}:a\_n=\frac{1}{1^2}+\frac{1}{2^2}+···+\frac{1}{n^2},n=1,2,···,数列收敛么？$

考点：用到后面的单调有界准则：一个数列单调增有上界必收敛，单调减有下界必收敛？

分析：

①$a\_{n+1}-a\_n=\frac{1}{(n+1)^2}>0 ⇒ {a\_n}单调增$

②$a\_n=\frac{1}{1^2}+\frac{1}{2^2}+···+\frac{1}{n^2}$

$=\frac{1}{1×1}+\frac{1}{2×2}+···+\frac{1}{n×n}<1+\frac{1}{1}-\frac{1}{2}+\frac{1}{2}-\frac{1}{3}+···+\frac{1}{n-1}-\frac{1}{n}$

$=2-\frac{1}{n}<2 ⇒ {a\_n}有上界为2$

故根据单调有界准则，{an}收敛。

### 2、三角函数

#### （1）三角函数基本关系

$\cscα=\frac{1}{\sinα}，\secα=\frac{1}{\cosα}，\cotα=\frac{1}{\tanα}，\tanα=\frac{\sinα}{\cosα}，\cotα=\frac{\cosα}{\sinα}$

$\sin^2α+\cos^2α=1，1+\tan^2α=\sec^2α，1+\cot^2α=\csc^2α$

#### （2）诱导公式

| 角θ  | $\frac{π}{2}-α$ | $\frac{π}{2}+α$ | $π-α$  | $π+α$  | $\frac{3}{2}π-α$ | $\frac{3}{2}π+α$ | $2π-α$ |
| ---- | --------------- | --------------- | ------ | ------ | ---------------- | ---------------- | ------ |
| 函数 | 90°-α           | 90°+α           | 180°-α | 180°+α | 270°-α           | 270°+α           | 360°-α |
| sinθ | cosα            | cosα            | sinα   | -sinα  | -cosα            | -cosα            | -sinα  |
| cosθ | sinα            | -sinα           | -cosα  | -cosα  | -sinα            | sinα             | cosα   |
| tanθ | cotα            | -cotα           | -tanα  | tanα   | cotα             | -cotα            | -tanα  |
| cotθ | tanα            | -tanα           | -cotα  | cotα   | tanα             | -tanα            | -cotα  |

$$
小结=\left{
\begin{aligned}
\sin(\frac{π}{2}-α)  & = & \cosα \\
\sin(π-α)  & = & \sinα \\
\cos(\frac{π}{2}-α)  & = & -\sinα \\
\cos(π-α)  & = & -\cosα \\
\end{aligned}
\right.
$$

#### （3）特殊的三角函数值

|      |      |      |      |      |      |      |      |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
|      |      |      |      |      |      |      |      |
|      |      |      |      |      |      |      |      |
|      |      |      |      |      |      |      |      |
|      |      |      |      |      |      |      |      |

注：

（1）

（2）

#### （4）重要公式

①倍角公式

$\sin2α=2\sinα\cosα$

$\cos2α=\cos^2α-\sin^2α=1-2\sin^2α=2cos^2α-1$

### 3、指数运算法则

### 4、对数运算法则

### 5、一元二次方程基础

### 6、因式分解公式

### 7、阶乘与双阶乘

①$n!=1·2·3·····n,规定0!=1$

②$（2n）!!=2·4·6·····（2n）=2^n·n!$

③$（2n-1）!!=1·3·5·····（2n-1）$

### 8、常用不等式

（1）设a、b为实数，则

①$|a±b|≤|a|+|b|$

②$\big||a|-|b|\big|≤|a-b|$

a、b在0的同一侧时，可以取等号，同正同负时。

（2）$\sqrt\[3]{abc}≤\sqrt{\frac{a^2+b^2+c^2}{3}}(a,b,c>0)$

（3）若$a>b>0$，则$\left{
\begin{aligned}
当n>0时，a^n>b^n, \\
当n<0时，a^n\<b^n, \\
\end{aligned}
\right.$

后面第二讲夹逼准则会用到

（4）若$0\<a\<x\<b,0\<c\<y\<d$，则$\frac{c}{b}<\frac{y}{x}<\frac{d}{a}$

---

---
url: /Java/架构设计/高可用/0_概述/0_高可用架构设计概述.md
---

# 高可用架构设计概述

作为软件开发人员，特别是架构师，在系统设计初期，必须考虑如何保证系统的高可用性，高扩展性，高性能。当然最重要的要先保证高可用性，系统不支持高可用，再高的性能和可扩展性都是无源之水，无本之木。

那什么是系统的高可用性？用什么指标来衡量系统可用性？实现系统高可用性的设计原则和方案有哪些？

## 一、系统高可用性

### 1.1、什么是高可用性

高可用性：（High Availability）通常用来描述一个系统经过专门的设计，从而减少停工时间，保证其服务的高度可用性。也就是系统大部分时间都是可用的，可以为用户提供服务的，即使发生了硬件故障或系统在进行升级。

### 1.2、如何衡量系统可用性

系统可用性指标，业界通常用多少个9来描述一个系统的可用性，比如4个9，99.99%就叫系统的可用性指标，表示该系统在所有运行时间中只有0.01%的时间是不可用的。

计算公式：`可用性指标=（1-年度不可用时间/年度总时间）X 100，反映系统的可用程度`。

![系统可用性指标 ](/assets/ef3f42acea6ddf09f5efc8310b14d55a.BbNVnd1B.png)

通过上图可以发现，

1个九和两个九的系统年故障时间是按天算的，日故障时间是按小时算的，也就是每年总有那么几天或每天总有那么几个小时系统不可用，满足该指标的系统只能算基本可用，也是最容易达到的，基本上靠人工运维的方式就能实现。

3个九的系统年故障时间从3天锐减为8小时，系统具有较高可用性。

4个九的系统年故障时间为50多分钟，这种系统不能依靠人工运维方式实现高可用，而是需要系统具有自动恢复能力。

5个9的系统年故障时间为5分钟，满足该指标的系统具有极高的可用性，不过实现的难度和代价也是非常大的，只有公司的核心业务系统或国家级基础设置才会确保满足5个9甚至6个9的指标，比如支付宝，微信支付，电信运营商，国家银行系统，其他非核心系统满足4个9的指标已经够用了。

可用性差异在于面对各种故障时，系统的高可用架构设计方案做的是否足够好。

## 二、高可用架构设计方案

### 2.1、解耦

解耦是用于解决系统耦合度过高的问题，耦合度过高会导致系统维护困难，任何一个小的改动都可能引入意想不到的bug和系统奔溃，导致系统不可用。

软件技术的进化离不开解耦，TCP/IP协议的分层将每层的职责做了明确区分，通过制定不同的协议解耦了标准与实现，软件与硬件的关系。MVC框架进一步将展示逻辑与控制逻辑解耦，方便前后端开发的分离。

两种低耦合设计原则：

组件的低耦合原则：

* 无循环依赖原则，即技术组件之间不能循环依赖，不能查权限的接口依赖于用户接口，用户接口又依赖权限接口；
* 稳定依赖原则，即被依赖的组件尽量稳定，尽量少因为业务变化而变化，不能今天因为某个业务改了接口定义，明天又因为另一个业务调整接口参数；
* 稳定抽象原则，即要想使组件稳定，组件就要更加抽象，组件发布出去之后非万不得已，不改变结构。

面向对象的低耦合原则：

* 开闭原则，即对修改封闭、对扩展开放，对象可以扩展新功能，但是不能修改代码；
* 依赖倒置原则，即高层对象不能依赖低层对象，而是要依赖抽象接口，而抽象接口属于高层；通常逻辑是上游对象直接依赖下游对象的接口或实现，依赖倒置原则是下游对象依赖上游对象的接口，并实现下游对象的接口，这样的好处是上游对象自身是稳定的，不需要关心下游对象接口的改变，下游对象也可以自己去单独演化，只需要保证上游接口的实现。
* 接口隔离原则，不要强迫使用者依赖它们不需要的方法，要用接口对方法进行隔离。

### 2.2、隔离

解耦是逻辑分隔，隔离是物理分隔，即将低耦合的组件进行独立部署，将不同组件在物理上隔离开来。每个组件有自己独立的代码仓库；每个组件可以独立发布，互不影响；每个组件有自己独立的容器进行部署，互不干扰。后端同学是不是很熟悉？这不是微服务架构吗！微服务将一个复杂的单体应用拆解成多个细粒度的微服务，这些微服务之间互相依赖，实现原来单体应用的功能。然后每个微服务可以独立开发和部署，微服务之间通过RPC框架进行调用，这就是微服务架构。

隔离使得系统间关系和边界更加清晰，同时也可以隔离开故障，更加快速的发现问题和解决问题，提高系统的可用性。

当然，**隔离必须在低耦合的基础上进行才有意义**。如果系统耦合度高，隔离只会让系统关系更混乱，更脆弱。

### 2.3、异步

异步是在隔离基础上的进一步解耦，将物理上已经分割的组件之间的依赖关系进一步切断，使故障无法扩散，提高系统可用性。异步在架构上的实现手段主要是使用消息队列。

用户注册同步模式

![用户注册同步模式](/assets/37c0d8589d2c1626aa23fc8ac4f01384.DUkywnve.png)

用户注册异步模式

![用户注册异步模式](/assets/419f383e65c5863fbf7b57447aaba9f2.BU6lDG0C.png)

### 2.4、备份

备份主要解决硬件故障下系统的可用性，即一个服务部署在多个服务器上，当某个服务器故障时，请求切换到其他服务器上继续处理，保证服务是可用的。所以，备份与失败转移（failover）总是成对出现的，共同构成一个高可用解决方案。

最常见的备份就是负载均衡，用户通过负载均衡服务器nginx访问应用服务时，nginx会判断应用服务器集群中每台机器是否可用，如果服务器宕机，则会停止分发请求到该服务器。

数据库或存储服务为了保证高可用，也会采用备份的手段，比如MySQL主备模式。

### 2.5、重试

系统业务多数情况下比较复杂，一般由多个步骤来组成完整的业务逻辑，每个步骤都可能因为某些原因而失败，特别是微服务架构时代，远程服务可能会由于线程阻塞、垃圾回收或者网络抖动，而无法及时返回响应，调用者可以通过重试的方式修复单次调用的故障。

需要注意的是，重试是有风险的。比如一个转账操作，由于网络请求超时，导致第一次请求转账后没有响应，如果这个时候进行重试，那么可能会导致重复转账，反而造成重大问题。所以，支持重试的服务必须是幂等的。所谓幂等，即服务重复调用和调用一次产生的结果是相同的。在做架构设计的时候，一定要注意服务如果要支持重试，必须考虑服务的幂等性。

### 2.6、熔断

重试主要解决偶发的因素导致的单次调用失败，但是如果某个服务器一直不稳定，甚至已经宕机，再请求这个服务器或者进行重试都没有意义了。所以为了保证系统整体的高可用，对于不稳定或宕机的服务器需要进行熔断。

熔断的主要方式是使用断路器阻断对故障服务器的调用，断路器状态图如下：

![在这里插入图片描述](/assets/d27be1437df1a46e4bc822dc352a9df1.BdiWJQig.png)

断路器有三种状态，关闭、打开、半开。断路器正常情况下是关闭状态，每次服务调用后都通知断路器。如果失败了，失败计数器就 +1，如果超过开关阈值，断路器就打开，这个时候就不再请求这个服务了。过一段时间，达到断路器预设的时间窗口后，断路器进入半开状态，发送一个请求到该服务，如果服务调用成功，那么说明服务恢复，断路器进入关闭状态，即正常状态；如果服务调用失败，那么说明服务故障还没修复，断路器继续进入到打开状态，服务不可用。

### 2.7、补偿

前面几种方案都是故障发生时如何处理，而补偿则是故障发生后，如何弥补错误或者避免损失扩大。比如将处理失败的请求放入一个专门的补偿队列，等待失败原因消除后进行补偿，重新处理。

补偿最典型的使用场景是事务补偿。在一个分布式应用中，多个相关事务操作可能分布在不同的服务器上，如果某个服务器处理失败，那么整个事务就是不完整的。按照传统的事务处理思路，需要进行事务回滚，即将已经成功的操作也恢复到事务以前的状态，保证事务的一致性。

传统的事务回滚主要依赖数据库的特性，当事务失败的时候，数据库执行自己的 undo日志，就可以将同一个事务的多条数据记录恢复到事务之初的状态。但是分布式服务没有undo日志，所以需要开发专门的事务补偿代码，当分布式事务失效的时候，调用事务补偿服务，将事务状态恢复如初。

### 2.8、限流

在高并发场景下，如果系统的访问量超过了系统的承受能力，可以通过限流对系统进行保护。限流是指对进入系统的用户请求进行流量限制，如果访问量超过了系统的最大处理能力，就会丢弃一部分用户请求，保证整个系统可用。这样虽然有一部分用户的请求被丢弃，但大部分用户还是可以访问系统的，总比整个系统崩溃，所有的用户都不可用要好。

### 2.9、降级

降级是保护系统高可用的另一种手段。有一些系统功能是非核心的，但是也给系统产生了非常大的压力，比如电商系统中有确认收货这个功能，即便用户不确认收货，系统也会超时自动确认。

但实际上确认收货是一个非常重的操作，因为它会对数据库产生很大的压力：它要进行更改订单状态，完成支付确认，并进行评价等一系列操作。如果在系统高并发的时候去完成这些操作，那么会对系统雪上加霜，使系统的处理能力更加恶化。

解决办法就是在系统高并发的时候（例如京东618，11.11大促），将确认收货、评价等非核心的功能关闭，也就是对系统进行降级，把宝贵的系统资源留下来，给正在购物的人，让他们去完成交易。

熔断和降级有什么区别呢？

熔断是部分服务器不可用，请求转移到其他服务器；降级是部分功能不可用，需要人为关闭这些功能的请求处理开关，降级是功能设计的一部分。

### 2.10、多活

多活，即异地多活，在多个地区建立数据中心，并都可以对用户提供服务，任何地区级的灾难都不会影响系统的可用。比如我们部署应用时，都会要求夸机房部署，这就是一种异地多活的方式。

异地多活的架构需要考虑的重点是，用户请求如何分发到不同的机房去。这个主要可以在域名解析的时候完成，也就是用户进行域名解析的时候，会根据就近原则或者其他一些策略，完成用户请求的分发。另一个至关重要的技术点是，因为是多个机房都可以独立对外提供服务，所以也就意味着每个机房都要有完整的数据记录。用户在任何一个机房完成的数据操作，都必须同步传输给其他的机房，进行数据实时同步。

数据库实时同步最需要关注的就是数据冲突问题。同一条数据，同时在两个数据中心被修改了，该如何解决？某些容易引起数据冲突的服务采用类似MySQL的主主模式，也就是说多个机房在某个时刻是有一个主机房的，某些请求只能到达主机房才能被处理，其他的机房不处理这一类请求，以此来避免关键数据的冲突。

除了以上的高可用架构方案，还有一些高可用的运维方案，比如通过自动化测试减少系统的Bug，通过自动化监控尽早发现系统的故障，通过预发布验证发现测试环境无法发现的Bug，通过灰度发布降低软件错误带来的影响等。

**参考资料**

\[1]. <https://blog.csdn.net/ITqige/article/details/134588411>

\[2]. <https://www.cnblogs.com/xiekun/p/15867034.html>

---

---
url: /Python/AI大模型应用开发/11_给AI模型读文件的能力.md
---

# 给AI模型读文件的能力

## 一、RAG 怎么让AI知道私人数据？检索增强生成

当大语言模型（LLM）面临 “知识覆盖不足”（如小众领域、未训练数据）或 “实时性缺失” 问题时，**检索增强生成（Retrieval-Augmented Generation，简称 RAG）** 成为关键解决方案。它通过 “连接外部知识库” 让模型基于实时、专属数据生成准确回答，以下从核心问题、实现流程、优劣势对比三方面展开整理：

### 1、RAG 的核心应用场景：解决 LLM 的固有局限

LLM 的知识依赖训练数据，存在天然短板，RAG 正是为弥补这些短板而生：

1. **小众 / 专业领域知识不足**：若训练数据中某领域文本覆盖少（如特定行业技术文档、学术研究），LLM 无法生成精准回答。
2. **私有数据无法访问**：企业内部数据（如员工手册、客户档案）、个人私密文档（如日记、医疗记录）不会纳入公开 LLM 的训练数据，LLM 无法直接回答相关问题。
3. **知识时效性缺失**：LLM 的训练数据有 “截止日期”（如 GPT-4 截止到 2023 年 10 月），无法回答训练后出现的新信息（如 2024 年新政策、新事件）。

RAG 的核心价值：让 LLM “实时访问外部知识库”，无需重新训练模型，即可基于专属 / 最新数据生成回答，典型应用包括：

* 企业知识库问答（如员工查询内部制度）；
* 个人文档问答（如基于 PDF 简历回答职业经历）；
* 行业专业工具（如基于医疗文献回答病症问题）。

### 2、RAG 的实现流程：三步完成 “检索 - 增强 - 生成”

RAG 的完整流程分为 “数据准备”“相似检索”“结合生成” 三大核心步骤，环环相扣确保模型获取有效外部信息：

#### 步骤 1：数据准备（离线阶段）—— 构建可检索的向量数据库

此阶段为后续检索打基础，核心是将 “非结构化外部文档” 转化为 “结构化向量数据”：

1. 文档加载与分割：
   * 加载外部文档（如 PDF、TXT、Word），由于 LLM 上下文窗口有限（无法处理超长文本），需将文档切分成**短文本块**（如每块 200-500 字符），避免信息超出窗口或分割过细导致语义断裂。
2. 文本向量化（嵌入）：
   * 通过 “嵌入模型（Embedding Model）” 将每个文本块转化为**固定长度的向量**（如 1536 维数字串）。
   * 关键特性：向量需保留文本的 “语义 / 语法关联”—— 相似文本的向量在 “向量空间” 中的距离更近（如 “猫抓老鼠” 和 “猫咪捕捉老鼠” 向量距离近），无关文本距离远（如 “猫抓老鼠” 和 “行星运行” 向量距离远），为后续相似检索提供数学依据。
3. 向量存储：
   * 将所有文本块的向量存入 “向量数据库”（如 Pinecone、Chroma、FAISS），向量数据库专门优化 “相似性查询” 效率，可快速找到与目标向量最接近的结果。

#### 步骤 2：相似检索（在线阶段）—— 匹配用户问题与知识库

当用户提出问题时，需从向量数据库中找到最相关的文本块：

1. **问题向量化**：将用户的问题（如 “这份文档中提到的产品定价策略是什么？”）通过同一嵌入模型转化为向量。
2. **相似性查询**：向量数据库计算 “问题向量” 与 “所有文本块向量” 的距离，筛选出**距离最近的 Top N 个文本块**（如 Top 3）—— 这些文本块是知识库中与用户问题最相关的内容。

#### 步骤 3：结合生成（在线阶段）—— LLM 基于检索结果生成回答

将 “用户问题 + 相关文本块” 合并为提示，传给 LLM 生成精准回答：

1. 构建提示：按 “问题 + 上下文” 格式组合内容，示例：

   > “基于以下上下文回答问题：
   > 【上下文】文档中提到，2024 年产品定价策略为：基础版 99 元 / 月，专业版 299 元 / 月，企业版按用户数计费（10 人以内 1999 元 / 月，每增加 10 人加 1000 元）。
   > 【问题】这份文档中提到的产品定价策略是什么？”

2. **LLM 生成回答**：LLM 以 “相关文本块” 为依据，避免依赖自身固有知识，生成与知识库内容一致的准确回答。

### 3、RAG 与 “直接传入全文” 的对比：何时该用 RAG？

随着 LLM 上下文窗口增大（如 GPT-4 Turbo 支持 128k Token），部分场景可 “直接将全文 + 问题传给模型”，但 RAG 在特定场景下仍不可替代，二者对比如下：

| 维度         | 直接传入全文（无 RAG）                                       | 检索增强生成（RAG）                                          |
| ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **适用场景** | 知识库规模小（全文可容纳进 LLM 上下文窗口），对细节准确度要求极高 | 知识库规模大（超窗口上限）、私有 / 实时数据、需控制成本      |
| **优点**     | 1. 无文本分割导致的信息损失，回答准确度可能更高；2. 实现逻辑简单（无需向量数据库） | 1. 支持超大规模知识库；2. 响应速度快（仅传相关文本）；3. 成本低（少消耗 Token，可用小窗口模型） |
| **缺点**     | 1. 响应慢（模型需处理全文）；2. 成本高（大窗口模型收费贵 + 多消耗 Token）；3. 无法支持超窗口数据 | 1. 文本分割可能丢失跨块语义（需优化分割策略）；2. 需额外维护向量数据库，实现复杂度高 |

### 4、RAG 的核心价值总结

1. **无需训练，快速适配新领域**：无需对 LLM 进行微调（Fine-tuning），仅需更新知识库，即可让模型处理新领域问题，降低技术门槛与成本。
2. **知识可控且可追溯**：回答基于明确的外部文档，可追溯信息来源（如 “回答来自文档第 3 章”），避免 LLM “幻觉”（生成虚假信息）。
3. **灵活支持私有 / 实时数据**：企业可将内部数据构建为私有知识库，个人可接入私密文档，且能通过更新知识库实现 “知识实时迭代”（如新增 2024 年数据）。

通过 RAG，可轻松构建 “专属领域 AI 工具”（如法律文档问答、医疗文献解读），或实现 “PDF/Word 文档问答” 等实用功能，是 LLM 落地行业场景的关键技术之一。

## 二、Document Loader 把外部文档加载进来

在 RAG（检索增强生成）流程中，“文档加载器（Document Loader）” 是数据准备的第一步，负责将不同来源、不同格式的内容（如本地文本、PDF、网络资源）统一加载为 LangChain 可处理的`Document`对象（含文本内容与元数据）。LangChain 社区提供了丰富的加载器，覆盖主流文档格式与资源类型，以下从核心概念、常用加载器实战、扩展类型三方面展开整理：

### 1、文档加载器的核心概念

#### 1.1. 核心作用

将非结构化 / 结构化数据（如 TXT、PDF、网页内容）转化为**标准化的`Document`对象列表**，每个`Document`包含两个关键属性：

* `page_content`：文本内容本身（字符串），是后续分割、嵌入的核心数据；
* `metadata`：元数据（字典），记录文本的来源信息（如文档路径、页码、URL、语言等），用于追溯数据来源或筛选内容。

#### 1.2. 设计优势

* **格式统一**：无论原始数据是 TXT、PDF 还是网页，加载后均为`Document`对象，后续分割、嵌入步骤无需适配不同格式，降低开发成本；
* **来源广泛**：支持本地文件、网络资源、数据库等多种数据源，满足不同场景的知识库构建需求。

### 2、常用文档加载器实战

#### 2.1. TextLoader：加载纯文本文件（TXT）

纯文本文件（如`.txt`）无格式干扰，是最基础的数据源，`TextLoader`可直接读取其内容。

使用步骤

```python
# 1. 导入TextLoader
from langchain_community.document_loaders import TextLoader

# 2. 初始化加载器：传入TXT文件路径
loader = TextLoader("demo.txt")  # 替换为你的TXT文件路径

# 3. 执行加载：返回Document对象列表
documents = loader.load()

# 4. 查看加载结果
print(f"加载的Document数量：{len(documents)}")
print(f"\n第一个Document元素的文本内容：\n{documents[0].page_content}")  # 查看第一个Document元素的文本内容
print(f"\n第一个Document的内容：\n{documents[0].page_content[:200]}...")  # 打印前200字符
print(f"\n第一个Document的元数据：\n{documents[0].metadata}")  # 元数据（含文件路径等）
```

关键说明：

* 若 TXT 文件较大（如万字以上），`load()`会返回一个包含 1 个`Document`的列表（整文件为一个文本块），后续需通过文本分割器切分为更小的块；
* 元数据默认包含`source`（文件路径），可用于追溯文本来源。

#### 2.2. PyPDFLoader：加载 PDF 文件

PDF 文件含格式信息（如页码、字体、排版），需依赖`PyPDF`库解析文本，`PyPDFLoader`会按页码拆分文本，每一页对应一个`Document`对象。

安装`PyPDF`依赖（解析 PDF 的核心库）：

```sh
pip install pypdf
```

使用步骤

```python
# 1. 导入PyPDFLoader
from langchain_community.document_loaders import PyPDFLoader

# 2. 初始化加载器：传入PDF文件路径
loader = PyPDFLoader("report.pdf")  # 替换为你的PDF文件路径

# 3. 执行加载：返回按页码拆分的Document列表（一页一个Document）
documents = loader.load()

# 4. 查看加载结果
print(f"PDF总页数（Document数量）：{len(documents)}")
print(f"\n第1页Document元素的文本内容：\n{documents[0].page_content}")  # 查看第一个Document元素的文本内容
print(f"\n第1页Document的内容：\n{documents[0].page_content[:200]}...")
print(f"\n第1页Document的元数据：\n{documents[0].metadata}")  # 元数据含"page"（页码）、"source"（路径）
```

关键说明

* 加载结果中，每个`Document`对应 PDF 的一页，元数据的`page`字段标记页码，便于后续定位内容来源；
* 若 PDF 含扫描图（非文字内容），`PyPDFLoader`无法提取文本，需先通过 OCR 工具（如 Tesseract）将图片转为文字，再用`TextLoader`加载。

#### 2.3. WikipediaLoader：加载维基百科词条内容

支持直接从维基百科加载指定词条的内容，无需手动复制粘贴，适合构建含权威信息的知识库。

安装`wikipedia`依赖（调用维基百科 API 的库）：

```sh
pip install wikipedia
```

使用步骤

```python
# 1. 导入WikipediaLoader
from langchain_community.document_loaders import WikipediaLoader

# 2. 初始化加载器：配置词条、语言、加载数量
loader = WikipediaLoader(
    query="人工智能",  # 维基百科词条名（中文/英文均可）
    lang="zh",         # 语言："zh"（中文）、"en"（英文）等
    load_max_docs=2    # 最多加载的相关文档数量（避免内容过多）
)

# 3. 执行加载：返回相关词条的Document列表
documents = loader.load()

# 4. 查看加载结果
print(f"加载的维基百科文档数量：{len(documents)}")
print(f"\n第1个Document元素的文本内容：\n{documents[0].page_content}")  # 查看第一个Document元素的文本内容

for i, doc in enumerate(documents, 1):
    print(f"\n【文档{i}】标题：{doc.metadata['title']}")
    print(f"内容预览：\n{doc.page_content[:300]}...")
    print(f"来源URL：{doc.metadata['source']}")  # 元数据含词条URL
```

关键参数说明

* `query`：必填，维基百科词条名（如 “牛顿第二定律”“ChatGPT”）；
* `lang`：可选，默认 “en”（英文），需指定 “zh” 获取中文词条；
* `load_max_docs`：可选，默认加载所有相关词条，建议设较小值（如 2-5）避免内容冗余。

### 3、LangChain 支持的其他文档加载器（扩展类型）

除上述三种，LangChain 社区还支持数十种文档加载器，覆盖主流格式与资源，部分常用类型如下：

| 加载器类型                     | 适用场景                                       | 依赖库 / 注意事项                                |
| ------------------------------ | ---------------------------------------------- | ------------------------------------------------ |
| `CSVLoader`                    | 加载 CSV 表格文件（如 Excel 导出的结构化数据） | 无需额外依赖，支持指定列提取文本                 |
| `Docx2txtLoader`               | 加载 Word 文档（.docx 格式）                   | 需安装`docx2txt`库                               |
| `UnstructuredPowerPointLoader` | 加载 PPT 文档（.pptx 格式）                    | 需安装`unstructured`库，提取幻灯片文本           |
| `WebBaseLoader`                | 加载网页内容（通过 URL）                       | 需安装`beautifulsoup4`库，支持解析 HTML          |
| `YouTubeLoader`                | 加载 YouTube 视频的字幕内容                    | 需安装`youtube-transcript-api`库，支持多语言字幕 |
| `GitHubLoader`                 | 加载 GitHub 仓库中的代码 / 文档                | 需配置 GitHub Token，支持指定仓库 / 文件路径     |

查看所有加载器

可访问 LangChain 官方文档的[Document Loaders 列表](https://python.langchain.com/docs/integrations/document_loaders/)，按 “格式”“来源” 筛选所需加载器，每个加载器均有详细使用示例。

### 4、文档加载的通用注意事项

1. **编码问题**：加载 TXT 文件时，若遇编码错误（如中文乱码），可在`TextLoader`中指定编码格式，示例：

   ```python
   loader = TextLoader("demo.txt", encoding="utf-8")  # 常用编码：utf-8、gbk
   ```

2. **大文件处理**：加载超大文件（如 100MB 以上的 TXT/PDF）时，建议先分割文件或使用 “流式加载”（部分加载器支持`load_incrementally=True`），避免内存溢出；

3. **元数据利用**：加载后可通过元数据筛选内容（如仅保留 PDF 的第 1-10 页），示例：

   ```python
   # 筛选PDF的第1-5页Document
   filtered_docs = [doc for doc in documents if 1 <= doc.metadata["page"] <= 5]
   ```

## 三、Text Splitter 上下文窗口有限？文本切成块

在 RAG（检索增强生成）流程中，“文本分割” 是衔接 “文档加载” 与 “向量嵌入” 的关键步骤 —— 由于大语言模型（LLM）上下文窗口有限，需将超长文档切分为 “语义完整、长度可控” 的文本块，为后续精准检索和嵌入奠定基础。以下从核心原理、工具选择、参数配置及中文适配展开整理：

### 1、文本分割的核心意义与挑战

#### 1. 核心意义

* **适配 LLM 窗口限制**：若文档长度远超模型上下文窗口（如一本 10 万字的书），无法直接传入模型，需分割为短文本块（如每块 500 字符），确保后续能被模型处理。
* **保障语义完整性**：分割后的文本块需是 “可理解的最小单元”（如完整句子、段落），避免切分在半句话、关键概念中间，导致 AI 无法理解文本含义。

#### 2. 核心挑战

* **避免语义断裂**：若分割符号选择不当（如在 “牛顿第二定律” 中间切分），会破坏文本逻辑，后续检索和生成都会出错。
* **平衡长度与连贯性**：文本块过长可能仍超窗口，过短则丢失上下文关联（如仅切分单个短句，无法体现段落逻辑）。

### 2、LangChain 核心分割器：RecursiveCharacterTextSplitter（字符递归分割器）

LangChain 中最常用、适配性最强的分割器是`RecursiveCharacterTextSplitter`（字符递归分割器），其核心逻辑是 “按优先级使用分割符，递归切分直到文本块符合长度要求”，尤其适合处理多格式、多语言文档。

#### 1. 前期准备：安装与导入

* **安装依赖**：该分割器属于`langchain-text-splitters`库，需先安装：

  ```sh
  pip install langchain-text-splitters
  ```

* **导入模块**：

  ```python
  from langchain_text_splitters import RecursiveCharacterTextSplitter
  ```

#### 2. 关键参数解析

创建分割器实例时，需配置 4 个核心参数，直接影响分割效果：

| 参数名            | 作用                                                         | 示例值       |
| ----------------- | ------------------------------------------------------------ | ------------ |
| `chunk_size`      | 单个文本块的最大长度（单位：字符，部分场景可设为 Token 数）  | 500          |
| `chunk_overlap`   | 相邻文本块的重叠长度（用于保持上下文连贯性，避免分割处信息丢失） | 50           |
| `separators`      | 分割符列表（按优先级排序，优先用靠前的分割符切分，失败则尝试下一个） | 中文适配列表 |
| `length_function` | 计算文本长度的函数（默认`len`，即字符数；可自定义为 Token 计数器） | `len`        |

##### （1）`chunk_size`与`chunk_overlap`：平衡长度与连贯性

* `chunk_size`：需根据模型上下文窗口调整（如 GPT-3.5 支持 4k Token≈3000 字符，可设`chunk_size=2000`），演示时可设较小值（如 500）方便测试。
* `chunk_overlap`：通常设为`chunk_size`的 10%-20%（如`chunk_size=500`时设`overlap=50`），确保相邻块在分割处有重叠（如 “第 1 块结尾 50 字符 = 第 2 块开头 50 字符”），避免关键信息断裂。

##### （2）`separators`：适配中文的分割符配置

默认`separators`为英文场景设计（含空格、英文标点），中文文档需自定义分割符列表，按 “从大到小” 的语义单元排序（优先按段落、再按句子、最后按短句），示例：

```python
# 中文适配的分割符列表（优先级从高到低）
chinese_separators = [
    "\n\n",  # 段落分隔（优先按段落切分）
    "\n",    # 换行分隔（段落内按换行切分）
    "。",    # 中文句号（句子结束）
    "！",    # 感叹号
    "？",    # 问号
    "，",    # 逗号（短句分隔，尽量避免，仅在必要时使用）
    "、",    # 顿号
    ""       # 空字符串（最后兜底，任意位置切分，避免无法分割）
]
```

* 逻辑：先尝试用 “段落分隔（\n\n）” 切分，若单个段落仍超`chunk_size`，再用 “换行（\n）”，以此类推，最后用空字符串兜底，确保所有文本都能被分割。

#### 3. 文本分割实战

以 “加载后的中文文档” 为例，完整分割流程如下

```python
#!pip install langchain_text_splitters

from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
# 加载一份中文产品文档
loader = TextLoader("./demo.txt")
docs = loader.load()
# 创建中文适配的字符递归分割器
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,  # 单个文本块最大500字符
    chunk_overlap=40,  # 相邻块重叠40字符
    # separators=chinese_separators,  # 中文分割符列表
    # length_function=len      # 按字符数计算长度
    separators=["\n\n", "\n", "。", "！", "？", "，", "、", ""]
)
# 执行分割（输入Documents列表，输出分割后的Documents列表）
texts = text_splitter.split_documents(docs)
texts

# 验证结果（查看分割后的文本块数量与长度）
# # print(f"分割前文档数：{len(docs)}")
# print(f"分割后文本块数：{len(texts)}")
# print(f"第一个文本块长度：{len(texts[0].page_content)}")  # 应≤500
```

```sh
[Document(page_content='罗浮宫（法语：Musée du Louvre,英语 /ˈluːv(rə)/ ），正式名称为罗浮博物馆，位于法国巴黎市中心的塞纳河边，原是建于12世纪末至13世纪初的王宫，现在是一所综合博物馆，亦是世界上最大的艺术博物馆之一，以及参观人数最多的博物馆，是巴黎中心最知名的地标。\n\n罗浮宫的建筑物始建于1190年左右，并在近代曾多次进行扩建，今天所见的模样则一个巨大的翼楼和亭阁建筑群，主要组成部分的总面积则超过60,600平方公尺（652,000平方英尺），馆内永久收藏则包括雕塑、绘画、美术工艺及古代东方、古代埃及和古希腊罗马等7个分类，主要收藏1860年以前的艺术作品与考古文物，罗浮宫博物馆在1793年8月10日开幕起正式对公众开放，平均每天有15,000名游客到此参观，其中65%是外国游客。\n\n位置\n\n罗浮宫与杜乐丽花园的卫星照片\n罗浮宫博物馆位于巴黎市中心的卢浮宫内，位于塞纳河右岸，毗邻杜乐丽花园。最近的两个地铁站是皇家宫-罗浮宫站和卢浮-里沃利站，前者有直达地下购物中心 Carrousel du Louvre 的地下通道。', metadata={'source': './demo.txt'}),
 Document(page_content='在1980年代末和1990年代大改建之前，罗浮宫共有好几个街道入口，目前大部分入口已经永久关闭。自1993年以来，博物馆的正门位置位于拿破仑广场金字塔底下的地下空间，游客可以从金字塔本身、旋转阶梯处连接到博物馆的通道。\n\n博物馆的参观时间随著时代的推移而变化。自18世纪开放以来，只有艺术家和来自外国的观光游客享有特权参观，这项特权后在1850年代才消失。当博物馆从1793年首次开放时，新历法法国共和历规定了“十天周”（法语：décades），其中前六天为艺术家和外国人访问，后三天为将军访问，民众仅能在最后一天参观，后在在1800年代初期在恢复七天周后，民众在每周只有4小时的时间能在罗浮宫参观，周六和周日则是缩减至下午2点至下午4点期间参观。\n\n从1824年开始的一项新规定允许公众在星期日和节假日时参观，然而其他日子只对艺术家和外国游客开放，这种情况到1855年才发生了变化，博物馆更改成除了周一外全天向公众免费开放，直到1922年才开始收费。\n\n当前自1946年开始，罗浮宫除了在周二公休和特殊假日外，通常向游客全面开放参观，内部允许使用照相机和录像机，但禁止使用闪光灯。', metadata={'source': './demo.txt'})]
```

```python
print(texts[0].page_content)
```

```sh
罗浮宫（法语：Musée du Louvre,英语 /ˈluːv(rə)/ ），正式名称为罗浮博物馆，位于法国巴黎市中心的塞纳河边，原是建于12世纪末至13世纪初的王宫，现在是一所综合博物馆，亦是世界上最大的艺术博物馆之一，以及参观人数最多的博物馆，是巴黎中心最知名的地标。

罗浮宫的建筑物始建于1190年左右，并在近代曾多次进行扩建，今天所见的模样则一个巨大的翼楼和亭阁建筑群，主要组成部分的总面积则超过60,600平方公尺（652,000平方英尺），馆内永久收藏则包括雕塑、绘画、美术工艺及古代东方、古代埃及和古希腊罗马等7个分类，主要收藏1860年以前的艺术作品与考古文物，罗浮宫博物馆在1793年8月10日开幕起正式对公众开放，平均每天有15,000名游客到此参观，其中65%是外国游客。

位置

罗浮宫与杜乐丽花园的卫星照片
罗浮宫博物馆位于巴黎市中心的卢浮宫内，位于塞纳河右岸，毗邻杜乐丽花园。最近的两个地铁站是皇家宫-罗浮宫站和卢浮-里沃利站，前者有直达地下购物中心 Carrousel du Louvre 的地下通道。
```

#### 4. 分割效果验证

* **长度合规**：所有分割后的文本块`page_content`长度均≤`chunk_size`，无超窗口风险。
* **语义完整**：文本块以 “段落、句子” 结尾（如 “。”“！”），无半句话、断裂概念（如不会出现 “牛顿第二定” 这样的不完整短语）。

### 3、文本分割的注意事项

① 根据文档类型调整参数：

* 长文档（如书籍）：`chunk_size`可设大（如 1000 字符），`overlap`设 100-200 字符，确保段落逻辑连贯。
* 短文档（如新闻稿）：`chunk_size`可设小（如 300 字符），`overlap`设 30-50 字符，避免单块过长。

② 中文与英文分割符区分：

* 英文文档可用默认`separators`（`["\n\n", "\n", ". ", " ", ""]`），依赖空格和英文句号；
* 中文文档必须自定义`separators`，避免用空格（中文无空格分隔习惯），优先用中文标点和换行。

③ 长度计算方式选择：

* 若需精准匹配模型 Token 限制（如 GPT-4 的 Token 窗口），可将`length_function`设为 Token 计数器（如`tiktoken.encoding_for_model("gpt-4").encode`），确保`chunk_size`按 Token 数计算，避免字符数与 Token 数差异导致超窗口。

### 4、后续流程衔接

文本分割完成后，下一步将进入 “向量嵌入” 阶段 —— 通过嵌入模型（如 OpenAI Embeddings、Sentence-BERT）将每个`split_documents`中的文本块转化为向量，最终存入向量数据库，为后续 RAG 的 “相似检索” 做准备。

## 四、Text Embedding 文本变数字？神奇的嵌入向量

在 RAG（检索增强生成）流程中，“文本嵌入（Embedding）” 是将 “分割后的文本块” 转化为 “机器可理解的向量” 的核心步骤 —— 通过嵌入模型捕捉文本的语义与语法关系，为后续 “相似性检索” 提供数学基础。以下从核心原理、工具选择、OpenAI Embeddings 实战三方面展开整理：

### 1、文本嵌入的核心原理与价值

#### 1.1、什么是文本嵌入？

文本嵌入是通过**嵌入模型**将非结构化文本（如句子、段落）转化为**固定长度的数值向量**（如 1536 维、3072 维数字串）的过程。

* 关键特性：向量需保留文本的 “语义关联性”——
  * 相似文本（如 “猫抓老鼠” 和 “猫咪捕捉老鼠”）的向量在 “向量空间” 中的距离更近；
  * 无关文本（如 “猫抓老鼠” 和 “行星运行轨道”）的向量距离更远。
* 核心价值：将 “文本语义匹配” 转化为 “向量数学计算”（如计算余弦相似度），让向量数据库能快速找到与用户问题最相关的文本块。

#### 1.2、LangChain 的嵌入逻辑

LangChain 本身不直接实现嵌入功能，而是通过**集成第三方嵌入模型**（如 OpenAI、百度文心一言、Sentence-BERT）提供统一接口，开发者无需关注模型底层细节，只需调用封装好的方法即可完成嵌入。

### 2、主流嵌入模型与工具选择

常用的嵌入模型分为 “商业模型” 和 “开源模型” 两类，需根据场景选择：

| 类型     | 代表模型                                       | 特点                                 | 适用场景                         |
| -------- | ---------------------------------------------- | ------------------------------------ | -------------------------------- |
| 商业模型 | OpenAI Embeddings（如 text-embedding-3-large） | 语义捕捉精准，API 调用便捷，需付费   | 企业级应用、对精度要求高的场景   |
| 开源模型 | Sentence-BERT（如 all-MiniLM-L6-v2）           | 免费，可本地部署，精度略低于商业模型 | 个人项目、预算有限、数据隐私敏感 |

以**OpenAI Embeddings**为例（最常用的商业嵌入模型），讲解具体实现流程。

### 3、OpenAI Embeddings 实战步骤

#### 3.1、前期准备：依赖安装与 API 密钥配置

##### （1）安装依赖

需安装`openai`库（用于调用 OpenAI API）和 LangChain 相关模块：

```sh
pip install openai langchain-openai
```

##### （2）配置 API 密钥

* 方式 1：将 API 密钥存入环境变量（推荐，避免硬编码）：
  * Windows：`set OPENAI_API_KEY="你的API密钥"`
  * macOS/Linux：`export OPENAI_API_KEY="你的API密钥"`
* 方式 2：在代码中直接传入密钥（仅用于测试，不推荐生产环境）。

#### 3.2、初始化 OpenAI Embeddings 实例

从`langchain-openai`导入`OpenAIEmbeddings`，并配置模型参数：

```python
from langchain_openai import OpenAIEmbeddings

# 初始化嵌入模型实例
embeddings = OpenAIEmbeddings(
    model="text-embedding-3-large",  # 指定嵌入模型（可替换为text-embedding-3-small等）
    openai_api_key="你的API密钥"      # 若已设环境变量，可省略该参数（自动读取）
)
```

##### 关键参数说明：

* `model`：嵌入模型名称，OpenAI 官方支持的模型包括：
  * `text-embedding-3-large`：高维度（默认 3072 维），精度高，适合复杂语义匹配；
  * `text-embedding-3-small`：低维度（默认 1536 维），速度快、成本低，适合简单场景；
  * 旧模型（如`text-embedding-ada-002`）：仍可用，但推荐优先使用 v3 系列。
* `dimensions`（可选）：自定义向量维度（仅 v3 系列支持），如`dimensions=1024`—— 可在 “精度” 和 “存储 / 计算成本” 间平衡（维度越小，向量数据库存储压力越小，检索速度越快）。

#### 3.3、文本嵌入核心操作

##### （1）单文本 / 多文本嵌入

通过`embed_query`（单文本，常用于用户问题嵌入）或`embed_documents`（多文本，常用于文本块嵌入）方法实现：

```python
#!pip install openai
from langchain_openai import OpenAIEmbeddings

embeddings_model = OpenAIEmbeddings(model="text-embedding-3-large")

# 使用其他API，则需要提供额外参数
# embeddings_model = OpenAIEmbeddings(model="text-embedding-3-large",
#                                     openai_api_key="<你的API密钥>",
#                                     openai_api_base="https://api.aigc369.com/v1")
```

```python
# 多文本嵌入（分割后的文本块列表，返回向量列表）
text_chunks = [
    "LangChain是一个用于构建LLM应用的框架",
    "OpenAI Embeddings可将文本转化为向量",
    "RAG流程包括文档加载、分割、嵌入、检索、生成"
]
# 对文本块列表进行嵌入，返回向量列表（每个向量对应一个文本块）
embeded_result = embeddings.embed_documents(text_chunks)
len(embeded_result)
```

```sh
2
```

```python
embeded_result
```

```sh
[[-0.005549268744966659,
  -0.016023970990018652,
  -0.01250122560200001,
  ...]]
```

```python
# 查看其他结果
print(f"文本块数量：{len(text_chunks)}")
print(f"向量数量：{len(embeded_result)}")  # 与文本块数量一致
print(f"单个向量维度：{len(embeded_result[0])}")  # 如3072（text-embedding-3-large默认）
```

```python
# 如果希望嵌入向量维度更小，可以通过dimensions参数进行指定
embeddings_model = OpenAIEmbeddings(model="text-embedding-3-large", dimensions=1024)
embeded_result = embeddings_model.embed_documents(["Hello world!", "Hey bro"])
len(embeded_result[0])
```

```sh
1024
```

```python
# 单文本嵌入（用户问题，返回单个向量）
user_query = "什么是RAG流程？"
query_embedding = embeddings.embed_query(user_query)
print(f"用户问题向量维度：{len(query_embedding)}")  # 与文本块向量维度一致（确保可计算相似度）
```

```sh
1024
```

##### （2）向量的本质

`chunk_embeddings`和`query_embedding`均为**浮点数列表**，示例（简化）：
`[0.023, -0.012, 0.056, ..., 0.031]`（共 3072 个浮点数，对应 3072 维向量）。
这些数值无直观含义，但通过数学计算（如余弦相似度）可衡量文本间的语义关联。

#### 4. 嵌入结果的后续用途

文本块的向量（`chunk_embeddings`）会被存入**向量数据库**（如 Pinecone、Chroma），用户问题的向量（`query_embedding`）会用于在向量数据库中 “相似性检索”—— 找到与问题向量距离最近的文本块向量，进而获取对应的原始文本块，为 LLM 生成回答提供上下文。

## 五、Vector Store 向量数据库，AI模型的海马体

在 RAG（检索增强生成）流程中，“向量数据库” 是衔接 “文本嵌入” 与 “相似性检索” 的核心组件 —— 它专门存储文本块的向量，并通过 “相似性搜索” 快速匹配用户问题与知识库内容，解决传统数据库无法处理非结构化数据语义匹配的痛点。以下从核心原理、工具实战、检索流程三方面展开整理：

### 1、向量数据库的核心价值：为何不用传统数据库？

传统数据库与向量数据库的核心差异在于 “数据类型适配” 和 “查询机制”，向量数据库的优势集中在**非结构化数据的语义匹配**：

| 维度             | 传统数据库（如 MySQL、PostgreSQL）                           | 向量数据库（如 FAISS、Chroma、Pinecone）                     |
| ---------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **适配数据类型** | 结构化数据（如员工 ID、入职日期，有固定格式和字段定义）      | 非结构化数据的向量（如文本嵌入向量、图像向量，无固定格式）   |
| **查询机制**     | 精准匹配（如 “员工 ID=002”“工资 > 5000”），依赖关键词或数值比对 | 相似性搜索（如计算向量间余弦相似度），基于语义关联匹配，不依赖精确关键词 |
| **典型场景局限** | 无法处理 “语义相似但关键词不同” 的查询（如查 “擅长财务预算”，无法匹配 “预算控制与财务规划经验丰富”） | 擅长处理语义匹配场景，可快速找到与用户问题 “意思相近” 的文本块 |

**结论**：在 RAG 中，需用向量数据库存储文本块向量，实现 “用户问题→语义匹配→相关文本块” 的高效检索，这是传统数据库无法替代的。

### 2、主流向量数据库与工具选择

常用的向量数据库分为 “开源本地型” 和 “商业云服务型”，本节以**FAISS**（Facebook 开源，轻量易上手，适合本地测试）为例，讲解具体实现流程：

| 类型       | 代表数据库         | 特点                                         | 适用场景                             |
| ---------- | ------------------ | -------------------------------------------- | ------------------------------------ |
| 开源本地   | FAISS、Chroma      | 免费，可本地部署，无需网络，适合小体量知识库 | 个人项目、本地测试、数据隐私敏感场景 |
| 商业云服务 | Pinecone、Weaviate | 支持大规模数据，高可用，需付费               | 企业级应用、超大规模知识库           |

### 3、FAISS 向量数据库实战步骤

#### 3.1. 前期准备：依赖安装与导入

安装 FAISS 依赖

FAISS 提供 CPU 和 GPU 版本，本地测试优先安装 CPU 版本：

```
pip install faiss-cpu langchain-community
```

导入核心模块

需导入 LangChain 的`FAISS`向量存储类，以及前期准备好的 “分割后文本块” 和 “嵌入模型实例”：

```python
# 导入FAISS向量数据库
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import TextLoader
# 导入前期准备的组件（文本块+嵌入模型）
from langchain_text_splitters import RecursiveCharacterTextSplitter  # 已分割好的文本块依赖
from langchain_openai import OpenAIEmbeddings  # 已初始化的嵌入模型
```

#### 3.2. 文本块向量存储：一键生成并存储向量

LangChain 封装了`FAISS.from_documents`方法，可自动完成 “文本块嵌入→向量存储” 的全流程，无需手动处理向量生成：

```python
# 假设前期已完成文本分割，得到split_documents（分割后的Documents列表）
# 假设已初始化嵌入模型embeddings（如OpenAIEmbeddings实例）

# 1. 生成文本块向量并存储到FAISS数据库
vector_db = FAISS.from_documents(
    documents=split_documents,  # 分割后的文本块列表（Documents类型）
    embedding=embeddings        # 嵌入模型实例（用于将文本块转为向量）
)

# （可选）保存FAISS数据库到本地，后续可直接加载（避免重复嵌入）
vector_db.save_local("faiss_local_db")  # 保存到当前目录的faiss_local_db文件夹

# （可选）从本地加载FAISS数据库
# vector_db = FAISS.load_local("faiss_local_db", embeddings)
```

* **核心逻辑**：`from_documents`方法会遍历`split_documents`中的每个文本块，通过`embeddings`模型生成向量，再将 “向量 + 原始文本块” 一起存入 FAISS 数据库。
* **优势**：无需手动调用嵌入模型，LangChain 自动衔接 “嵌入→存储” 流程，简化代码。

#### 3.3. 相似性检索：找到与用户问题最相关的文本块

向量数据库的核心功能是 “相似性检索”，LangChain 通过 “检索器（Retriever）” 封装检索逻辑，步骤如下：

（1）创建检索器

调用向量数据库的`as_retriever`方法，生成检索器实例，可配置 “返回文本块数量”（默认返回 Top 4）：

```python
# 创建检索器，设置返回Top 3个最相关的文本块
retriever = vector_db.as_retriever(search_kwargs={"k": 3})
```

* `search_kwargs={"k": 3}`：控制检索结果数量，k 值越大，返回的相关文本块越多（需平衡 “覆盖度” 与 “精准度”，通常 k=3~5 即可）。

（2）执行相似性检索

检索器实现了 LangChain 的`Runnable`接口，可直接调用`invoke`方法传入用户问题，返回最相关的文本块列表：

```python
# 用户问题（示例：查询与“财务预算”相关的内容）
user_query = "如何制定公司的财务预算？"

# 执行检索，返回Top 3个相关文本块（Documents列表）
relevant_chunks = retriever.invoke(user_query)

# 查看检索结果
print(f"检索到的相关文本块数量：{len(relevant_chunks)}")
for i, chunk in enumerate(relevant_chunks, 1):
    print(f"\n【相关文本块{i}】")
    print(f"内容：{chunk.page_content}")  # 原始文本块内容
    print(f"来源：{chunk.metadata}")      # 文本块元数据（如文档路径、页码，可选）
```

（3）检索结果说明

* 结果排序：`relevant_chunks`按 “相似度从高到低” 排序，第一个文本块与用户问题语义最接近。
* 语义匹配逻辑：检索器通过计算 “用户问题向量” 与 “数据库中所有文本块向量” 的余弦相似度，筛选出相似度最高的文本块，即使关键词不完全匹配（如用户问 “财务预算”，可匹配 “预算控制与财务规划” 相关文本）。

#### 3.4. 完整代码

```python
# -------------------------- 1. 安装依赖（仅首次执行需运行） --------------------------
# 安装FAISS向量数据库的CPU版本（用于本地存储文本向量，轻量易上手）
# !pip install faiss-cpu


# -------------------------- 2. 导入核心模块（对应RAG各环节组件） --------------------------
# 1. 文档加载器：从本地加载TXT纯文本文件
from langchain_community.document_loaders import TextLoader
# 2. 向量数据库：FAISS，用于存储文本块向量并支持相似性检索
from langchain_community.vectorstores import FAISS
# 3. 嵌入模型：OpenAI Embeddings，用于将文本块转化为语义向量
from langchain_openai.embeddings import OpenAIEmbeddings
# 4. 文本分割器：递归字符分割器，将长文本切分为语义完整的短文本块
from langchain_text_splitters import RecursiveCharacterTextSplitter


# -------------------------- 3. 步骤1：加载本地TXT文档（RAG-数据准备） --------------------------
# 初始化TextLoader，传入TXT文件路径（需确保文件在当前代码运行目录下）
# 作用：将TXT文件内容加载为LangChain标准的Document对象（含文本内容和元数据）
loader = TextLoader("./demo2.txt")
# 执行加载操作，返回Document对象列表（1个Document对应整个TXT文件，后续会分割）
docs = loader.load()


# -------------------------- 4. 步骤2：文本分割（RAG-数据处理） --------------------------
# 初始化递归字符分割器，配置分割参数（适配中文文本特性）
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,          # 单个文本块的最大长度（单位：字符），避免超LLM上下文窗口
    chunk_overlap=40,        # 相邻文本块的重叠长度（40字符），保持语义连贯性（如避免分割在句子中间）
    separators=["\n\n", "\n", "。", "！", "？", "，", "、", ""]  # 中文适配分割符优先级
    # 分割逻辑：优先按段落（\n\n）→换行（\n）→句子结尾（。！？）→短句分隔（，、）→兜底（任意位置）
)

# 执行分割：将加载的Document列表（整个TXT）切分为多个短文本块Document
# 输出texts为分割后的Document列表，每个元素是一个语义完整的短文本块
texts = text_splitter.split_documents(docs)


# -------------------------- 5. 步骤3：文本嵌入与向量存储（RAG-数据存储） --------------------------
# 初始化OpenAI嵌入模型（需确保环境变量已配置OPENAI_API_KEY，或通过openai_api_key参数传入）
# 作用：将文本块转化为含语义信息的向量（默认text-embedding-3-small模型，1536维向量）
embeddings_model = OpenAIEmbeddings()

# 1. 将分割后的文本块（texts）通过嵌入模型生成向量
# 2. 自动将“向量+原始文本块”存入FAISS向量数据库
# 输出db为FAISS数据库实例，后续可用于相似性检索
db = FAISS.from_documents(texts, embeddings_model)


# -------------------------- 6. 步骤4：创建检索器（RAG-检索准备） --------------------------
# 将FAISS数据库包装为检索器（Retriever），简化相似性检索调用
# 检索器是LangChain的标准Runnable组件，支持invoke方法快速查询
retriever = db.as_retriever()
# 可通过search_kwargs配置检索参数，如search_kwargs={"k":3}（返回Top3相关文本块，默认k=4）


# -------------------------- 7. 步骤5：相似性检索（RAG-检索执行） --------------------------
# 第一次检索：查询“卢浮宫这个名字怎么来的？”
# invoke方法会自动完成：问题→向量转化→FAISS相似性计算→返回TopN相关文本块
retrieved_docs = retriever.invoke("卢浮宫这个名字怎么来的？")
# 打印第一个最相关文本块的内容（page_content为文本块核心内容）
print("【检索结果1：卢浮宫名字由来】")
print(retrieved_docs[0].page_content)
print("-" * 50)  # 分隔线，便于区分不同查询结果

# 第二次检索：查询“卢浮宫在哪年被命名为中央艺术博物馆”
retrieved_docs = retriever.invoke("卢浮宫在哪年被命名为中央艺术博物馆")
# 打印第一个最相关文本块的内容
print("【检索结果2：卢浮宫命名为中央艺术博物馆的年份】")
print(retrieved_docs[0].page_content)
```

### 4、当前 RAG 流程进展与后续衔接

截至目前，已完成 RAG 的前两步核心流程：

1. **数据准备**：文档加载→文本分割→文本嵌入→向量存储（存入 FAISS）；
2. **相似检索**：用户问题→问题嵌入→向量数据库相似性搜索→返回相关文本块。

下一步需完成 RAG 的最后一步：**结合生成**—— 将 “用户问题 + 相关文本块” 合并为提示，传给 LLM 生成基于知识库的精准回答。此外，若需实现 “带记忆的连续对话”，还需将检索器与 “对话记忆”“提示模板” 结合，但 LangChain 提供了更简化的方案（如`RetrievalChain`），无需手动拼接流程。

## 六、Retrieval Chain 开箱即用的检索增强对话链

在完成 “文档加载→分割→嵌入→向量存储→相似检索” 后，核心需求是 “让 AI 结合检索到的外部文档 + 对话记忆生成回答”。LangChain 提供的`Conversational Retrieval Chain`（检索增强对话链）已封装好全流程，无需手动拼接 “问题 + 文档 + 记忆”，以下从**核心组件、链创建、使用方法、定制化功能**四方面整理：

### 1、Conversational Retrieval Chain 的核心价值

该链是 RAG 与 “对话记忆” 的结合体，解决两大关键问题：

1. **检索增强**：自动将用户问题对应的 “相关文档片段” 作为上下文传给 AI，避免 AI 依赖固有知识（减少幻觉）；
2. **对话记忆**：维持多轮对话连贯性，AI 能关联历史对话（如用户追问 “它的具体时间”，AI 知道 “它” 指代上一轮提到的 “卢浮宫命名事件”）。

相比普通`ConversationChain`（仅带记忆）或`RetrievalChain`（仅带检索），它同时具备 “检索外部知识” 和 “记忆上下文” 的能力，是实现 “带知识库的连续对话 AI” 的核心工具。

### 2、链创建前的核心组件准备

需提前准备 3 个关键组件（均为前序步骤已涉及的内容，可直接复用）：

| 组件类型                | 作用                                   | 实现方式（示例）                                     |
| ----------------------- | -------------------------------------- | ---------------------------------------------------- |
| **聊天模型（LLM）**     | 生成回答的核心，需支持对话格式         | 使用`ChatOpenAI`（如 GPT-3.5/4）                     |
| **检索器（Retriever）** | 从向量数据库中检索与问题相关的文档片段 | 从 FAISS/Chroma 等向量数据库通过`as_retriever()`生成 |
| **对话记忆（Memory）**  | 储存历史对话，维持多轮连贯性           | 使用`ConversationBufferMemory`等记忆类型，需特殊配置 |

#### 关键：记忆组件的特殊配置

`Conversational Retrieval Chain`对记忆的**变量名有固定要求**，需确保：

* `memory_key="chat_history"`：链默认通过`chat_history`键读取 / 更新历史对话，记忆实例的`memory_key`必须与此一致；
* `return_messages=True`：记忆需储存为消息对象列表（而非字符串），确保链能正确解析历史对话；
* `output_key="answer"`：链默认将 AI 的回答存入`answer`键，记忆需指定该键以更新对话历史。

示例代码（初始化记忆）：

```python
from langchain.memory import ConversationBufferMemory

# 初始化符合链要求的记忆实例
memory = ConversationBufferMemory(
    memory_key="chat_history",  # 必须为"chat_history"，与链的变量名匹配
    return_messages=True,       # 储存为消息列表
    output_key="answer"         # 链的输出结果中，AI回答对应"answer"键
)
```

### 3、创建 Conversational Retrieval Chain

#### 3.1. 导入核心模块

```python
# 导入链、聊天模型、记忆（前序步骤已导入检索器和向量数据库）
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
```

#### 3.2. 初始化其他组件（复用前序代码）

```python
# -------------------------- 步骤1：加载文档 --------------------------
# 初始化文本加载器，指定要加载的TXT文件路径
loader = TextLoader("./demo2.txt")
# 执行加载，返回包含文档内容的列表（每个元素是一个Document对象）
docs = loader.load()


# -------------------------- 步骤2：文本分割 --------------------------
# 初始化递归字符分割器，配置中文适配参数
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,          # 单个文本块最大长度（500字符）
    chunk_overlap=40,        # 相邻文本块重叠长度（40字符，保持上下文连贯）
    separators=["\n", "。", "！", "？", "，", "、", ""]  # 中文分割符优先级
)

# 将加载的文档分割为多个短文本块
texts = text_splitter.split_documents(docs)


# -------------------------- 步骤3：创建向量数据库 --------------------------
# 初始化OpenAI嵌入模型（用于将文本转换为向量）
embeddings_model = OpenAIEmbeddings()
# 将分割后的文本块转换为向量并存储到FAISS数据库
db = FAISS.from_documents(texts, embeddings_model)


# -------------------------- 步骤4：创建检索器 --------------------------
# 将向量数据库转换为检索器，用于后续查询相关文本块
retriever = db.as_retriever()


# -------------------------- 步骤5：初始化核心组件 --------------------------
# 初始化聊天模型（使用GPT-3.5-turbo）
model = ChatOpenAI(model="gpt-3.5-turbo")

# 初始化对话记忆（适配ConversationalRetrievalChain的要求）
memory = ConversationBufferMemory(
    return_messages=True,       # 以消息对象列表形式存储记忆
    memory_key='chat_history',  # 记忆在链中的变量名（需与链要求一致）
    output_key='answer'         # 链输出中AI回答的键名（需与链要求一致）
)
```

#### 3.3. 调用`from_llm`方法创建链

链的创建通过`ConversationalRetrievalChain.from_llm()`实现，参数需包含 “模型、检索器、记忆” 三大核心组件：

```python
# 创建检索增强对话链
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,                # 聊天模型
    retriever=retriever,    # 文档检索器
    memory=memory,          # 对话记忆
    verbose=False           # 可选：True则打印链的运行日志，便于调试
)
```

### 4、使用链实现 “带记忆的 RAG 对话”

#### 4.1. 核心调用方式：`invoke`方法

链的输入是**含 “question” 键的字典**（`question`对应用户当前问题），输出是含 “answer”“chat\_history” 等键的字典：

```python
# 第一轮对话：用户提问（关于外部文档的问题）
user_question1 = "卢浮宫这个名字怎么来的？"
response1 = qa_chain.invoke({"chat_history": memory, "question": user_question1})

# 查看输出结果
print(f"用户问题1：{response1['question']}")
print(f"AI回答1：{response1['answer']}\n")

# 第二轮对话：追问（验证记忆，依赖上一轮上下文）
# user_question2 = "对应的拉丁语是什么呢？"
user_question2 = "它在哪年被命名为中央艺术博物馆？"  # “它”指代卢浮宫
response2 = qa_chain.invoke({"chat_history": memory, "question": user_question2})

print(f"用户问题2：{response2['question']}")
print(f"AI回答2：{response2['answer']}")
print(f"历史对话：{response2['chat_history']}")  # 查看储存的历史对话
```

#### 4.2. 关键逻辑说明

* **检索自动触发**：调用`invoke`时，链会先将`question`传入检索器，获取相关文档片段，再将 “历史对话（chat\_history）+ 问题（question）+ 相关文档” 合并为提示传给 LLM；
* **记忆自动更新**：每轮对话后，链会将 “用户问题 + AI 回答” 自动存入`memory`，无需手动调用`save_context`；
* **上下文连贯性**：第二轮追问中，AI 能识别 “它” 指代 “卢浮宫”，证明记忆生效；同时回答基于检索到的文档片段，证明 RAG 生效。

### 5、链的定制化功能

#### 5.1. 返回参考文档片段（验证 AI 回答可信度）

默认情况下，链仅返回 AI 的回答，若需验证 “回答是否来自外部文档”（避免幻觉），可在创建链时设置`return_source_documents=True`：

```python
# 创建链时开启“返回参考文档”
qa_chain_with_source = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=memory,
    return_source_documents=True  # 开启后，输出会包含"source_documents"键
)

# 调用并查看参考文档
response = qa_chain_with_source.invoke({"chat_history": memory, "question": "卢浮宫名字怎么来的？"})
print(f"AI回答：{response['answer']}")
print("\n【参考文档片段（Top1相关）】")
print(response["source_documents"][0].page_content)  # 打印最相关的文档片段
print(f"文档来源：{response['source_documents'][0].metadata}")  # 打印文档元数据（如路径、页码）
```

* 作用：`source_documents`是检索到的相关文档片段列表（按相似度排序），可直接定位 AI 回答的信息来源，验证回答真实性。

#### 5.2. 潜在问题：文档片段过长导致超窗口

当前默认逻辑是 “将所有检索到的文档片段（如 Top3）全部传给 LLM”，若片段过长或数量过多，可能超过模型上下文窗口限制。解决方案（后续讲解）包括：

* 限制检索片段数量（如`retriever = db.as_retriever(search_kwargs={"k":2})`）；
* 对检索到的片段进行二次总结（用`create_doc_summary`参数）；
* 使用支持大窗口的模型（如 GPT-4 Turbo 128k）。

### 6、核心流程总结

| 步骤           | 操作                          | 链的作用                                               |
| -------------- | ----------------------------- | ------------------------------------------------------ |
| 1. 组件准备    | 初始化 LLM、Retriever、Memory | 为链提供 “生成核心”“知识来源”“上下文记忆”              |
| 2. 创建链      | 调用`from_llm`整合组件        | 封装 “检索→拼接提示→生成回答→更新记忆” 全流程          |
| 3. 发起对话    | 调用`invoke`传入用户问题      | 自动触发检索，结合历史对话生成回答，无需手动拼接上下文 |
| 4. 验证 / 定制 | 开启`return_source_documents` | 查看参考文档，验证回答可信度，避免 AI 幻觉             |

通过`Conversational Retrieval Chain`，可快速实现 “带知识库 + 带记忆” 的对话 AI（如企业知识库问答、文档助手），是 LangChain 中 RAG 落地的核心工具之一。

## 七、Documents Chain 把外部文档塞给模型的不同方式

在 RAG（检索增强生成）中，“文档传递策略” 决定了如何将检索到的相关文本片段传给 LLM 生成回答。除默认的`stuff`（填充）法外，LangChain 还支持`map_reduce`（映射规约）、`refine`（优化）、`map_rerank`（映射重排序）3 种策略，分别适配 “片段过长 / 过多”“需精准迭代优化”“需快速筛选最优片段” 等场景。以下从**原理、优缺点、适用场景、代码实现**四方面展开整理：

### 1、4 种文档传递策略核心原理

所有策略的前提是 “已通过检索器获取 Top N 相关文本片段”，核心差异在于 “片段如何处理并传给 LLM”：

#### 1.1. Stuff（填充法）：默认策略，简单直接

##### 原理

将**所有检索到的文本片段拼接成一个长文本**，与用户问题、对话记忆合并为一个提示，一次性传给 LLM 生成回答。

* 流程：检索片段 → 拼接片段 → 单次 LLM 调用 → 生成回答。

##### 优缺点

| 优点                                          | 缺点                                             |
| --------------------------------------------- | ------------------------------------------------ |
| 1. 仅需 1 次 LLM 调用，速度快、成本低；       | 1. 片段总长度易超 LLM 上下文窗口限制；           |
| 2. LLM 能看到所有片段的完整关联，信息无割裂； | 2. 仅适合片段数量少（如 Top3）、单片段短的场景。 |

##### 适用场景

* 知识库片段短（如单片段≤500 字符）、检索结果数量少（如 Top2-3）；
* 对响应速度和成本敏感，且 LLM 上下文窗口足够容纳所有片段（如 GPT-3.5 4k Token）。

#### 1.2. Map Reduce（映射规约法）：多片段融合

##### 原理

分 “Map（映射）” 和 “Reduce（规约）” 两阶段处理，解决 “片段总长度超窗口” 问题：

1. **Map 阶段**：将每个检索片段单独传给 LLM，生成该片段对应的 “局部回答”（1 个片段→1 个局部回答，N 个片段→N 次 LLM 调用）；
2. **Reduce 阶段**：将所有 “局部回答” 拼接成 “回答合集”，传给 LLM 生成 “整合所有信息的最终回答”（1 次 LLM 调用）。

* 流程：检索片段 → 逐个片段生成局部回答（Map） → 合并局部回答 → 生成最终回答（Reduce）。

##### 优缺点

| 优点                                                     | 缺点                                                  |
| -------------------------------------------------------- | ----------------------------------------------------- |
| 1. 支持超长篇段 / 多片段（单片段不超窗口即可）；         | 1. 需 N+1 次 LLM 调用（N 为片段数），成本高、速度慢； |
| 2. 能融合多个片段的信息，适合复杂查询；                  | 2. Reduce 阶段可能遗漏局部回答的细节；                |
| 3. 可并行处理 Map 阶段，提升效率（LangChain 默认支持）。 | 3. 局部回答间若有冲突，LLM 需手动判断，易出错。       |

##### 适用场景

* 检索片段数量多（如 Top5-10）或单片段较长，但每个片段独立包含部分信息；
* 需整合多来源信息的复杂查询（如 “总结文档中 3 个产品的定价策略”）。

#### 1.3. Refine（优化法）：迭代式精准优化

##### 原理

按片段顺序**逐次迭代优化回答**，让 LLM 基于新片段不断修正已有回答，而非独立处理每个片段：

1. 第 1 轮：用 “第 1 个片段 + 用户问题” 生成初始回答；
2. 第 2 轮：用 “初始回答 + 第 2 个片段 + 用户问题” 生成优化后的回答；
3. 第 N 轮：用 “上一轮回答 + 第 N 个片段 + 用户问题” 生成最终回答（N 个片段→N 次 LLM 调用）。

* 流程：检索片段 → 基于第 1 片段生成初始回答 → 结合第 2 片段优化 → ... → 结合第 N 片段生成最终回答。

##### 优缺点

| 优点                                                | 缺点                                                        |
| --------------------------------------------------- | ----------------------------------------------------------- |
| 1. 回答精度高，LLM 能基于新信息逐次修正，减少遗漏； | 1. 需 N 次 LLM 调用，成本最高、速度最慢；                   |
| 2. 能处理超长篇段 / 多片段，且保留上下文关联；      | 2. 片段顺序影响最终结果（若关键片段在后，前期回答易偏差）； |
| 3. 适合需要深度理解片段逻辑的场景。                 | 3. 无法并行处理，必须按顺序迭代。                           |

##### 适用场景

* 片段间有逻辑关联（如文档章节顺序），需逐步深入理解的查询（如 “解析论文的实验方法与结论”）；
* 对回答精度要求极高，可接受高成本和慢速度（如专业领域问答、学术解读）。

#### 1.4. Map Rerank（映射重排序法）：快速筛选最优

##### 原理

分 “Map（映射）” 和 “Rerank（重排序）” 两阶段，聚焦 “筛选最优片段” 而非 “融合信息”：

1. **Map 阶段**：将每个检索片段单独传给 LLM，要求 LLM 生成 “局部回答” 并对 “该片段与问题的相关性” 打分（如 1-10 分，N 个片段→N 次 LLM 调用）；
2. **Rerank 阶段**：筛选出 “相关性得分最高” 的局部回答，直接作为最终回答（无需额外 LLM 调用）。

* 流程：检索片段 → 逐个片段生成局部回答 + 打分（Map） → 选择最高分回答作为最终结果（Rerank）。

##### 优缺点

| 优点                                                | 缺点                                                    |
| --------------------------------------------------- | ------------------------------------------------------- |
| 1. 相比 Map Reduce，少 1 次 Reduce 调用，成本略低； | 1. 不融合多片段信息，仅用最优片段回答，易遗漏其他信息； |
| 2. 速度比 Refine 快，且能解决超窗口问题；           | 2. 依赖 LLM 打分准确性，若打分偏差，结果会出错；        |
| 3. 适合只需单个片段即可回答的场景。                 | 3. 需在提示中明确打分规则，提示设计较复杂。             |

##### 适用场景

* 问题答案仅存在于某一个片段中（如 “文档中提到的卢浮宫命名年份是多少”）；
* 需快速得到答案，且可接受 “不融合多片段信息”（如简单事实查询）。

### 2、4 种策略对比总结

| 对比维度       | Stuff（填充）    | Map Reduce（映射规约） | Refine（优化）       | Map Rerank（映射重排序） |
| -------------- | ---------------- | ---------------------- | -------------------- | ------------------------ |
| LLM 调用次数   | 1 次             | N+1 次（N 为片段数）   | N 次                 | N 次                     |
| 响应速度       | 最快             | 中等（可并行 Map）     | 最慢                 | 中等（可并行 Map）       |
| 成本           | 最低             | 较高                   | 最高                 | 较高                     |
| 信息融合能力   | 强（全片段可见） | 较强（整合局部回答）   | 强（逐次优化）       | 弱（仅用最优片段）       |
| 上下文窗口限制 | 易超限制         | 无（单片段不超即可）   | 无（单片段不超即可） | 无（单片段不超即可）     |
| 适用问题类型   | 简单事实查询     | 复杂多信息整合查询     | 深度逻辑理解查询     | 单片段事实查询           |

### 3、代码实现：指定 Chain Type

在 LangChain 中，通过`ConversationalRetrievalChain`创建链时，只需给`chain_type`参数指定对应策略名称，即可切换文档传递方式。以下是完整代码示例（基于前序 RAG 流程）：

#### 3.1. 前期准备（复用组件）

```python
# -------------------------- 导入核心模块 --------------------------
# 检索增强对话链：结合检索、记忆和LLM生成回答
from langchain.chains import ConversationalRetrievalChain
# 对话记忆：存储历史对话，支持连续对话
from langchain.memory import ConversationBufferMemory
# 文档加载器：加载本地TXT文本文件
from langchain_community.document_loaders import TextLoader
# 向量数据库：FAISS，用于存储文本向量并支持相似检索
from langchain_community.vectorstores import FAISS
# 聊天模型：OpenAI的对话模型（如GPT-3.5/4）
from langchain_openai import ChatOpenAI
# 嵌入模型：将文本转换为向量的模型
from langchain_openai.embeddings import OpenAIEmbeddings
# 文本分割器：将长文本切分为语义完整的短文本块
from langchain_text_splitters import RecursiveCharacterTextSplitter


# -------------------------- 步骤1：加载文档 --------------------------
# 初始化文本加载器，指定要加载的TXT文件路径
loader = TextLoader("./demo2.txt")
# 执行加载，返回包含文档内容的列表（每个元素是一个Document对象）
docs = loader.load()


# -------------------------- 步骤2：文本分割 --------------------------
# 初始化递归字符分割器，配置中文适配参数
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,          # 单个文本块最大长度（500字符）
    chunk_overlap=40,        # 相邻文本块重叠长度（40字符，保持上下文连贯）
    separators=["\n", "。", "！", "？", "，", "、", ""]  # 中文分割符优先级
)

# 将加载的文档分割为多个短文本块
texts = text_splitter.split_documents(docs)


# -------------------------- 步骤3：创建向量数据库 --------------------------
# 初始化OpenAI嵌入模型（用于将文本转换为向量）
embeddings_model = OpenAIEmbeddings()
# 将分割后的文本块转换为向量并存储到FAISS数据库
db = FAISS.from_documents(texts, embeddings_model)


# -------------------------- 步骤4：创建检索器 --------------------------
# 将向量数据库转换为检索器，用于后续查询相关文本块
retriever = db.as_retriever()


# -------------------------- 步骤5：初始化核心组件 --------------------------
# 初始化聊天模型（使用GPT-3.5-turbo）
model = ChatOpenAI(model="gpt-3.5-turbo")

# 初始化对话记忆（适配ConversationalRetrievalChain的要求）
memory = ConversationBufferMemory(
    return_messages=True,       # 以消息对象列表形式存储记忆
    memory_key='chat_history',  # 记忆在链中的变量名（需与链要求一致）
    output_key='answer'         # 链输出中AI回答的键名（需与链要求一致）
)
```

#### 3.2. 切换不同 Chain Type

只需修改`chain_type`参数的值（支持`"stuff"`“`map_reduce`”“`refine`”“`map_rerank`”）：

##### （1）Stuff 策略（默认，可省略 chain\_type 参数）

```python
# 创建Stuff策略的链
qa_chain_stuff = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=memory,
    chain_type="stuff",  # 默认值，可省略
    return_source_documents=True  # 可选：返回参考片段
)

# 调用链
response = qa_chain_stuff.invoke({"question": "卢浮宫名字怎么来的？"})
print("Stuff策略回答：", response["answer"])
```

##### （2）Map Reduce 策略

```python
qa_chain_map_reduce = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=memory,
    chain_type="map_reduce",
    return_source_documents=True
)

response = qa_chain_map_reduce.invoke({"question": "总结文档中提到的3个博物馆特点"})
print("Map Reduce策略回答：", response["answer"])
```

##### （3）Refine 策略

```python
qa_chain_refine = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=memory,
    chain_type="refine",
    return_source_documents=True
)

response = qa_chain_refine.invoke({"question": "解析文档中实验的步骤与结论"})
print("Refine策略回答：", response["answer"])
```

##### （4）Map Rerank 策略

```python
qa_chain_map_rerank = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=memory,
    chain_type="map_rerank",
    return_source_documents=True,
    # 可选：指定打分规则（需与LLM提示匹配）
    chain_type_kwargs={
        "prompt": "请回答问题，并对该片段与问题的相关性打分（1-10分，格式：回答：xxx；得分：x）"
    }
)

response = qa_chain_map_rerank.invoke({"question": "卢浮宫在哪年被命名为中央艺术博物馆？"})
print("Map Rerank策略回答：", response["answer"])
```

### 4、关键注意事项

1. **提示模板适配**：`map_reduce`/`refine`/`map_rerank`需 LLM 理解特定指令（如 “生成局部回答”“打分”），LangChain 默认提供基础提示模板，若需定制（如专业领域术语），可通过`chain_type_kwargs={"prompt": 自定义提示}`修改。
2. **片段数量控制**：`map_reduce`/`refine`/`map_rerank`的 LLM 调用次数与片段数（N）正相关，建议通过`retriever`的`search_kwargs={"k": 3}`控制 N（通常 3-5 为宜），平衡精度与成本。
3. **模型选择**：复杂策略（如`refine`）建议用更擅长逻辑推理的模型（如 GPT-4），简单策略（如`stuff`）可用 GPT-3.5 降低成本。

通过选择合适的文档传递策略，可在 “精度、速度、成本” 之间找到最优平衡，让 RAG 系统适配不同场景的需求（如快速查询、深度分析、多信息整合）。

---

---
url: /Python/AI大模型应用开发/9_给AI模型添加记忆.md
---

# 给AI模型添加记忆

## 一、Memory—让AI模型不再忘掉对话

### 1、核心问题：模型缺乏上下文记忆

AI 模型本身不具备 “上文记忆” 能力，若仅单次传递当前提示（如先问 “李白是谁”，再问 “他是哪国人”），模型会因缺失前序对话信息而无法正确响应。

**解决方案**：将历史对话记录存入 “消息列表”，每轮对话时将 “历史消息 + 当前提示” 一并传给模型，让模型基于完整上下文生成回应。

### 2、关键工具：ConversationBufferMemory（对话缓冲记忆）

LangChain 的`memory`模块提供`ConversationBufferMemory`类，专门用于储存和管理历史对话，核心功能如下：

#### 2.1. 初始化记忆实例

需设置`return_messages=True`：确保储存的历史对话以 “消息对象列表” 形式存在（而非字符串），便于后续拼接进提示。

示例：

```python
from langchain.memory import ConversationBufferMemory

memory = ConversationBufferMemory(return_messages=True)
```

#### 2.2. 查看记忆内容

调用`load_memory_variables`方法（传入空字典），可查看当前记忆中储存的历史对话，初始时`history`对应空列表：

```python
memory.load_memory_variables({})  # 输出：{"history": []}
```

#### 2.3. 储存历史对话

通过`save_context`方法手动储存单轮对话（用户输入 + AI 输出），参数为两个字典（分别对应用户和 AI 的内容）：

```python
# 储存“用户问李白是谁”和“AI的回应”
memory.save_context(
    {"input": "李白是谁？"},
    {"output": "李白是唐代著名诗人，被誉为‘诗仙’。"}
)
```

再次查看记忆时，`history`列表会包含该轮对话的消息对象。

### 3、构建带记忆的提示模板

需在提示模板中预留 “历史对话” 的位置，确保历史消息能拼接在当前提示前（系统消息需放在最前面），关键依赖`MessagesPlaceholder`：

#### 3.1. 用 MessagesPlaceholder 占位历史消息

`langchain.prompts`模块的`MessagesPlaceholder`类，用于在提示模板中为 “历史消息列表” 占位，需指定`variable_name="history"`（与记忆中储存历史的键名一致）。

#### 3.2. 完整提示模板结构

示例（系统消息 + 历史消息占位 + 当前用户提示）：

```python
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate, SystemMessagePromptTemplate

prompt = ChatPromptTemplate.from_messages([
    SystemMessagePromptTemplate.from_template("你是一个知识问答助手，基于上下文回答问题。"),
    MessagesPlaceholder(variable_name="history"),  # 历史消息占位
    HumanMessagePromptTemplate.from_template("{user_input}")  # 当前用户输入
])
```

### 4、构建带记忆的对话链（手动实现）

#### 4.1. 组件串联逻辑

1. 从`memory`中加载历史对话（`history`列表）。
2. 将 “历史对话 + 当前用户输入” 传入提示模板，生成完整提示。
3. 调用模型生成回应。
4. 用`memory.save_context`储存本轮 “用户输入 + AI 回应”，更新历史记忆。

#### 4.2. 示例流程

```python
# 1. 加载历史对话
history = memory.load_memory_variables({})["history"]

# 2. 生成完整提示（历史+当前输入）
user_input = "他是哪国人？"
prompt_value = prompt.invoke({"history": history, "user_input": user_input})

# 3. 模型生成回应（假设model为已定义的聊天模型）
response = model.invoke(prompt_value)

# 4. 储存本轮对话到记忆
memory.save_context({"input": user_input}, {"output": response.content})
```

手动实现需反复调用 “加载记忆 - 生成提示 - 储存记忆”，流程较繁琐。LangChain 已提供**现成的带记忆对话链**（无需手动管理记忆）。

## 二、ConversationChain—开箱即用的带记忆对话链

### 1、ConversationChain：简化带记忆的对话实现

LangChain 的`chains`模块提供`ConversationChain`（对话链），是专门用于实现 “带上下文记忆对话” 的现成工具。它已封装好 “记忆加载、提示拼接、模型调用、记忆更新” 的完整流程，无需手动管理历史对话，大幅简化代码。

### 2、核心使用步骤

#### 2.1. 准备依赖组件

需提前创建两个核心组件，作为`ConversationChain`的参数：

* **聊天模型（如 ChatModel）**：用于生成对话回应。
* **记忆实例（如 ConversationBufferMemory）**：用于储存历史对话，需确保`return_messages=True`（储存为消息列表）。

示例：

```python
from langchain.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferMemory

# 1. 创建模型
model = ChatOpenAI()
# 2. 创建记忆（自动储存历史对话）
memory = ConversationBufferMemory(return_messages=True)
```

#### 2.2. 初始化 ConversationChain

通过`ConversationChain`类，传入`llm`（模型）和`memory`（记忆）参数，即可完成对话链构建：

```python
from langchain.chains import ConversationChain

# 构建对话链
conversation_chain = ConversationChain(
    llm=model,
    memory=memory
)
```

#### 2.3. 调用对话链实现多轮对话

通过`invoke`方法与链交互，参数为含`input`键的字典（`input`对应当前用户提示）：

```python
# 第一轮对话
response1 = conversation_chain.invoke({"input": "李白是谁？"})
print(response1["response"])  # 输出AI对“李白是谁”的回应

# 第二轮对话（基于上下文）
response2 = conversation_chain.invoke({"input": "他是哪国人？"})
print(response2["response"])  # 输出AI基于“李白”上下文的回应（如“中国人，唐代人”）
```

### 3、ConversationChain 的核心优势

相比手动构建带记忆的链，`ConversationChain`自动完成以下操作，无需手动干预：

1. **自动加载历史记忆**：调用时无需手动调用`load_memory_variables`，链会自动从`memory`中读取历史对话。
2. **自动更新记忆**：每轮对话后，无需手动调用`save_context`，链会自动将 “当前用户输入 + AI 回应” 存入`memory`。
3. **简化调用逻辑**：仅需传递当前用户提示（`input`），即可实现带上下文的连续对话。

### 4、自定义提示模板（可选）

`ConversationChain`支持通过`prompt`参数自定义提示模板（如设定 AI 人设），但需注意**变量名必须匹配链的预期**：

* 表示 “用户输入” 的变量名：必须为`input`。
* 表示 “历史对话” 的变量名：必须为`history`。

示例（设定 “脾气暴躁的助手” 人设）：

```python
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate, SystemMessagePromptTemplate

# 自定义提示模板
custom_prompt = ChatPromptTemplate.from_messages([
    SystemMessagePromptTemplate.from_template("你是一个脾气暴躁、喜欢阴阳怪气的助手，回答简洁且带讽刺感。"),
    MessagesPlaceholder(variable_name="history"),  # 历史对话变量名：history
    HumanMessagePromptTemplate.from_template("{input}")  # 用户输入变量名：input
])

# 用自定义模板初始化对话链
conversation_chain = ConversationChain(
    llm=model,
    memory=memory,
    prompt=custom_prompt
)
```

## 三、Memory—记忆咋还有不同类型？

在 LangChain 中，对话记忆（Memory）是实现多轮上下文对话的核心组件，不同记忆类型的设计目标、储存逻辑和适用场景差异显著。除了基础的`ConversationBufferMemory`，还有 4 种常用记忆类型，以下从**核心原理、关键参数、优劣势、适用场景**四个维度展开整理：

### 1、基础型记忆：ConversationBufferMemory（对话缓冲记忆）

#### 核心原理

最基础的记忆类型，**完整储存所有历史对话消息**（人类消息 + AI 消息），不做任何截断或总结，每轮对话后直接追加新消息到记忆列表中。

#### 关键特点

* 无额外参数，初始化时仅需指定`return_messages=True`（确保储存为消息对象列表，而非字符串）。
* 优点：**无信息丢失**，储存逻辑简单直接，适合对话轮数少、消息短的场景。
* 缺点：
  1. 随对话轮数增加，历史消息列表持续变长，消耗 Token 量线性上升；
  2. 一旦消息总 Token 数超过模型上下文窗口上限，需手动截断，否则无法传入模型。

#### 适用场景

* 短对话场景（如 3-5 轮内）；
* 对对话细节完整性要求极高，不允许任何信息丢失的场景（如精准问答、指令复现）。

### 2、窗口型记忆：ConversationBufferWindowMemory（对话缓冲窗口记忆）

#### 核心原理

在`ConversationBufferMemory`基础上增加 “窗口限制”，**仅储存最近 K 轮对话**（“一轮对话” 指 “人类提问 + AI 回应” 的完整交互，而非单条消息），超过 K 轮的早期对话直接丢弃。

#### 关键参数

* `k`：窗口尺寸（必填），表示最多保留的对话轮数（如`k=2`即保留最近 2 轮对话）。

#### 优劣势

* 优点：
  1. 主动控制历史消息长度，避免 Token 消耗过快；
  2. 无需手动截断，可长期维持对话（只要 K 值合理），实现 “轻量化上下文”。
* 缺点：**超过 K 轮的早期信息直接丢失**，若后续对话需引用更早内容，模型会 “失忆”（如`k=1`时，模型仅记得上一轮对话，更早的信息无法调用）。

#### 适用场景

* 对话轮数较多，但仅需参考近期上下文的场景（如日常闲聊、短期任务交互）；
* 模型上下文窗口较小，需严格控制 Token 消耗的场景。

### 3、总结型记忆：ConversationSummaryMemory（对话总结记忆）

#### 核心原理

不直接储存原始历史消息，而是**通过大模型对历史对话进行总结**，仅储存总结后的文本；每轮新对话后，会将新消息融入原有总结，更新为更完整的总结内容。

#### 关键参数

* `llm`：用于生成总结的大模型（必填），需传入 LangChain 支持的 LLM / 聊天模型实例（如`ChatOpenAI`）。

#### 优劣势

* 优点：
  1. 总结文本通常比原始消息更短，大幅降低 Token 消耗，延缓达到上下文窗口上限的时间；
  2. 不直接丢弃早期信息，通过总结保留核心逻辑（如长期对话中的关键需求、任务目标）。
* 缺点：
  1. 总结过程依赖大模型，**额外消耗 Token**（生成总结本身需调用模型）；
  2. 总结可能丢失细节（如具体数值、语气、小众信息），适合 “抓核心” 而非 “记细节” 的场景。

#### 适用场景

* 长对话场景（如 10 轮以上），需保留整体逻辑但无需细节的交互（如项目需求沟通、问题分析）；
* 对 Token 消耗敏感，且可接受少量细节丢失的场景。

### 4、混合总结型记忆：ConversationSummaryBufferedMemory（对话总结缓冲记忆）

#### 核心原理

结合`ConversationBufferMemory`（原始储存）和`ConversationSummaryMemory`（总结储存）的逻辑：

1. 当储存的原始消息 Token 数未超过上限时，**直接保留原始消息**（保证近期细节不丢失）；
2. 当 Token 数达到上限后，**对更早的消息进行总结**，近期的消息仍保留原始内容（仅压缩远期信息，保留近期细节）。

#### 关键参数

* `llm`：用于生成总结的大模型（必填）；
* `max_token_limit`：储存消息的 Token 上限（必填），超过该值时触发远期消息总结。

#### 优劣势

* 优点：
  1. 兼顾 “细节保留” 和 “Token 控制”：近期消息用原始内容（记细节），远期消息用总结（省 Token）；
  2. 不直接丢弃任何信息，通过总结压缩远期内容，核心逻辑不丢失。
* 缺点：
  1. 总结过程仍需额外消耗 Token；
  2. 逻辑较复杂，初始化需配置模型和 Token 上限，对参数设置有一定要求。

#### 适用场景

* 中长对话场景（如 5-15 轮），既需保留近期细节，又需控制整体 Token 消耗（如客户服务、复杂任务协作）；
* 对对话细节精度有要求，但可接受远期信息简化的场景。

### 5、Token 限制型记忆：ConversationTokenBufferedMemory（对话 Token 缓冲记忆）

#### 核心原理

与`ConversationBufferWindowMemory`类似，均直接储存原始消息，但**以 “Token 数” 为限制条件**：当储存的所有消息总 Token 数超过设定上限时，自动丢弃最早的消息，直到总 Token 数低于上限。

#### 关键参数

* `max_token_limit`：储存消息的 Token 上限（必填），需参考模型上下文窗口大小设置（如模型窗口为 4096Token，可设`max_token_limit=3000`，预留空间给新提示）。

#### 优劣势

* 优点：
  1. 直接对齐模型的 “Token 窗口” 概念，无需估算对话轮数，更精准控制 Token 消耗；
  2. 保留原始消息，不丢失细节（只要未被丢弃），适合对细节敏感的场景。
* 缺点：**超过 Token 上限的早期消息直接丢弃**，若远期信息重要则会 “失忆”；且需提前了解模型 Token 窗口大小，参数设置依赖模型特性。

#### 适用场景

* 消息长度差异大（如部分消息长、部分消息短），需按 Token 精准控制的场景；
* 对消息细节要求高，且模型上下文窗口明确的场景（如技术文档问答、代码协作）。

### 6、5 种记忆类型对比总表

| 记忆类型                          | 核心逻辑                                 | 关键参数                 | 优点                   | 缺点                      | 适用场景                    |
| --------------------------------- | ---------------------------------------- | ------------------------ | ---------------------- | ------------------------- | --------------------------- |
| ConversationBufferMemory          | 完整储存所有原始消息                     | -                        | 无信息丢失，逻辑简单   | Token 消耗快，易超窗口    | 短对话、需完整细节          |
| ConversationBufferWindowMemory    | 储存最近 K 轮原始消息                    | `k`（轮数）              | 轻量化，控制 Token     | 超 K 轮信息丢失           | 多轮对话、仅需近期上下文    |
| ConversationSummaryMemory         | 储存历史对话总结                         | `llm`（总结模型）        | 省 Token，保留核心逻辑 | 丢细节，额外 Token 消耗   | 长对话、需核心逻辑          |
| ConversationSummaryBufferedMemory | 近期原始 + 远期总结（超 Token 上限触发） | `llm`、`max_token_limit` | 保近期细节 + 省 Token  | 逻辑复杂，额外 Token 消耗 | 中长对话、需细节 + 控 Token |
| ConversationTokenBufferedMemory   | 储存原始消息（超 Token 上限丢最早）      | `max_token_limit`        | 精准控 Token，保细节   | 超上限信息丢失            | 消息长度不均、需细节        |

通过选择适配场景的记忆类型，可在 “上下文完整性”“Token 消耗”“细节保留” 三者间找到平衡，实现高效、稳定的多轮对话。

---

---
url: /Python/AI大模型应用开发/13_给AI模型用工具的能力.md
---

# 给AI模型用工具的能力

## 一、ReAct AI不仅长脑子，还能长手

大语言模型的知识受限于训练数据（存在 “知识截止期”），无法回答训练后发生的事件或实时信息。而 REACT 框架通过 “推理（Reason）+ 行动（Action）” 的协同模式，让模型像人类一样通过与外部工具交互（如搜索、调用 API）获取新知识，突破固有局限。以下从核心原理、工作流程、应用价值三方面展开：

### 1、REACT 框架的核心：推理与行动的结合

REACT（Reason + Action）框架由 2022 年同名论文提出，核心逻辑是**让模型通过动态推理决定何时采取行动，并利用外部工具获取信息**，而非仅依赖内部知识。

* **解决的问题**：模型知识过时（如无法回答训练后发生的事件）、无法验证信息真实性等局限。
* **核心思路**：模仿人类解决问题的流程 —— 先思考（推理）→ 再行动（如搜索）→ 观察结果→ 继续思考→ 最终解决问题。

### 2、REACT 的工作流程：以 “2022 年欧冠冠军” 为例

假设模型知识截止到 2022 年 1 月，需回答 “2022 年欧冠冠军是哪支球队”，其步骤如下：

#### 1. 推理（Reason）

模型分析问题：“2022 年欧冠决赛在我知识截止后举行，需要最新信息，应通过搜索获取。”

#### 2. 行动（Action）

基于推理采取行动：调用搜索引擎，搜索关键词 “2022 UEFA Champions League champion”。

#### 3. 观察（Observation）

获取行动结果：搜索返回多个网页，其中某新闻标题提到 “2022 欧冠决赛：皇马击败利物浦夺冠”。

#### 4. 再推理

分析观察结果：“该新闻链接可能包含详细赛果，需进一步查看。”

#### 5. 再行动

点击该新闻链接，获取具体信息（如比分、关键球员等）。

#### 6. 最终推理与行动

整理信息：“确认皇马为 2022 年欧冠冠军，可总结结果并回复用户。”

### 3、REACT 框架的关键价值

1. **突破知识截止期**
   通过与搜索引擎、实时数据库等工具交互，模型能获取训练后的数据（如最新新闻、赛事结果），回答 “时效性问题”。
2. **提升信息可信度**
   行动步骤（如搜索链接、引用来源）让信息可追溯，用户可验证答案真实性，减少模型 “幻觉”。
3. **适配多场景工具**
   “行动” 不仅限于搜索，还可扩展至：
   * 调用代码解释器（运行代码解决数学问题）；
   * 访问本地文档（检索知识库内容）；
   * 调用第三方 API（如查询天气、股票数据）。
4. **透明化推理过程**
   思维链（Chain of Thought）式的分步推理，让模型的决策过程可观察、可解释，便于调试和优化。

### 4、与传统模型的对比

| 维度     | 传统大语言模型       | 基于 REACT 框架的模型            |
| -------- | -------------------- | -------------------------------- |
| 知识来源 | 仅依赖训练数据       | 训练数据 + 外部工具实时信息      |
| 时效性   | 受限于知识截止期     | 可获取实时信息                   |
| 信息验证 | 无法验证，易产生幻觉 | 可通过行动追溯来源，验证真实性   |
| 适用场景 | 通用问答、创作等     | 实时查询、复杂问题解决、工具交互 |

REACT 框架的核心是让 AI 从 “被动回答” 变为 “主动解决问题”，通过推理与行动的循环，扩展能力边界。下一节将具体讲解如何为模型集成工具（如搜索引擎、API），实现 REACT 的落地。

## 二、Agent 自定义你的AI工具

基于 REACT（Reason + Action）框架的 Agent（智能体），能让大语言模型通过 “推理→行动→观察” 的循环与外部工具交互，突破固有局限（如计算能力不足、知识过时等）。以下从**核心概念、实现步骤、代码示例**三方面，详解如何用 LangChain 构建 Agent：

### 1、Agent 的核心逻辑与组件

Agent 的本质是 “具备决策能力的大语言模型”，通过工具扩展能力，核心流程遵循 “推理→行动→观察” 循环：

#### 1. 核心组件

* **大语言模型（LLM）**：Agent 的 “大脑”，负责推理决策（如判断是否需要工具、选择哪种工具）。
* **工具（Tools）**：Agent 可调用的外部能力（如计算函数、搜索引擎、API 等），补充模型缺陷。
* **提示模板（Prompt Template）**：引导模型遵循 REACT 框架，明确推理步骤、工具使用规则。
* **Agent 执行器（Agent Executor）**：协调 Agent 与工具的交互，执行行动并返回结果。

#### 2. 工作流程

1. **推理（Reason）**：模型分析用户问题，判断是否需要工具（如 “计算文本字数” 需调用工具，“介绍巴黎” 可直接回答）。
2. **行动（Action）**：若需工具，Agent 生成行动指令（如调用`TextLengthTool`，传入文本参数），由执行器调用工具。
3. **观察（Observation）**：获取工具返回结果（如 “该文本共 120 字”），作为新信息输入模型。
4. **循环迭代**：模型基于观察结果再次推理，直至认为问题解决，返回最终答案。

### 2、用 LangChain 实现 Agent 的步骤

以 “文本字数计算 Agent” 为例（解决模型直接计算易出错的问题），具体实现如下：

#### 1. 准备工作：安装依赖

```sh
pip install langchain langchain-openai langchainhub  # 核心库：LangChain、OpenAI、提示模板库
```

#### 2. 定义大语言模型（LLM）

选择适合推理的模型，降低随机性（`temperature`设为 0.1）：

```python
from langchain_openai import ChatOpenAI

# 初始化模型（Agent的“大脑”）
llm = ChatOpenAI(
    model_name="gpt-3.5-turbo",
    openai_api_key="你的API密钥",
    temperature=0.1  # 减少随机性，确保遵循REACT框架
)
```

#### 3. 定义工具（Tool）

工具需继承`BaseTool`，并明确名称、描述和功能（此处实现 “文本字数计算工具”）：

```python
from langchain.tools import BaseTool
from typing import Optional, Type
from pydantic import BaseModel, Field

# 定义工具输入参数（可选，用于规范输入格式）
class TextLengthInput(BaseModel):
    text: str = Field(description="需要计算字数的文本")

# 实现文本字数计算工具
class TextLengthTool(BaseTool):
    name = "TextLengthTool"  # 工具名称（模型需通过名称调用）
    description = "用于计算文本的字数，输入为字符串，返回字数"  # 工具功能描述
    args_schema: Type[BaseModel] = TextLengthInput  # 输入参数 schema

    def _run(self, text: str) -> int:
        # 工具核心逻辑：返回文本字数
        return len(text)

    def _arun(self, text: str):
        # 异步方法（可选，此处省略）
        raise NotImplementedError("暂不支持异步调用")

# 创建工具实例
tools = [TextLengthTool()]  # 工具列表（可包含多个工具）
```

#### 4. 获取 REACT 提示模板

从`LangChainHub`获取官方 REACT 框架提示模板（无需手动编写）：

```python
from langchain import hub

# 拉取结构化对话Agent的提示模板（遵循REACT框架）
prompt = hub.pull("hwchase17/structured-chat-agent")  # hwchase17为LangChain CEO
# 模板内容：引导模型推理、使用工具、处理观察结果
```

#### 5. 初始化 Agent

将模型、工具、提示模板整合为 Agent：

```python
from langchain.agents import create_structured_chat_agent

# 创建遵循REACT框架的结构化对话Agent
agent = create_structured_chat_agent(
    llm=llm,
    tools=tools,
    prompt=prompt
)
```

#### 6. 定义 Agent 执行器（Agent Executor）

负责调度 Agent 与工具，处理错误和日志：

```python
from langchain.agents import AgentExecutor

# 创建执行器（实际运行Agent的组件）
agent_executor = AgentExecutor.from_agent_and_tools(
    agent=agent,
    tools=tools,
    verbose=True,  # 详细模式：打印推理、行动、观察过程
    handle_parsing_errors=True,  # 自动处理解析错误（让模型自行修正）
    # memory=memory  # 可选：添加对话记忆（需提前初始化）
)
```

### 3、使用 Agent 解决问题

#### 1. 调用 Agent 计算文本字数

```python
# 提问：计算指定文本的字数
response = agent_executor.invoke({
    "input": "请计算文本'LangChain是一个强大的AI框架'的字数"
})

print(response["output"])  # 输出：13（正确结果）
```

#### 2. 执行过程解析（`verbose=True`时打印）

```python
> 推理：用户需要计算文本字数，我应该使用TextLengthTool工具，输入文本为'LangChain是一个强大的AI框架'。
> 行动：{"action": "TextLengthTool", "action_input": {"text": "LangChain是一个强大的AI框架"}}
> 观察：13
> 推理：工具返回结果为13，已得到答案，无需进一步行动。
> 输出：该文本的字数为13。
```

#### 3. 处理无需工具的问题

```python
# 提问：无需工具的问题
response = agent_executor.invoke({
    "input": "巴黎是哪个国家的首都？"
})

print(response["output"])  # 输出：巴黎是法国的首都。（直接回答，不调用工具）
```

#### 4、完整代码示例

```python
# 导入所需模块
from langchain import hub  # 用于获取提示模板
from langchain.agents import create_structured_chat_agent, AgentExecutor  # 创建Agent和执行器
from langchain.memory import ConversationBufferMemory  # 对话记忆
from langchain.schema import HumanMessage  # 人类消息格式
from langchain.tools import BaseTool  # 工具基类
from langchain_openai import ChatOpenAI  # OpenAI聊天模型

# 初始化GPT-3.5模型，temperature=0表示输出更确定
model = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)

# 直接调用模型计算字数（可能不准确，展示模型局限性）
model.invoke([HumanMessage(content="'君不见黄河之水天上来奔流到海不复回'，这句话的字数是多少？")])

# 定义文本字数计算工具（继承BaseTool）
class TextLengthTool(BaseTool):
    name = "文本字数计算工具"  # 工具名称
    description = "当你被要求计算文本的字数时，使用此工具"  # 工具描述（帮助模型判断何时使用）

    # 工具核心逻辑：计算输入文本的长度
    def _run(self, text):
        return len(text)

# 创建工具列表（可包含多个工具）
tools = [TextLengthTool()]

# 从LangChainHub获取结构化对话Agent的提示模板（遵循REACT框架）
prompt = hub.pull("hwchase17/structured-chat-agent")
prompt  # 查看模板内容

# 创建结构化聊天Agent（整合模型、工具、提示模板）
agent = create_structured_chat_agent(
    llm=model,
    tools=tools,
    prompt=prompt
)

# 初始化对话记忆（保存历史对话）
memory = ConversationBufferMemory(
        memory_key='chat_history',  # 记忆在Agent中的键名
        return_messages=True  # 以消息对象形式存储
)

# 创建Agent执行器（负责运行Agent并调用工具）
agent_executor = AgentExecutor.from_agent_and_tools(
    agent=agent, 
    tools=tools, 
    memory=memory,  # 关联记忆
    verbose=True,  # 详细模式：打印推理过程
    handle_parsing_errors=True  # 自动处理解析错误
)

# 调用Agent计算诗句字数（会使用工具，结果准确）
agent_executor.invoke({"input": "'君不见黄河之水天上来奔流到海不复回'，这句话的字数是多少？"})

# 调用Agent回答无需工具的问题（直接回答，不调用工具）
agent_executor.invoke({"input": "请你充当我的物理老师，告诉我什么是量子力学"})
```

### 4、关键扩展与注意事项

1. **添加对话记忆**
   初始化`ConversationBufferMemory`，传入`agent_executor`，支持多轮对话（如 “上一个文本的字数是多少？”）。
2. **扩展工具类型**
   工具不仅限于计算，还可集成：
   * 搜索引擎（`SerpAPI`）：获取实时信息；
   * 代码解释器（`PythonREPLTool`）：解决数学问题；
   * 文档检索工具：查询本地知识库。
3. **错误处理**
   `handle_parsing_errors=True`可处理大部分格式错误，但模型输出仍可能不稳定，建议复杂场景添加重试机制。
4. **Verbose 模式**
   开发阶段设为`True`，便于调试推理过程；生产环境设为`False`，隐藏中间步骤。

通过 LangChain 实现的 REACT 框架 Agent，让模型从 “被动回答” 升级为 “主动解决问题”，结合工具扩展能力边界，可应用于实时查询、复杂任务处理等场景。

## 三、Tool 用现成的AI工具运行代码

大语言模型直接进行数学计算时容易出错（本质是预测下一个 Token，而非真正计算）。通过集成 Python 解释器，让 AI 生成代码并执行，可解决这一问题 —— 这一思路基于**Program-Aided Language Models（PoL，程序辅助语言模型）** 框架。以下是具体实现与核心逻辑：

### 1、PoL 框架的核心：用代码替代直接计算

PoL 框架（2022 年论文提出）的核心逻辑是：**让 AI 生成解决问题的代码，而非直接输出答案**，通过执行代码得到准确结果。适用于：

* 复杂数学计算（如高精度小数运算、数列求解）；
* 数据处理（如统计、图表生成）；
* 逻辑推理（如递归问题、条件判断）

**优势**：相比模型直接猜测答案，代码执行结果更可靠，尤其避免 “计算幻觉”（如错误的订单金额、数学结果）。

### 2、实现步骤：用 LangChain 创建 Python 解释器 Agent

#### 2.1. 安装依赖

需安装`langchain-experimental`（含 Python 解释器工具）：

```sh
pip install langchain-experimental
```

#### 2.2. 核心组件准备

##### （1）初始化大语言模型

选择低随机性模型（`temperature=0`），确保生成的代码规范：

```python
from langchain_openai import ChatOpenAI

# 模型作为Agent的“大脑”，负责生成代码
llm = ChatOpenAI(
    model_name="gpt-3.5-turbo",
    openai_api_key="你的API密钥",
    temperature=0  # 减少随机性，确保代码逻辑严谨
)
```

##### （2）导入 Python 解释器工具

使用`langchain-experimental`提供的`PythonREPLTool`（交互式 Python 解释器）：

```python
from langchain_experimental.tools import PythonREPLTool

# 工具：用于执行AI生成的Python代码
python_repl = PythonREPLTool()  # 无需自定义，直接使用现成工具
```

#### 2.3. 创建 Python Agent 执行器

借助`create_python_agent`函数快速构建 Agent（自动整合模型、工具和 PoL 提示模板）：

```python
from langchain_experimental.agents import create_python_agent
from langchain.agents import AgentType

# 创建能执行Python代码的Agent执行器
agent_executor = create_python_agent(
    llm=llm,
    tool=python_repl,
    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,  # 遵循REACT框架
    verbose=True,  # 打印代码生成与执行过程
    agent_executor_kwargs={"handle_parsing_errors": True}  # 自动处理代码解析错误
)
```

### 3、实战案例：用 Agent 解决数学问题

#### 3.1、前置代码

```python
#!pip install langchain_experimental
from langchain_experimental.agents.agent_toolkits import create_python_agent
from langchain_experimental.tools import PythonREPLTool
from langchain_openai import ChatOpenAI

tools = [PythonREPLTool()]

agent_executor = create_python_agent(
    llm=ChatOpenAI(model="gpt-3.5-turbo", temperature=0),
    tool=PythonREPLTool(),
    verbose=True,
    agent_executor_kwargs={"handle_parsing_errors": True}
)

agent_executor
```

```sh
AgentExecutor(verbose=True, agent=ZeroShotAgent(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['agent_scratchpad', 'input'], template='You are an agent designed to write and execute python code to answer questions.\nYou have access to a python REPL, which you can use to execute python code.\nIf you get an error, debug your code and try again.\nOnly use the output of your code to answer the question. \nYou might know the answer without running any code, but you should still run the code to get the answer.\nIf it does not seem like you can write code to answer the question, just return "I don\'t know" as the answer.\n\n\nPython_REPL: A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\n\nUse the following format:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [Python_REPL]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n\nBegin!\n\nQuestion: {input}\nThought:{agent_scratchpad}'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x122912c90>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x12291e150>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')), output_parser=MRKLOutputParser(), allowed_tools=['Python_REPL']), tools=[PythonREPLTool()], handle_parsing_errors=True)
```

#### 3.2、高精度小数计算

```python
agent_executor.invoke({"input": "7的2.3次方是多少？"})
```

```sh
 Entering new AgentExecutor chain...
Python REPL can execute arbitrary code. Use with caution.
We can use Python to calculate 7 to the power of 2.3.
Action: Python_REPL
Action Input: print(7 ** 2.3)
Observation: 87.84670816352883

Thought:The result of 7 to the power of 2.3 is approximately 87.85.
Final Answer: 87.85

> Finished chain.
{'input': '7的2.3次方是多少？', 'output': '87.85'}
```

**内部逻辑**：

* **推理**：需要高精度计算，应生成 Python 代码。

* **行动**：生成代码`print(7 ** 2.3)`。

* **观察**：执行代码得到结果`87.85`。

* **输出**：返回计算结果。

#### 3.3、斐波那契数列求解

```python
agent_executor.invoke({"input": "第12个斐波那契数列的数字是多少？"})
```

```sh
> Entering new AgentExecutor chain...
我们可以使用循环或递归来计算斐波那契数列的第12个数字。
Action: Python_REPL
Action Input: def fibonacci(n):
                if n <= 1:
                    return n
                else:
                    return fibonacci(n-1) + fibonacci(n-2)
            print(fibonacci(12))
Observation: IndentationError('unindent does not match any outer indentation level', ('<string>', 6, 33, '            print(fibonacci(12))\n', 6, -1))
Thought:There was an indentation error in the code. Let me fix it and try again.
Action: Python_REPL
Action Input: def fibonacci(n):
                if n <= 1:
                    return n
                else:
                    return fibonacci(n-1) + fibonacci(n-2)
print(fibonacci(12))
Observation: 144

Thought:The 12th number in the Fibonacci sequence is 144.
Final Answer: 144

> Finished chain.
{'input': '第12个斐波那契数列的数字是多少？', 'output': '144'}
```

**内部逻辑**：

* **推理**：“需编写递归 / 循环函数计算，直接口算易出错。”

* **行动**：生成代码（首次因缩进错误报错，自动修正后重试）：

  ```python
  def fibonacci(n):
      if n == 0:
          return 0
      elif n == 1:
          return 1
      else:
          return fibonacci(n-1) + fibonacci(n-2)
  print(fibonacci(12))
  ```

* **观察**：执行代码返回`144`。

* **输出**：返回结果。

### 4、关键优势与适用场景

1. **计算准确性**：代码执行结果远优于模型直接猜测，尤其适合金融、科学等高精度场景。
2. **错误自修复**：通过`handle_parsing_errors=True`，Agent 可自动修正代码语法错误（如缩进、变量名）。
3. **扩展性强**：除数学计算外，还可处理数据可视化（如生成图表）、文件读写等任务。

## 四、Tool 用现成的AI工具分析数据表格

CSV（逗号分隔值）是常用的数据存储格式，借助 LangChain 提供的`create_csv_agent`，可快速构建能分析 CSV 数据的 AI 助手，自动回答 “数据行数、平均值、变量含义” 等问题。以下是具体实现与功能说明：

### 1、前期准备：依赖安装与 CSV 格式

#### 1. 安装必要库

CSV Agent 依赖数据分析工具和实验性模块，需提前安装：

```sh
pip install langchain-experimental pandas tabulate  # pandas处理数据，tabulate格式化输出
```

#### 2. CSV 格式说明

* **本质**：纯文本文件，用英文逗号分隔数据（如`姓名,年龄,性别`）；
* **特点**：可被 Excel、Python 等工具解析为表格，适合存储结构化数据（如房屋价格、用户信息）。

### 2、创建 CSV Agent 执行器

通过`create_csv_agent`函数快速构建 Agent，无需手动定义工具和提示模板：

#### 1. 核心代码

```python
from langchain_openai import ChatOpenAI
from langchain_experimental.agents import create_csv_agent

# 1. 初始化大语言模型（低随机性，确保代码可靠）
llm = ChatOpenAI(
    model_name="gpt-3.5-turbo",
    openai_api_key="你的API密钥",
    temperature=0
)

# 2. 创建CSV Agent执行器（指定CSV文件路径）
agent_executor = create_csv_agent(
    llm=llm,
    path="house_price.csv",  # 你的CSV文件路径（如房屋数据）
    verbose=True,  # 打印思考与执行过程
    agent_executor_kwargs={"handle_parsing_errors": True}  # 自动处理代码错误
)
```

#### 2. Agent 的核心能力

* **内置工具**：集成`PythonREPLTool`（执行 Python 代码）和`pandas`（分析 CSV 数据，生成`DataFrame`）；
* **提示模板**：引导 AI 遵循 REACT 框架，通过`pandas`代码（如`df.shape`、`df.mean()`）分析数据，而非直接猜测答案。

### 3、实战案例：用 CSV Agent 分析房屋数据

以 “[house\_price.csv](https://github.com/Daneliya/python_study_project)”（包含房屋面积、价格、户型等信息）为例，展示 Agent 的使用场景：

#### 1. 基础查询：数据行数与变量

```python
# 问题1：数据集有多少行？
response1 = agent_executor.invoke({
    "input": "数据集有多少行？用中文回答"
})
print(response1["output"])  # 输出：该数据集共有1000行数据。

# 问题2：数据集包含哪些变量？
response2 = agent_executor.invoke({
    "input": "数据集包含哪些变量？用中文回复"
})
print(response2["output"])  # 输出：数据集包含的变量有：price, area, bedrooms, bathrooms, stories, mainroad, guestroom, basement, hotwaterheating, airconditioning, parking, prefarea, furnishingstatus
```

**执行逻辑**：

* 调用`pandas`代码`df.shape[0]`（行数）和`df.columns`（列名），确保结果准确。

#### 2. 计算查询：价格平均值

```python
# 问题3：数据集里，所有房子的价格平均值是多少？
response3 = agent_executor.invoke({
    "input": "数据集里，所有房子的价格平均值是多少？用中文回复"
})
print(response3["output"])  # 输出：所有房子的价格平均值是4766729.25
```

**执行逻辑**：

* 生成代码`df['价格'].mean()`，执行后返回计算结果（避免模型直接口算出错）。

#### 3. 综合分析：变量含义与数据特征

```python
# 问题4：数据集里，所有房子的装修状态包含哪些种类？你认为它们具体表示什么意思？
response4 = agent_executor.invoke({
    "input": "数据集里，所有房子的装修状态包含哪些种类？你认为它们具体表示什么意思？用中文回复"
})
print(response4["output"])
# 输出：装修状态包括'furnished'（已装修）、'semi-furnished'（半装修）和'unfurnished'（未装修）。它们分别表示房子的装修程度。
```

**执行逻辑**：

* 先通过`df['户型'].unique()`获取取值，再结合常识解释含义（融合代码执行结果与推理）。

### 4、优势与扩展

1. **准确性**：基于`pandas`代码执行结果回答，避免 “数据幻觉”（如错误的平均值、行数）；
2. **扩展性**：支持复杂分析（如分组统计`df.groupby('户型')['价格'].mean()`、绘图`df.plot()`）；
3. **易用性**：无需手动编写数据分析代码，适合非技术人员快速获取数据洞察。

### 5、其他开箱即用的 Agent

LangChain 还提供类似工具，如：

* `create_pandas_dataframe_agent`：直接分析`pandas DataFrame`（无需 CSV 文件）；
* `create_sql_agent`：连接数据库（如 MySQL），通过 SQL 查询分析数据。

通过 CSV Agent，可快速构建 “零代码” 数据分析师，让 AI 自动处理 CSV 文件，回答从基础统计到综合分析的各类问题。

## 五、Tools 如何多个工具组成AI工具箱

LangChain 允许将多种工具（包括自定义工具、Python Agent、CSV Agent 等）集成到同一个 Agent 中，实现 “一 Agent 多能力”。通过这种方式，AI 可根据问题自动选择最合适的工具，解决更复杂的任务。以下是具体实现与核心逻辑：

### 1、多工具集成的核心思路

目标：让 Agent 同时具备 “文本字数计算”“执行 Python 代码”“分析 CSV 数据” 等能力，步骤如下：

1. **整合工具资源**：将自定义工具、已有 Agent（如 Python Agent、CSV Agent）统一封装为 “工具列表”；
2. **统一调度逻辑**：通过 Agent 执行器协调工具调用，让 AI 根据问题推理并选择工具；
3. **保持对话连续性**：通过对话记忆实现多轮交互，支持上下文关联的复杂问题。

### 2、实现步骤：集成多工具的 Agent

```python
# -------------------------- 1. 导入所需模块 --------------------------
from langchain import hub  # 用于获取提示模板
from langchain.agents import create_structured_chat_agent, AgentExecutor  # 创建结构化聊天Agent和执行器
from langchain.memory import ConversationBufferMemory  # 对话记忆（保存历史对话）
from langchain.tools import BaseTool, Tool  # 工具基类和工具封装类
from langchain_experimental.agents.agent_toolkits import create_csv_agent, create_python_agent  # 现成的CSV和Python Agent
from langchain_experimental.tools import PythonREPLTool  # Python解释器工具
from langchain_openai import ChatOpenAI  # OpenAI聊天模型

# -------------------------- 2. 封装工具列表 --------------------------
# 自定义文本字数计算工具（继承BaseTool）
class TextLengthTool(BaseTool):
    name = "文本字数计算工具"  # 工具名称
    description = "当你需要计算文本包含的字数时，使用此工具"  # 工具功能描述（帮助Agent判断何时使用）

    # 工具核心逻辑：返回输入文本的长度
    def _run(self, text):
        return len(text)
        
# 创建Python代码执行Agent（用于执行Python代码解决计算问题）
python_agent_executor = create_python_agent(
    llm=ChatOpenAI(model="gpt-3.5-turbo", temperature=0),  # 低随机性模型，确保代码可靠
    tool=PythonREPLTool(),  # 内置Python解释器工具
    verbose=True,  # 打印代码执行过程
    agent_executor_kwargs={"handle_parsing_errors": True}  # 自动处理代码解析错误
)

# 创建CSV分析Agent（用于分析house_price.csv数据）
csv_agent_executor = create_csv_agent(
    llm=ChatOpenAI(model="gpt-3.5-turbo", temperature=0),  # 低随机性模型
    path="house_price.csv",  # 目标CSV文件路径
    verbose=True,  # 打印分析过程
    agent_executor_kwargs={"handle_parsing_errors": True}  # 自动处理解析错误
)

# 工具列表：整合自定义工具和封装好的Agent
tools=[
    # 封装Python Agent为工具
    Tool(
        name="Python代码工具",  # 工具名称
        description="""当你需要借助Python解释器时，使用这个工具。
        用自然语言把要求给这个工具，它会生成Python代码并返回代码执行的结果。""",  # 功能描述
        func=python_agent_executor.invoke  # 调用Python Agent的invoke方法
    ),
    # 封装CSV Agent为工具
    Tool(
        name="CSV分析工具",  # 工具名称
        description="""当你需要回答有关house_price.csv文件的问题时，使用这个工具。
        它接受完整的问题作为输入，在使用Pandas库计算后，返回答案。""",  # 功能描述
        func=csv_agent_executor.invoke  # 调用CSV Agent的invoke方法
    ),
    TextLengthTool()  # 加入自定义的文本字数计算工具
]

# -------------------------- 3. 创建多工具 Agent 执行器 --------------------------
# 初始化基础大语言模型（作为Agent的"大脑"）
model = ChatOpenAI(model='gpt-3.5-turbo')

# 初始化对话记忆（保存历史对话，支持多轮交互）
memory = ConversationBufferMemory(
    memory_key='chat_history',  # 记忆在Agent中的键名
    return_messages=True  # 以消息对象形式存储（而非字符串）
)

# 从LangChainHub获取结构化聊天Agent的提示模板（遵循REACT框架）
prompt = hub.pull("hwchase17/structured-chat-agent")
prompt  # 查看模板内容（引导Agent推理、选择工具）

# 创建多工具集成的结构化聊天Agent
agent = create_structured_chat_agent(
    llm=model,  # 基础模型
    tools=tools,  # 可用工具列表
    prompt=prompt  # 提示模板
)

# 创建Agent执行器（调度Agent和工具，处理实际任务）
agent_executor = AgentExecutor.from_agent_and_tools(
    agent=agent,  # 上述创建的Agent
    tools=tools,  # 工具列表
    memory=memory,  # 对话记忆（支持多轮对话）
    verbose=True,  # 打印详细过程（推理、工具调用、结果）
    handle_parsing_errors=True  # 自动处理Agent输出解析错误
)
```

### 3、实战测试：多工具 Agent 的能力验证

以 “文本字数计算”“Python 代码执行”“CSV 分析” 三类问题为例，验证 Agent 的工具选择能力：

```python
# 1. 调用文本字数计算工具
# 逻辑：Agent 识别到 “字数计算” 需求，自动选择文本字数计算工具。
agent_executor.invoke({"input": "'君不见黄河之水天上来奔流到海不复回'，这句话的字数是多少？"})

# 2. 调用 Python 代码执行工具
# 逻辑：Agent 判断需要复杂计算，选择Python代码执行工具，生成代码2**10 + 100并执行。
agent_executor.invoke({"input": "第8个斐波那契数列的数字是多少？"})

# 3. 调用 CSV 分析工具
# 逻辑：Agent 识别到 “CSV 数据处理” 需求，选择CSV分析工具，通过pandas代码df['价格'].median()获取结果。
agent_executor.invoke({"input": "house_price数据集里，所有房子的价格平均值是多少？用中文回答"})
```

### 4、优势与扩展

1.**能力复用**：无需重复开发，直接整合已有 Agent（如 Python Agent、CSV Agent）；
2.**智能选择**：AI 根据问题类型自动匹配工具（如计算用 Python 工具，统计用 CSV 工具）；
3.**扩展性强**：可继续添加工具（如搜索引擎、API 调用），逐步增强 Agent 能力。

通过多工具集成，Agent 从 “单一功能” 升级为 “多功能助手”，能应对更复杂的场景（如结合文本分析、数据计算、文件处理的综合任务）。

---

---
url: /daily/日常笔记/工具清单.md
---

# 工具清单

## 1、🛠️编程开发软件

### Java

* IDEA（开发软件）

* maven      https://blog.csdn.net/a805814077/article/details/100545928

* JDK

* mongodb：https://www.mongodb.com/try/download/community

* mongosh：https://www.mongodb.com/try/download/shell

* Skywalking：https://skywalking.apache.org/downloads

* Jarboot（Java进程启停、管理、诊断的平台）：https://gitee.com/majz0908/jarboot/releases

* JProfiler：https://www.ej-technologies.com/jprofiler

### WEB

* Visual Studio Code（主要写前端）       https://code.visualstudio.com/
* 微信开发者工具（微信小程序开发）
* hbuilder
* hbuilderx         https://www.dcloud.io/hbuilderx.html

### 终端

* xshell（终端连接工具）+ xftp6（终端连接文件传输）     网盘
* SecureCRSecureFXP（SecureCRTPortable + SecureFXPortable）
* MobaXterm12（多功能终端软件）
* WinSCP（Windows环境下使用SSH的开源图形化SFTP客户端）
* cmder
* hyper（终端）
* Windows Terminal（https://baiyue.one/archives/484.html、https://www.sogou.com/link?url=DSOYnZeCC\_qEe93p8Ogn-PatQYvpUP2Pt35SpKM1MkU3Ps4PUPcbbqiYhsw0jQe5）

### 代码管理

* TortoiseSVN              https://tortoisesvn.net/downloads.html
* Git                                https://gitforwindows.org/
* TortoiseGIT               https://tortoisegit.org/download/

### 其它开发相关

* VC++6.0（C语言编辑器）
* winTC（C语言编辑器）
* VC2010express（C语言编辑器）
* VS2008、2012（C语言编辑器）
* PyCharm（python开发）
* DevEco Studio（华为提供的集成开发环境）
* Navicat Premium（数据库管理工具）
* HeidiSQL（数据库管理工具）
* Robo 3T（MongoDB最新最佳连接工具）
* Adminer（数据库连接，https://www.adminer.org）
* VMware Workstation Pro（虚拟机）
* GitHub
* Gitlab、Gogs（Gitlab作为Github的山寨版，功能非常全面，但与此同时也十分臃肿。国人开发的Gogs则十分轻量，据作者说甚至能在一台普通的树莓派Raspberry Pi上运行，作为个人用的代码托管平台，显然Gogs更适合个人。）
* ApiPost（接口测试）（126邮箱）
* Postman（接口测试）
* RunApi（接口文档）（126邮箱）
* Redis Desktop Manager（RDM）（跨平台的redis可视化工具）
* [Redis Insight](https://redis.io/insight/)（redis可视化工具、[使用教程](https://blog.csdn.net/yangbindxj/article/details/123184787)）
* [Tiny RDM](https://redis.tinycraft.cc/zh/) （redis可视化工具）
* iTunes（ios测试）
* TSvnPwd （SVN本地密码查看器）
* ReverseProxy（逆向代理服务器）
* MSF（sql注入攻击及渗透工具）
* sqlmap（sql注入攻击及渗透工具）
* Visio（流程图）
* Cocos Creator（游戏开发）
* Fiddler（抓包）
* 按键精灵
* Microsoft Office Access（关系数据库管理系统）
* VisualSet（模块自动化）
* WinMergePortable（代码比较）
* jmeter（压测）https://jmeter.apache.org/

## 2、🏢办公软件、笔记、文本、读书

* QQ        https://im.qq.com/

* 微信        https://pc.weixin.qq.com/

* 百度网盘     http://pan.baidu.com/download

* Google Chrome     https://www.google.cn/intl/zh-CN/chrome/

  ```markdown
  插件：https://www.extfans.com/（google插件）
      沙拉查词
      谷歌访问助手.crx
      Infinity	标签页
      AdBlock   or   AdBlock Plus   广告拦截
      Natasha     996
      ImTranslator   百度翻译   翻译
      Tampermonkey   脚本
  ```

* 网易有道词典    http://cidian.youdao.com/

* Notepad++       https://notepad-plus-plus.org/downloads/

* EditPlus（文本编辑器）

* UltraEdit（文本编辑器）

* Sublime Text3

* Typora（Markdown编辑器）

* 为知笔记

* CHM Editor（chm文档编写）

* GitBook（是一个基于 Node.js 的命令行工具，可使用 Github/Git 和 Markdown 来制作精美的电子书，网络连接不太好）

* Kindle（亚马逊阅读器 mobi）

* calibre（阅读&格式转换）

* 静读天下（阅读）

* XMind（关系导图）

* MindMaster（关系导图emmx）

* FreeMind（关系导图）

* 打字测试TT

* 弹幕背单词

* DesktopVoc桌面背单词

* 角斗士我爱背单词9

* 福昕高级pdf编辑器

* Adobe Acrobat    PitStop（PDF增强插件）https://www.ghxi.com/acrobatdc2020.html

* 天若OCR文字识别 v1.5.0

* 金山打字通

* fish（文库下载器）

* ###### Figma（设计，教程：https://blog.csdn.net/aimeeth/article/details/134139926）

* OfficeTool（Office 部署工具）

## 3、📸多媒体制作

* Adobe Photoshop Pro

* Adobe Premiere Pro（专业版视频剪辑）

  http://www.dakongbai.cn/show.asp?id=4733（向天歌李兴兴PR剪辑上分攻略（B站现价99元））

* Adobe Premiere Elements（家庭版视频剪辑）

* 剪映（对于短视频、信息流或者口播类的更加友好，各种模板特效，ai文字配音等等，很适合信息流视频）

* 达芬奇（风格化的商业广告片，tvc等对于视频拍摄素材的规格和质量要求更高的片子）

* Final Cut Pro（mac混剪）

* Vegas

* 会声会影（家庭版视频剪辑）

* OBS（直播录屏）

* UTAU（歌声合成软件）

* Melodyne（调音，鬼畜）

* freepiano（电脑键盘模拟演奏钢琴软件）

* Foobar2000（音频播放器）

* [KMPlayer](https://www.ghxi.com/kmplayerpc.html)（视频播放器）

* Tinymediamanager（影片搜刮）

* Screenbox（现代化的媒体播放器）

* 格式工厂

* 唧唧Down（B站视频下载）

* 自媒体导航：http://www.zimeiti135.com/

* ACDSee（图片文件管理，查重）

* [qBittorrent](https://github.com/c0re100/qBittorrent-Enhanced-Edition)（基于Qt工具箱和libtorrent-rasterbar，并用C++编写的BitTorrent客户端）

* BitComet（下载）

* uTorrent（下载）

* eagle（图片管理软件）

## 4、🖥️windows系统相关

* ###### [CPU-Z](https://www.ghxi.com/cpu-z.html)（硬件信息查看工具）

* TrayS（win10任务栏透明软件）

* One Commander（多栏文件管理器）

* TeamViewer（远程控制）

* IDM（下载器）

* WizMouse（Win7鼠标焦点不点击滑动事件触发）

* Advanced Archive Password Recovery（rar破解）

* 图新地球

* 光速搜索

* Snipaste（截图）

* QR\_Research（二维码扫描）

* Windows On Top（软件窗口置顶）

* Opera 浏览器

* Opera GX 浏览器（游戏浏览器）

* DiskGenius（数据恢复）

* 迅龙数据恢复

* ziperello（压缩包密码破解）

* Advanced PDF Password Recovery Pro（pdf加密）

* 印章制作大师

* MicrosoftToolkit（老牌的激活工具）

* ETH超级矿工 + ETH超级矿工-监控器（挖矿）

* xmrig（CPU挖矿）

* ccminer-x64（N卡挖矿）

* WINDOWSJimCoinInstaller（比特币钱包）

* [spacesniffer](https://www.ghxi.com/spacesniffer.html)（磁盘空间分析）

* wiztree（磁盘空间分析）

* EXIF（EXIF照片地址查看）

* Spacedesk（IPAD作为电脑扩展屏）

* WiredXDisplay（⭐IPAD作为电脑扩展屏）

* EmptyFolderFinder（空文件夹查找器）

* TinyTools（小而美的工具包）

* GreenHub（https://o.extensiondock.com）

* Everything（文件搜索工具）

* 傲梅分区助手（推荐分区工具）

* Cisco PacketTracer（思科路由器模拟器）

* v2rayN（机场，适用于 Windows、Linux 和 macOS）

* bongocat（按键桌宠）

* NFC Tools PRO/NFC Tasks（NFC写入软件）

* crossover（Mac电脑上的类虚拟机软件，可以在Mac和Linux系统上运行Microsoft Windows应用）

资源付费（自动发卡平台，小次方）

## 5、手机软件

* 酷安
* IDM+（酷安下载）
* Cellular-Z（卫星）
* AccuBattery（手机电池寿命）
* 一个木函（工具包）

## 6、自媒体

主流平台：

百家号、

头条号（https://mp.toutiao.com/profile\_v3/index）qq，178 xxl666、

大鱼号（https://mp.dayu.com/）126 Xxl66666 178、

企鹅号（https://om.qq.com/userAuth/index收益比较低）、

趣头条（https://baijiahao.baidu.com/builder/app/choosetype）178 xxl666

一点号（http://www.yidianzixun.com/）

## 7、国外撸美金问卷

觅米小站

问卷荟平台

攒粒：<https://www.zanli.com/>

沃克短视频volkvlog

givvy videos（听音乐看视频赚钱）

---

---
url: /10.配置/01.主题配置/50.功能页配置.md
---

# 功能页配置

## 私密文章（登录页）

什么是登录页？在导航栏 功能页 -> 登录页 点击查看效果。

您可以通过 `teek-login-page` 插槽自定义登录页。

使用登录页需要 2 个步骤：

* 创建一个登录页，如何创建请看 [登录页](/reference/function-page#登录页)
* 开启私密文章监听

```ts
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  private: {
    enabled: true,
  },
});
```

此时已经开启了私密文章功能，下一步您需要配置登录相关的配置，如果要想了解所有配置请看 更多配置项。

这里仅仅给出一些配置项例子，有关私密文章更详细的设计、介绍、使用，请看 [私密文章](/guide/private)

::: code-group

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  private: {
    enabled: false,
    expire: "1d",
    session: true,
    siteLogin: false,
    site: [
      { username: "teek-site-1", password: "teek", role: "common", expire: "1d", session: true, strategy: "once" },
      { username: "teek-site-2", password: "teek", role: "admin", expire: "1d", session: false, strategy: "always" },
    ],
    pages: [
      { username: "teek-pages-1", password: "teek", expire: "1d", session: true, strategy: "once" },
      { username: "teek-pages-2", password: "teek", expire: "1d", session: false, strategy: "always" },
    ],
    realm: {
      blog: [
        { username: "teek-blog-1", password: "teek", expire: "1d", session: true, strategy: "once" },
        { username: "teek-blog-2", password: "teek", expire: "1d", session: false, strategy: "always" },
      ],
      comment: [
        { username: "teek-comment-1", password: "teek", expire: "1d", session: true, strategy: "always" },
        { username: "teek-comment-2", password: "teek", expire: "1d", session: false, strategy: "always" },
      ],
    },
    // onFocus: (value, formName) => {},
    // onBlur: (value, formName) => {},
    // doLogin: (loginInfo, type, nativeExecLogin) => true,
    // doValidate: (type, frontmatter, nativeExecLogin) => true,
    // encrypt: (value, frontmatter) => value,
    // decrypt: (value, frontmatter) => value,
  },
});
```

```ts [更多配置项]
interface Private {
  /**
   * 是否启用私密功能
   *
   * @default false
   */
  enabled?: boolean;
  /**
   * 登录过期时间：1d 代表 1 天，1h 代表 1 小时，仅支持这两个单位，不加单位代表秒。过期后访问私密文章重新输入用户名和密码。默认一天
   *
   * @default '1d'
   */
  expire?: string;
  /**
   * 开启是否在网页关闭或刷新后，清除登录状态，这样再次访问网页，需要重新登录
   *
   * @default false
   */
  session?: boolean;
  /**
   * 是否使用站点级别登录功能，即第一次进入网站需要验证
   *
   * @default false
   */
  siteLogin?: boolean;
  /**
   * 站点级别登录信息，进入站点时需要认证，当 siteLogin 为 true 时生效
   */
  site?: (LoginInfo & { role?: "common" | "admin" })[];
  /**
   * 全局页面级登录信息，登录一次后其他全局页面级别的文章都可以访问
   */
  pages?: LoginInfo[];
  /**
   * 领域页面级别登录信息，登录一次后其他相同领域的文章都可以访问
   */
  realm?: { [key: string]: LoginInfo[] };
  /**
   * 输入框聚焦回调
   */
  onFocus?: (value: string, formName: "username" | "password" | "verifyCode") => void;
  /**
   * 输入框失焦回调
   */
  onBlur?: (value: string, formName: "username" | "password" | "verifyCode") => void;
  /**
   * 自定义登录逻辑，如果返回 boolean 代表自定义逻辑成功或者失败（内部会删除提示语），返回 undefined 代表结束登录逻辑
   *
   * @param nativeExecLogin 内置的登录函数，通过调用该函数来实现内置的登录功能
   */
  doLogin?: (
    loginInfo: { username: string; password: string },
    type: "site" | "pages" | "realm" | "page",
    nativeExecLogin: () => boolean
  ) => boolean | undefined;
  /**
   * 自定义验证逻辑
   *
   * @param nativeExecLogin 内置的登录函数，通过调用该函数来实现内置的登录功能
   */
  doValidate?: (
    type: "site" | "pages" | "realm" | "page",
    frontmatter: Record<string, any>,
    nativeValidate: () => boolean
  ) => boolean;
  /**
   * 自定义加密逻辑
   */
  encrypt?: (value: string, frontmatter: Record<string, any>) => string;
  /**
   * 自定义解密逻辑
   */
  decrypt?: (value: string, frontmatter: Record<string, any>) => string;
}

interface LoginInfo {
  /**
   * 用户名
   */
  username: string;
  /**
   * 密码
   */
  password: string;
  /**
   * 登录过期时间：1d 代表 1 天，1h 代表 1 小时，仅支持这两个单位，不加单位代表秒。过期后访问私密文章重新输入用户名和密码。默认一天
   *
   * @default 1d
   */
  expire?: string;
  /**
   * 开启是否在网页关闭或刷新后，清除登录状态，这样再次访问网页，需要重新登录
   *
   * @default false
   */
  session?: boolean;
  /**
   * 登录策略，once 代表一次登录，always 代表每次访问都登录
   *
   * @default 'once'
   */
  strategy?: "once" | "always";
}
```

:::

## 风险链接提示页&#x20;

什么是风险链接提示页？在导航栏 功能页 -> 风险链接提示页 点击查看效果。

您可以通过 `teek-risk-link-page` 插槽自定义险链接提示页。

使用风险链接提示页需要 2 个步骤：

* 创建一个风险链接提示页，如何创建请看 [风险链接提示页](/reference/function-page#风险链接提示页)
* 开启监听外部链接跳转拦截功能（监听 `a` 标签的点击事件）

::: code-group

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  riskLink: {
    enabled: true,
  },
});
```

```ts [更多配置项]
interface RiskLink {
  /**
   * 是否启用风险链接提示功能
   *
   * @default false
   */
  enabled?: boolean;
  /**
   * 白名单，支持正则表达式
   */
  whitelist?: Array<RegExp | string>;
  /**
   * 黑名单，支持正则表达式
   *
   * @remark 如果设置了黑名单，则只拦截黑名单的链接
   */
  blacklist?: Array<RegExp | string>;
}
```

:::

此时 Teek 会监听所有 `a` 标签的点击事件，如果点击的链接是风险链接，则会先跳转到风险链接提示页。

::: tip 什么是风险链接？

Teek 把非本域名下的跳转链接视为风险链接。

:::

如果您需要对一些链接放行或者专门拦截，请使用白名单和黑名单功能。

```ts
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  riskLink: {
    enabled: true,
    whitelist: ["http://vp.teek.top", /https:\/\/github.com/],
    blacklist: [],
  },
});
```

白名单和黑名单支持字符串和正则表达式：

* 当为字符串时，Teek 先完全匹配跳转的链接，如果匹配失败，则匹配跳转的链接开头部分（`startsWith` 方法），因此你可以配置某个域名或者某个完整的链接
* 当为正则表达式时，按照正则表达式进行匹配跳转的链接

当配置了黑名单，则只拦截黑名单的链接，其他链接全部放行，如果黑名单的链接在白名单里，则也会放行（白名单优先级最高）。

---

---
url: /10.配置/30.功能页配置.md
---

# 功能页配置

Teek 支持构建分类页、标签页、归档页，文章清单页、登录页、风险链接提示页，这些页面本质上是一个 Markdown 文档，通过在 `frontmatter` 配置来开启功能，因此可以与其他文档一起放到任意目录下，并且和正常 Markdown 文档一样可以进行内容编写。

但是 Teek 建议放在 `@pages` 目录下，因为 Teek 不会对 `@pages` 目录下的文档做任何处理（自动生成侧边栏、站点分析，自动生成 `frontmatter` 等）。

```sh
.
├─ @pages
│  ├── archivesPage.md
│  ├── articleOverviewPage.md
│  ├── categoriesPage.md
│  ├── loginPage.md
│  ├── riskLinkPage.md
│  ├── tagsPage.md
```

## 分类页

在 `frontmatter` 中配置 `categoriesPage: true` 和 `layout: home` 来开启分类页。

```yaml
---
title: 分类
permalink: /categories
categoriesPage: true
layout: home
article: false
---
```

`permalink` 需要配置为 `/categories`，如果你希望修改为 `/c`，需要在主题配置中配置 `category.path` 为 `/c`：

```ts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  category: {
    path: "/c",
  },
});
```

## 标签页

在 `frontmatter` 中配置 `tagsPage: true` 和 `layout: home` 来开启标签页。

```yaml
---
title: 标签
permalink: /tags
tagsPage: true
layout: home
article: false
---
```

`permalink` 需要配置为 `/tags`，如果你希望修改为 `/t`，需要在主题配置中配置 `tag.path` 为 `/t`：

```ts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  tag: {
    path: "/t",
  },
});
```

## 归档页

有两种方式可以开启归档页：

1. 在 `frontmatter` 配置 `archivesPage: true` 和 `layout: page` 来开启归档页
2. 在 `frontmatter` 配置 `layout: TkCataloguePage` 来开启归档页

::: code-group

```yaml [方式 1]
---
title: 归档
permalink: /archives
archivesPage: true
layout: page
article: false
sidebar: false
---
```

```yaml [方式 2]
---
title: 归档
permalink: /archives
layout: TkCataloguePage
article: false
sidebar: false
---
```

:::

`permalink` 没有强制要求为 `/archives`，你可以根据自己的需求进行配置，然后在导航栏配置 `url` 为 `permalink` 的值即可。

通过 `frontmatter`，你可以定制归档页的部分文字：归档页所有可以配置的 `frontmatter` 如下：

```yaml
---
title: 归档
permalink: /archives
layout: TkCataloguePage
totalCount: 总共 {count} 篇文章
year: 年
month: 月
count: 篇
notFound: 未指定
---
```

这些文字优先级会覆盖 Teek 的国际化文字。

## 文章清单页&#x20;

有两种方式可以开启文章清单页：

1. 在 `frontmatter` 配置 `articleOverviewPage: true` 和 `layout: page` 来开启文章清单页
2. 在 `frontmatter` 配置 `layout: TkArticleOverviewPage` 来开启文章清单页

::: code-group

```yaml [方式 1]
---
title: 归档
permalink: /articleOverview
articleOverviewPage: true
layout: page
article: false
sidebar: false
---
```

```yaml [方式 2]
---
title: 归档
permalink: /articleOverview
layout: TkArticleOverviewPage
article: false
sidebar: false
---
```

:::

`permalink` 没有强制要求为 `/articleOverview`，你可以根据自己的需求进行配置，然后在导航栏配置 `url` 为 `permalink` 的值即可。

您可以通过 `publishDateFormat` 来设置发布时间的格式，比如 `yyyy-MM-dd`，默认为 `yyyy-MM-dd hh:mm:ss`。

```yaml {6}
---
title: 文章清单
permalink: /articleOverview
articleOverviewPage: true
layout: page
publishDateFormat: yyyy-MM-dd
article: false
sidebar: false
---
```

## 登录页&#x20;

有两种方式可以开启登录页：

1. 在 `frontmatter` 配置 `loginPage: true` 和 `layout: false` 来开启登录页，此时登录页不含有导航
2. 在 `frontmatter` 配置 `layout: TkLoginPage` 来开启登录页，此时登录页顶部有导航

::: code-group

```yaml [方式 1]
---
permalink: /login
layout: false
loginPage: true
logo: /teek-logo-large.png
name: VitePress Theme Teek
leftImg: /login/bg-1.png
article: false
---
```

```yaml [方式 2]
---
permalink: /login
layout: TkLoginPage
logo: /teek-logo-large.png
name: VitePress Theme Teek
leftImg: /login/bg-1.png
article: false
---
```

:::

`leftImg: /login/bg-1.png`  是添加左侧图片的配置项，如果添加该配置，左侧将会出现图片，右侧为登录框；如果不添加该配置，则登录框位于屏幕中间。

## 风险链接提示页&#x20;

有两种方式可以开启风险链接提示页：

1. 在 `frontmatter` 配置 `riskLinkPage: true` 和 `layout: false` 来开启登录页，此时风险链接提示页不含有导航
2. 在 `frontmatter` 配置 `layout: TkRiskLinkPage` 来开启登录页，此时风险链接提示页顶部有导航

::: code-group

```yaml [方式 1]
---
permalink: /risk-link
layout: false
riskLinkPage: true
logo: /teek-logo-large.png
name: VitePress Theme Teek # 与 desc 二选一
desc: 即将离开 VitePress Theme Teek，请注意财产安全 # 与 name 二选一
linkIllegal: 链接安全性校验中，请稍后 ...
article: false
---
```

```yaml [方式 2]
---
permalink: /risk-link
layout: TkRiskLinkPage
logo: /teek-logo-large.png
name: VitePress Theme Teek # 与 desc 二选一
desc: 即将离开 VitePress Theme Teek，请注意财产安全 # 与 name 二选一
linkIllegal: 链接安全性校验中，请稍后 ...
article: false
---
```

:::

---

---
url: /01.指南/40.开发/05.贡献指南.md
---

# 贡献指南

感谢您使用 Teek。

以下是关于向 Teek 提交反馈或代码的指南。在向 Teek 提交 Issue 或者 PR 之前，请先花几分钟时间阅读以下内容。

## Issue 规范

* 遇到问题时，请先确认这个问题是否已经在 Issue 中有记录或者已被修复
* 提 Issue 时，请用简短的语言描述遇到的问题，并添加出现问题时的环境和复现步骤，必要时需提供可复现问题最小代码仓库

环境包含

* `浏览器` 版本
* `操作系统` 版本
* `node` 版本
* `vitepress` 版本
* `Teek` 版本

## 参与开发

参考 [开发指南](/guide/dev)。

### 代码规范

在编写代码时，请注意：

* 确保代码可以通过仓库的 `ESLint` 校验
* 确保代码格式是规范的，使用 `prettier` 进行代码格式化

## 提交 Pull Request

### 参考指南

如果你是第一次在 GitHub 上提 Pull Request ，可以阅读下面这两篇文章来学习：

* [第一次参与开源](https://github.com/firstcontributions/first-contributions/blob/main/translations/README.zh-cn.md)
* [如何优雅地在 GitHub 上贡献代码](https://segmentfault.com/a/1190000000736629)

### Pull Request 规范

在提交 Pull Request 时，请注意：

* 保持你的 PR 足够小，一个 PR 只解决单个问题或添加单个功能
* 在 PR 中请添加合适的描述，并关联相关的 Issue

### Pull Request 流程

1. fork 主仓库，如果已经 fork 过，请同步主仓库的最新代码
2. 基于 fork 后仓库的 dev 分支新建一个分支，比如 feature/docs
3. 在新分支上进行开发，开发完成后，提 Pull Request 到主仓库的 dev 分支
4. Pull Request 会在 Review 通过后被合并到主仓库
5. 等待 Teek 发布新版本

### Pull Request 标题格式

Pull Request 的标题应该遵循以下格式：

```sh
type(scoped)：commit message
```

示例：

* docs: add contribution.md
* build: optimize build speed
* fix(component(icon)): incorrect style
* feat(composables(useVpRouter)): add new function

可选的类型：

* feat
* fix
* docs
* style
* refactor
* perf
* test
* build
* ci
* chore
* revert
* wip
* types

## 同步最新代码

提 Pull Request 前，请依照下面的流程同步主仓库的最新代码：

```sh
# 添加主仓库到 remote
git remote add upstream https://github.com/Kele-Bingtang/vitepress-theme-teek.git

# 拉取主仓库最新代码
git fetch upstream

# 切换至 dev 分支
git checkout dev

# 合并主仓库代码
git merge upstream/dev
```

---

---
url: /@pages/archivesPage.md
---


---

---
url: /daily/开源项目/2_规则引擎LiteFlow.md
---

# 规则引擎LiteFlow

## 一、LiteFlow简介

> https://gitee.com/dromara/liteFlow
>
> https://liteflow.cc/

搭建系统要先搭建框架，而搭建框架最重要的事情之一就是要设计好流程引擎。流程引擎可以带来诸如\*\*【避免冗余】【最小修改】【方便追踪】【利于分工】【可读性好】【灵活多变】\*\*等好处，这些好处是伴随着整个系统生命周期的。

市面上这样的引擎其实还挺多的，知名的包括**Activiti、Flowable、camunda** 等。这些流程引擎功能强大，还提供了可视化的能力，很多大厂也在使用。**LiteFlow** 是相比他们来说更轻量级一些的流程引擎，很容易上手，取之即用。

官方对LiteFlow的介绍：

> *LiteFlow是一个轻量且强大的国产规则引擎框架，可用于复杂的组件化业务的编排领域，独有的DSL规则驱动整个复杂业务，并可实现平滑刷新热部署，支持多种脚本语言规则的嵌入。帮助系统变得更加丝滑且灵活。*
>
> *LiteFlow于2020年正式开源，2021年获得开源中国年度最受欢迎开源软件殊荣。于2022年获得Gitee最有价值开源项目(GVP)荣誉。是一个正处在高速发展中的开源项目。*
>
> *LiteFlow是一个由社区驱动的项目，拥有一个2500多人的使用者社区*

虽然相比Acitiviti、Flowable来说，LiteFlow的知名度要低得多，功能也没有这些知名成熟引擎那么强大，但LiteFlow还是有诸多优点，能够满足你绝大部分的场景。这些优点包括：

**【使用便捷】**：引几个jar包、实现几个接口、写一个流程编排文件，就能run。

**【编排丰富】**：支持串行、并行、选择、循环、异常处理、嵌套等各种编排方式。

**【支持脚本】**：支持各种主流脚本语言。

**【配置源丰富】**：支持将流程定义放到ZK、DB、Etcd、Nacos等。相当于可以实现动态配置更改。

## 二、快速开始

### :palm\_tree:依赖

增加liteflow-spring-boot-starter依赖包，提供自动装配功能

```xml
<dependency>
    <groupId>com.yomahub</groupId>
    <artifactId>liteflow-spring-boot-starter</artifactId>
    <version>2.12.0</version>
</dependency>
```

### :palm\_tree:配置

SpringBoot配置文件

```yaml
liteflow:
  rule-source: config/flow.el.xml
```

在resources下的`config/flow.el.xml`中定义规则：

```xml
<?xml version="1.0" encoding="UTF-8"?>
<flow>
    <chain name="chain1">
        THEN(a, b, c);
    </chain>
</flow>
```

### :palm\_tree:组件

定义并实现一些组件，确保SpringBoot会扫描到这些组件并注册进上下文。

```java
@Component("a")
public class ACmp extends NodeComponent {

	@Override
	public void process() {
		//do your business
	}
}
```

以此类推再分别定义b,c组件：

```java
@Component("b")
public class BCmp extends NodeComponent {

	@Override
	public void process() {
		//do your business
	}
}
@Component("c")
public class CCmp extends NodeComponent {

	@Override
	public void process() {
		//do your business
	}
}
```

### :palm\_tree:执行

声明启动类：

```java
/**
 * 启动类
 *
 * @author xxl
 * @date 2024/5/6 22:48
 */
@ComponentScan({"com.xxl.liteflow"}) //把定义的组件扫入Spring上下文中
@SpringBootApplication
public class SpringbootLiteFlowApplication {
    public static void main(String[] args) {
        SpringApplication.run(SpringbootLiteFlowApplication.class, args);
    }
}
```

然后就可以在Springboot任意被Spring托管的类中拿到flowExecutor，进行执行链路：

```java
@Component
public class YourClass{
    
    @Resource
    private FlowExecutor flowExecutor;
    
    public void testConfig(){
        LiteflowResponse response = flowExecutor.execute2Resp("chain1", "arg");
    }
}
```

## 三、实战

### :palm\_tree:简单的串行流程

以一个简单的用户转账接口为例：

提供一个用户转账接口，用户可以给收款方转账，转账成功后生成一个转账账单。

#### :seedling:【step1】确定流程

一个转账行为的步骤如下：

![image-20240511221120610](/assets/image-20240511221120610.DM68508G.png)

#### :seedling:【step2】实现组件

根据上面这个流程，实现如下组件：

【UserTransferComponent】：用户转账组件，负责执行转账操作。

【UserTransferBillComponent】：用户转账账单组件，记录转账账单。

【UserTransferSuccessComponent】：转账成功组件，设置转账成功标志。

这三个组件的关键是要继承LiteFlow的NodeComponent类，并实现process()方法，如下所示：

UserTransferComponent

```java
@Component("userTransfer")
public class UserTransferComponent extends NodeComponent {

    @Autowired
    private TransferService transferService;

    @Override
    public void process() throws Exception {
        MyRequestContext requestContext = this.getContextBean(MyRequestContext.class);
        //执行转账
        String orderId = transferService.pay(requestContext.getPayerId(), requestContext.getPayeeId(), requestContext.getMoney());
        this.getContextBean(MyResponseContext.class).setOrderId(orderId);
    }
}
```

UserTransferBillComponent

```java
@Component("userTransferBill")
public class UserTransferBillComponent extends NodeComponent {

    @Autowired
    private TransferBillService transferBillService;

    @Override
    public void process() throws Exception {
        MyRequestContext requestContext = this.getContextBean(MyRequestContext.class);
        MyResponseContext responseContext = this.getContextBean(MyResponseContext.class);
        //记录账单
        transferBillService.recordBill(responseContext.getOrderId(), requestContext.getPayerId(), requestContext.getPayeeId());
    }
}
```

UserTransferSuccessComponent

```java
@Component("userTransferSuccess")
public class UserTransferSuccessComponent extends NodeComponent {

    @Override
    public void process() throws Exception {
        //设置转账成功标志
        this.getContextBean(MyResponseContext.class).setSuccess(true);
    }

}
```

#### :seedling:【step3】编排组件

接着，我们要新建一个流程配置文件，并在里面描述我们想要的流程。

我们先新建一个flow.el.xml文件，并在application.yml中配置

application.yml

```yaml
liteflow:
  rule-source: config/flow.el.xml
```

接着，就是最重要的一步，在flow.el.xml中编写流程，如下：

```xml
<?xml version="1.0" encoding="UTF-8"?>
<flow>
    <chain name="userTransferChainSimple">
        THEN(userTransfer, userTransferBill, userTransferSuccess);
    </chain>
</flow>
```

文件中包含这些内容：

【1】需要对这个chain取一个name，这个name在后面调用流程时需要用到。

【2】THEN中可以看到，编排了上面编写的三个组件，并且有顺序关系。

#### :seedling:【step4】实现一个controller调用接口

```java
/**
 * 转账流程调用接口
 *
 * @author xxl
 * @date 2024/5/11 22:14
 */
@RestController
public class UserTransferController {

    //LiteFlow核心bean，用来触发流程
    @Resource
    private FlowExecutor flowExecutor;

    @GetMapping(path = "/userTransferChainSimple")
    public String userTransferChainSimple(@RequestParam String payerId, @RequestParam String payeeId, @RequestParam int money) {
        //1. 构建请求上下文对象，用来传递参数进流程组件
        RequestContext requestContext = new RequestContext();
        requestContext.setPayerId(payerId);
        requestContext.setPayeeId(payeeId);
        requestContext.setMoney(money);

        //2. 调用流程引擎，传入请求上下文和结果上下文，结果上下文用于组件在其中设置各种结果
        ResponseContext responseContext = new ResponseContext();
        flowExecutor.execute2Resp("userTransferChainSimple", null, requestContext, responseContext);

        //3. 构造返回对象
        return buildResult(responseContext);
    }
}
```

其中需要注意的是：启动流程时需要构建上下文对象并传入流程引擎。这个上下文对象完全可以自己定义，上图示例仅是我的习惯（分开了请求上下文和结果上下文）。

#### :seedling:【step5】run！

执行运行一下看下结果：

![image-20240511235744663](/assets/image-20240511235744663.wzfdd-J8.png)

下面是控制台的输出：

```sh
2024-05-11 23:57:17.975  INFO 20216 --- [nio-8082-exec-6] com.yomahub.liteflow.slot.Slot           : [3fb498306f40485894840641e017f128]:CHAIN_NAME[userTransferChainSimple]
userTransfer<0>==>userTransferBill<0>==>userTransferSuccess<0>
2024-05-11 23:57:17.975  INFO 20216 --- [nio-8082-exec-6] com.yomahub.liteflow.slot.DataBus        : [3fb498306f40485894840641e017f128]:slot[2] released
```

从上面控制台输出图中可以看到组件执行的过程。

看下整个代码的目录结构：

![image-20240511235954917](/assets/image-20240511235954917.zelkQtd_.png)

使用了流程引擎liteflow后，整个代码的目录也很清爽清楚。

### :palm\_tree:涉及到条件选择(if)的流程

#### :seedling:【step1】流程

延展上面的转账流程，加入选择的逻辑。整个流程变成如下这样：

![image-20240512001719343](/assets/image-20240512001719343.CLolCq02.png)

新增入参校验和转账失败节点。

#### :seedling:【step2】新增组件

【入参校验】需要用LiteFlow的“条件组件”来实现，而【转账失败】组件则和转账成功组件一样实现，如下所示：

用户入参检查组件：UserCheckComponent

需要注意的是v2.12之前使用NodeIfComponent，之后版本后使用NodeBooleanComponent。

```java
// 历史版本
@Component("userCheck")
public class UserCheckComponent extends NodeIfComponent {

    @Override
    public boolean processIf() throws Exception {
        RequestContext requestContext = this.getContextBean(RequestContext.class);
        int money = requestContext.getMoney();
        if (money <= 0) {
            return false;
        }
        return true;
    }
}
// v2.12
@Component("userCheck")
public class UserCheckComponent extends NodeBooleanComponent {

    @Override
    public boolean processBoolean() throws Exception {
        MyRequestContext requestContext = this.getContextBean(MyRequestContext.class);
        int money = requestContext.getMoney();
        if (money <= 0) {
            return false;
        }
        return true;
    }
}
```

转账失败组件：UserTransferFailComponent

```java
@Component("userTransferFail")
public class UserTransferFailComponent extends NodeComponent {

    @Override
    public void process() throws Exception {
        this.getContextBean(ResponseContext.class).setSuccess(false);
    }

}
```

#### :seedling:【step3】新增配置

在流程定义文件中的定义：

```xml
<flow>
    <chain name="userTransferChainIfElse">
        THEN(
            IF(userCheck, THEN(userTransfer, userTransferBill, userTransferSuccess)).ELSE(userTransferFail)
        );
    </chain>
</flow>
```

#### :seedling:【step4】运行测试

两种执行结果

```sh
userCheck<0>==>userTransfer<0>==>userTransferBill<12>==>userTransferSuccess<0>

userCheck<0>==>userTransferFail<0>
```

### :palm\_tree:涉及到异常处理的流程

#### :seedling:【step1】流程

涉及到异常的流程。流程如下：

![image-20240512005734162](/assets/image-20240512005734162.CfpDRo6O.png)

#### :seedling:【step2】新增配置

在流程定义文件中的定义如下：

```xml
<flow>
    <chain name="userTransferChainCatchException">
        CATCH(
            THEN(userTransfer, userTransferBill, userTransferSuccess)
        ).DO(userTransferFail);
    </chain>
</flow>
```

通过CATCH关键字包裹需要捕获异常的流程，然后使用DO来指定异常处理流程即可。

#### :seedling:【step3】运行测试

代码中添加一段异常，直接运行看一下异常情况下控制台的输出：

```
userTransfer<3>==>userTransferFail<0>
```

从控制台可以看出，捕获了预埋的异常，然后走进了userTransferFail节点。

### :palm\_tree:将流程配置放到DB，并实现热刷新

**LiteFlow 还支持把流程定义放在数据库里维护，并实现热刷新（修改流程后不需要重新启动服务）**。

#### :seedling:【step1】添加依赖

需要添加以下额外插件依赖：

```xml
<dependency>
    <groupId>com.yomahub</groupId>
    <artifactId>liteflow-rule-sql</artifactId>
    <version>2.12.0</version>
</dependency>
```

#### :seedling:【step2】增加配置

依赖了插件包之后，无需再配置`liteflow.ruleSource`路径。

只需要配置插件的额外参数即可，在`application.yml`中，添加如下配置：

```yaml
liteflow:
#  rule-source: config/flow.el.xml
  rule-source-ext-data-map:
    #数据源基础配置，项目中配置过spring.datasource无需再次配置
#    url: jdbc:mysql://127.0.0.1:3306/x_file_storage
#    driverClassName: com.mysql.cj.jdbc.Driver
#    username: root
#    password: xxl666
    applicationName: liteflowdemo
    #配置流程的表名
    chainTableName: LITE_FLOW_CHAIN
    #注意，下面xxxField结尾的，都是表中的列名
    #应用名
    chainApplicationNameField: application_name
    #流程名
    chainNameField: chain_name
    #流程表达式
    elDataField: el_data
```

LiteFlow支持了使用项目中已存在的Datasource来进行数据库连接。如果项目中已有链接配置，无需再次配置。

#### :seedling:【step3】建立数据库表

下面插入一个原本写在流程定义文件中的流程配置：

```sql
create table LITE_FLOW_CHAIN (
  `id` int(11) NOT NULL auto_increment,
  `application_name` varchar(64) NOT NULL,
  `chain_name` varchar(64) NOT NULL,
  `el_data` varchar(2046) NOT NULL,
  PRIMARY KEY (`id`)
);

insert into LITE_FLOW_CHAIN values (1, "liteflowdemo", "userTransferChainFromDB", "THEN(userTransfer, userTransferBill, userTransferSuccess)");
```

#### :seedling:【step4】自动刷新

测试执行的“流程链条”。

```sh
userTransfer<4>==>userTransferBill<11>==>userTransferSuccess<0>
```

然后我们在数据库修改这个流程，中间加入一个UserTransferNotifyComponent

```sql
UPDATE LITE_FLOW_CHAIN SET el_data = "THEN(userTransfer, userTransferBill, userTransferNotify, userTransferSuccess)" WHERE id = 1;
```

这个时候再去访问链接，还是原来的流程链。

这是因为**LiteFlow不支持（v2.11.1之前版本不支持，之后的版本是默认关闭的）DB作为数据源的热加载**（如果使用ZK，etcd等作为数据源，自动实现热加载，详情可以去官网看看）。

**v2.11.1之前主动刷新缓存**

所以在DB修改完以后，要手动刷新一下LiteFlow的本地缓存。

我实现了一个简单的接口让我们可以主动刷新缓存，如下：

```java
    /**
     * 主动刷新缓存
     */
    @GetMapping(path = "/reloadLiteFlowConfigure")
    public void reloadLiteFlowConfigure() {
        flowExecutor.reloadRule();
    }
```

调用完这个接口以后，我们再访问一下链接，就是如下效果了：

```sh
userTransfer<0>==>userTransferBill<0>==>userTransferNotify<0>==>userTransferSuccess<0>
```

**v2.11.1+轮询自动刷新**

LiteFlow支持SQL数据源轮询模式的自动刷新机制。可以在配置项中通过`pollingEnabled: true`来开启自动刷新：

```yaml
liteflow:
  rule-source-ext-data-map:
    ...
    pollingEnabled: true
    ##以下非必须，默认1分钟
    pollingIntervalSeconds: 60
    pollingStartSeconds: 60
    ...
```

轮询模式的自动刷新根据预设的时间间隔定时拉取SQL中的数据，与本地保存的数据SHA值进行对比来判断是否需要更新数据。

```sh
2024-05-13 00:40:09.153  INFO 22552 --- [  SQL-Polling-1] c.y.l.parser.sql.read.AbstractSqlRead    : query sql: SELECT * FROM LITE_FLOW_CHAIN WHERE application_name='liteflowdemo'
2024-05-13 00:40:09.155  INFO 22552 --- [  SQL-Polling-1] c.y.l.parser.sql.util.LiteFlowJdbcUtil   : use dataSourceName[dataSource],has found liteflow config
2024-05-13 00:41:09.154  INFO 22552 --- [  SQL-Polling-1] c.y.l.parser.sql.read.AbstractSqlRead    : query sql: SELECT * FROM LITE_FLOW_CHAIN WHERE application_name='liteflowdemo'
2024-05-13 00:41:09.155  INFO 22552 --- [  SQL-Polling-1] c.y.l.parser.sql.util.LiteFlowJdbcUtil   : use dataSourceName[dataSource],has found liteflow config
```

定时轮询存在些微的性能消耗；受轮询间隔限制，数据更新有一定延迟性。

## 参考资料

苏三说技术-聊聊LiteFlow

---

---
url: /01.指南/10.使用/25.国际化.md
---

# 国际化

Teek 默认使用中文，如果你希望使用其他语言，你可以参考下面的方案。

## 全局语言配置&#x20;

在 `Teek.Layout` 组件传入 `locale` 参数，即可设置默认语言。

```ts
// .vitepress/config.mts
import Teek, { en } from "vitepress-theme-teek";
import "vitepress-theme-teek/index.css";

export default {
  extends: Teek,
  Layout: h(Teek.Layout, { locale: en }),
};
```

如果希望根据 VitePress 的国际化动态切换语言，可以定义一个组件 `TeekLayoutProvider.vue`。

```vue [TeekLayoutProvider.vue]
<script setup lang="ts">
import Teek, { zhCn, en } from "vitepress-theme-teek";
import { useData } from "vitepress";
import { computed } from "vue";

const { lang } = useData();

const locale = computed(() => {
  if (lang.value === "zh-CN") return zhCn;
  return en;
});
</script>

<template>
  <Teek.Layout :locale />
</template>
```

然后在 `.vitepress/theme/index.ts` 中传入 `TeekLayoutProvider` 组件。

```ts
// .vitepress/theme/index.ts
import Teek, { en } from "vitepress-theme-teek";
import "vitepress-theme-teek/index.css";
import TeekLayoutProvider from "../components/TeekLayoutProvider.vue";

export default {
  extends: Teek,
  Layout: TeekLayoutProvider,
};
```

`lang` 是 VitePress 国际化的一个配置项：

```ts
import { defineConfig } from "vitepress";

export default defineConfig({
  locales: {
    root: { lang: "zh-CN" },
    en: { lang: "en" },
  },
});
```

### 语言列表

Teek 目前支持以下语言：

* 简体中文（zh-cn）
* English（en）

### 自定义语言

如果你需要使用其他的语言，可以添加一个语言配置文件。

比如需要添加繁体中文（zh-tw）语言，步骤如下：

1. 创建 `.vitepress/theme/locale/zh-tw.ts` 文件（路径位置任意）
2. 然后参考 Teek 现有的任一 [语言文件](https://github.com/Kele-Bingtang/vitepress-theme-teek/tree/master/vitepress-theme-teek/src/locale/lang)，将里面的内容拷贝到 `zh-tw.ts` 文件中，并将所有内容（Value）修改为 `zh-tw` 语言
3. 最后通过 `locale` 属性传入到 `Teek.Layout` 组件中

```vue [TeekLayoutProvider.vue] {4,10}
<script setup lang="ts">
import Teek from "vitepress-theme-teek";
import { useData } from "vitepress";
import zhTw from "../locale/zh-tw";

const { lang } = useData();
</script>

<template>
  <Teek.Layout :locale="zhTw" />
</template>
```

### provide 方式

除了通过 `locale` 属性传入语言配置，Teek 也支持通过 `provide` 方式传入语言配置：

```vue {5-8}
<script setup lang="ts">
import Teek, { localeContextKey, en } from "vitepress-theme-teek";
import { provide } from "vue";

provide(
  localeContextKey,
  computed(() => en) // 必须是 computer 或者 ref
);
</script>

<template>
  <Teek.Layout />
</template>
```

## 国际化下配置文件

### VitePress 配置

假设国际化环境下，配置文件目录如下：

```sh
.vitepress
├─ locales
│  ├─ zh.ts       # 中文配置
│  ├─ shared.ts   # 共享配置
│  ├─ en.ts       # 英文配置
│  ├─ xx.ts       # 其他语言配置
├─ config.mts
```

`.vitepress/config.mts` 内容如下：

```ts
// .vitepress/config.mts
import { defineConfig } from "vitepress";
import shared from "./locales/shared";
import zh from "./locales/zh";
import en from "./locales/en";

export default defineConfig({
  ...shared,
  locales: {
    root: { label: "简体中文", ...zh },
    en: { label: "English", ...en },
  },
});
```

VitePress 默认会对 `shared.ts` 和当前语言的配置文件进行合并，且配置同名时，以当前语言配置为主，如 `zh.ts` 和 `en.ts` 会覆盖 `shared.ts` 中的同名配置。

利用这个机制，你可以在 `shared.ts` 中定义一些通用的配置，然后 `zh.ts` 和 `en.ts` 里配置不同语言的配置，如：

::: code-group

```ts [shared.ts]
import { defineConfig } from "vitepress";

export default defineConfig({
  title: "Hd Security",
  cleanUrls: false,
  lastUpdated: true,

  head: [
    ["link", { rel: "icon", type: "image/svg+xml", href: "/teek-logo-mini.svg" }],
    ["link", { rel: "icon", type: "image/png", href: "/teek-logo-mini.png" }],
    ["meta", { property: "og:type", content: "website" }],
    ["meta", { property: "og:locale", content: "zh-CN" }],
    ["meta", { property: "og:title", content: "Teek | VitePress Theme" }],
    ["meta", { name: "author", content: "Teek" }],
    ["link", { rel: "icon", href: "/favicon.ico", type: "image/png" }],
    ["link", { rel: "stylesheet", href: "//at.alicdn.com/t/font_2989306_w303erbip9.css" }], // 阿里在线矢量库
  ],

  markdown: {
    lineNumbers: true,
  },

  // https://vitepress.dev/reference/default-theme-config
  themeConfig: {
    logo: "https://testingcf.jsdelivr.net/gh/Kele-Bingtang/static/user/avatar1.png",
    search: {
      provider: "local",
    },
  },
});
```

```ts [zh.ts]
import { defineConfig } from "vitepress";

const description = ["Teek 使用文档", "VitePress 主题"].toString();

export default defineConfig({
  lang: "zh-CN",
  description: description,
  head: [
    ["meta", { name: "description", description }],
    ["meta", { name: "keywords", description }],
  ],
  markdown: {
    container: {
      tipLabel: "提示",
      warningLabel: "警告",
      dangerLabel: "危险",
      infoLabel: "信息",
      detailsLabel: "详细信息",
    },
  },
  themeConfig: {
    darkModeSwitchLabel: "主题",
    sidebarMenuLabel: "菜单",
    returnToTopLabel: "返回顶部",
    lastUpdatedText: "上次更新时间",
    outline: {
      level: [2, 4],
      label: "本页导航",
    },
    docFooter: {
      prev: "上一页",
      next: "下一页",
    },
    nav: [
      { text: "首页", link: "/" },
      { text: "指南", link: "/guide/intro" },
      { text: "配置", link: "/reference/config" },
      { text: "开发", link: "/develop/intro" },
      { text: "归档", link: "/archives" },
    ],
    editLink: {
      text: "在 GitHub 上编辑此页",
      pattern: "https://github.com/Kele-Bingtang/vitepress-theme-teek/edit/master/docs/:path",
    },
  },
});
```

```ts [en.ts]
import { defineConfig } from "vitepress";

const description = ["Teek Documentation", "VitePress Theme"].toString();

export default defineConfig({
  lang: "en-US",
  description: description,
  head: [
    ["meta", { name: "description", description }],
    ["meta", { name: "keywords", description }],
  ],
  markdown: {
    container: {
      tipLabel: "Tip",
      warningLabel: "Warning",
      dangerLabel: "Danger",
      infoLabel: "Info",
      detailsLabel: "Details",
    },
  },
  themeConfig: {
    ...teekConfig.themeConfig,
    darkModeSwitchLabel: "Theme",
    sidebarMenuLabel: "Menu",
    returnToTopLabel: "To Top",
    lastUpdatedText: "LastUpdated",
    outline: {
      level: [2, 4],
      label: "Page Navigation",
    },
    docFooter: {
      prev: "prev",
      next: "next",
    },
    nav: [
      { text: "index", link: "/en" },
      { text: "guide", link: "/guide/intro" },
      { text: "reference", link: "/reference/config" },
      { text: "develop", link: "/develop/intro" },
      { text: "archives", link: "/en/archives" },
    ],
    editLink: {
      text: "Edit this page on GitHub",
      pattern: "https://github.com/Kele-Bingtang/vitepress-theme-teek/edit/master/docs/:path",
    },
  },
});
```

:::

### Teek 配置

在非国际化配置文件里下，您可以直接在 VitePress 的配置里使用 `extends` 来继承 Teek 的配置，但是在国际化配置文件下，`extends` 配置会失效。

因此需要将 Teek 配置的 `themeConfig` 手动添加到 VitePress 的 `themeConfig` 里。

::: code-group

```ts [shared.ts]
import { defineConfig } from "vitepress";
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  // 公共配置 ...
});

export default defineConfig({
  extends: teekConfig,
  // ...
});
```

```ts [zh.ts]
import { defineConfig } from "vitepress";
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  // zh 配置下配置 ...
});

export default defineConfig({
  themeConfig: {
    ...teekConfig.themeConfig,
    // ...
  },
});
```

```ts [en.ts]
import { defineConfig } from "vitepress";
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  // en 环境下配置 ...
});

export default defineConfig({
  themeConfig: {
    ...teekConfig.themeConfig,
    // ...
  },
});
```

:::

举个例子，您可以在中文和英文环境下分别取不同的名字：

::: code-group

```ts [zh.ts]
import { defineConfig } from "vitepress";
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  blogger: {
    name: "天客",
    slogan: "朝圣的使徒，正在走向编程的至高殿堂！",
  },
});

export default defineConfig({
  themeConfig: {
    ...teekConfig.themeConfig,
    // ...
  },
});
```

```ts [en.ts]
import { defineConfig } from "vitepress";
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  blogger: {
    name: "Teeker",
    slogan: "Code Pilgrims march to the summit of code mastery!",
  },
});

export default defineConfig({
  themeConfig: {
    ...teekConfig.themeConfig,
    // ...
  },
});
```

:::

## 给 root 语言添加目录

这里对一个特殊场景进行说明。

VitePress 支持的国际化文档目录如下：

```sh
docs/
├─ es/
│  ├─ foo.md
├─ fr/
│  ├─ foo.md
├─ foo.md
```

根目录下的 `foo.md` 是 root 语言（默认语言）的文档，当 Markdown 文件多起来时，根目录下文件显得很拥挤，那么可以将这些文档放到一个目录下，假设默认语言是 `zh`，则：

```sh
docs/
├─ es/
│  ├─ foo.md
├─ fr/
│  ├─ foo.md
├─ zh/
│  ├─ foo.md
```

但是 VitePress 无法感知到 root 语言（默认语言）的文档已经放到 `zh` 目录下，它依然只扫描根目录的 Markdown 文件作为默认语言的文档，因此需要使用 VitePress 提供的 `rewrites` 进行重定向，同时 Teek 也无法感知文档进行了移动，因此需要配置 `vitePlugins.sidebarOption.localeRootDir`

```ts {6-10,15-17}
import { defineConfig } from "vitepress";
import { defineTeekConfig } from "vitepress-theme-teek/config";

// Teek 主题配置
const teekConfig = defineTeekConfig({
  vitePlugins: {
    sidebarOption: {
      localeRootDir: "zh",
    },
  },
});

// VitePress 配置
export default defineConfig({
  rewrites: {
    "zh/:rest*": ":rest*",
  },
});
```

---

---
url: /daily/高等数学/03_函数极限与连续性.md
---

# 函数极限与连续性

---

---
url: /Java/Java开发技巧/02.函数式编程/0_函数式编程概念.md
---

# 函数式编程概念

## 一、为什么学？

* 能够看懂公司里的代码
* 大数量下处理集合效率高
* 代码可读性高
* 消灭嵌套地狱

```java
//查询未成年作家的评分在70以上的书籍 由于洋流影响所以作家和书籍可能出现重复，需要进行去重
List<Book> bookList = new ArrayList<>();
Set<Book> uniqueBookValues = new HashSet<>();
Set<Author> uniqueAuthorValues = new HashSet<>();
for (Author author : authors) {
    if (uniqueAuthorValues.add(author)) {
        if (author.getAge() < 18) {
            List<Book> books = author.getBooks();
            for (Book book : books) {
                if (book.getScore() > 70) {
                    if (uniqueBookValues.add(book)) {
                        bookList.add(book);
                    }
                }
            }
        }
    }
}
System.out.println(bookList);
```

```java
List<Book> collect = authors.stream()
    .distinct()
    .filter(author -> author.getAge() < 18)
    .map(author -> author.getBooks())
    .flatMap(Collection::stream)
    .filter(book -> book.getScore() > 70)
    .distinct()
    .collect(Collectors.toList());
System.out.println(collect);
```

## 二、函数式编程思想

### 2.1、概念

```
面向对象思想需要关注用什么对象完成什么事情。而函数式编程思想就类似于我们数学中的函数。它主要关注的是对数据进行了什么操作。
```

### 2.2、优点

* 代码简洁，开发快速
* 接近自然语言，易于理解
* 易于"并发编程"

---

---
url: /Java/Java开发技巧/02.函数式编程/3_函数式接口.md
---

# 函数式接口

## 一、概述

### 1.1、什么是函数式接口？

只有一个**抽象方法**的接口我们称之为函数接口。

JDK的函数式接口都加上了\*\*@FunctionalInterface\*\* 注解进行标识。

但是无论是否加上该注解只要接口中只有一个抽象方法，都是函数式接口。

> jdk自带的一些常用的一些接口Callable、Runnable、Comparator等在JDK8中都添加了@FunctionalInterface注解。

### 1.2、FunctionalInterface注解说明

1. 该注解只能标记在"有且仅有一个抽象方法"的接口上。
2. JDK8接口中的静态方法和默认方法，都不算是抽象方法。
3. 接口默认继承java.lang.Object，所以如果接口显示声明覆盖了Object中方法，那么也不算抽象方法。
4. 该注解不是必须的，如果一个接口符合"函数式接口"定义，那么加不加该注解都没有影响。
5. 为了规范，建议给所有需要使用函数式编程的接口都加上注解。

## 二、自定义函数式接口

只有一个抽象方法的情况

```java
@FunctionalInterface
public interface MyInterface {
    
    int test(int i);
}
```

有多个方法，但只有一个抽象方法，其他方法有默认实现

```java
@FunctionalInterface
public interface MyInterface {

    int test(int i);

    default int add(int a, int b) {
        return a + b;
    }

    default void sum(int a, int b) {
        System.out.println("sum:" + a + b);
    }
}
```

使用一下，看看效果

```java
public static void main(String[] args) {
    // 使用lombda实现test方法
    MyInterface myInterface = i -> i*2;
    
    // 调用刚刚实现的方法
    System.out.println(myInterface.test(22));
    
    // 调用默认重写的方法
    System.out.println(myInterface.add(10,20));
    myInterface.sum(100,20);
}
```

结果

```sh
44
30
sum:10020
```

## 三、JDK自带函数式接口

大多数情况下我们不需要自己去撰写函数式接口，因此jdk自带的函数接口应该是日常开发中，使用频率最高的函数式接口了，这里已经整理成表格的形式方便后续查阅使用。

| 接口             | 输入参数 | 返回类型 | 使用说明                     |
| :--------------- | :------- | :------- | :--------------------------- |
| `Predicate`      | T        | boolean  | 断言                         |
| `Consumer`       | T        | /        | 消费一个数据                 |
| `Function`       | T        | R        | 输入T输出R的函数             |
| `Supplier`       | /        | T        | 提供一个数据                 |
| `UnaryOperator`  | T        | T        | 一元函数（输入输出类型相同） |
| `BiFunction`     | (T,U)    | R        | 两个输入的函数               |
| `BinaryOperator` | (T,T)    | T        | 二元函数（输入输出类型相同） |

### 3.1、Consumer 消费接口

根据其中抽象方法的参数列表和返回值类型知道，我们可以在方法中对传入的参数进行消费。

![image-20211028145622163](/assets/image-20211028145622163-16354041894551.-SywCmqI.png)

### 3.2、Function 计算转换接口

根据其中抽象方法的参数列表和返回值类型知道，我们可以在方法中对传入的参数计算或转换，把结果返回

![image-20211028145707862](/assets/image-20211028145707862-16354042291112.C0EwnEcv.png)

### 3.3、Predicate 判断接口

根据其中抽象方法的参数列表和返回值类型知道，我们可以在方法中对传入的参数条件判断，返回判断结果

![image-20211028145818743](/assets/image-20211028145818743-16354043004393.DIkNSBHm.png)

### 3.4、Supplier 生产型接口

根据其中抽象方法的参数列表和返回值类型知道，我们可以在方法中创建对象，把创建好的对象返回

![image-20211028145843368](/assets/image-20211028145843368-16354043246954.Ddwj7Y9a.png)

### 3.5、使用

```java
public static void main(String[] args) {
    // 断言
    Predicate<Integer> predicate = i -> i > 0;
    System.out.println(predicate.test(-1));

    // 消费
    Consumer<String> consumer = s -> System.out.println(s);
    consumer.accept("张三");

    // 输入T输出R
    Function<Integer, String> function = i -> i >= 0 ? "正数" : "负数";
    function.apply(7);

    // 提供一个数据
    Supplier<String> stringSupplier = () -> "测试";
    System.out.println(stringSupplier.get());

    // 一元函数
    UnaryOperator<String> unaryOperator = s -> "处理后" + s;
    System.out.println(unaryOperator.apply("Test"));

    // 两个输入的函数
    BiFunction<String,Boolean,Integer> biFunction = (s, b) -> {
        // 如果b为true，返回0；如果s为test返回1，否则返回-1
        if (b){
            return 0;
        }
        if ("test".equals(s)){
            return 1;
        }
        return -1;
    };
    System.out.println(biFunction.apply("test", false));

    // 二元函数
    BinaryOperator<String> binaryOperator = (s, s2) -> s+s2;
    System.out.println(binaryOperator.apply("雷军", "666"));
}
```

结果

```sh
false
张三
测试
处理后Test
1
雷军666
```

## 四、常用的默认方法

* and

  我们在使用Predicate接口时候可能需要进行判断条件的拼接。而and方法相当于是使用&&来拼接两个判断条件

  例如：

  打印作家中年龄大于17并且姓名的长度大于1的作家。

  ```java
  List<Author> authors = getAuthors();
  Stream<Author> authorStream = authors.stream();
  authorStream.filter(new Predicate<Author>() {
      @Override
      public boolean test(Author author) {
          return author.getAge()>17;
      }
  }.and(new Predicate<Author>() {
      @Override
      public boolean test(Author author) {
          return author.getName().length()>1;
      }
  })).forEach(author -> System.out.println(author));
  ```

* or

  我们在使用Predicate接口时候可能需要进行判断条件的拼接。而or方法相当于是使用||来拼接两个判断条件。

  例如：

  打印作家中年龄大于17或者姓名的长度小于2的作家。

  ```java
  //        打印作家中年龄大于17或者姓名的长度小于2的作家。
  List<Author> authors = getAuthors();
  authors.stream()
      .filter(new Predicate<Author>() {
          @Override
          public boolean test(Author author) {
              return author.getAge()>17;
          }
      }.or(new Predicate<Author>() {
          @Override
          public boolean test(Author author) {
              return author.getName().length()<2;
          }
      })).forEach(author -> System.out.println(author.getName()));
  ```

* negate

  Predicate接口中的方法。negate方法相当于是在判断添加前面加了个! 表示取反

  例如：

  打印作家中年龄不大于17的作家。

  ```java
  //        打印作家中年龄不大于17的作家。
  List<Author> authors = getAuthors();
  authors.stream()
      .filter(new Predicate<Author>() {
          @Override
          public boolean test(Author author) {
              return author.getAge()>17;
          }
      }.negate()).forEach(author -> System.out.println(author.getAge()));
  ```

---

---
url: /daily/开发文档/基于Taro开发小程序.md
---

# 基于Taro开发小程序

> 微信官方文档：https://developers.weixin.qq.com/miniprogram/dev/framework/
>
> 开发工具：https://developers.weixin.qq.com/miniprogram/dev/devtools/download.html

## 项目搭建

> Taro官网：https://taro.zone/
>
> Taro官方文档：https://taro-docs.jd.com/docs/

建议使用 3.X 版本，node.js >= 16，npm>=18

### 安装Taro

![image-20240403201214902](/assets/image-20240403201214902.BAoN7-iO.png)

### 项目初始化

```
taro init 项目名称
```

![image-20240403201238331](/assets/image-20240403201238331.CWWEDczx.png)

模板选择

* 没什么需求：就用默认。

* 如果要用官方推荐的组件库 taro-ui：就用taro-ui模板（推荐）。

* 不想自己搭后端，云开发：就用wxcloud 云开发模板。

最后提示安装依赖失败

使用开发工具打开项目，`npm install`自己手动安装。

```json
// package.json
  "scripts": {
    "build:weapp": "taro build --type weapp",
    "build:swan": "taro build --type swan",
    "build:alipay": "taro build --type alipay",
    "build:tt": "taro build --type tt",
    "build:h5": "taro build --type h5",
    "build:rn": "taro build --type rn",
    "build:qq": "taro build --type qq",
    "build:jd": "taro build --type jd",
    "build:quickapp": "taro build --type quickapp",
    "dev:weapp": "npm run build:weapp -- --watch",
    "dev:swan": "npm run build:swan -- --watch",
    "dev:alipay": "npm run build:alipay -- --watch",
    "dev:tt": "npm run build:tt -- --watch",
    "dev:h5": "npm run build:h5 -- --watch",
    "dev:rn": "npm run build:rn -- --watch",
    "dev:qq": "npm run build:qq -- --watch",
    "dev:jd": "npm run build:jd -- --watch",
    "dev:quickapp": "npm run build:quickapp -- --watch"
  }
```

项目初始化完成后，先不要急着改代码，而是要先快速安装完依赖、运行、验证能否正常运行

build:weapp：打包上线前再使用，体积通常更小

dev:weapp：开发测试时使用，可以即时编译，自动更新小程序的效果

\--watch：监听参数

### UI组件库

推荐和 Taro 官方框架兼容的组件库，否则会出现跨端后样式丢失的问题:

* taro-ui：https://taro-ui.taro.zone/#/docs/introduction（最淮荐，兼容性最好）

* nut-ui

### 怎么向后端发送请求

axios?

适配器库：https://github.com/bigmeow/taro-platform/tree/master/packages/axios-taro-adapter

传统开发方式：每一个请求写一个发送请求的代码

更高效的开发方式: 根据后端接口文档自动生成请求：https://www.npmjs.com/package/@umijs/openapi

### 开启多端同步调试

/config/index.js

```js
outputRoot: `dist/${process.env.TARO_ENV}`
```

### 部署

1. 需要正式的appld (到微信小程序官方申请

2. 执行build 命令打包代码

3. 在微信小程序开发工具上传代码

   如果有报错，可以多去微信开发者社区交流

### 小程序中接入广告

https://developers.weixin.qq.com/community/develop/doc/000aecc6584fd81887969ded456c04

### 接入美团联盟

> 美团联盟官网：https://union.meituan.com/

2022年8月16日不支持个人推广

美团联盟专门针对个人开发了一个公众号叫“外卖美天赚”

https://www.bilibili.com/read/cv21971143/

https://jiubaoyou.cn/mulu/52590.html

https://www.bilibili.com/video/BV1RU4y1G75Y

https://dtmbw.com/

### 文件存储

又拍云：https://console.upyun.com/

参考资料

https://www.cnblogs.com/redFeather/p/13517999.html

https://zhuanlan.zhihu.com/p/360563720

https://tool.oschina.net/commons?type=3

https://www.bilibili.com/video/BV1Up4y1S7qq

https://www.bilibili.com/video/BV1Mu411p7cV

---

---
url: /常用框架/Thymeleaf/2_实战.md
---

# 基于Thymeleaf模板生成HTML并转成PDF

## 一、步骤

1. 准备HTML模板
2. 基于模板生成HTML
3. HTML转PDF

## 二、引入 Thymeleaf 相关依赖

```xml
<dependency>
   <groupId>org.springframework.boot</groupId>
   <artifactId>spring-boot-starter-thymeleaf</artifactId>
</dependency>

<dependency>
   <groupId>ognl</groupId>
   <artifactId>ognl</artifactId>
   <version>3.2</version>
</dependency>
```

## 三、准备 HTML 模板

`resources/templates`下新建页面**EvaluateTwoLevel.html**

```html
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
        "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!--suppress HtmlUnknownTag -->
<html xmlns:th="http://www.thymeleaf.org" xmlns="http://www.w3.org/1999/html" lang="zh">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
    <title>二级评分表</title>
    <style type="text/css">
        .tabtop13 {
            margin-top: 13px;
        }

        .tabtop13 td {
            background-color: #ffffff;
            height: 25px;
            line-height: 150%;
        }

        .font-center {
            text-align: center
        }

        .btbg {
            background: #e9faff !important;
        }

        .btbg1 {
            text-align: center;
            background: #f2fbfe !important;
        }

        .btbg2 {
            background: #f3007f !important;
        }

        .btbg3 {
            background: #d0f3eb !important;
            text-align: center
        }

        .btbg4 {
            background: #f3e7f0 !important;
            text-align: center
        }

        .tenant {
            font-family: 微软雅黑, serif;
            font-size: 20px;
            border-bottom: 1px dashed #CCCCCC;
            color: #255e95;
        }

        .biaoti {
            font-family: 微软雅黑, serif;
            font-size: 26px;
            font-weight: bold;
            border-bottom: 1px dashed #CCCCCC;
            color: #255e95;
        }

        .titfont {
            font-family: 微软雅黑, serif;
            font-size: 16px;
            font-weight: bold;
            color: #255e95;
            background-color: #e9faff;
        }
    </style>
</head>
<body>

<!-- 表头信息 colspan="3" 横向合并  rowspan="2" 纵向合并-->
<!-- 边框线 border="0" 虚线 border="none" 不显示 -->
<table width="100%" border="none" cellspacing="0" cellpadding="0" align="center">
    <tr border="none">
        <td align="center" class="tenant" width="20%" height="60" rowspan="2"><img width="100%" height="60" th:src="${evaluateTwoItem.logo}"></td>
        <td align="center" class="tenant" width="40%" height="60" th:text="${evaluateTwoItem.tenantName}"></td>
        <td align="center" class="tenant" width="20%" height="60" rowspan="2"><img th:src="${evaluateTwoItem.qrcode}"></td>
    </tr>
    <tr border="none">
        <td align="center" class="biaoti" height="60" th:text="${evaluateTwoItem.title}"></td>
    </tr>
    <tr border="none">
        <td align="center" width="33%" height="25" th:text="${evaluateTwoItem.evaluateUserName}"></td>
        <td align="center" width="33%" height="25" th:text="${evaluateTwoItem.jobTitleName}"></td>
        <td align="center" width="33%" height="25" th:text="${evaluateTwoItem.deptName}"></td>
    </tr>
</table>

<!-- 评分内容信息 -->
<table width="100%" border="0" cellspacing="1" cellpadding="4" bgcolor="#cccccc" class="tabtop13" align="center">
    <tr>
        <td width="10%" class="btbg font-center titfont">项目</td>
        <td width="10%" class="btbg font-center titfont">分值</td>
        <td width="60%" class="btbg font-center titfont">评分标准</td>
        <td width="10%" class="btbg font-center titfont">分值</td>
        <td width="10%" class="btbg font-center titfont">得分</td>
    </tr>

    <div>
        <th:block th:each="evaluateContent, contentStats : ${evaluateTwoItem.evaluateContentList}">
            <tr>
                <td th:text="${evaluateContent.twoItem}" th:rowspan="${evaluateContent.children.size()}"
                    th:class="${contentStats.index % 2 == 0} ? 'btbg4':'btbg3'"></td>
                <td th:text="${evaluateContent.twoScore}" th:rowspan="${evaluateContent.children.size()}"
                    th:class="${contentStats.index % 2 == 0} ? 'btbg4':'btbg3'"></td>
                <td th:class="${contentStats.index % 2 == 0} ? 'btbg4':'btbg3'"
                    th:text="${evaluateContent.children.get(0).content}"></td>
                <td th:class="${contentStats.index % 2 == 0} ? 'btbg4':'btbg3'"
                    th:text="${evaluateContent.children.get(0).scorePoints}"></td>
                <td th:class="${contentStats.index % 2 == 0} ? 'btbg4':'btbg3'"
                    th:text="${evaluateContent.children.get(0).score}"></td>
            </tr>
            <tr th:each="child, childStats : ${evaluateContent.children}" th:if="${!childStats.first}">
                <td th:class="${(contentStats.index + childStats.index) % 2 == 0} ? 'btbg4':'btbg3'"
                    th:text="${child.content}"></td>
                <td th:class="${(contentStats.index + childStats.index) % 2 == 0} ? 'btbg4':'btbg3'"
                    th:text="${child.scorePoints}"></td>
                <td th:class="${(contentStats.index + childStats.index) % 2 == 0} ? 'btbg4':'btbg3'"
                    th:text="${child.score}"></td>
            </tr>
        </th:block>
    </div>
</table>
<!-- 底部信息 -->
<table width="100%" border="0" cellspacing="0" cellpadding="0" align="center">
    <tr>
        <td align="right" height="25" th:text="${evaluateTwoItem.time}"></td>
    </tr>
</table>
</body>
</html>

```

## 四、基于模板生成HTML

**EvaluateTwoController.java**

```java
/**
 * @Author: xxl
 * @Date: 2024/11/27 18:14
 */
@RunWith(SpringRunner.class)
@SpringBootTest
public class EvaluateTwoController {

    @Resource
    private TemplateEngine templateEngine;

    @Test
    public void test() throws Exception {
        // 准备数据
        List<EvaluateTwoContent> evaluateTwoContents = initData();
        // 转换成html
        EvaluateTwoItem build = EvaluateTwoItem.builder()
                .tenantName("哈 佛 大 学 附 属 医 院")
                .logo("https://ek02.oss-cn-qingdao.aliyuncs.com/微信图片_20240125105054.png")
                .qrcode("https://oss.lcsxs.com/shandong/jinan/ykyxfstasrmyy/icon1732086712610.png")
                .title("教师教学质量评价表")
                .time("打印时间：2024年11月28日 10:00")
                .evaluateUserName("被评价人：张可欣")
                .jobTitleName("职称：主任医师")
                .deptName("科室：神经内科")
                .evaluateContentList(evaluateTwoContents)
                .build();
        String html = parseHtml(build);

        System.out.println(html);
        // HTML 转换成 PDF
        PdfUtils.convertToPdf(html);
    }

    /**
     * 将 EvaluateTwoItem 中的数据渲染到 EvaluateTwoLevel 这个模板上
     *
     * @param EvaluateTwoItem 包含信息的 EvaluateTwoItem
     * @return 数据渲染成功后的 HTML 字符串
     */
    public String parseHtml(EvaluateTwoItem EvaluateTwoItem) {
        Context context = new ContextBuilder().set("evaluateTwoItem", EvaluateTwoItem).build();
        return templateEngine.process("EvaluateTwoLevel.html", context);
    }

    /**
     * 准备数据
     *
     * @return
     */
    public List<EvaluateTwoContent> initData() {
        List<EvaluateTwoContent> appInfoList1 = new ArrayList<>();
        EvaluateTwoContent build1 = EvaluateTwoContent.builder().content("遵守劳动纪律").scorePoints(4.0).score(4.0).build();
        EvaluateTwoContent build2 = EvaluateTwoContent.builder().content("为人师表，以身作则").scorePoints(4.0).score(4.0).build();
        EvaluateTwoContent build3 = EvaluateTwoContent.builder().content("言传身教，对学生带教认真").scorePoints(4.0).score(4.0).build();
        EvaluateTwoContent build4 = EvaluateTwoContent.builder().content("吃苦耐劳、有奉献精神").scorePoints(4.0).score(4.0).build();
        EvaluateTwoContent build5 = EvaluateTwoContent.builder().content("仪表端庄、礼仪规范").scorePoints(4.0).score(4.0).build();
        appInfoList1.add(build1);
        appInfoList1.add(build2);
        appInfoList1.add(build3);
        appInfoList1.add(build4);
        appInfoList1.add(build5);
        List<EvaluateTwoContent> appInfoList2 = new ArrayList<>();
        EvaluateTwoContent build6 = EvaluateTwoContent.builder().content("专业知识扎实").scorePoints(5.0).score(5.0).build();
        EvaluateTwoContent build7 = EvaluateTwoContent.builder().content("理论与实践相结合").scorePoints(5.0).score(5.0).build();
        EvaluateTwoContent build8 = EvaluateTwoContent.builder().content("积极指导学生，能清楚而正确的回答学生提问").scorePoints(5.0).score(5.0).build();
        EvaluateTwoContent build9 = EvaluateTwoContent.builder().content("教学时语言表达准确易懂，能容讲解详细全面，能明确指出临床问题及解决问题").scorePoints(5.0).score(5.0).build();
        EvaluateTwoContent build10 = EvaluateTwoContent.builder().content("培养学生看、听、说、写、做综合能力，指导实习生书写各类病历文书").scorePoints(5.0).score(5.0).build();
        EvaluateTwoContent build11 = EvaluateTwoContent.builder().content("规范指导实习生按照临床程序进行诊疗").scorePoints(5.0).score(5.0).build();
        appInfoList2.add(build6);
        appInfoList2.add(build7);
        appInfoList2.add(build8);
        appInfoList2.add(build9);
        appInfoList2.add(build10);
        appInfoList2.add(build11);
        List<EvaluateTwoContent> appInfoList3 = new ArrayList<>();
        EvaluateTwoContent build12 = EvaluateTwoContent.builder().content("操作前讲解注意事项").scorePoints(6.0).score(6.0).build();
        EvaluateTwoContent build13 = EvaluateTwoContent.builder().content("操作正确、熟练、规范、标准").scorePoints(6.0).score(6.0).build();
        EvaluateTwoContent build14 = EvaluateTwoContent.builder().content("具有良好的临床应急能力，临床基本技能操作能力强").scorePoints(6.0).score(6.0).build();
        EvaluateTwoContent build15 = EvaluateTwoContent.builder().content("运用教学技能促进实习生学习，注重实习临床评判性思维能力培养，提供技能操作机会，放手不放眼").scorePoints(6.0).score(6.0).build();
        EvaluateTwoContent build16 = EvaluateTwoContent.builder().content("学生操作时能做到及时指导、帮助").scorePoints(6.0).score(6.0).build();
        appInfoList3.add(build12);
        appInfoList3.add(build13);
        appInfoList3.add(build14);
        appInfoList3.add(build15);
        appInfoList3.add(build16);
        List<EvaluateTwoContent> appInfoList4 = new ArrayList<>();
        EvaluateTwoContent build17 = EvaluateTwoContent.builder().content("对实习生要求严格，有带教计划，能按照本科室“实习生带教路径”进行教学").scorePoints(5.0).score(5.0).build();
        EvaluateTwoContent build18 = EvaluateTwoContent.builder().content("带教方法灵活，因人施教").scorePoints(5.0).score(5.0).build();
        EvaluateTwoContent build19 = EvaluateTwoContent.builder().content("与实习生沟通，给学生有效的反馈").scorePoints(5.0).score(5.0).build();
        EvaluateTwoContent build20 = EvaluateTwoContent.builder().content("认真落实实习生的各项考核工作，公平公正的对待每位实习生，并及时反馈考核信息").scorePoints(5.0).score(5.0).build();
        appInfoList4.add(build17);
        appInfoList4.add(build18);
        appInfoList4.add(build19);
        appInfoList4.add(build20);

        List<EvaluateTwoContent> appInfoList5 = new ArrayList<>();
        EvaluateTwoContent build25 = EvaluateTwoContent.builder().content("总分").scorePoints(100.0).score(100.0).children(appInfoList2).build();
        appInfoList5.add(build25);

        // 一级
        List<EvaluateTwoContent> evaluateContentList = new ArrayList<>();
        EvaluateTwoContent build21 = EvaluateTwoContent.builder().twoItem("态度").twoScore(getTotalScore(appInfoList1)).children(appInfoList1).build();
        EvaluateTwoContent build22 = EvaluateTwoContent.builder().twoItem("知识").twoScore(getTotalScore(appInfoList2)).children(appInfoList2).build();
        EvaluateTwoContent build23 = EvaluateTwoContent.builder().twoItem("技能").twoScore(getTotalScore(appInfoList3)).children(appInfoList3).build();
        EvaluateTwoContent build24 = EvaluateTwoContent.builder().twoItem("管理").twoScore(getTotalScore(appInfoList4)).children(appInfoList4).build();
        EvaluateTwoContent build26 = EvaluateTwoContent.builder().twoItem("总分").twoScore(100.0).children(appInfoList5).build();
        evaluateContentList.add(build21);
        evaluateContentList.add(build22);
        evaluateContentList.add(build23);
        evaluateContentList.add(build24);
        evaluateContentList.add(build26);

        return evaluateContentList;
    }

    /**
     * 标准得分相加总分
     *
     * @param appInfoList
     * @return
     */
    private Double getTotalScore(List<EvaluateTwoContent> appInfoList) {
        return appInfoList.stream().mapToDouble(EvaluateTwoContent::getScore).sum();
    }
}
```

**EvaluateTwoItem.java**

```java
@Data
@Builder
public class EvaluateTwoItem {

    /**
     * 标题
     */
    private String title;

    /**
     * 时间
     */
    private String time;

    /**
     * 租户
     */
    private String tenantName;

    /**
     * logo
     */
    private String logo;

    /**
     * 二维码
     */
    private String qrcode;

    /**
     * 被评价人
     */
    private String evaluateUserName;

    /**
     * 职称
     */
    private String jobTitleName;

    /**
     * 科室
     */
    private String deptName;

    /**
     * 内容
     */
    private List<EvaluateTwoContent> evaluateContentList;
}
```

**EvaluateTwoContent.java**

```java
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class EvaluateTwoContent {

    /**
     * 一级项目
     */
    private String oneItem;

    /**
     * 二级项目
     */
    private String twoItem;

    /**
     * 二级分值
     */
    private Double twoScore;

    /**
     * 标准内容
     */
    private String content;

    /**
     * 占分
     */
    private Double scorePoints;

    /**
     * 得分
     */
    private Double score;

    /**
     * 子集
     */
    private List<EvaluateTwoContent> children;
}
```

## 五、使用itextpdf将HTML转换成PDF

引入 itextpdf 相关依赖

```xml
<dependency>
    <groupId>com.itextpdf</groupId>
    <artifactId>font-asian</artifactId>
    <version>7.1.13</version>
</dependency>
<dependency>
    <groupId>com.itextpdf</groupId>
    <artifactId>html2pdf</artifactId>
    <version>4.0.5</version>
</dependency>
```

Html 转 Pdf 工具类

```java
public class PdfUtils {

    /**
     * HTML 转换成 PDF
     *
     * @param html
     * @throws Exception
     */
    public static void convertToPdf(String html) throws Exception {
        // 将 HTML 内容字符串转换为 PDF
        String outPath = "D:\\activity.pdf";
        OutputStream outputStream = new FileOutputStream(outPath);
        PdfWriter pdfWriter = new PdfWriter(outputStream);
        PdfDocument pdfDocument = new PdfDocument(pdfWriter);
        pdfDocument.setDefaultPageSize(PageSize.A4);
        HtmlConverter.convertToPdf(html, pdfDocument, initProperties());
        // 关闭资源
        pdfDocument.close();
        outputStream.close();
    }

    /**
     * pdf 特殊处理
     *
     * @return
     * @throws Exception
     */
    private static ConverterProperties initProperties() throws Exception {
        // 添加中文字体支持
        ConverterProperties properties = new ConverterProperties();
        FontProvider fontProvider = new FontProvider();
        String fontPath = "C:/WINDOWS/Fonts/simsun.ttc,0";
        PdfFont microsoft = PdfFontFactory.createFont(fontPath, PdfEncodings.IDENTITY_H);
        fontProvider.addFont(microsoft.getFontProgram(), PdfEncodings.IDENTITY_H);
        properties.setFontProvider(fontProvider);
        return properties;
    }
}

```

## 六、导出效果

![image-20241129001642581](/assets/image-20241129001642581.Wgw6ugEy.png)

---

---
url: /daily/软件设计师/01_计算机组成与体系结构.md
---

# 计算机组成与体系结构

## 计算机组成与体系结构

大纲

![image-20240401000844314](/assets/image-20240401000844314.CjijLmHd.png)

考情分析

![image-20240401000917877](/assets/image-20240401000917877.Bn6uuwbV.png)

## 一、数据的表示

考点1：进制转换

考点2：码制（原码/反码/补码/移码）

考点3：浮点数的表示

考点4：逻辑运算

### 数据的表示考点1-进制转换

![image-20231206000745395](/assets/image-20231206000745395.DnFqCGg-.png)

#### :cat:按权展开法

**R进制转十进制**使用按权展开法，其具体操作方式为: 将R进制数的每一位数值用R^k形式表示，即幂的底数是R，指数为k，k与该位和小数点之间的距离有关。当该位位于小数点左边，k值是该位和小数点之间数码的个数，而当该位位于小数点右边，k值是负值，其绝对值是该位和小数点之间数码的个数加1。

数码\*位权

示例一：二进制转十进制
$$
10100.01
\\=10000+0+100+0+0+0.01
\\=1*10^4+0+1*10^2+0+0+1*10^-2
\\=1*2^4+1*2^2+1*2^{-2}
$$

示例二：七进制转十进制
$$
604.01
\=6*7^2+4*7^0+1\*7^{-2}
$$

#### :cat:短除法

**十进制转R进制**使用短除法（除基取余法）。

商为0截止

余数从下往上记录

![image-20231207000930254](/assets/image-20231207000930254.C1rooJiX.png)

![image-20231207001212409](/assets/image-20231207001212409.xRcN1G8S.png)

#### :cat:减法

**十进制转二进制**使用减法。

例如将94转换为二进制数。
$$
2^{0}=1,
\2^{1}=2,
\2^{2}=4,
\2^{3}=8,
\2^{4}=16,
\2^{5}=32,
\2^{6}=64,
\2^{7}=128,
\2^{8}=256,
\2^{9}=512,
\2^{10}=1024
$$

$$
小于且离94最近的乘幂为64：94-64=30 (2^6=64)\\
小于且离30最近的乘幂为16：30-16=14 (2^4=16)\\
小于且离14最近的乘幂为8：14-8=6 (2^3=8)\\
小于且离6最近的乘幂为4：6-4=2 (2^2=4)\\
小手且离2最近的乘幂为2：2-2=0 (2^1=2)\\
结束
$$

| 位号 | 6    | 5    | 4    | 3    | 2    | 1    | 0    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| 取值 | 1    | 0    | 1    | 1    | 1    | 1    | 0    |

#### :cat:二进制转八进制与十六进制

二进制转八进制

```
010 001 110
2  1   6
```

二进制转十六进制

```
1000 1110
8    E
```

### 数据的表示考点2-码制（原码/反码/补码/移码）

原码：最高位是符号位，其余低位表示数值的绝对值。

反码：正数的反码与原码相同，负数的反码是其绝对值按位取反（符号位不变）。

补码：正数的补码与原码相同，负数的补码是其反码末位加1（符号位不变）。

移码：补码的符号位按位取反。

|      | 数值1     | 数值-1    | 1-1       |                            |
| ---- | --------- | --------- | --------- | -------------------------- |
| 原码 | 0000 0001 | 1000 0001 | 1000 0010 | 后面变成10，即-2，偏差较大 |
| 反码 | 0000 0001 | 1111 1110 | 1111 1111 | -0，实际中没有负数，也不对 |
| 补码 | 0000 0001 | 1111 1111 | 0000 0000 | +0，结果正确               |
| 移码 | 1000 0001 | 0111 1111 | 1000 0000 |                            |

定点整数、定点小数、数码个数

什么是定点？小数点在数的前面还是后面，小数点在数的后面是纯整数，小数点在数的前面是纯小数。

为什么需要定点？计算机不认识小数，只有纯整数、纯小数。

| 码制 | 定点整数                        | 定点小数                        | 数码个数  |
| ---- | ------------------------------- | ------------------------------- | --------- |
| 原码 | $-（2^{n-1}-1）~+（2^{n-1}-1）$ | $-（2^{n-1}-1）~+（2^{n-1}-1）$ | $2^{n-1}$ |
| 反码 | $-（2^{n-1}-1）~+（2^{n-1}-1）$ | $-（2^{n-1}-1）~+（2^{n-1}-1）$ | $2^{n-1}$ |
| 补码 | $-2^{n-1}~+（2^{n-1}-1）$       | $-1~+（1-2^{-(n-1)}）$          | $2^{n}$   |
| 移码 | $-2^{n-1}~+（2^{n-1}-1）$       | $-1~+（1-2^{-(n-1)}）$          | $2^{n}$   |

定点整数示例

n=3时，位数是3位，每位由0/1组成：0/1、0/1、0/1

编码形式，第一位是符号位：000、001、010、011、100、101、110、111

最高位是符号位，4位正数、4位负数，

范围为+0—+3，-0—-3

11补一位变成100，2^2-1，即公式中的
$$
2^{n-1}-1
$$
定点小数示例

第一位依然是符号位，最小0.00，最大0.11，-0.00，-0.11

0.11补整变成2的k次方形式：0.11+0.01=1.00，即为公式中的
$$
1-2^{-(n-1)}
$$

### 数据的表示考点3：浮点数的表示

#### :rabbit:浮点的运算

:sunny:浮点数表示
$$
N=尾数\*基数^{指数}
$$

解释：

* 浮点：指的是小数点不固定
* 尾数：定点小数
* 基数：可以叫阶码，一般是定点整数
* 以科学计数法为例，1.25\*10^6

:sunny:运算过程

对阶>尾数计算>结果格式

:sunny:特点

1、一般尾数用**补码**，阶码用移码

2、**阶码的位数**决定数的**表示范围**，位数越多范围越大

3、**尾数的位数**决定数的**有效精度**，位数越多精度越高

4、对阶时，**小数向大数看齐**

5、对阶是通过**较小数的尾数右移**实现的

#### :rabbit:例题

![image-20240324204145333](/assets/image-20240324204145333.C9oW-9UC.png)

![image-20240324204310975](/assets/image-20240324204310975.C7WQ0Vuq.png)

![image-20240324204725322](/assets/image-20240324204725322.BtafxHKE.png)

### 数据的表示考点4：逻辑运算

#### :wolf:关系运算符

:nail\_care:关系运算符及其优先次序

优先级相同（高）

* <（小于）
* <=（小于等于）
* \>（大于）
* \>=（大于等于）

优先级相同（低）

* \==（等于）
* !=（不等于）

说明：

:nail\_care:关系运算符的优先级低于算术运算符

:nail\_care:关系运算符的优先级高于赋值运算符

#### :wolf:逻辑运算

逻辑变量之间的运算称为逻辑运算。二进制数1和0在逻辑上可以代表“真”与“假”。

逻辑运算

* 逻辑或（||、+、U、V、OR）：连接的两个逻辑值全0时才取0
* 逻辑与（&&、\*、·、∩、∧、AND）：连接的两个逻辑值全1时才取1
* 逻辑异或（⊕、XOR）：连接的两个逻辑值不相同时才取1，相同则取0
* 逻辑非（！、¬、~、NOT、-）：将原逻辑值取反即可

真值表：描述一个逻辑表达式与其变量之间的关系

| A    | B    | ！A  | A+B（A||B） | A\*B（A&\&B） | A⊕B  |
| ---- | ---- | ---- | ------------- | ----------- | ---- |
| 0    | 0    | 1    | 0             | 0           | 0    |
| 0    | 1    | 1    | 1             | 0           | 1    |
| 1    | 0    | 0    | 1             | 0           | 1    |
| 1    | 1    | 0    | 1             | 1           | 0    |

#### :wolf:逻辑运算符

**逻辑运算符**

* &&（逻辑与）相当于其他语言中的AND
* ||（逻辑或）相当于其他语言中的OR
* ！（逻辑非）相当于其他语言中的NOT

例：a&\&b 若a，b为真，则a&\&b为真。

a||b 若a，b之一为真，则a||b为真。

!a 若a为真，则!a为假。

**优先次序：**

！（非）->&&（与）->||（或）

逻辑运算符中的"&&"和"||"低于关系运算符，“！”高于算术运算符

因此运算符的优先顺序为: ！>算术运算符>关系运算符>&8>||>赋值运算符

**短路原则**
在逻辑表达式的求解中，并不是所有的逻辑运算符都要被执行。

* （1）a&\&b&\&c 只有a为真时，才需要判断b的值，只有a和b都为真时，才需
  要判断c的值。
* （2）a||b||c 只要a为真，就不必判断b和c的值，只有a为假，才判断b。a和b
  都为假才判断c。

例：（m=a>b）&&（n=c>d）

当a=1,b=2,c=3,d=4,m和n的原值为1时，由于"a>b"的值为0，因此m=0，而
"n=c>d"不被执行，因此n的值不是0而仍保持原值1。

#### :wolf:例题

![image-20240324220821795](/assets/image-20240324220821795.C883Rvgg.png)

![image-20240324221022777](/assets/image-20240324221022777.JxwCkBrV.png)

## 二、校验码

考点1：奇偶校验码

考点2：CRC循环冗余校验码

考点3：海明校验码:star:

### 考点1：奇偶校验码

#### :dog:基础知识

码距：任何一种编码都由许多码字构成，任意两个码字之间最少变化的二进制位数就称为数据校验码的码距。

例如，用4位二进制表示16种状态，则有16个不同的码宇，此时码距为1。如0000与0001。

#### :dog:奇偶校验

奇偶校验码的编码方法是：由若干位有效信息（如一个字节），再加上一个二进制位(校验位)组成校验码。

奇校验：整个校验码 (有效信息位和校验位)中"1”的个数为奇数。（奇数校验码：在发送端增加一位校验位且根据数据中已有的1的个数改变校验位的状态使整段二进制码中1的个数为奇数。接收端根据对应规则判断二进制中数据是否为奇来确定是否出错）

偶校验：整个校验码 (有效信息位和校验位)中 “1”的个数为偶数。

**奇偶校验，可检查1位（奇数位）的错误，不可纠错。**

#### :dog:例题

```
以下关于采用一位奇校验方法的叙述中，正确的是 (C)。
A、若所有奇数位出错，则可以检测出该错误但无法纠正错误
B、若所有偶数位出错，则可以检测出该错误并加以纠正
C、若有奇数个数据位出错，则可以检测出该错误但无法纠正错误
D、若有偶数个数据位出错，则可以检测出该错误并加以纠正
```

### 考点2：CRC循环冗余校验码

#### :mouse:CRC循环冗余校验码

**CRC校验，可检错，不可纠错。**

* CRC的编码方法是：**在k位信息码之后拼接r位校验码**（信息位+校验位=生成多项式）。应用CRC码的关键是如何从k 位信息位简便地得到r位校验位（编码），以及如何从k+r位信息码判断是否出错。
* 把接收到的CRC码用约定的生成多项式G(X)去除（**模二除法**），如果正确，则余数为0；如果某一位出错，则余数不为0。不同的位数出错其余数不同，余数和出错位序号之间有惟一的对应关系。

#### :mouse:例题

```
在（D）校验方法中，采用模2运算来构造校验位。
A、水平奇偶
B、垂直奇偶
C、海明码
D、循环冗余
```

### 考点3：海明校验码

#### :hamster:海明校验码

**海明校验，可检错，也可纠错。**

海明校验码的原理是：在有效信息位中加入几个校验位形成海明码，使码距比较均匀地拉大，并把海明码的

每个二进制位分配到几个奇偶校验组中。当某一位出错后，就会引起有关的几个校验位的值发生变化，这不

但可以发现错误，还能指出错误的位置，为自动纠错提供了依据。
$$
2^r>=m+r+1（m是信息位）
\2^r-1>=m+r
$$

#### :hamster:三种校验码对比​

|                 | 校验码位数             | 校验码位置       | 检错         | 纠错     | 校验方式                                                     |
| --------------- | ---------------------- | ---------------- | ------------ | -------- | ------------------------------------------------------------ |
| 奇偶校验        | 1                      | 一般拼接在头部   | 可检奇数位错 | 不可检错 | 奇校验：最终1的个数是奇数个；偶校验：最终1的个数是偶数个； |
| CRC循环冗余校验 | 生成多项式最高次幂决定 | 拼接在信息位尾部 | 可检错       | 不可检错 | 模二除法求余数，拼接作为校验位                               |
| 海明校验        | 2^r>=m+r+1             | 插入在信息位中间 | 可检错       | 可检错   | 分组奇偶校验                                                 |

:hamster:例题

![image-20240326004629650](/assets/image-20240326004629650.DbxtMzOc.png)

![image-20240326005251764](/assets/image-20240326005251764.DESZAbQR.png)

## 三、CPU组成（运算器与控制器）

### :frog:指令的基本概念

一条指令就是机器语言的一个语句，它是一组有意义的二进制代码，指令的基本格式如下：

操作码字段+地址码字段

OP+A1+A2

### :frog:寻址方式

1. 立即寻址方式
   * 特点：**操作数**直接在指令中，度快，灵活性差。
   * ![在这里插入图片描述](/assets/202006031102462.B-0GIpgH.png)
2. 直接寻址方式
   * 特点：指令中存放的是操作数的**地址**。
   * ![img](/assets/20200603111516809.BhjCweFG.png)
3. 间接寻址方式
   * 特点：指令中存放了一个地址，这个地址对应的内容是操作数的地址。
   * ![img](/assets/20200603112234286.BSLEoyqV.png)
4. 寄存器寻址方式
   * 特点：寄存器存放操作数。
   * ![img](/assets/20200603111009473.MTU5uQ57.png)
5. 寄存器间接寻址方式
   * 特点：寄存器内存放的是操作数的地址。
   * ![img](/assets/20200603111348925.DrgB0ZnA.png)

### :frog:例题

```markdown
在机器指令的地址字段中，直接指出操作数本身的寻址方式称为 (C)。
A、隐含寻址
B、寄存器寻址
C、立即寻址
D、直接寻址
```

参考讲解：

[计算机组成原理——9种常用寻址方式\_隐含寻址-CSDN博客](https://blog.csdn.net/qq_44997784/article/details/106519374)

[关于寻址方式一篇就够了 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/370204019)

## 四、CISC与RISC

### :koala:CISC与RISC对比

| 指令系统类型 | 指令                                                         | 寻址方式   | 实现方式                                             | 其它                       |
| ------------ | ------------------------------------------------------------ | ---------- | ---------------------------------------------------- | -------------------------- |
| CISC（复杂） | 数量多，使用频率差别大，可变长格式                           | 支持多种   | 微程序控制技术（微码）                               | 研制周期长                 |
| RISC（精简） | 数量少，使用频率接近，定长格式，大部分为单周期指令，操作寄存器，只有Load/Store操作内存 | 支持方式少 | 增加了通用寄存器；硬布线逻辑控制为主；适合采用流水线 | 优化编译，有效支持高级语言 |

### :koala:CISC与RISC比较，分哪些维度？

指令数量、指令使用频率，寻址方式，寄存器，流水线支持，高级语言支持

* CISC：复杂，指令数量多，频率差别大，多寻址
* RISC：精筒，指令数量少，操作寄存器，单周期，少寻址，多配用奇存器，流水线

### :koala:例题

```
以下关于RISC(精简指令系统计算机)技术的叙述中，错误的是(B)
A、指令长度固定、指令种类尽量少
B、指令功能强大、寻址方式复杂多样
C、增加寄存器数目以减少访存次数
D、用硬布线电路实现指令解码，快速完成指令译码
```

## 五、流水线技术

### :tiger:概念​

* 相关参数计算：**流水线执行时间计算、流水线吞吐率**、流水线加速比、流水线效率
* 流水线是指在程序执行时**多条指令重叠进行操作**的一种准并行处理实现技术。各种部件同时处理是针对不同指令而言的，它们可同时为多条指令的不同部分进行工作，以提高各部件的利用率和指令的平均执行速度。

![image-20240326233027213](/assets/image-20240326233027213.ING5mq6a.png)

理论公式与实践公式结果会不一样，默认优先理论公式

![image-20240327001110579](/assets/image-20240327001110579.CcKKoQB-.png)

一条指令的执行过程可以分解为取指、分析和执行三步，在取指时间 t取值=3△t、分析时间 t分析=2△t、执行时间 t执行=4△t的情况下，若按串行方式执行，则10条指令全部执行完需要（90）△t；若按流水线的方式执行，流水线周期为（4）△t，则10条指令全部执行完需要（45）△t。

```
k=3，t=4
串行就是顺序执行，（3+2+4）*10=90
按流水线的方式，流水线周期就是执行时间最长的一段4
按流水线的方式，全部执行完 理论：9+（10-1）*4=45   实践：3*4+（10-1）*4=48
```

### :tiger:流水线计算

理论公式：
$$
（t\_1+t\_2+...+t\_k）+（n-1）\*t
$$

![image-20240327003133771](/assets/image-20240327003133771.a-Xh_0vF.png)

实践公式：
$$
k^t+（n-1）\*t
$$
![image-20240327003149870](/assets/image-20240327003149870.CppHVh0j.png)

### :tiger:流水线吞吐率计算

流水线的吞吐率（Though Put rate，TP）是指在**单位时间内流水线所完成的任务数量或输出的结果数量**。计算流水线吞吐率的最基本的公式如下（流水线执行时间使用理论公式）：
$$
TP=\frac{指令条数}{流水线执行时间}
$$
流水线最大吞吐率（流水线执行时间使用实践公式）：
$$
TP\_{max}=\displaystyle \lim\_{n \to \infty}{\frac{n}{(k+n-1)}}=\frac{1}{t}
$$
一条指令的执行过程可以分解为取指、分析和执行三步，取指时间 t取值=3△t、分析时间 t分析=2△t、执行时间 t执行=4△t。10条指令的吞吐率？最大吞吐率？

```
吞吐率=10/45△t
最大吞吐率=1/4△t
```

### :tiger:例题

```markdown
下列关于流水线方式执行指令的叙述中，不正确的是(A)。
A、流水线方式可提高单条指令的执行速度
B、流水线方式下可同时执行多条指令（不是并行，只是准并行）
C、流水线方式提高了各部件的利用率
D、流水线方式提高了系统的吞吐率

将一条指令的执行过程分解为取指、分析和执行三步，按照流水方式执行，若取指时间t取指=4△t、分析时间t分析=2△t、执行时间t执行=3△t，则执行完100条指令，需要的时间为 (D) △t
A、200
B、300
C、400
D、405

例题：若指令流水线一条指令分为取指、分析、执行三个阶段，并且这三个阶段的时间分别为取指1ns，分析2ns，执行1ns，则流水线的周期为多少？100条指令全部执行完毕需要执行的时间是多少？
分析：流水线的周期为花费时间最长的阶段所花费的时间，所以流水线的周期就是2ns。
    根据理论公式，T=（1+2+1）+（100-1）*2=4+99*2=202ns
    根据实践公式，T=（3+100-1）*2=204ns
在这里，需要注意的是，因为流水线的理论公式和实践公式的结果不一样，但是在考试过程中可能都会考到，所以，在应用时，先考虑理论公式，后考虑实践公式。
```

## 六、存储系统

考点1：层次化存储体系

考点2：Cache

考点3：主存编址计算

### 考点1：层次化存储体系

#### :bear:层次化存储结构

![img](/assets/20210526162358127.CgXhkIw-.png)

CPU：最快，但容量小，成本高

Cache：按内容存取

内存（主存）：分两类：随机存储器（RAM）和只读存储器（ROM）

外存（辅存）：硬盘、光盘、U盘等

三级存储体系：Cache——主存——辅存

**局部性原理**是层次化存储结构的支撑

时间局部性：刚被访问的内容，立即又被访问。（重复使用，循环体）

空间局部性：刚被访问的内容，临近的空间很快又被访问。（顺序结构）

[计算机组成原理与体系结构——层次化存储结构\_层次化片上存储架构-CSDN博客](https://blog.csdn.net/qq_36362721/article/details/117295930)

#### :bear:层次化存储结构-分类

1、存储器位置：内存&外存

2、存取方式

（1）按内容存取：相联存储器（如Cache）

（2）按地址存取：

* 随机存取存储器（如内存）
* 顺序存取存储器（如磁带）
* 直接存取存储器（如磁盘）

3、工作方式

（1）随机存取存储器RAM（如内存DRAM）（掉电丢失）

（2）只读存储器ROM（如BIOS）（掉电保留）

* DRAM：动态随机存取存储器
* SRAM：静态随机存取存储器
* Cache：高速缓存
* EEPROM：电可擦可编程只读存储器

#### :bear:例题

```
CPU访问存储器时，被访问数据一般聚集在一个较小的连续存储区域中。若一个存储单元已被访问，则其邻近的存储单元有可能还要被访问，该特性被称为 (C)
A、数据局部性
B、指令局部性
C、空间局部性
D、时间局部性

虚拟存储体系由 (A) 两级存储器构成。
A、主存-辅存
B、寄存器-Cache
C、寄存器-主存
D、Cache-主存

在微机系统中，BIOS(基本输入输出系统) 保存在 (A) 中。
A、主板上的ROM
B、CPU的寄存器
C、主板上的RAM
D、虚拟存储器
```

### 考点2：Cache

#### :pig:Cache-概念

1、在计算机的存储系统体系中，Cache是访问速度**最快**的层次（**若有寄存器，则寄存器最快**）。

2、使用Cache改善系统性能的依据是程序的局部性原理。

* **时间局部性**
* **空间局部性**

3、如果以h代表对Cache的访问命中率，t1表示Cache的周期时间，t2表示主存储器周期时间，以读操作为例，使用“Cache+主存储器”的系统的平均周期为t3，则：
$$
t\_3=h\*t+（1-h）\*t\_2
$$
其中， （1-h）又称为失效率（未命中率）

4、直接相联映像：**硬件电路**较简单，但**冲突率很高**。

5、全相联映像：电路难于设计和实现，只适用于小容量的cache，但**冲突率较低**。

* 组相联映像：直接相联与全相联的折中。
* **注：主存与Cache之间的地址映射的硬件直接完成。**

6、地址映像是将主存与Cache的存储空间划分为若干大小相同的页（或称为块）。

例如，某机的主存容量为1GB，划分为2048页，每页512KB；Cache容量为8MB，划分为16页，每页512KB。

#### :pig:Cache-直接相联映像

#### :pig:Cache-全相联映像

#### :pig:Cache-组相联映像

#### :pig:Cache-映像

中级考试只需要掌握特点就可以。

|              | 冲突率（高、中、低） | 电路复杂度（复杂、简单、折中） |
| ------------ | -------------------- | ------------------------------ |
| 直接相联映像 | 高                   | 简单                           |
| 全相联映像   | 低                   | 复杂                           |
| 组相联映像   | 中                   | 折中                           |

#### :pig:Cache-例题

```
以下关于Cache (高速缓冲存储器)的叙述中，不正确的是 (A)
A、Cache的设置扩大了主存的容量
B、Cache的内容是主存部分内容的拷贝
C、Cache的命中率并不随其容量增大线性地提高
D、Cache 位于主存与CPU 之间

在程序执行过程中，高速缓存(Cache) 与主存间的地址映射由 (D)
A、操作系统进行管理
B、存储管理软件进行管理
C、程序员自行安排
D、硬件自动完成

主存与Cache的地址映射方式中， (A) 方式可以实现主存任意一块装入Cache中任意位置，只有装满才需要替换。
A、全相联
B、直接映射
C、组相联
D、串并联
```

### 考点3：主存编址计算

#### :pig\_nose:主存-编址

#### :pig\_nose:主存-编址与计算

字节、字、字长等多个概念

```markdown
字节：
	1个字节等于8位，即1Byte=8bit
字：
	在计算机中，一串数码作为一个整体来处理或运算的，称为一个字。字的位数称为字长；字通常分若干个字节。
理解：若计算机字长64位，则一次可以处理的字位64/8=8B，进而计算按字寻址的范围。
地址线：存储单元的个数=存储容量=2 地 址 线 的 条 数 2^{地址线的条数}2 
地址线的条数，一个存储单元占一个字节(1B,也就是8位)。字节用来计量存储容量。一个cpu的N根地址总线，则可以说这个CPU的地址总线宽度为N。这样cpu最多可以寻址2^N个内存单元
字长：
	表示机器CPU的处理能力，即CPU在单位时间内能处理的最大二进制数的位数称为字长，即寄存器一次能处理的位数。
解释：若字长位32位，则1字(word) = 4字节(Byte)=32比特(bit)，表示存储器一次能存取4个存储单元，指令的长度位4个存储单元。

按字节寻址：
	一组地址线的每个不同状态对应一个字节的地址，存储空间的最小编址单位是字节。
	例如，对24位地址线的主存而言（也就是有24根地址线），按字节寻址，每根线有两个状态，那么24根地址线组成的地址信号就有2^24个不同的状态，每个状态对应一个字节的地址空间的话，那么24根地址线的可寻址空间为2^24B，即16MB。

按字寻址：
	一组地址线的每个不同状态对应一个字的地址，存储空间的最小编址单位是字。
	一个字由若干个字节构成，所以计算机在寻址过程中会区分字里面的字节，即会给字里面的字节编址，这样就会占用部分地址线，例如有24根地址线，机器字长为16位，若按字寻址的话，16位=2个字节，需要占用一根地址线用来字内寻址，这样就剩下23根地址线，故按字寻址范围是2^23W（W是字长的意思），也就是8MW。【真正用于按字寻址的地址线只有24-1=23根】
```

* 存储单元
  * **存储单元个数=最大地址-最小地址+1**
* 编址内容
  * 按字编址：存储体的存储单元是字存储单元，即最小寻址单位是一个字
  * 按字节编地：存储体的存储单元是字节存储单元，即最小寻址单位是一个字节。
* **总容量=存储单元个数\*编址内容**
* 根据存储器所要求的容量和选定的存储芯片的容量，就可以计算出所需芯片的总数，即：**总片数=总容量/每片的容量**

#### :pig\_nose:主存-例题

```
内存按字节编址，地址从A0000H到CFFFFH的内存，共有 (D) 字节，
若用存储容量为64K×8bit的存储器芯片构成该内存空间，至少需要 (B)片。
A、80KB  B、96KB  C、160KB  D、192KB
A、2   B、3   C、5   D、8

设机器字长为64位，存储器的容量为512MB,若按字编址，它可寻址的单位个数是（）。
```

## 七、输入输出技术

### :cow:数据传输控制方式

1. 程序控制（查询）方式：分为无条件传送和程序查询方式两种。
   * 方法简单，硬件开销小，但I/O能力不高，严重影响CPU的利用率。
2. 程序中断方式
   * 与程序控制方式相比，中断方式因为CPU无需等待而提高了传输请求的响应速度。（CPU与数据传输并行）
3. DMA方式（直接内存存取）
   * DMA方式是为了在主存与外设之间实现高速、批量数据交换而设置的。DMA方式比程序控制方式与中断方式都高效。
   * （DMAC向总线裁决逻辑提出总线请求；**CPU执行完当前总线周期即可释放总线控制权**。此时DMA响应，通过DMAC通知！I/O接口开始DMA传输。）
4. 通道方式
5. I/O处理机

1-5效率越来越高

中断处理过程：

1. CPU无需等待也不必查询I/O状态；
2. 当I/O系统准备好以后，发出**中断请求信号**通知CPU；
3. CPU接到中断请求后，保存正在执行程序的现场（**保存现场**），打断的程序当前位置即为**断点**；
4. （通过中**断向量表**）转入I/O中的服务程序的执行，完成I/O系统的数据交换；
5. 返回被打断的程序继续执行（恢复现场）。

### :cow:例题

```
计算机系统中常用的输入/输出控制方式有无条件传送、中断、程序查询和DMA方式等。当采用(D)方式时，不需要CPU执行程序指令来传送数据。
A.中断
B.程序查询
C.无条件传送
D.DMA

计算机运行过程中，遇到突发事件，要求CPU暂时停止正在运行的程序转去为突发事件服务，服务完毕，再自动返回原程序继续执行，这个过程称为 (B)，其处理过程中保存现场的目的是(C)。
B中断
A阻塞
C动态绑定
D静态绑定
A防止丢失数据
B防止对其他部件造成影响
C返回去继续执行原程序
D为中断处理程序提供数据

CPU是在 (D) 结束时响应DMA请求的。
A一条指令执行
B一段程序
C一个时钟周期
D一个总线周期
```

## 八、总线

### :boar:总线概念

一条总线同一时刻仅允许一个设备发送，但允许多个设备接收。（分时双工）

总线的分类

1. 数据总线 (DataBus): 在CPU与RAM之间来回传送需要处理或是需要储存的数据。
2. 地址总线 (Address Bus) : 用来指定在RAM (Random AccessMemory) 之中储存的数据的地址。
3. 控制总线 (Control Bus) : 将微处理器控制单元 (Control Unit)的信号，传送到周边设备。

### :boar:例题

```
以下关于总线的叙述中，不正确的是 (C)
A、并行总线适合近距离高速数据传输
B、串行总线适合长距离数据传输
C、单总线结构在一个总线上适应不同种类的设备，设计简单且性能很高
D、专用总线在设计上可以与连接设备实现最佳匹配
```

## 九、可靠性

1-2分

### :monkey\_face:系统可靠性分析—可靠性指标

平均无故障时间—>（MTTF）MTTF=1/λ，λ为失效率

平均故障修复时间—>（MTTR）MTTR=1/μ，μ为修复率

平均故障间隔时间—>（MTBF）MTBF=MTTR+MTTF

系统可用性—>MTTF/（MTTR+MTTF）\*100%

![image-20240330225231455](/assets/image-20240330225231455.Do_UN2JM.png)

在实际应用中，一般MTTR很小，所以通常认为MTBF≈MTTF。

可靠性可以用\*\*MTTF/（1+MTTF）\*\*来度量。

### :monkey\_face:串联系统与并联系统

#### 串联模型

![image-20240330225445796](/assets/image-20240330225445796.BETVrmT1.png)

串联模型：N个子系统串在一起形成一个系统。所有的子系统都必须正常运行，整个系统才正常，只要有一个环节出问题了，整个系统就会出问题。

**串联模型可靠度计算**：R = R1 X R2 X R3 X … X Rn

比如R1,R2,R3都是0.9，则R=0.9^3 = 72.9%

**串联模型失效率计算**：失效率有可能是会大于1的，它只是一个简化公式，当失效率极低的时候可以快速计算结果。

#### 并联系统

![在这里插入图片描述](/assets/20210612112617730.DqmccDOI.png)

并联系统：是由多个系统并联在一起。多个子系统只要有一个能正常运行，系统就能正常运行。换句话说，只有全部子系统都出问题，则系统出问题。

**并联系统可靠度计算**

思路是计算全部子系统出问题的概率，得出不出问题的概率。

设R1是子系统1的可靠度，则失效是 1 - R1

R = 1 - （1-R1）x （1-R2） x （1-R3）… x （1-Rn）

比如R1,R2,R3均为0.1 则 R = 1-0.1^3 = 99.9%

[2.18 串联系统与并联系统可靠度计算\_并联系统的可靠性计算-CSDN博客](https://blog.csdn.net/qq_30353463/article/details/117842059)

### :monkey\_face:N模冗余系统

![image-20240331225050983](/assets/image-20240331225050983.CxMYfgeE.png)

提高系统可靠性可以用冗余的方式进行。

如上图，R1，R2。。。Rn都做同样的计算，系统最终通过表决决定采纳哪个结果。

假设R1得到结果是1，R2也是1，R3得到0，通过表决器，输出应该是1，1占多数。此处的R3相当于出故障了，但是不会影响整个系统的运行。

几乎不会考。

### :monkey\_face:N模混合系统

重点

![在这里插入图片描述](/assets/20210612114344727.B4QGRbGN.png)

如上图，先判断他大体是串联还是并联。可以看出这个系统可以看分成3份独立系统的串联，大结构是串联，小结构是并联的系统。

1. 算出第二个系统的可靠度 1-（1-R）3
2. 算出第三个系统的可靠度 1-（1-R）2
3. 用串联公式计算整体可靠度 R X (1-（1-R）^3) x (1-（1-R）^2)

### :monkey\_face:例题

```
软件可靠性是指系统在给定的时间间隔内、在给定条件下无失效运行的概率。若MTTF和MTTR分别表示平均无故障时间和平均修复时间，则公式(A)可用于计算软件可靠性。
A、MTTF/(1+MTTF)
B、1/(1+MTTF)
C、MTTR/(1+MTTR)
D、1/(1+MTTR)

某系统由3个部件构成，每个部件的千小时可靠度都为R，该系统的千小时可靠度为 (1- (1-R)^2) R,则该系统的构成方式是(C)
A、3个部件串联 R*R*R=R^3
B、3个部件并联 1-(1-R)(1-R)(1-R)=1-(1-R)^3
C、前两个部件并联后与第三个部件串联   (1-(1-R)^2)R
D、第一个部件与后两个部件并联构成的子系统串联  (1-(1-R)^2)R
```

## 十、性能指标

### :monkey:系统性能设计-性能指标

* 字长和数据通路宽度
  * 字长32bit：数据通路宽度=2^32=4G
* 主存容量和存取速度
* 主频与CPU时钟周期
  * 主频2.4GHZ：时钟周期=1/主频=1s/2.4GHZ
* 运算速度
* CPI与IPC
  * 平均每条指令的平均时钟周期个数 (CPl，clock perinstruction)
  * 每 (时钟)周期运行指令条数 (IPC，instruction per clocy)
* 吞吐量与吞吐率
* MIPS与MFLOPS
  * 百万条指令每秒 (MIPS，MillionInstructions Per Second)
  * 每秒百万个浮点操作 (MFLOPS，Milion Floating-pointOperations per Second)
  * MIPS=指令条数/（执行时间×10^6）=主频/CPI=主频×IPC
  * MFLOPS=浮点操作次数/（执行时间×10^6）
* 响应时间 (RT) 与完成时间 (TAT)
  * 响应时间 (RT，Response Time)
* 兼容性

### :monkey:例题

```
软件质量属性中， (B)是指软件每分钟可以处理多少个请求。
A、响应时间
B、吞吐量
C、负载
D、容量

某计算机系统的CPU主频为2.8GHz。某应用程序包括3类指令，各类指令的CPI(执行每条指令所需要的时钟周期数)及指令比例如下表所示。执行该应用程序时的平均CPI为 (C); 运算速度用MIPS表示，约为（B）
     指令A  指令B  指令C
比例  35%   45%    20%
CPI   4     2      6
A.25
B.3
C.3.5 加权平均数=4*0.35+2*0.45+6*0.20=0.35
D.4
A.700
B.800
C.930
D.1100

1G=10^3M=10^6K=10^9
1/(3.5*(1/2.8G))=1/(3.5*(1/2.8*10^3M))=1/(1/0.8*10^3M)
```

---

---
url: /daily/日常笔记/加密算法.md
---

# 加密算法

https://www.python100.com/html/883TWNV35E4Q.html

---

---
url: /01.指南/01.简介/01.简介.md
---

# 简介

Teek 是一个轻量、简洁高效、灵活配置、易于扩展的 VitePress 主题 ✨，是在默认主题的基础上进行拓展，支持 VitePress 的所有功能、配置，完全可以零成本迁移过来。

使用 Teek 可以很方便的搭建一个结构化的知识库或博客。

::: warning

* Node.js `18.0.0` 及以上版本
* 在使用 Teek 前，要求至少会 VitePress 的基本使用和默认主题的基本配置，然后再查看本文档
* 本文档仅负责介绍 Teek 对 VitePress 默认主题的扩展部分，更多配置请移步 [VitePress 中文文档](https://vitepress.dev/zh/)

:::

## 特性

> **知识管理**

包含三种典型的知识管理形态：结构化、碎片化、体系化。轻松打造属于你自己的知识管理平台。

> **结构化 & 体系化**

自动生成侧边栏、目录页、索引页、面包屑等，轻松构建一个结构化知识库。

> **碎片化 & 个性化**

博客功能提供快速构建知识的碎片化形态，并提供大量个性化的主题配置。

> **文档风 & 博客风**

支持通过配置随意切换文档风和博客风，支持个人博客、文档站、知识库等场景。

## 拓展功能

相较于 VitePress 主题，Teek 主要实现了博客风格的功能，这些功能也兼容文档风格，您现在正在阅读的是 Teek 的文档风格。

> 全局

* 侧边栏自动生成，根据目录自动生成侧边栏，无需手动配置
* 提供目录页，根据 `Markdown` 文件路径自动生成目录
* 自动生成 `frontmatter`，并且支持拓展 `frontmatter` 格式
* 自动生成一级标题
* 全站背景图片
* `Markdown` 拓展：居中、居右容器、卡片容器、`Demo` 容器、`TODO` 列表、`Video` 容器
* 主题多元化：4 种布局模式、8 种主题风格选择，且支持自定义扩展新的主题风格
* 移动端适配：自动适配移动端
* ...

> 首页

* `Banner` 功能：提供 3 种风格选择：局部背景色、局部图片、全屏图片，提供打印个性签名、切换个性签名选择，提供 `Feature` 功能
* 文章列表：支持切换列表和卡片模式，展示文章标题、封面图、作者、创建时间、更新时间、标签、分类，且支持重写文章列表
* 博客卡片栏：博主信息栏、精选文章栏、分类栏、标签栏、友情链接栏、站点信息栏
* 全屏壁纸模式：只保留 `Banner` 背景图片或全站背景图片，且禁止滚动、打开开发者工具、右键功能
* 页脚：展示社交图标、版权信息、备案信息、自定义信息
* ...

> 文章页

* 文章信息：展示面包屑、作者、创建时间、更新时间、标签、分类、字数、阅读时长
* 评论区：提供 `Giscus`、`Twikoo`、`Waline`、`Artalk` 四种评论提供商选择，并且支持自定义评论区
* 代码块：UI 升级，支持一键折叠/展开
* 文章页风格书页化：提供 3 种风格选择：VitePress 原生、整体卡片化、片段卡片化
* 文章打赏：支持 3 种打赏风格选择
* 文章分享：提供一键复制文章链接功能
* 最近更新栏：展示最近更新文章
* ...

> 功能页

* 分类页
* 标签页
* 归档页
* 清单页
* 登录页
* 风险链接提示页

除了上述功能，Teek 也提供了各种 `CSS` 文件来增强 VitePress 的样式，并提供大量的插槽支持二次开发。

如果您是其他主题的用户，也可以按需引入 Teek 的功能，增强自己的站点风格。

---

---
url: /Java/JVM性能调优/02.JVM性能监控与调优/接口超时排查.md
---

# 接口超时排查

生产环境突然报系统繁忙，通过查看当时的日志发现接口报 **java.io.IOException:Broken pipe** ，复现抓包看到网关报 504(Gateway Timeout)。

## 原因分析

随着数据越来越多，分页的页数也越来越多，当翻页过多的时候，就会产生深分页，导致查询效率急剧下降。

## 问题定位

### 使用arthas定位问题

一般通过代码review，就能大概猜测问题点。如果项目基建成熟，使用skywalking可以看到每次请求耗时。也可以通过arthas工具，帮忙定位下具体的耗时方法。

在服务器上下载arthas

```shell
// wget https://arthas.aliyun.com/arthas-boot.jar

xxl@xxl-ubuntu:~$ cd /home/xxl/tools
xxl@xxl-ubuntu:~$ mkdir arthas
xxl@xxl-ubuntu:~$ wget https://arthas.aliyun.com/arthas-boot.jar
```

启动arthas

```shell
xxl@xxl-ubuntu:~$ java -jar arthas-boot.jar
```

选择要抓取的jar包

```shell
xxl@xxl-ubuntu:~/tools/arthas$ java -jar arthas-boot.jar 
[INFO] JAVA_HOME: /usr/lib/jvm/java-11-openjdk-amd64
[INFO] arthas-boot version: 3.7.1
[INFO] Can not find java process. Try to run `jps` command lists the instrumented Java HotSpot VMs on the target system.
Please select an available pid.
xxl@xxl-ubuntu:~/tools/arthas$ java -jar arthas-boot.jar 
[INFO] JAVA_HOME: /usr/lib/jvm/java-11-openjdk-amd64
[INFO] arthas-boot version: 3.7.1
[INFO] Found existing java process, please choose one and input the serial number of the process, eg : 1. Then hit ENTER.
* [1]: 13059 xxl-ubuntu-springboot-0.0.1-SNAPSHOT.jar
1
```

使用 stack 命令可以看到方法内部调用命令 :point\_right:：​[trace | arthas (aliyun.com)](https://arthas.aliyun.com/doc/trace.html)

```shell
// 类名+方法名
stack com.xxl.xxl.controller.UbuntuController helloUbuntu
```

下载插件 `arthas idea`，在方法名上右键 `Arthas Command` ，`Trace`。

到shell中粘贴命令，查看方法内详细耗时信息

```shell
[arthas@16342]$ trace com.xxl.xxl.controller.UbuntuController helloUbuntu2  -n 5 --skipJDKMethod false 
Press Q or Ctrl+C to abort.
Affect(class count: 1 , method count: 1) cost in 68 ms, listenerId: 2
`---ts=2023-09-24 19:50:59;thread_name=http-nio-8080-exec-5;id=16;is_daemon=true;priority=5;TCCL=org.springframework.boot.web.embedded.tomcat.TomcatEmbeddedWebappClassLoader@7ba8c737
    `---[0.281693ms] com.xxl.xxl.controller.UbuntuController:helloUbuntu2()
        `---[54.11% 0.152427ms ] com.xxl.xxl.controller.UbuntuController:add() #20

```

### Mybatis-Plus整合p6spy打印SQL耗时

添加依赖

```xml
<dependency>
	<groupId>p6spy</groupId>
	<artifactId>p6spy</artifactId>
	<version>3.9.1</version>
</dependency>
```

修改数据库连接配置

`driver-class-name`用`p6spy`提供的驱动类

`url`前缀为`jdbc:p6spy`跟着冒号，后面对应数据库连接地址

```yaml
spring:
    datasource: 
        type: com.alibaba.druid.pool.DruidDataSource
        driver-class-name: com.p6spy.engine.spy.P6SpyDriver
        url: jdbc:p6spy:mysql://127.0.0.1:3306/test?useUnicode=true&characterEncoding=utf-8&useSSL=true&serverTimezone=UTC
```

增加配置文件：在resources文件夹下创建p6spy的配置文件`spy.properties`

```properties
#3.2.1以上使用
modulelist=com.baomidou.mybatisplus.extension.p6spy.MybatisPlusLogFactory,com.p6spy.engine.outage.P6OutageFactory
#3.2.1以下使用或者不配置
#modulelist=com.p6spy.engine.logging.P6LogFactory,com.p6spy.engine.outage.P6OutageFactory
# 自定义日志打印
logMessageFormat=com.baomidou.mybatisplus.extension.p6spy.P6SpyLogger
#日志输出到控制台
#appender=com.baomidou.mybatisplus.extension.p6spy.StdoutLogger
# 使用日志系统记录 sql
appender=com.p6spy.engine.spy.appender.Slf4JLogger
# 设置 p6spy driver 代理
deregisterdrivers=true
# 取消JDBC URL前缀
useprefix=true
# 配置记录 Log 例外,可去掉的结果集有error,info,batch,debug,statement,commit,rollback,result,resultset.
excludecategories=info,debug,result,commit,resultset
# 日期格式
dateformat=yyyy-MM-dd HH:mm:ss
# 实际驱动可多个
#driverlist=org.h2.Driver
# 是否开启慢SQL记录
outagedetection=true
# 慢SQL记录标准 2 秒
outagedetectioninterval=2
```

## 解决

1、分页关闭查询总数

如果使用mybatis-plus提供的分页插件，会默认开启查询总数的SQL，如果不需要可以关闭减少一次查询。

```java
Page pageReq = request.plusPage();
pageReq.setSearchCount(false);
```

2、游标分页

游标翻页，只要命中索引，速度就非常快，和消息量基本没关系

[mysql分页使用游标\_mob64ca12d1a59e的技术博客\_51CTO博客](https://blog.51cto.com/u_16213305/7029596)

[解密高效分页查询：Java实现MySQL分页的奥秘 (baidu.com)](https://baijiahao.baidu.com/s?id=1773258061474240109\&wfr=spider\&for=pc)

[mysql 分页的方法技巧有哪些？ - 知乎 (zhihu.com)](https://www.zhihu.com/question/590227060/answer/2974873775?utm_id=0)

[MySQL分页到了后面越来越慢，有什么好的解决办法？ - 知乎 (zhihu.com)](https://www.zhihu.com/question/432910565/answer/2557661575)

---

---
url: /Java/解决方案/接口设计/2_接口加密.md
---

# 接口加密

---

---
url: /Java/解决方案/接口设计/1_接口幂等.md
---

# 接口幂等

---

---
url: /Java/解决方案/接口设计/3_接口限流.md
---

# 接口限流

---

---
url: /Java/解决方案/接口设计/4_接口优化.md
---

# 接口优化

---

---
url: /daily/软件设计师/06_结构化开发方法.md
---

# 结构化开发方法

## 一、系统分析与设计概述

## 二、结构化分析方法

## 三、结构化设计方法

## 四、WebApp分析与设计

## 五、用户界面设计

---

---
url: /01.指南/01.简介/20.结构化目录.md
---

# 结构化目录

## 目录结构

在运行或构建 Teek 时，Teek 会按照目录结构自动生成一个结构化的 **侧边栏**、**目录树**、**面包屑**、**文章列表**、**文章分析** 等数据。

侧边栏和目录树的标题获取有如下特性：

* 针对文件夹，先分别扫描该文件夹下的 `index.md`、`index.MD`、`[文件夹名].md` 文件，并尝试获取文件的 `frontmatter.title` 或一级标题，如果获取到标题，则使用，否则使用文件夹名
* 针对 Markdown 文档，其获取顺序：`frontmatter.title` > Markdown 文件一级标题 > Markdown 文件名

面包屑的标题默认按照 Markdown 文件所在的目录层级名进行获取。

Teek 建议给每个 Markdown 文件设置 `frontmatter.title`，且文件的命名与 `frontmatter.title` 保持一致。

::: info
侧边栏数据会在文章页左侧生成菜单，目录树数据会在目录页生成目录，面包屑数据会在文章页生成面包屑。
:::

## 特殊目录

Teek 的自动生成侧边栏，自动生成 `frontmatter`、自动生成 `h1` 标题、站点分析功能在 VitePress 启动后，从根目录扫描 Markdown 文件，但是有部分目录会忽略扫描：

* `@pages`：该目录初衷是存放归档页、分类页、标签页等非文章的 Markdown 文件，它完全不会被 Teek 扫描，它是非常纯净的目录
* `.scripts`：该目录初衷是存放一些脚本、工具文件，它完全不会被 Teek 扫描，它是非常纯净的目录
* `@fragment`：该目录下的 Markdown 文件不会自动生成侧边栏，因此一些 **碎片化** 文章建议放在该目录下
* `/目录页/`：该命名的目录（正则表达式，如 `01.目录页` 也符合）初衷是提供一个专门存放目录页的 Markdown 文件，因此统计站点文章数、文章总数等功能不会扫描该目录，文章列表和归档页也不会扫描该目录

::: details 碎片化文章说明
特点

* 简短且独立：每个碎片化文章通常只涵盖一个具体的知识点或主题，篇幅较短
* 灵活性高：可以随时添加、修改或删除，不受整体结构的严格限制
* 易于管理：由于其独立性，管理和查找特定知识点更加方便
* 补充性质：通常作为主干内容的补充，提供额外的信息或细节

示例场景

* 技术笔记：例如某个命令的用法、某个库的简单示例等
* 个人心得：如读书笔记、会议纪要等
* 临时记录：开发过程中遇到的问题及解决方案

:::

## 命名约定

### 博客风

如果你搭建的是博客风的站点，那么 Teek 建议和 vdoing 一样，使用以下命名约定：

* 无论是文件还是文件夹，请为其名称添加上正确的正整数序号和 `.`，从 `00` 或 `01` 开始累计，如 `01.文件夹`、`02.文件.md`，我们将会按照序号的顺序来决定其在侧边栏当中的顺序
* 同一级别目录别内即使只有一个文件或文件夹也要为其加上序号

序号只是用于决定先后顺序，并不一定需要连着，如 `01、02、03...`，实际可能会在两个文章中间插入一篇新的文章，因此为了方便可以采用间隔序号 `10、20、30...`，后面如果需要在 `10` 和 `20` 中间插入一篇新文章，可以给定序号 `15`。

当然可以使用非序号的命名，这并不影响使用，添加序号只是为了排序，且更具有结构化，如果同一个目录下同时存在带序号和不带序号的文件，在生成侧边栏时，Teek 会分为两个区：带序号区和不带序号区，两个区内部按照各自的逻辑排序，在最终生成侧边栏的时候，不带序号区始终放在带序号区的后面。

::: tip
从维护性、可读性的角度分析，带有序号的文件名在本地目录看起来更加直观；从站点渲染的角度分析，在生成侧边栏时，Teek 会根据文件名的序号进行排序。
:::

如果不希望 URL 上带有序号，请给每一个 Markdown 指定一个永久链接 [frontmatter.permalink](http://localhost:5173/reference/frontmatter.html#permalink)。

### 文档风&#x20;

如果你搭建的是文档风，那么大部分场景下，Markdown 文件名为英文格式，此时正好作为 URL 访问，此时会遇到一个问题：在生成侧边栏时，怎么按照自己的要求进行排序呢？

* 仿照博客风给文件名添加序号，但是访问时 URL 会带上序号，这样看起来比较难受，为解决这个问题，你需要指定一个永久链接 `frontmatter.permalink`
* 文件名不添加序号，通过 `frontmatter.sidebarSort` 指定排序，数值越小越靠前，数值的命名规则可以参考博客风：序号之间添加间隔

如果不指定 `frontmatter.sidebarSort`，那么 Teek 给每一个文件默认设置为 `9999`，因此如果给某一个文件的 `frontmatter.sidebarSort` 设置大于 9999，则排在侧边栏的最后面。

> 我可以自定义默认 9999 为其他序号吗？或者希望文件名本身有序号时，则替换 9999？

这是可以的，配置如下：

```ts {6-8}
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  vitePlugins: {
    sidebarOption: {
      sort: true, // 开启 frontmatter.sidebarSort 功能，默认已经开启，无需设置
      defaultSortNum: 9999, // 没有指定 frontmatter.sidebarSort 时的默认值，用于侧边栏排序
      sortNumFromFileName: false, // 是否用文件名的前缀序号作为其侧边栏 Item 的排序序号。如果为 true，当文件名存在序号前缀，则使用序号前缀，否则使用 defaultSortNum
      // ... 更多配置
    },
  },
});
```

更多的其他配置请看 [SideBar 配置项](https://github.com/Kele-Bingtang/vitepress-theme-teek/blob/master/plugins/vitepress-plugin-sidebar-resolve/src/types.ts)。

## 目录层级

Teek 生成侧边栏或目录树的数据时，虽然支持无限层嵌套，但是建议不要超过 5 层。

## 目录结构例子

这里以博客风的命名约定为例：

```sh
.
│ (不参与数据生成)
├─ .vitepress
├─ .scripts
├─ @pages
├─ @fragment
├─ index.md
├─ package.json
│
│  (以下部分参与数据生成)
├─ 01.指南
│  │  index.md
│  ├─ 01.指南 - 使用
│  │  ├── 04.使用 - 登录认证.md
│  │  ├── 07.使用 - 权限认证.md
│  │  ├── 10.使用 - 登出下线.md
│  │  ├── 13.使用 - 注解鉴权.md
│  │  ├── 16.使用 - 路由拦截鉴权.md
│  │  ├── 19.使用 - Session 会话.md
│  │  ├── 22.使用 - 框架配置.md
│  │  ├── 25.使用 - 自定义 Token.md
│  │  ├── 28.使用 - 临时 Token 认证.md
│  │  ├── 31.使用 - 记住我模式.md
│  │  ├── 34.使用 - 二级认证.md
│  │  ├── 37.使用 - 身份切换.md
│  │  ├── 40.使用 - 账号封禁.md
│  │  ├── 43.使用 - 会话查询.md
│  │  ├── 46.使用 - Http Basic 认证.md
│  │  ├── 49.使用 - 全局侦听器.md
│  │  ├── 52.使用 - 全局过滤器.md
│  │  ├── 55.使用 - 多账号认证.md
│  │  ├── 58.使用 - 自定义注解.md
│  │  └─ 99.三级目录测试
│  │  │  ├── 01.测试1.md
│  │  │  ├── 03.测试2.md
│  │  │  ├── index.md
│  │  │  │   └─ 99.四级目录测试
│  │  │  │   ├── 01.测试1.md
│  │  │  │   ├── 03.测试2.md
│  │  │  │   ├── index.md
│  ├─ 05.指南 - 环境集成
│  │  ├── 04.环境集成 - Spring Boot.md
│  │  ├── 07.环境集成 - Spring WebFlux.md
│  │  ├── 99.环境集成 - 上下文组件开发指南.md
│  └─ 10.指南 - 插件
│  │  ├── 04.插件 - 持久层集成 Redis.md
│  │  ├── 07.插件 - 持久层拓展.md
│  │  ├── 10.插件 - AOP 注解鉴权.md
│  │  ├── 13.插件 - Token 集成 JWT.md
│  │  ├── 99.插件 - 插件开发指南.md
├─ 05.设计
│  │  00.目录.md
│  ├─ 01.设计 - 思路
│  │  │  01.设计 - 思路设计.md
│  │  │  04.设计 - 模块设计.md
│  │  │  07.设计 - 术语说明.md
│  │  │  10.设计 - 全局配置.md
│  │  │  13.设计 - 策略模式.md
│  │  │  16.设计 - 异常模型.md
│  │  │  18.设计 - 管理者模型.md
│  ├─ 03.设计 - Helpers
│  │  ├── 01.设计 - Helpers 说明.md
│  │  ├── 07.设计 - 账号登录.md
│  │  ├── 10.设计 - 账号登出.md
│  │  ├── 13.设计 - 账号封禁.md
│  │  ├── 16.设计 - 二级认证.md
│  │  ├── 19.设计 - 身份切换.md
│  │  ├── 22.设计 - 账号认证.md
│  │  ├── 25.设计 - 临时 Token.md
│  │  ├── 28.设计 - 同源 Token.md
│  │  ├── 31.设计 - Http Basic 认证.md
```

---

---
url: /Java/设计模式/03.结构型/1.md
---

# 结构型

---

---
url: /daily/博客文档/VitPress/8_静态部署.md
---

# 静态部署

## Base

::: warning 注意

base必须配置，否则打包会丢失css样式！！

根目录配置 `/`，那么对应 `https://xxx.github.io/`

仓库 `vitepress` 配置 `/vitepress/` ，那么对应 `https://xxx.github.io/vitepress`

我们根据自己的需求，选择相应的的配置

:::

```js
export default defineConfig({
    base: '/', //网站部署的路径，默认根目录
    // base: '/vitepress/', //网站部署到github的vitepress这个仓库里
})
```

另一个要注意的点，部署到非根目录，Fav图标路径也要变动一下

```js
export default defineConfig({

  //fav图标
  head: [
    ['link',{ rel: 'icon', href: '/logo.png'}], //部署到根目录  // [!code --]
    ['link',{ rel: 'icon', href: '/vitepress/logo.png'}], //部署到vitepress仓库  // [!code ++]
  ],

})
```

## 部署

### 手动部署

构建完成后，在dist文件夹上传到Github即可

```sh
npm run docs:build
```

默认的构建输出目录 `.vitepress/dist` ，将生成的所有文件上传到 Github 即可

上传成功后，在GitHub仓库 - 设置 - page里把分支改成main，默认root，保存

等创建成功后即可获得访问链接

### 自动部署1-脚本（推荐）

根目录下新建`deploy.sh`

```sh
#!/usr/bin/env sh

# 确保脚本抛出遇到的错误
set -e

# 生成静态文件
# npm run docs:build
# npm run set NODE_OPTIONS=--openssl-legacy-provider && vuepress build .

# 进入生成的文件夹
cd docs/.vitepress/dist
#cd ./public

# 如果是发布到自定义域名
echo 'http://luckilyxxl.xyz' > CNAME

git init
git add -A
git commit -m 'deploy'

# 如果发布到 https://<USERNAME>.github.io
# git push -f git@github.com:<USERNAME>/<USERNAME>.github.io.git master
git push -f git@github.com:Daneliya/Daneliya.github.io.git master
#git push -f git@gitee.com:xu_xiaolong/xu_xiaolong.git master

# 如果发布到 https://<USERNAME>.github.io/<REPO>
# git push -f git@github.com:<USERNAME>/<REPO>.git master:gh-pages

cd -
```

执行构建`docs:build`

进入根目录下执行`./deploy.sh`

### 自动部署2-Vercel

[Vercel](https://vercel.com/docs/concepts/get-started) ：非常推荐，可以参考 [Vercel注册到部署](https://yiov.top/website/pages/vercel.html)

### 自动部署3—Github Actions 工作流

在仓库 `Actions` 里新建一个工作流 中创建一个 `deploy.yml` 脚本文件

每次你更新代码后，系统会自动给你打包上传并部署

::: tip 注意

名字可以自定义，不用非得用 `deploy` ，只要下面配置名和这个一致就行

分支默认是 `main`

:::

```yaml
# 构建 VitePress 站点并将其部署到 GitHub Pages 的示例工作流程
#
name: Deploy VitePress site to Pages

on:
  # 在针对 `main` 分支的推送上运行。如果你
  # 使用 `master` 分支作为默认分支，请将其更改为 `master`
  push:
    branches: [main]

  # 允许你从 Actions 选项卡手动运行此工作流程
  workflow_dispatch:

# 设置 GITHUB_TOKEN 的权限，以允许部署到 GitHub Pages
permissions:
  contents: read
  pages: write
  id-token: write

# 只允许同时进行一次部署，跳过正在运行和最新队列之间的运行队列
# 但是，不要取消正在进行的运行，因为我们希望允许这些生产部署完成
concurrency:
  group: pages
  cancel-in-progress: false

jobs:
  # 构建工作
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # 如果未启用 lastUpdated，则不需要
      # - uses: pnpm/action-setup@v3 # 如果使用 pnpm，请取消此区域注释
      #   with:
      #     version: 9
      # - uses: oven-sh/setup-bun@v1 # 如果使用 Bun，请取消注释
      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: npm # 或 pnpm / yarn
      - name: Setup Pages
        uses: actions/configure-pages@v4
      - name: Install dependencies
        run: npm ci # 或 pnpm install / yarn install / bun install
      - name: Build with VitePress
        run: npm run docs:build # 或 pnpm docs:build / yarn docs:build / bun run docs:build
      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: docs/.vitepress/dist

  # 部署工作
  deploy:
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    needs: build
    runs-on: ubuntu-latest
    name: Deploy
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
```

## 报错

```
(!) Found dead link ./index in file
```

解决：https://blog.csdn.net/sinat\_31213021/article/details/143791615

---

---
url: /StableDiffusion/Midjourney/5_局部重绘.md
---

# 局部重绘

---

---
url: /10.配置/01.主题配置/20.卡片栏配置.md
---

# 卡片栏配置

## homeCardSort

* 类型：`("topArticle" | "category" | "tag" | "friendLink" | "docAnalysis")[]`
* 默认值：`["topArticle", "category", "tag", "friendLink", "docAnalysis"]`

首页卡片的位置排序，当设置了 `homeCardSort` 但没有全部补全内容，Teek 会将剩余内容按照 `homeCardSort` 的顺序进行添加。

::: code-group

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  homeCardSort: ["topArticle", "category", "tag", "friendLink", "docAnalysis"],
});
```

```yaml [index.md]
---
tk:
  homeCardSort:
    - topArticle
    - category
    - tag
    - friendLink
    - docAnalysis
---
```

:::

## tagColor&#x20;

* 类型：`string[]`
* 默认值：

```json
[
  { "border": "#bfdbfe", "bg": "#eff6ff", "text": "#2563eb" },
  { "border": "#e9d5ff", "bg": "#faf5ff", "text": "#9333ea" },
  { "border": "#fbcfe8", "bg": "#fdf2f8", "text": "#db2777" },
  { "border": "#a7f3d0", "bg": "#ecfdf5", "text": "#059669" },
  { "border": "#fde68a", "bg": "#fffbeb", "text": "#d97706" },
  { "border": "#a5f3fc", "bg": "#ecfeff", "text": "#0891b2" },
  { "border": "#c7d2fe", "bg": "#eef2ff", "text": "#4f46e5" }
]
```

标签背景色，用于精选文章卡片的 `top + sticky` 功能和标签卡片的标签，背景色按顺序显示。

当在文章页的 `frontmatter` 配置时，如果颜色值有 `#` 号时请添加引号。

::: code-group

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  bgColor: ["#e74c3c", "#409EFF", "#DAA96E", "#0C819F", "#27ae60", "#ff5c93", "#fd726d", "#f39c12", "#9b59b6"],
});
```

```yaml [index.md]
---
tk:
  bgColor:
    - "#e74c3c"
    - "#409EFF"
    - "#DAA96E"
    - "#0C819F"
    - "#27ae60"
    - "#ff5c93"
    - "#fd726d"
    - "#f39c12"
    - "#9b59b6"
---
```

:::

## blogger

博主信息，显示在首页左边第一个卡片。

::: code-group

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  blogger: {
    name: "天客", // 博主昵称
    avatar: "https://testingcf.jsdelivr.net/gh/Kele-Bingtang/static/user/avatar1.png", // 博主头像
    slogan: "朝圣的使徒，正在走向编程的至高殿堂！", // 博主签名
    shape: "square", // 头像风格：square 为方形头像，circle 为圆形头像，circle-rotate 可支持鼠标悬停旋转，circle-rotate-last 将会持续旋转 59s
  },
});
```

```yaml [index.md]
---
tk:
  blogger:
    name: 天客
    avatar: https://testingcf.jsdelivr.net/gh/Kele-Bingtang/static/user/avatar1.png
    slogan: 朝圣的使徒，正在走向编程的至高殿堂！
    shape: square
---
```

```ts [更多配置项]
interface Blogger {
  /**
   * 博主昵称
   */
  name: string;
  /**
   * 博主头像
   */
  avatar: string;
  /**
   * 博主签名
   */
  slogan?: string;
  /**
   * 头像风格：square 为方形头像，circle 为圆形头像，circle-rotate 可支持鼠标悬停旋转，circle-rotate-last 将会持续旋转 59s
   *
   * @default 'square'
   */
  shape?: TkAvatarProps["shape"] | "circle-rotate";
  /**
   * 背景图片地址，仅当 shape 为 circle 相关值时有效
   *
   * @since v1.1.5
   */
  circleBgImg?: string;
}
```

:::

## topArticle

精选文章卡片配置。

::: code-group

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  topArticle: {
    enabled: true, // 是否启用精选文章卡片
    limit: 5, // 一页显示的数量
    autoPage: false, // 是否自动翻页
    pageSpeed: 4000, // 翻页间隔时间，单位：毫秒。autoPage 为 true 时生效
    dateFormat: "yyyy-MM-dd hh:mm:ss", // 精选文章的日期格式
  },
});
```

```yaml [index.md]
---
tk:
  topArticle:
    enabled: true
    limit: 5
    autoPage: false
    pageSpeed: 4000
---
```

```ts [更多配置项]
import type { VpRouter } from "@teek/composables";

interface TopArticle {
  /**
   * 是否启用精选文章卡片
   *
   * @default true
   */
  enabled?: boolean;
  /**
   * 首页卡片标题
   *
   * @default '${icon}精选文章'
   */
  title?: string | ((icon: string) => string);
  /**
   * 精选文章为空时的标签
   *
   * @default '暂无精选文章'
   */
  emptyLabel?: string;
  /**
   * 一页显示的数量
   *
   * @default 5
   */
  limit?: number;
  /**
   * 是否自动翻页
   *
   * @default false
   */
  autoPage?: boolean;
  /**
   * 翻页间隔时间，单位：毫秒。autoPage 为 true 时生效
   *
   * @default 4000 (4秒)
   */
  pageSpeed?: number;
  /**
   * 精选文章的日期格式
   *
   * @default 'yyyy-MM-dd hh:mm:ss'
   */
  dateFormat?: "yyyy-MM-dd" | "yyyy-MM-dd hh:mm:ss" | ((date: number | string) => string);
  /**
   * 点击标题时触发，可以通过 router.go 跳转到其他页面，也可以通过 window.open 打开新窗口
   *
   * @since v1.1.2
   */
  titleClick?: (router: VpRouter) => void;
}
```

:::

## category

分类卡片配置。

::: code-group

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  category: {
    enabled: true, // 是否启用分类卡片
    limit: 5, // 一页显示的数量
    autoPage: false, // 是否自动翻页
    pageSpeed: 4000, // 翻页间隔时间，单位：毫秒。autoPage 为 true 时生效
  },
});
```

```yaml [index.md]
---
tk:
  category:
    enabled: true
    limit: 5
    autoPage: false
    pageSpeed: 4000
---
```

```ts [更多配置项]
interface Category {
  /**
   * 是否启用分类卡片
   *
   * @default true
   */
  enabled?: boolean;
  /**
   * 分类页访问地址
   *
   * @default '/categories'
   */
  path?: string;
  /**
   * 分类页卡片标题
   *
   * @default '${icon}全部分类'
   */
  pageTitle?: string | ((icon: string) => string);
  /**
   * 首页卡片标题
   *
   * @default '${icon}文章分类'
   */
  homeTitle?: string | ((icon: string) => string);
  /**
   * 查看更多分类标签
   *
   * @default '更多 ...'
   */
  moreLabel?: string;
  /**
   * 分类为空时的标签
   *
   * @default '暂无文章分类'
   */
  emptyLabel?: string;
  /**
   * 一页显示的数量
   *
   * @default 5
   */
  limit?: number;
  /**
   * 是否自动翻页
   *
   * @default false
   */
  autoPage?: boolean;
  /**
   * 翻页间隔时间，单位：毫秒。autoPage 为 true 时生效
   *
   * @default 4000 (4秒)
   */
  pageSpeed?: number;
}
```

:::

## tag

标签卡片配置。

::: code-group

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  tag: {
    enabled: true, // 是否启用标签卡片
    limit: 21, // 一页显示的数量
    autoPage: false, // 是否自动翻页
    pageSpeed: 4000, // 翻页间隔时间，单位：毫秒。autoPage 为 true 时生效
  },
});
```

```yaml [index.md]
---
tk:
  tag:
    enabled: true
    limit: 5
    autoPage: false
    pageSpeed: 4000
---
```

```ts [更多配置项]
interface Tag {
  /**
   * 是否启用标签卡片
   *
   * @default true
   */
  enabled?: boolean;
  /**
   * 标签页访问地址
   *
   * @default '/tags'
   */
  path?: string;
  /**
   * 标签页页卡片标题
   *
   * @default '${icon}全部标签'
   */
  pageTitle?: string | ((icon: string) => string);
  /**
   * 首页卡片标题
   *
   * @default '${icon}热门标签'
   */
  homeTitle?: string | ((icon: string) => string);
  /**
   * 查看更多分类标签
   *
   * @default '更多 ...'
   */
  moreLabel?: string;
  /**
   * 标签为空时的标签
   *
   * @default '暂无标签'
   */
  emptyLabel?: string;
  /**
   * 一页显示的数量
   *
   * @default 21
   */
  limit?: number;
  /**
   * 是否自动翻页
   *
   * @default false
   */
  autoPage?: boolean;
  /**
   * 翻页间隔时间，单位：毫秒。autoPage 为 true 时生效
   *
   * @default 4000 (4秒)
   */
  pageSpeed?: number;
}
```

:::

## friendLink

友情链接卡片配置。

::: code-group

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  friendLink: {
    enabled: true, // 是否启用友情链接卡片
    list: [
      {
        name: "Teeker",
        desc: "朝圣的使徒，正在走向编程的至高殿堂！",
        avatar: "https://testingcf.jsdelivr.net/gh/Kele-Bingtang/static/user/avatar2.png",
        link: "http://notes.teek.top/",
      },
      {
        name: "vuepress-theme-vdoing",
        desc: "🚀一款简洁高效的VuePress 知识管理&博客 主题",
        avatar: "https://doc.xugaoyi.com/img/logo.png",
        link: "http://notes.teek.top/",
      },
      {
        name: "vuepress-theme-vdoing",
        desc: "🚀一款简洁高效的VuePress 知识管理&博客 主题",
        avatar: "https://doc.xugaoyi.com/img/logo.png",
        link: "https://doc.xugaoyi.com/",
      },
      {
        name: "One",
        desc: "明心静性，爱自己",
        avatar: "https://onedayxyy.cn/img/xyy-touxiang.png",
        link: "https://onedayxyy.cn/",
      },
      {
        name: "Hyde Blog",
        desc: "人心中的成见是一座大山",
        avatar: "https://teek.seasir.top/avatar/avatar.webp",
        link: "https://teek.seasir.top/",
      },
      {
        name: "二丫讲梵",
        desc: "💻学习📝记录🔗分享",
        avatar: "https://wiki.eryajf.net/img/logo.png",
        link: " https://wiki.eryajf.net/",
      },
    ], // 友情链接数据列表
    limit: 5, // 一页显示的数量
    autoScroll: false, // 是否自动滚动
    scrollSpeed: 2500, // 滚动间隔时间，单位：毫秒。autoScroll 为 true 时生效
    autoPage: false, // 是否自动翻页
    pageSpeed: 4000, // 翻页间隔时间，单位：毫秒。autoPage 为 true 时生效
  },
});
```

```yaml [index.md]
---
tk:
  friendLink:
    enabled: true
    list:
      - name: 测试1
        desc: 这是一个友链测试1
        avatar: /img/bg1.jpg
        link: https://github.com/Kele-Bingtang
      - name: 测试2
        desc: 这是一个友链测试2222111啊
        avatar: /img/ui.png
    limit: 5
    autoScroll: false
    scrollSpeed: 2500
    autoPage: false
    pageSpeed: 4000
---
```

```ts [更多配置项]
import type { VpRouter } from "vitepress-theme-teek";

interface FriendLink {
  /**
   * 是否启用友情链接卡片
   *
   * @default true
   */
  enabled?: boolean;
  /**
   * 友情链接数据列表
   */
  list?: {
    /**
     * 友链名称
     */
    name: string;
    /**
     * 友链头像
     */
    avatar?: string;
    /**
     * 友链描述
     */
    desc?: string;
    /**
     * 友链链接
     */
    link?: string;
    /**
     * img 标签的 alt
     *
     * @default name
     */
    alt?: string;
  }[];
  /**
   * 首页卡片标题
   *
   * @default '${icon}友情链接'
   */
  title?: string | ((icon: string) => string);
  /**
   * 友情链接为空时的标签
   *
   * @default '暂无友情链接'
   */
  emptyLabel?: string;
  /**
   * 一页显示的数量
   *
   * @default 5
   */
  limit?: number;
  /**
   * 是否自动滚动
   *
   * @default false
   */
  autoScroll?: boolean;
  /**
   * 滚动间隔时间，单位：毫秒。autoScroll 为 true 时生效
   *
   * @default 2500 (2.5秒)
   */
  scrollSpeed?: number;
  /**
   * 是否自动翻页
   *
   * @default false
   */
  autoPage?: boolean;
  /**
   * 翻页间隔时间，单位：毫秒。autoPage 为 true 时生效
   *
   * @default 4000 (4秒)
   */
  pageSpeed?: number;
  /**
   * 点击标题时触发，可以通过 router.go 跳转到其他页面，也可以通过 window.open 打开新窗口
   *
   * @since v1.1.2
   */
  titleClick?: (router: VpRouter) => void;
}
```

:::

## docAnalysis

站点信息卡片配置。

::: code-group

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  docAnalysis: {
    enabled: true,
    createTime: "2021-10-19",
    wordCount: true,
    readingTime: true,
    statistics: {
      provider: "busuanzi",
      siteView: true,
      pageView: true,
    },
    overrideInfo: [
      {
        key: "lastActiveTime",
        label: "活跃时间",
        value: (_, currentValue) => (currentValue + "").replace("前", ""),
        show: true,
      },
    ],
    appendInfo: [{ key: "index", label: "序号", value: "天客 99" }],
  },
});
```

```yaml [index.md]
---
tk:
  docAnalysis:
    enabled: true
    createTime: 2021-10-19
    wordCount: true
    readingTime: true
    statistics:
      provider: "busuanzi"
      siteView: true
      pageView: true
    appendInfo:
      - key: "index"
        label: "序号"
        value: "天客 99"
---
```

```ts [更多配置项]
interface DocAnalysis {
  /**
   * 是否启用站点信息卡片
   *
   * @default true
   */
  enabled?: boolean;
  /**
   * 首页卡片标题
   *
   * @default '${icon}站点信息'
   */
  title?: string | ((icon: string) => string);
  /**
   * 项目创建时间
   */
  createTime?: string;
  /**
   * 是否开启文章页的字数统计
   *
   * @default true
   */
  wordCount?: boolean;
  /**
   * 是否开启文章页的阅读时长统计
   *
   * @default true
   */
  readingTime?: boolean;
  /**
   * 访问量、访客数统计配置
   */
  statistics?: {
    /**
     * 自建网站流量统计的 URL，支持的 URL 格式与提供商 provider 绑定
     */
    url?: string;
    /**
     * 网站流量统计提供商
     */
    provider?: "" | "busuanzi" | "vercount";
    /**
     * 是否开启首页的访问量和排名统计
     *
     * @default true
     */
    siteView?: boolean;
    /**
     * 是否开启文章页的浏览量统计
     *
     * @default true
     */
    pageView?: boolean;
    /**
     * 如果请求网站流量统计接口失败，是否重试，类型 boolean
     *
     * @default false
     */
    tryRequest?: boolean;
    /**
     * 重试次数，仅当 tryRequest 为 true 时有效
     *
     * @default 5
     */
    tryCount?: number;
    /**
     * 重试间隔时间，单位毫秒，仅当 tryRequest 为 true 时有效
     *
     * @default 2000
     */
    tryIterationTime?: number;
    /**
     * 是否只统计永久链接的浏览量，如果为 false，则统计 VitePress 默认的文档目录链接
     *
     * @default true
     */
    permalink?: boolean;
    /**
     * 自定义请求函数，返回 UvPvData 数据
     *
     * @param url 统计接口地址
     * @param createScriptFn 创建一个 script 标签的函数
     */
    requestFn?: (url: string | undefined, createScriptFn: typeof createScript) => UvPvData | Promise<UvPvData>;
  };
  /**
   * 自定义现有信息
   * originValue 为计算前的数据，currentValue 为计算后的数据（加单位的数据），针对 lastActiveTime 这些需要判断 N 分、N 时、N 天的 key，originValue 为具体的时间，需要自行计算
   */
  overrideInfo?: (Omit<PartialKey<DocAnalysisInfo, "label">, "value"> & {
    value?: (originValue: string | number, currentValue?: string | number) => string;
  })[];
  /**
   * 自定义额外信息，类型和 overrideInfo 一样
   * @default []
   */
  appendInfo?: (Omit<DocAnalysisInfo, "key"> & { key: string })[];
}

interface DocAnalysisInfo {
  /**
   * 站点信息唯一标识
   */
  key:
    | "totalPosts"
    | "weekAddNum"
    | "monthAddNum"
    | "runtime"
    | "totalWordCount"
    | "lastActiveTime"
    | "viewCount"
    | "visitCount"
    | string;
  /**
   * 站点信息标签
   */
  label: string;
  /**
   * 站点信息值的描述
   */
  value: string;
  /**
   * 是否显示在站点信息
   *
   * @default true
   */
  show?: boolean;
}
```

:::

::: tip
如果想开启访问量、访客数统计，请使用 `statistics.provider` 选择网站流量统计提供商。
:::

当想修改站点信息内置的信息时，可以使用 `overrideInfo` 配置项，该配置项是一个数组对象，对象的 `key` 为信息标识，`value` 是一个函数，接收两个参数 `originValue` 和 `currentValue`：

* originValue：站点信息卡片的原始值，如创建时间为 2021-10-19
* currentValue：站点信息卡片当前渲染的值，如创建时间渲染的值为 N 天前

比如想将 `文章数目` 改为 `文章总数目`：

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  docAnalysis: {
    overrideInfo: [{ key: "totalPosts", label: "文章总数目" }],
  },
});
```

比如想隐藏最后活动时间：

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  docAnalysis: {
    overrideInfo: [{ key: "lastActiveTime", show: false }],
  },
});
```

key 可选值如下：

* `totalPosts`：文章总数
* `weekAddNum`：近一周新增
* `monthAddNum`：近一月新增
* `runtime`：已运行时间
* `totalWordCount`：本站总字数
* `lastActiveTime`：最后活动时间
* `viewCount`：本站被访问了
* `visitCount`：本站曾来访过

---

---
url: /15.主题开发/70.开发技巧.md
---

# 开发技巧

介绍 Teek 开发路程的一些技巧。

## 规范

Teek 建议在进行项目开发时，一个文件的代码行数推荐 300 行以下，最好不超过 500 行，禁止超过 1000 行。

如果超过 300+ 行，应该考虑下是否可以拆分为多个文件，这是一个良好的 **结构化思维和分治思维**。

::: tip
Teek 建议您在开发前先思考有哪些模块，然后分别创建模块文件，而不是先在一个文件写完，再拆分。
:::

举个例子：

代码合在一个文件：

```html
<div>
  <div class="header">
    <img src="logo.png" />
    <h1>网站名称</h1>
  </div>
  <div class="main-content">
    <div class="banner">
      <ul>
        <li><img src="banner1.png" /></li>
        <!--   省略n行代码      -->
      </ul>
    </div>
    <div class="post-list">
      <!--   省略n行代码      -->
    </div>
    <!--   省略n行代码      -->
  </div>
</div>
```

将代码进行模块化，根据功能/布局/逻辑等进行拆分：

```html
<div>
  <header />
  <main>
    <Banner />
    <PostList />
    <Card />
    <AboutMe />
  </main>
  <footer />
</div>
```

假设您没有参与过该项目开发，现在由您来开始开发 `PostList` 模块的功能，我相信您更喜欢第二种，因为它已经明确的在向您挥手。

## SSR 兼容

在 VitePress 主题开发时，请考虑 SSR 兼容性。否则在构建项目时，报错：`window/document is not defined`。

关于 SSR 兼容性，VitePress 官方提供了 [SSR 兼容性](https://vitepress.dev/zh/guide/ssr-compat) 的文档可以参考，文档里介绍了几个方式如何兼容 SSR。

但是 Teek 在这里提供一个 VitePress 官方没有 **直接** 说明的一种方式，这也是 Teek 兼容 SSR 的方式，即：

**在任何访问浏览器或调用 DOM API 的代码前加上 SSR 环境校验**。

首先自定义一个 SSR 环境检验变量：

```ts
const isClient = typeof window !== "undefined" && typeof document !== "undefined";
```

然后在使用 DOM API 之前加上这个校验，这样就防止构建时报错：

```vue {6}
<script setup lang="ts">
// Teek 已经内置了 isClient 函数
import { isClient } from "vitepress-theme-teek";

const init = () => {
  if (!isClient) return;

  const xxDom = document.querySelector(".xx");
  // ...
};

init();
</script>

<template></template>
```

如果您的代码在 Vue 组件的 `beforeMount` 或 `mounted` 钩子中执行，则无需考虑 SSR，Vue 已经处理了。

## 利用对象/数组减少 HTML 编写

### 对象形式

**在 `template` 用 `if`、`if-else` 判断**。

```vue
<script setup lang="ts">
import { A, B, C, D } from "./components";
import { useData } from "vitepress";

const { theme } = useData();
const provider = theme.value.provider;
</script>

<template>
  <template v-if="provider === 'a'" name="a"><A /></template>
  <template v-else-if="provider === 'b'" name="b"><B /></template>
  <template v-else-if="provider === 'd'" name="c"><C /></template>
  <template v-else-if="provider === 'd'" name="d"><D /></template>
</template>
```

可以将其转为对象：

```vue
<script setup lang="ts">
import { A, B, C, D } from "./components";
import { useData } from "vitepress";

const { theme } = useData();
const provider = theme.value.provider;

const providerMap = {
  a: { el: A, props: { name: "a" } },
  b: { el: B, props: { name: "b" } },
  c: { el: C, props: { name: "c" } },
  d: { el: D, props: { name: "d" } },
};
</script>

<template>
  <component v-if="provider" :is="providerMap[provider]?.el" v-bind="providerMap[provider]?.props" />
</template>
```

可以在组件 [Layout](https://github.com/Kele-Bingtang/vitepress-theme-teek/blob/master/packages/components/theme/Layout/index.vue) 的评论区相关代码或者 [HomeBanner](https://github.com/Kele-Bingtang/vitepress-theme-teek/blob/master/packages/components/theme/HomeBanner/src/index.vue) 查看具体使用。

### 数组形式

**在 `template` 编写类似的重复 HTML**。

```vue
<script setup lang="ts"></script>

<template>
  <div>
    <div>
      <span class="title">A</span>
      <span class="desc">测试 A</span>
    </div>
    <div>
      <a class="link" href="/b">B</a>
      <span class="desc">测试 B</span>
    </div>
    <div>
      <img class="link" src="/c.png" />
      <span class="desc">测试 C</span>
    </div>
  </div>
</template>
```

可以将其转为数组：

```vue
<script setup lang="ts">
const list = [
  { title: "A", desc: "测试 A", className: "title" },
  { title: "B", desc: "测试 B", isLink: true, className: "link", link: "/b" },
  { desc: "测试 C", isImg: true, className: "img", link: "/c.png" },
];
</script>

<template>
  <div>
    <div v-for="item in list" :key="item.title">
      <a v-if="item.isLink" :class="item.className" :href="item.link">{{ item.title }}</a>
      <img v-else-if="item.isImg" :class="item.className" :src="item.link" />
      <span v-else :class="item.className">{{ item.title }}</span>

      <span class="desc">{{ item.desc }}</span>
    </div>
  </div>
</template>
```

仅限于重复度接近 90% 以上或者简单的 HTML，否则不建议使用数组 + `for` 循环，可读性会变差。

可以在组件 [ArticleInfo](https://github.com/Kele-Bingtang/vitepress-theme-teek/blob/master/packages/components/theme/ArticleInfo/src/index.vue) 查看具体使用。

## 配置项支持方式

### config 配置

如果配置项仅支持在 `.vitepress/config.mts` 配置：

在组件里这样使用：

```vue {6-8}
<script setup lang="ts">
import { useData } from "vitepress";

const { theme } = useData();

// 赋予默认值
const { enabled = true, name = "", obj = {}, arr = [] } = { ...theme.xxx };
// 或者
// const { enabled = true, name = "", obj = {}, arr = [] } = theme.xxx || {};
</script>

<template></template>
```

这样避免了获取 `theme.xxx` 里的属性时报 `undefined`（没配置 `xxx`），同时如果 `theme.xxx` 里的某些属性没有配置，则赋予默认值。

### config 和 frontmatter 配置

配置项同时支持在 `.vitepress/config.mts` 和 Markdown 的 `frontmatter` 配置，当两种都配置，则以 `frontmatter` 为准。

在组件里这样使用：

```vue
<script setup lang="ts">
import { computed } from "vue";
import { useData } from "vitepress";

const { theme, frontmatter } = useData();

const themeConfig = computed(() => ({
  enabled: true,
  name: "",
  obj: {},
  arr: [],
  ...theme.xxx,
  ...frontmatter.value.xxx,
  ...frontmatter.value.tk.xxx,
}));

// 使用
console.log(themeConfig.value.xxx);
</script>

<template></template>
```

`frontmatter.value.tk.xxx` 是在首页 `index.md` 配置 `frontmatter` 时，额外添加了 `tk`，这是为了避免与 VitePress 自带配置的命名冲突。

支持 `frontmatter` 配置时，一定要用 `computed` 监听 `frontmatter` 变化，因为不同 Markdown 的 `frontmatter` 有可能不一样，如果没有监听 `frontmatter` 变化，会导致切换 Markdown 文章后，新文章的配置不会重新生效。

如：

::: code-group

```ts [config]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
 comment: {
    provider: "giscus",
    options: {
      repo: "your repo",
      repoId: "your repoId",
      category: "your category",
      categoryId: "your categoryId",
    }
 };
});
```

```yaml [index.md]
---
tk:
  comment:
    provider: "giscus"
    options:
      repo: "your repo"
      repoId: "your repoId"
      category: "your category"
      categoryId: "your categoryId"
---
```

```yaml [文章页.md]
---
comment:
  provider: "giscus"
  options:
    repo: "your repo"
    repoId: "your repoId"
    category: "your category"
    categoryId: "your categoryId"
---
```

:::

---

---
url: /15.主题开发/01.开发思路.md
---

# 开发思路

::: note 摘要

只要会编写 `Vue` 组件，那么就可以发挥您天马行空的想象力，来构建自己的主题。

::: right

2025-03-17 @Teek

:::

本系列为 **主题开发**，主要介绍 Teek 的开发思路，当然这只是提供思路，不会细到每一个文件的逻辑讲解。

在阅读完本系列内容后，您可以去阅读 Teek 的源码，了解 Teek 的实现思路，也许对您的主题开发之路有些帮助。

基于 VitePress 开发一个主题是非常简单的，在 [自定义主题](https://vitepress.dev/zh/guide/custom-theme) 和 [拓展默认主题](https://vitepress.dev/zh/guide/extending-default-theme) 已经详细的介绍了如何开发一个主题。

## Layout 函数

VitePress 默认内置了一套主题，如果觉得内置主题的功能不满足需求或者想额外添加一些功能，可以编写组件来替换/拓展 VitePress 主题。

首先 VitePress **必须** 需要接收一个 `Layout` 函数，该函数需要返回一个 `vue` 组件作为 **入口组件**：

```ts {5}
import DefaultTheme from "vitepress/theme";

export default {
  extends: DefaultTheme,
  Layout: /* Vue 组件 */,
  enhanceApp({ app, router, siteData }) {},
};
```

在 `Layout` 实现一个组件主要有 2 种方式：

1. `h` 函数 + `.vue` 组件

```ts {7}
import DefaultTheme from "vitepress/theme";
import MyComponent from "./MyComponent.vue";
import { h } from "vue";

export default {
  extends: DefaultTheme,
  Layout: () => h(MyComponent),
  enhanceApp({ app, router, siteData }) {},
};
```

2. `defineComponent` 函数生成 `vue` 组件

```ts {7-14}
import DefaultTheme from "vitepress/theme";
import MyComponent from "./MyComponent.vue";
import { h } from "vue";

export default {
  extends: DefaultTheme,
  Layout: defineComponent({
    name: "ConfigProvider",
    setup(_, { slots }) {
      // 自定义一些全局逻辑

      return () => h(MyComponent, null, slots);
    },
  }),
  enhanceApp({ app, router, siteData }) {},
};
```

可以看到，`defineComponent` 函数的返回值还是使用了 `h` 函数 + `.vue` 组件，但是这样好处在于可以添加一些全局逻辑或往所有组件里注入常用数据，因为这是在所有组件加载前执行的逻辑。

比如 Teek 注入了文章信息数据、并开启一些监听器：

```ts {9-16}
import DefaultTheme from "vitepress/theme";
import MyComponent from "./MyComponent.vue";
import { h, type Component } from "vue";

const configProvider = (Layout: Component) => {
  return defineComponent({
    name: "ConfigProvider",
    setup(_, { slots }) {
      const { theme } = useData();
      // 往主题注入数据
      provide(postsContext, theme.value.posts);

      // 开启监听器
      usePermalink().startWatch();
      useAnchorScroll().startWatch();
      useViewTransition();

      return () => h(Layout, null, slots);
    },
  });
};

export default {
  extends: DefaultTheme,
  Layout: configProvider(MyComponent),
  enhanceApp({ app, router, siteData }) {},
};
```

相比较直接用 `h` 函数来构建组件，`defineComponent` 函数更灵活，多了一个中间层方便实现一些逻辑。

## 入口组件

阅读内容前，您需要了解 VitePress 提供了哪些插槽，请看 [布局插槽](https://vitepress.dev/zh/guide/extending-default-theme#layout-slots)。

Teek 并不是完全脱离 VitePress 的主题，而是基于 VitePress 的主题开发，所以 Teek 在入口组件里继承 VitePress 的 `Layout` 组件，并通过 VitePress 提供的插槽来实现功能。

Teek 的入口文件伴随着迭代，已经有很多内容产出，这里给出 Teek 早期的模板：

```vue
<script setup lang="ts" name="TeekLayout">
import DefaultTheme from "vitepress/theme";
import { useData } from "vitepress";

const { theme } = useData();
const { teekTheme = true } = theme.value;

// 维护已使用的插槽，防止外界传来的插槽覆盖已使用的插槽
const usedSlots = [
  "home-hero-before",
  "nav-bar-content-after",
  // 其他模板里已使用的插槽 ...
];
</script>

<template>
  <!-- 使用 Teek 主题 -->
  <template v-if="teekTheme">
    <DefaultTheme.Layout>
      <template #home-hero-before>
        <slot name="home-hero-before" />
        <!-- 自定义首页 -->
      </template>

      <!-- 在 VP 的不同插槽自定义不同内容 -->
      <template #xx></template>

      <!-- 未使用的其他 VP 插槽 -->
      <template
        v-for="name in Object.keys($slots).filter(name => !usedSlots.includes(name))"
        :key="name"
        #[name]="slotData"
      >
        <slot :name="name" v-bind="slotData"></slot>
      </template>
    </DefaultTheme.Layout>
  </template>

  <template v-else>
    <DefaultTheme.Layout>
      <template v-for="(_, name) in $slots" :key="name" #[name]="slotData">
        <slot :name="name" v-bind="slotData"></slot>
      </template>
    </DefaultTheme.Layout>
  </template>
</template>
```

Teek 从 `theme` 中取出一个配置项 `teekTheme`，如果为 `true`，则使用 Teek 主题，否则使用 VitePress 的默认主题。

::: tip
`theme` 为项目里 `.vitepress/config.mts` 里的 `themeConfig` 内容。
:::

如果您完全不需要基于 VitePress 的主题开发，则不需要使用 `DefaultTheme.Layout` 组件，直接在该组件写入自己的内容即可，这也意味着您只是基于 Vite 环境构建您的专属风格，您将自己实现首页、侧边栏、导航栏、CSS 样式等，只有 Markdown 解析的内容不需要自己实现，VitePress 已经提供了全局组件 `<Content />` 来渲染 Markdown 内容。

::: tip
如果想完全脱离 VitePress 主题，在 `Layout` 函数处不要添加 `extends: DefaultTheme`
:::

在模板里可以看到这样两段代码：

```vue
<template v-for="name in Object.keys($slots).filter(name => !usedSlot.includes(name))" :key="name" #[name]="slotData">
  <slot :name="name" v-bind="slotData"></slot>
</template>

<template v-for="(_, name) in $slots" :key="name" #[name]="slotData">
  <slot :name="name" v-bind="slotData"></slot>
</template>
```

* 第一段代码：Teek 不仅自己使用 VitePress 的插槽，同时也允许用户使用 VitePress 的插槽，所以 Teek 先维护了已使用的插槽列表，然后通过了 `v-for` 遍历所有 未使用 VitePress 的插槽，并使用 `#[name]="slotData"` 将插槽内容传递给 VitePress
* 第二段代码：当不使用 Teek 主题时，则加载 VitePress 的默认主题，并使用 `v-for` 遍历所有插槽，将插槽内容传递给 VitePress。

如果不通过 `for` 循环，那么就需要这样写：

```vue
<template>
  <Layout>
    <!-- layout -->
    <template #layout-top>
      <slot name="layout-top" />
    </template>
    <template #layout-bottom>
      <slot name="layout-bottom" />
    </template>
    <!-- navbar -->
    <template #nav-bar-title-before>
      <slot name="nav-bar-title-before" />
    </template>
    <template #nav-bar-title-after>
      <slot name="nav-bar-title-after" />
    </template>
    <template #nav-bar-content-before>
      <slot name="nav-bar-content-before" />
    </template>
    <template #nav-bar-content-after>
      <slot name="nav-bar-content-after" />
    </template>
    <template #nav-screen-content-before>
      <slot name="nav-screen-content-before" />
    </template>
    <template #nav-screen-content-after>
      <slot name="nav-screen-content-after" />
    </template>
    <!-- sidebar -->
    <template #sidebar-nav-before>
      <slot name="sidebar-nav-before" />
    </template>
    <template #sidebar-nav-after>
      <slot name="sidebar-nav-after" />
    </template>
    <!-- page -->
    <template #page-top>
      <slot name="page-top" />
    </template>
    <template #page-bottom>
      <slot name="page-bottom" />
    </template>
    <!-- 404 -->
    <template #not-found>
      <slot name="not-found" />
    </template>
    <!-- home -->
    <template #home-hero-info>
      <slot name="home-hero-info" />
    </template>
    <template #home-hero-image>
      <slot name="home-hero-image" />
    </template>
    <template #home-hero-before>
      <slot name="home-hero-before" />
    </template>
    <template #home-hero-after>
      <slot name="home-hero-after" />
    </template>
    <template #home-features-before>
      <slot name="home-features-before" />
    </template>
    <template #home-features-after>
      <slot name="home-features-after" />
    </template>
    <!-- doc -->
    <template #doc-footer-before>
      <slot name="doc-footer-before" />
    </template>
    <template #doc-before>
      <slot name="doc-before" />
    </template>
    <template #doc-after>
      <slot name="doc-after" />
    </template>
    <template #doc-top>
      <slot name="doc-top" />
    </template>
    <template #doc-bottom>
      <slot name="doc-bottom" />
    </template>
    <!-- aside -->
    <template #aside-top>
      <slot name="aside-top" />
    </template>
    <template #aside-bottom>
      <slot name="aside-bottom" />
    </template>
    <template #aside-outline-before>
      <slot name="aside-outline-before" />
    </template>
    <template #aside-outline-after>
      <slot name="aside-outline-after" />
    </template>
    <template #aside-ads-before>
      <slot name="aside-ads-before" />
    </template>
    <template #aside-ads-after>
      <slot name="aside-ads-after" />
    </template>
  </Layout>
</template>
```

可以看到 VitePress 提供的插槽非常多，这样写起来非常麻烦，因此建议使用 `for` 循环方式。

接下来就可以根据自己的需求来写组件，然后传入 VitePress 的插槽中，Teek 也是在模板里不断补充内容才达到现在的效果。

假设您已经自定义了首页和评论区组件，则需要传入 VitePress 的插槽中，内容如下：

```vue
<script setup lang="ts" name="TeekLayout">
import DefaultTheme from "vitepress/theme";
import { useData } from "vitepress";
import HomePost from "./components/HomePost.vue"; // [!code focus:2]
import Comment from "./components/Comment.vue";

const { Layout } = DefaultTheme;
const { theme } = useData();
const { teekTheme = true } = theme.value;

// 维护已使用的插槽，防止外界传来的插槽覆盖已使用的插槽
const usedSlots = [
  "home-hero-before",
  "nav-bar-content-after",
  // 其他模板里已使用的插槽 ...
];
</script>

<template>
  <!-- 使用 Teek 主题 -->
  <template v-if="teekTheme">
    <Layout>
      <template #home-hero-before>
        <slot name="home-hero-before" />
        <!-- 自定义首页 -->
        // [!code focus:2]
        <HomePost />
      </template>

      <template #doc-after>
        <slot name="doc-after" />
        <!-- 自定义评论区 -->
        // [!code focus:2]
        <Comment />
      </template>

      <!-- 未使用的其他 VP 插槽 -->
      <template
        v-for="name in Object.keys($slots).filter(name => !usedSlots.includes(name))"
        :key="name"
        #[name]="slotData"
      >
        <slot :name="name" v-bind="slotData"></slot>
      </template>
    </Layout>
  </template>

  <template v-else>
    <Layout>
      <template v-for="(_, name) in $slots" :key="name" #[name]="slotData">
        <slot :name="name" v-bind="slotData"></slot>
      </template>
    </Layout>
  </template>
</template>
```

如果您不了解每个插槽分别作用于什么位置，可以在每个插槽里加一段文字如 `<div>${插槽名}</div>`，然后在页面查看输出的内容。

## 引入主题

假设您已经开发好了一个主题，则需要在项目的 `.vitepress/theme/index.ts` 文件中引入，如果没有请按照该路径依次创建。

```ts
import Teek from "vitepress-theme-teek";

export default {
  extends: Teek,
};
```

此时项目成功使用你的自定义主题。

---

---
url: /01.指南/40.开发/01.开发指南.md
---

# 开发指南

## 开发环境

| 类型          | 名称              | 版本             |
| :------------ | :---------------- | :--------------- |
| 操作系统      | Windows 11 专业版 | 26100.3476       |
| 开发工具      | Microsoft VS Code | 1.96.2           |
| 调试工具      | Microsoft Edge    | 134.0.3124.85    |
| 代码版本控制  | Git               | 2.47.0.windows.2 |
| 语言环境      | Node              | 22.12.0          |
| 包管理器      | npm               | 10.9.0           |
| 包管理器      | pnpm              | 9.15.4           |
| node 版本管理 | nvm               | 1.1.12           |
| npm 源管理    | nrm               | 1.2.6            |

## 项目结构

请看 [目录结构](/develop/catalogue)。

## 克隆仓库

```sh
git clone https://github.com/Kele-Bingtang/vitepress-theme-teek.git
```

如果 GitHub 克隆速度较慢，你也可以直接克隆 Gitee 上的镜像仓库，同步可能会存在时差。

```sh
git clone https://gitee.com/kele-bingtang/vitepress-theme-teek.git
```

## 依赖安装

只能用 pnpm 安装依赖。

```sh
pnpm install
```

## Teek 本地包构建

```bash
pnpm to:theme stub
```

## 文档网站预览

```sh
pnpm docs:dev
```

## 代码提交

```sh
pnpm cz # 仅提交本地

pnpm czp # 提交远程
```

::: tip
如果需要分次提交，可以先执行 `git add ./x/x`，再执行 `pnpm run git-cz`，最后执行 `git push origin dev`（或者其他分支）。
:::

## Teek 打包

先构建 build 本地包

```bash
pnpm to:build stub
```

最后执行打包

```sh
pnpm build
```

---

---
url: /daily/开源项目/0_开源项目.md
---

# 开源项目

## 🦄项目列表

| 名称             | 描述                       | 地址                                                         | 简介                                                         |
| ---------------- | -------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| screw            | 数据库表结构文档生成工具   | [screw](https://gitee.com/leshalv/screw)                     | 简洁好用的数据库表结构文档生成工具                           |
| 云笔记觅思文档   | 在线文档系统               | [MrDoc](https://gitee.com/zmister/MrDoc)                     | `MrDoc` 是基于`Python`开发的在线文档系统。MrDoc 适合作为个人和中小型团队的私有云文档、云笔记和知识管理工具，致力于成为优秀的私有化在线文档部署方案。 |
| 云策文档         | 知识管理工具               | [云策文档](https://github.com/fantasticit/think)[在线演示](http://think.kubeservice.cn/) | 云策文档是一款开源知识管理工具。通过独立的知识库空间，结构化地组织在线协作文档，实现知识的积累与沉淀，促进知识的复用与流通。 |
| 毕升Office       | 在线文件服务               | [毕升Office](https://bishengoffice.com/)[说明](https://blog.csdn.net/weixin_35770067/article/details/125610589) | 企业级在线OFFICE，云端网盘多人协作                           |
| liteflow         | 规则引擎                   | [liteflow](https://liteflow.cc/)                             | 轻量，快速，稳定可编排的组件式规则引擎                       |
| FlowLong         | 飞龙工作流                 | [FlowLong](https://gitee.com/aizuda/flowlong/tree/dev/)      | 仿钉钉审批流工具                                             |
| newsnow          | 全网热点聚合               | [newsnow](https://github.com/ourongxing/newsnow)             | NewsNow 是一款开源的网站，可以实时显示微博、知乎、抖音、头条、酷安等平台的每天热门新闻。支持部署到自己的服务器或 Cloudflare Pages 等平台，无需登录或缓存。 |
| gitdiagram       | 生成 GitHub 架构图         | [gitdiagram](https://github.com/ahmedkhaleel2004/gitdiagram) | 将GitHub仓库自动转换为交互式系统架构图，帮助开发者快速理解项目结构。 工具基于AI技术分析代码库，生成包含组件关系的图表，点击图表组件能跳转到对应源文件。 |
| feishu2md        | 飞书文档转换 Markdown 工具 | [feishu2md](https://github.com/Wsine/feishu2md)[介绍](https://blog.csdn.net/qq_39811006/article/details/135902394) | 这是一个下载飞书文档为 Markdown 文件的工具，使用 Go 语言实现。 |
| 云帆在线考试项目 | 在线考试                   | https://gitee.com/davz/yf-exam-lite                          | 云帆开源考试系统是一款基于java+vue、前后端分离、多角色的考试系统。考试系统功能包含用户管理、角色管理、部门管理、题库管理、试题管理、考试管理、试题导入导出、在线考试、错题训练等功能，特点：界面友好，代码规范，考试流程完善。 |

## 🦄AI相关

| 名称                    | 描述               | 地址                                                         | 简介                                                         |
| ----------------------- | ------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| cursor                  | AI编程工具         | [Cursor](https://cursor.com/cn)                              |                                                              |
| claude code             | AI编程工具         | [claude-code](https://docs.anthropic.com/zh-CN/docs/claude-code/overview) |                                                              |
| windsurf                | AI编程工具         | [Windsurf](https://windsurf.com/)                            |                                                              |
| easyAI                  | 人工智能小微模型   | [easyAI](https://gitee.com/dromara/easyAi)                   | 简易快速开发人工智能应用的JAVA引擎                           |
| sayOrder                | 轻量级智能客服助手 | [sayOrder](https://gitee.com/dromara/sayOrder)               | 基于easyAi引擎的JAVA高性能，低成本，轻量级智能客服助手       |
| ai-agents-for-beginners | AI agent入门课     | [ai-agents-for-beginners](https://github.com/microsoft/ai-agents-for-beginners) | 微软出品的AI agent入门课                                     |
| star-vector             | 生成SVG多模态模型  | [star-vector](https://github.com/joanrod/star-vector)        | StarVector 是开源的多模态视觉语言模型，专注于将图像和文本转换为可缩放矢量图形（SVG）代码。 |
| qlib                    | AI量化投资平台     | [qlib](https://github.com/microsoft/qlib)                    | Qlib 是一个面向人工智能的量化投资平台，旨在利用人工智能技术为量化研究赋能，从探索想法到实施制作。Qlib 支持多种 ML 建模范式，包括监督学习、市场动态建模和 RL，现在配备了自动化研发流程的 https://github.com/microsoft/RD-Agent。 |

## 🦄消息工具

| 项目名称     | 描述              | 项目地址                                          | 简介                                                         |
| ------------ | ----------------- | ------------------------------------------------- | ------------------------------------------------------------ |
| sms4j        | 短信发送工具      | [sms4j](https://gitee.com/dromara/sms4j)          | 统一各个厂商的发送方式和功能                                 |
| WePush       | 短信/消息推送工具 | [WePush](https://gitee.com/zhoubochina/WePush)    | 专注批量推送的小而美的工具                                   |
| SmsForwarder | 短信转发器        | [SmsForwarder](https://gitee.com/pp/SmsForwarder) | 监控Android手机短信、来电、APP通知，并根据指定规则转发到其他手机：钉钉群自定义机器人、钉钉企业内机器人、企业微信群机器人、企业微信应用消息、飞书群机器人、飞书企业应用、邮箱、bark、webhook、Telegram机器人、Server酱、PushPlus、手机短信等。 |
| 云湖         | AI社交聊天平台    | [云湖](https://www.yhchat.com/)                   | 云湖集成多种AI助手，支持全平台使用，拥有丰富的机器人生态和兴趣社区，让沟通更智能高效。 |

## 🦄工具类

| 项目名称       | 描述                     | 项目地址                                               | 简介                                                         |
|------------| ------------------------ | ------------------------------------------------------ | ------------------------------------------------------------ |
| yazi       | 终端文件管理器           | [yazi](https://github.com/sxyazi/yazi)[介绍](https://cn.x-cmd.com/pkg/yazi) | yazi 是用 Rust 开发的终端文件管理器，支持预览文本文件、pdf 文件、图像、视频，内置代码高亮功能。在内部，它使用 Tokio 作为其异步运行时，以非阻塞（事件驱动）的方式处理任务，效率高响应快。 |
| SQLynx     | 数据库管理和优化工具     | [SQLynx](https://www.maicongs.com/)                    | **SQLynx** 是一款安全性极高的数据库管理开发工具，支持多数据源管理，包括： MySQL、Oracle、PostgreSQL、Hadoop、SQL Server、MongoDB、达梦（DM）、人大金仓（Kingbase）、OceanBase、openGauss 和 ClickHouse 等。 |
| Pocketbase | 轻量级无代码后端解决方案 | [PocketBase](https://pocketbase.io/)[介绍](https://awesometop.cn/posts/6f4fef6aeb684504b7b911ffa5968162) | Pocketbase 是由 Vasil Valchev 创建的一个开源项目，旨在为开发者提供一个简单易用的后端解决方案。它集成了 RESTful API、文件存储、身份验证等功能，允许用户无需编写大量代码即可快速搭建起完整的 Web 或移动应用程序后端。Pocketbase 的设计理念是尽可能减少配置工作，同时保持足够的灵活性以适应各种应用场景。 |
| Jvedio     | 本地视频管理软件         | [Jvedio](https://github.com/hitchao/Jvedio)            | Jvedio 是本地视频管理软件，支持扫描本地视频并导入软件，建立视频库， 提取出视频的 唯一识别码，自动分类视频， 添加标签管理视频，使用人工智能识别演员，支持翻译信息， 基于 FFmpeg 截取视频图片，Window 桌面端流畅美观的应用软件 |
| HuLa       | 即时通讯桌面应用         | [HuLa](https://github.com/HulaSpark/HuLa)              | HuLa是一款基于Tauri v2+Vue3的跨平台即时通讯桌面应用（不仅仅是即时通讯），兼容Windows、MacOS、Linux、Android、IOS |
| Yearning   | SQL语句审核平台          | [Yearning](https://github.com/cookieY/Yearning)        | Yearning MYSQL SQL语句审核平台。提供查询审计，SQL审核，SQL回滚，自定义工作流等多种功能。 |
| ServBay    | 本地开发环境集成平台          | [ServBay](https://www.servbay.com)              | ServBay 是一个为 Web 开发者精心打造的本地开发环境集成平台。它集成了您日常开发所需的各种工具和组件，例如多种编程语言（PHP, Node.js, Python, Go, Java, .NET, Ruby, Rust 等）、常用的 SQL数据库（MySQL, MariaDB, PostgreSQL, MongoDB 等）、NoSQL数据库（Redis, Memcached 等）、高性能 Web 服务器（Caddy, Nginx, Apache 等）、DNS 服务、邮件服务、SSL 证书服务以及AI/LLM（Ollama 等）。 |
| 快脚本 | Python脚本 | [快脚本](https://www.kuaibote.com/)[视频介绍](https://www.bilibili.com/video/BV1P4gAzZEU8) | 通过AI对话轻松创建Python脚本，快格式自动解析生成可视化配置界面，智能错误检测让脚本运行无忧。 |
| RestCloud | 数据传输平台 | [RestCloud](https://www.etlcloud.cn/helpDocument.html) | ETLCloud数据集成平台是一款基于微服务架构技术开发的先进数据集成解决方案。该平台底层采用纯Java语言构建，并采用了前后端分离的架构设计，为用户提供了直观且高效的操作界面。 |
| wxdump | 微信聊天导出工具 | [wxdump](https://github.com/xaoyaoo/PyWxDump) | 获取微信信息；读取数据库，本地查看聊天记录并导出为csv、html等格式用于AI训练，自动回复等。支持多账户信息获取，支持所有微信版本。 |
| OpenCut | **视频剪辑应用** | [OpenCut](https://github.com/OpenCut-app/OpenCut) | 开源、跨平台、注重用户体验的视频剪辑工具，支持时间轴、多轨编辑、导出高质量 MP4/H264 视频，界面简洁友好 |

## 🦄后台管理系统

https://zhuanlan.zhihu.com/p/461510148

| 名称               | 描述 | 地址                                            | 简介                                                         |
| ------------------ | ---- | ----------------------------------------------- | ------------------------------------------------------------ |
| Vue element admin  |      | https://github.com/PanJiaChen/vue-element-admin | 老牌 admin 后台管理 求稳首选                                 |
| **Antd Pro Vue**   |      | https://pro.antdv.com/                          | **背靠阿里，代码过硬，大型项目首选**                         |
| 卡拉云             |      | https://kalacloud.com/                          | 新一代低代码开发工具，无需部署，无需任何前端技术，随心所欲搭建属于你的 admin 后台 |
| **Vue vben admin** |      | https://github.com/vbenjs/vue-vben-admin        | **宝藏后台管理 基于 Vue3 UI清新 功能扎实**                   |
| **iView Admin**    |      | https://github.com/vbenjs/vue-vben-admin        | **老牌 admin 代码工程化 建立生态 高价高品质**                |
| **D2Admin**        |      | https://github.com/d2-projects/d2-admin         | **趁手 好用 代码工程化 RBAC 权限管理**                       |
| **Naive Ui admin** |      | https://github.com/jekip/naive-ui-admin         | **基于 Vue3 的后台管理新星 适合小项目**                      |
| **mall admin web** |      | https://github.com/macrozheng/mall-admin-web    | **电商类 Vue admin 后台管理**                                |
| Fantastic-admin    |      | https://gitee.com/fantastic-admin/basic         | 一款**开箱即用**的 Vue3 中后台管理系统框架                   |
| onekeyadmin        |      | https://gitee.com/ikam/onekeyadmin              | 基于Thinkphp6+Element的插件化管理系统，网站、小程序、商城、CMS、APP、ERP、API接口一个系统全部搞定，无需脚手架开箱即用！ |
| vben admin         |      | https://doc.vvbin.cn/guide/introduction.html    | 基于 Vue3.0、Vite、 Ant-Design-Vue、TypeScript 的后台解决方案，目标是为开发中大型项目提供开箱即用的解决方案。 |

## 🦄脚本类

| 名称              | 描述         | 地址                                                         | 简介 |
| ----------------- | ------------ | ------------------------------------------------------------ | ---- |
| douyin-downloader | 抖音视频下载 | [douyin-downloader](https://github.com/jiji262/douyin-downloader) |      |

---

---
url: /Linux/Nginx/5_跨域配置与防盗链.md
---

# 跨域配置与防盗链

## 跨域配置

跨域资源共享（CORS）允许浏览器向不同域名的服务器发起请求。Nginx 可以通过设置 HTTP 响应头来实现 CORS 支持，配置示例：

```sh
server {
    listen 80;
    server_name example.com;

    location / {
        # 允许所有域名访问
        add_header 'Access-Control-Allow-Origin' '*';

        # 允许的请求方法
        add_header 'Access-Control-Allow-Methods' 'GET, POST, OPTIONS';

        # 允许的请求头
        add_header 'Access-Control-Allow-Headers' 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range';

        # 预检请求缓存时间
        add_header 'Access-Control-Max-Age' 1728000;

        # 允许携带凭证（如 Cookie）
        add_header 'Access-Control-Allow-Credentials' 'true';

        # 处理 OPTIONS 预检请求
        if ($request_method = 'OPTIONS') {
            return 204;
        }

        # 其他代理配置
        proxy_pass http://backend;
    }
}
```

### 关键配置项

* **`Access-Control-Allow-Origin`**：允许访问的域名，`*` 表示允许所有域名。
* **`Access-Control-Allow-Methods`**：允许的 HTTP 方法（如 GET、POST）。
* **`Access-Control-Allow-Headers`**：允许的请求头。
* **`Access-Control-Max-Age`**：预检请求的缓存时间（秒）。
* **`Access-Control-Allow-Credentials`**：是否允许携带凭证（如 Cookie）。

### 注意事项

如果允许特定域名访问，可以将 `*` 替换为具体的域名，例如：

```sh
add_header 'Access-Control-Allow-Origin' 'https://example.com';
```

> 如果需要支持带凭证的请求（如 Cookie），`Access-Control-Allow-Origin` 不能为 `*`，必须指定具体域名。

## 防盗链配置

防盗链（Hotlink Protection）用于防止其他网站直接引用你的资源（如图片、视频），从而减少带宽消耗和资源滥用。

```sh
server {
    listen 80;
    server_name example.com;

    location ~* \.(jpg|jpeg|png|gif|mp4)$ {
        # 允许空 Referer（直接访问）和本站域名访问
        valid_referers none blocked example.com *.example.com;

        # 如果 Referer 不合法，返回 403 或重定向到其他图片
        if ($invalid_referer) {
            return 403;
            # 或者重定向到一张提示图片
            # rewrite ^/.*$ /anti-hotlink.jpg break;
        }

        # 其他代理配置
        proxy_pass http://backend;
    }
}
```

### 关键配置项

* valid\_referers：定义合法的 Referer 来源：
  * `none`：允许空 Referer（直接访问）。
  * `blocked`：允许没有协议头（如 `http://` 或 `https://`）的 Referer。
  * `example.com`：允许指定域名访问。
  * `*.example.com`：允许子域名访问。
* `$invalid_referer`：如果 Referer 不合法，返回 403 或重定向。
* 防盗链配置通常用于静态资源（如图片、视频）。

---

---
url: /01.指南/01.简介/10.快速开始.md
description: Teek 是一个基于 VitePress 构建的主题，本文专门介绍如何快速安装 Teek。
---

# 快速开始 推荐

## 版本

[![Teek badge](https://img.shields.io/npm/v/vitepress-theme-teek.svg?style=flat-square)](https://www.npmjs.org/package/vitepress-theme-teek)

建议使用如下包管理器安装 `vitepress-theme-teek`：

* [pnpm](https://pnpm.io/)
* [yarn](https://classic.yarnpkg.com/lang/en/)
* [npm](https://www.npmjs.com/)

## 前言

如果你想快速构建一个和 Teek 文档类似的项目，可以直接拉取 [Teek 文档模板仓库](https://github.com/Kele-Bingtang/vitepress-theme-teek-docs-template)。

```sh
git clone https://github.com/Kele-Bingtang/vitepress-theme-teek-docs-template.git
```

如果你更喜欢从零开始慢慢研究 Teek，则可以按照下面的在线安装步骤构建并逐步丰富您的项目。

## VitePress 安装

有关 VitePress 的安装教程来源于 [VitePress 文档](https://vitepress.dev/zh/guide/getting-started)。如果安装失败，请阅读 VitePress 文档查看最新的安装教程。

::: code-group

```sh [pnpm]
pnpm add -D vitepress
```

```sh [yarn]
yarn add -D vitepress
```

```sh [npm]
npm add -D vitepress
```

:::

VitePress 附带一个命令行设置向导，可以帮助你构建一个基本项目。安装后，通过运行以下命令启动向导：

::: code-group

```sh [pnpm]
pnpm vitepress init
```

```sh [yarn]
yarn vitepress init
```

```sh [npm]
npx vitepress init
```

:::

将需要回答几个简单的问题：

```sh
┌ Welcome to VitePress!
│
◇ Where should VitePress initialize the config?
│ ./docs
│
◇ Where should VitePress look for your markdown files?
│ ./docs
│
◇ Site title:
│ My Awesome Project
│
◇ Site description:
│ A VitePress Site
│
◇ Theme:
│ Default Theme
│
◇ Use TypeScript for config and theme files?
│ Yes
│
◇ Add VitePress npm scripts to package.json?
│ Yes
│
◇ Add a prefix for VitePress npm scripts?
│ Yes
│
◇ Prefix for VitePress npm scripts:
│ docs
│
└ Done! Now run pnpm run docs:dev and start writing.
```

## Teek 在线安装

::: code-group

```sh [pnpm]
pnpm install vitepress-theme-teek -D
```

```sh [yarn]
yarn add vitepress-theme-teek -D
```

```sh [npm]
npm install vitepress-theme-teek -D
```

:::

## Teek 引入

根据 VitePress 的要求，需要在 `.vitepress/theme/index.ts` 文件中引入 Teek 主题。如果没有该路径，需要先创建它：

```ts
// .vitepress/theme/index.ts
import Teek from "vitepress-theme-teek";
import "vitepress-theme-teek/index.css";

export default {
  extends: Teek,
};
```

然后在 `.vitepress/config.mts` 文件中引入 Teek 的配置信息：

```ts
// .vitepress/config.mts
import { defineConfig } from "vitepress";
import { defineTeekConfig } from "vitepress-theme-teek/config";

// Teek 主题配置
const teekConfig = defineTeekConfig({});

// VitePress 配置
export default defineConfig({
  extends: teekConfig,
  // ...
});
```

有关 Teek 更多的配置信息，请从 [配置简介](/reference/config) 开始阅读。

## 启动运行

请查看你的 `package.json` 文件，确保存在下面 npm 脚本：

```json
{
  "scripts": {
    "docs:dev": "vitepress dev docs",
    "docs:build": "vitepress build docs",
    "docs:preview": "vitepress preview docs"
  }
}
```

`docs:dev` 脚本将启动具有即时热更新的本地开发服务器。使用以下命令运行它：

::: code-group

```sh [pnpm]
pnpm run docs:dev
```

```sh [yarn]
yarn docs:dev
```

```sh [npm]
npm run docs:dev
```

:::

`vitepress dev docs` 的 `docs` 并不是固定写死的，有三种场景可以进行选择：

* 如果 `.vitepress` 和 Markdown 文档在项目根目录下，则 `vitepress dev docs` 改为 `vitepress dev`
* 如果 `.vitepress` 和 Markdown 文档在项目 `src` 目录下，则 `vitepress dev docs` 改为 `vitepress dev src`
* 如果 `.vitepress` 在项目根目录下，Markdown 文档放在 `src` 目录下，则 `vitepress dev docs` 改为 `vitepress dev`，且需要在 `.vitepress/config.mts` 里配置 `srcDir: src`，`srcDir` 的作用请看 [VitePress - srcDir](https://vitepress.dev/zh/reference/site-config#srcdir)

总结：VitePress 以 `.vitepress` 所在的目录层级 + `srcDir` 为参照逐层对 Markdown 文档扫描解析。

## 版本更新

Teek 不定期提供新特性或者修复 Bug，届时只需要更新版本号即可：

::: code-group

```sh [pnpm]
pnpm add vitepress-theme-teek@latest -D
```

```sh [yarn]
yarn add vitepress-theme-teek@latest -D
```

```sh [npm]
npm add vitepress-theme-teek@latest -D
```

:::

---

---
url: /StableDiffusion/Midjourney/6_扩展图片ZoomOut功能.md
---

# 扩展图片ZoomOut功能

---

---
url: /常用框架/SpringBoot/SpringBoot与Web应用/7_拦截器.md
---

# 拦截器

---

---
url: /Java/JVM性能调优/01.JVM概念/1_类加载器.md
---

# 类加载器

## 什么是类加载器？

类加载器是一个用来加载类文件的类。Java 源代码通过 javac 编译器编译成类 文件。然后 JVM 来执行类文件中的字节码来执行程序。类加载器负责加载文件 系统、网络或其他来源的类文件。

## 类加载器有几种类型？

JVM在运行时会产生三个ClassLoader：Bootstrap (启动类加载器)、ExtensionClassLoader（扩展类装载器）和AppClassLoader（系统类装载器）。

1. 引导类加载器（BootstrapClassloader）：用C++编写，是JVM自带的类加载器；负责加载Java的核心类库。（该加载器无法直接获取）
2. 扩展类加载器（ExtClassloader）：负责加载/jre/lib/ext目录下的jar包。
3. 应用程序类加载器（AppClassloader）：负责加载java -classpath或-D java.class.path所指的目录下的类与jar包。（最常用的加载器）

## 自定义类加载器

上述3个是JVM在运行时会产生三个类加载器，除此之外还有第四种就是自定义类加载器，那么为什么需要自定义类加载器？又在什么情况下用到？

用户在需要的情况下，可以实现自己的自定义类加载器，一般而言，在以下几种情况下需要自定义类加载器：

1. 隔离加载类：某些框架为了实现中间件和应用程序的模块的隔离，就需要中间件和应用程序使用不同的类加载器，例如Tomcat，Tomcat可以运行多个应用程序，每个应用程序之间是相互隔离的。
2. 修改类加载的方式：类加载的双亲委派模型并不是强制的，用户可以根据需要在某个时间点动态加载类；
3. 扩展类加载源：例如从数据库、网络进行类加载；
4. 防止源代码泄露：Java代码很容易被反编译和篡改，为了防止源码泄露，可以对类的字节码文件进行加密，并编写自定义的类加载器来加载自己的应用程序的类。

**实现自定义类加载器**

若要实现自定义类加载器，只需要继承 `java.lang.ClassLoader` 类,并且重写其`findClass()`方法即可。

java.lang.ClassLoader类的基本职责就是根据一个指定的类的名称， 找到或者生成其对应的字节代码，然后从这些字节代码中生成一个`java.lang.Class` 实例。ClassLoader的核心方法如下:

1. getParent()：返回该类加载器的父类加载器。
2. loadClass(String name)：加载名称为二进制名称为name的类，返回的结果是java.lang.Class类的实例。
3. findClass(String name)：查找名称为name的类，返回的结果是java.lang.Class类的实例。
4. findLoadedClass(String name)：查找名称为name的已经被加载过的类，返回的结果是java.lang.Class类的实例。

## 双亲委派机制

1. 类加载器接收到一个加载请求时，他会委派给他的父加载器，实际上是去他父加载器的缓存中去查找是否有该类，如果有就加载返回，如果没有则继续委派给父类加载，直到顶层类加载器。
2. 如果顶层类加载器也没有加载该类，则会依次向下查找子加载器的加载路径，如果有就加载返回，如果都没有，则会抛出异常。

## 沙箱安全机制

了解

## 描述一下JVM加载class文件的原理机制？

JVM中类的装载是由类加载器（ClassLoader）和它的子类来实现的，Java中的类加载器是一个重要的Java运行时系统组件，它负责在运行时查找和装入类文件中的类。

由于Java的跨平台性，经过编译的Java源程序并不是一个可执行程序，而是一个或多个类文件。当Java程序需要使用某个类时，JVM会确保这个类已经被加载、连接（验证、准备和解析）和初始化。

类的加载是指把类的.class文件中的数据读入到内存中，通常是创建一个字节数组读入.class文件，然后产生与所加载类对应的Class对象。加载完成后，Class对象还不完整，所以此时的类还不可用。当类被加载后就进入连接阶段，这一阶段包括验证、准备（为静态变量分配内存并设置默认的初始值）和解析（将符号引用替换为直接引用）三个步骤。

最后JVM对类进行初始化，包括：1)如果类存在直接的父类并且这个类还没有被初始化，那么就先初始化父类；2)如果类中存在初始化语句，就依次执行这些初始化语句。 类的加载是由类加载器完成的，类加载器包括：根加载器（BootStrap）、扩展加载器（Extension）、系统加载器（System）和用户自定义类加载器（java.lang.ClassLoader的子类）。从Java 2（JDK 1.2）开始，类加载过程采取了父亲委托机制（PDM）。PDM更好的保证了Java平台的安全性，在该机制中，JVM自带的Bootstrap是根加载器，其他的加载器都有且仅有一个父类加载器。类的加载首先请求父类加载器加载，父类加载器无能为力时才由其子类加载器自行加载。

JVM不会向Java程序提供对Bootstrap的引用。

## 什么是 tomcat 类加载机制？

在 tomcat 中类的加载稍有不同，如下图：

![classLoader001](/assets/classLoader001.D3MPshLS.jpg)

当 tomcat启动时，会创建几种类加载器： Bootstrap 引导类加载器 加载 JVM启动所需的类，以及标准扩展类（位于jre/lib/ext 下） System 系统类加载器 加载 tomcat 启动的类，比如bootstrap.jar，通常在 catalina.bat 或者 catalina.sh 中指定。位于CATALINA\_HOME/bin 下。

---

---
url: /daily/高等数学/07_零点问题与微分不等式.md
---

# 零点问题与微分不等式

---

---
url: /01.指南/20.相关/05.路由钩子.md
---

# 路由钩子

VitePress 提供的 `useRouter` 有 4 个路由钩子，分别为：

* `onBeforeRouteChange`：路由变化前触发，如果在该钩子函数中返回 `false`，则不会进行路由跳转
* `onBeforePageLoad`：页面加载前执行，在 `onBeforeRouteChange` 之后触发，如果在该钩子函数中返回 `false`，则不会进行路由跳转
* `onAfterPageLoad`：页面加载后执行
* `onAfterRouteChange`：路由变化后触发，在 `onAfterPageLoad` 之后触发

Teek 内置的 4 个评论区组件使用了 `onAfterRouteChange` 钩子函数，且 `vitepress-plugin-permalink` 插件分别使用了 `onBeforeRouteChange` 和 `onAfterRouteChange` 两个路由钩子。

如果您也需要使用这些路由钩子，请不要直接这样使用：

```ts
router.onAfterRouteChange = (href: string) => {
  // 你的逻辑
};
```

`onAfterRouteChange` 是一个函数，您这样使用将会 **覆盖** Teek 在该钩子函数的逻辑，因此您需要这样使用：

```vue
<script setup lang="ts">
import { useRouter, useData } from "vitepress";

const router = useRouter();
const state = router.state || {};
const stateKey = "xx";

// 防止重复在 router 添加函数
if (!state[stateKey]) {
  const selfOnAfterRouteChange = router.onAfterRouteChange;

  router.onAfterRouteChange = async (href: string) => {
    // 调用可能已有的 onAfterRouteChange
    await selfOnAfterRouteChange?.(href);

    // 调用自己的函数
    myFunction();
  };

  router.state = { ...state, [stateKey]: true };
}

const myFunction = () => {
  /* */
};
</script>
```

`onBeforeRouteChange` 支持返回 false 来阻止路由跳转，因此请这样使用：

```vue
<script setup lang="ts">
import { useRouter, useData } from "vitepress";

const router = useRouter();
const state = router.state || {};
const stateKey = "xx";

// 防止重复在 router 添加函数
if (!state[stateKey]) {
  const selfOnAfterRouteChange = router.onAfterRouteChange;

  router.onBeforeRouteChange = async (href: string) => {
    // 调用可能已有的 onAfterRouteChange
    const selfResult = await selfOnBeforeRouteChange?.(href);
    if (selfResult === false) return false;

    // 调用自己的函数
    myFunction();
  };

  router.state = { ...state, [stateKey]: true };
}

const myFunction = () => {
  /* */
};
</script>
```

## useVpRouter

针对上面较为复杂的配置，Teek 已经封装了 Composables 函数 `useVpRouter`，该函数对 VitePress 的 `router` 钩子进行封装，因此您可以这样使用：

```vue
<script setup lang="ts">
import { useVpRouter } from "vitepress-theme-teek";

const vpRouter = useVpRouter();

vpRouter.bindAfterRouteChange("xx", () => {
  // 调用自己的函数
  myFunction();
});

const myFunction = () => {
  /* */
};
</script>
```

如果您想一次性绑定多个 `router` 钩子，可以这样使用：

```vue
<script setup lang="ts">
import { useVpRouter } from "vitepress-theme-teek";

const vpRouter = useVpRouter();

vpRouter.bindRouterFn("xx", router => {
  router.onBeforeRouteChange = async (href: string) => {
    // 调用可能已有的 onAfterRouteChange
    const selfResult = await selfOnBeforeRouteChange?.(href);
    if (selfResult === false) return false;

    // 调用自己的函数
    myFunction();
  };

  router.onAfterRouteChange = async (href: string) => {
    // 调用可能已有的 onAfterRouteChange
    await selfOnAfterRouteChange?.(href);

    // 调用自己的函数
    myFunction();
  };
});

const myFunction = () => {
  /* */
};
</script>
```

---

---
url: /Java/系统优化/系统优化/1_幂等设计.md
---

# 幂等设计

## 一、什么是幂等性？

在编程场景指的是：使用相同参数来调用同一接口，调用多次的结果跟单次产生的结果是一致的。

企业级项目的一些关键接口都需要幂等设计，比如支付扣款、发货等等。

设想一下，因为网络问题我们调用扣款接口超时了，并且没有进行重试，这样有可能给用户发货了，但是实际没扣款。因此这种情况下通常要重试扣款。但是如果重试了，假设之前超时的那次调用实际是成功了，只是响应结果的时候接口超时了，这样不是重复扣两次款了？

如果你是那个用户，买一个东西，平台竟然扣了你两次钱，你会怎么样？抓狂啊！投诉啊！

那如果我们是平台会怎样？一直被人投诉，渐渐流失用户，然后倒闭。。。

所以幂等设计在一些必须要保证业务一致性的情况下，非常关键，因为这种场景往往需要重试，重试就需要幂等。

当然，还是一些情况是用户误触的，比如多次点击按钮导致多次提交等。

需要注意，虽然前端可以通过将按钮置灰防止重复点击，但是纯前端无法完美实现幂等性！比如前端调用后端接口超时，有可能后端已经存储了数据，此时前端的按钮已经可点击，用户再次点击就会生成两条数据。

## 二、方案选型

业界主流的幂等性设计主要有以下几种：

### 1、数据库唯一索引

利用数据库唯一索引的一致性保证幂等性。

比如将数据库订单表中的订单号字段配置成唯一索引，用户生成订单会执行 insert 语句，MySQL 根据唯一索引天然阻止相同订单号数据的插入，我们可以 catch 往报错，让接口正常返回插入成功结果。

```java
try {
  insertOrder({id: xxx});  
} catch(DuplicateKeyException e) {
  return true;
}
```

对应的订单插入语句为：

```java
INSERT INTO order (id, orderNo, xx, updateTime) 
VALUES (1, 2, 3, "2024-05-28 15:55:18")
ON DUPLICATE KEY UPDATE updateTime = now();
```

这样同一笔订单，不论调用几次，结果都不会新增重复的订单记录。

### 2、数据库乐观锁

利用乐观锁在某些场景下也能实现幂等性。

比如需要对一个配置进行修改，同时记录修改的时间、旧配置、新配置、操作人等日志信息到操作记录表中，方便后面追溯。

```java
// update sys_config set config = "a" where id = 1;
updateConfig(); 

// insert opreation_log (createTime, oldConfig, newConfig, userId)
// value("2024-05-28 15:55:18", "b", "a", 1L)
addOpreationLog(); 
```

这个场景就很适合采用乐观锁来实现幂等。

乐观锁并不是真的加锁，而是可以给配置表加一个 version 版本号字段，每次修改需要验证版本号是否等于修改前的（没被别人同时修改），然后才能给版本号加 1。

如果配置表修改成功（通过影响行数来判断 1 表示成功，0 表示失败），才能添加操作日志。

因此，进行如下改造：

```java
// update sys_config set config = "a", version = version + 1 
//   where id = 1 and version = 1;
int updateEffect = updateConfig(); 

// insert opreation_log (createTime, oldConfig, newConfig, userId)
//               value("2024-05-28 15:55:18", "b", "a", 1L)
if (updateEffect == 1) {
    addOpreationLog();
}
```

### 3、天然幂等操作

比如一些 delete 操作，这种是天然幂等的，因为删除一次和多次都是一样的。

还有一些更新操作，例如 :

```sql
update sys_config set config = "a" where id = 1;
```

这样的 SQL 不论执行几遍，结果都是一样的。

如果接口里面仅包含上述的这些天然幂等的行为，那么对外就可以标记当前接口为幂等接口，不需要任何其他操作。

### 4、分布式锁

导致数据错乱的元凶很多时候都是“并发修改”。

很多时候的业务场景是这样的：

```sql
1、查找数据
2、if (不包含这个数据) {
  3、插入这条数据
}
```

在没有并发的情况下，这样的逻辑没任何问题，但是一旦出现并发，就会导致数据不一致的情况。

因为同时可能出现多个线程在同一时刻到达第 2 步的判断，这时候其实数据都没有插入，因此它们都能通过这个判断到达第 3 步，这就导致重复插入一样的数据。

针对这种场景，可以上一把分布式锁，杜绝了并发问题。

```sql
分布式锁 {
  1、查找数据
  2、if (不包含这个数据) {
    3、插入这条数据
    }
}
```

多个线程需要先抢占锁才能进行后续的业务操作，因此 1、2、3 这三个步骤在同一时刻仅会有一个线程执行，所以不会存在数据不一致的情况。

也因此加了锁之后，这个业务代码可以进行多次调用，因为除了第一次的调用，后续通过 if 判断，都不会插入数据。

### 几种方案比较

可以思考下，用户保存的场景更适合哪个方案？

1）数据库唯一索引，合适。（因为本来用的数据库就支持唯一 id，实现成本低）

2）数据库乐观锁，不合适。（因为是插入一条新记录，版本号没有地方可以设置，也不需要更新版本）

3）天然幂等操作，不合适。（因为是插入，不是删除和修改）

4）分布式锁，合适。

## 三、方案设计（数据库唯一索引）

### 业务流程

唯一索引大家应该很熟悉，但是给数据库表的哪个字段添加唯一索引呢？

id 一般是首要选择，因为本来就是唯一的。但由于是插入新数据，id 还没有生成，怎么办？

那我们造一个字段来作为唯一索引就好了！

1. 前端进入页面的时候，请求后端返回一个全局唯一 id；
2. 用户提交的时候，前端不仅提交用户保存的信息，同时也需要带上这个全局唯一 id；
3. 后端可以将这个全局唯一 id 保存到数据库的某个唯一索引字段，利用数据库实现幂等性。

这样改造后，每个用户提交保存时，即使提交多次，也能避免多条记录的产生。

### 唯一 id 生成

再来思考下，怎么获取到唯一 id？以及要将唯一 id 保存到数据库的哪个字段呢？

第一种方案是：生成随机 UUID 字符串，并且在数据库中新增一列唯一索引存储 UUID。

但其实没必要新增一列，因为表里面的主键本身就是唯一的，所以可以复用主键来进行唯一性判断。因为主键的类型是 bigint，所以只需要更换唯一 id 生成的策略，使用雪花算法来生成分布式全局唯一的自增 id 即可。

雪花算法的学习和实现成本也不高，可以使用 Hutool 工具类提供的 IdUtil 工具类来基于雪花算法生成 id。

```java
IdUtil.getSnowflakeNextId()
```

雪花算法原理图如下：

核心是通过时间戳（毫秒）保证递增，通过机器 id 、服务 id 和递增序号（同一毫秒内递增），保证唯一性。

## 四、源码开发

### 复用主键方案（推荐）

1）在 UserAnswerController 下新增生成 id 的接口：

```java
@GetMapping("/generate/id")
public BaseResponse<Long> generateUserAnswerId() {
    return ResultUtils.success(IdUtil.getSnowflakeNextId());
}
```

这个接口可能是能够复用的，也可以把它放到单独的 Controller 中。

2）修改创建 userAnswer 的验证规则，补充校验 id 的逻辑

代码如下：

```java
@Override
public void validUserAnswer(UserAnswer userAnswer, boolean add) {
    ThrowUtils.throwIf(userAnswer == null, ErrorCode.PARAMS_ERROR);
    // 从对象中取值
    Long appId = userAnswer.getAppId();
    Long id = userAnswer.getId();
    // 创建数据时，参数不能为空
    if (add) {
        // 补充校验规则
        ThrowUtils.throwIf(appId == null || appId <= 0, ErrorCode.PARAMS_ERROR, "appId 非法");
        ThrowUtils.throwIf(id == null || id <= 0, ErrorCode.PARAMS_ERROR, "id 不存在");
    }
    ...
}
```

4）提交用户答案的请求参数补充 id 字段，代码如下：

```java
@Data
public class UserAnswerAddRequest implements Serializable {
    /**
     * id
     */
    private Long id;
    
    /**
     * 应用 id
     */
    private Long appId;

    /**
     * 用户答案（JSON 数组）
     */
    private List<String> choices;

    private static final long serialVersionUID = 1L;
}
```

5）修改提交用户答案接口，将 id 填充到回答对象中并插入到数据库（默认会通过 BeanUtils.copyProperties 填充）。并且捕获主键重复的异常，直接忽略即可。

代码如下：

```java
// 写入数据库
try {
    boolean result = userAnswerService.save(userAnswer);
    ThrowUtils.throwIf(!result, ErrorCode.OPERATION_ERROR);
} catch (DuplicateKeyException e) {
    // ignore error
}
```

### 新增 UUID 字段方案（扩展）

1）修改 user\_answer 表，增加 serialNumber

SQL：

```java
alter table user_answer_0
    add serialNumber varchar(64) null comment '用户答题流水号' after userId;

create unique index serialNumber_unidx
    on user_answer_0 (serialNumber);

alter table user_answer_1
    add serialNumber varchar(64) null comment '用户答题流水号' after userId;

create unique index serialNumber_unidx
    on user_answer_1 (serialNumber);
```

问：之前的数据怎么办？之前用户提交结果没有流水号，那不是 NULL 了吗，跟唯一索引不就冲突了吗？

答：不是的，NULL 的含义是未知，未知跟未知不冲突，所以唯一索引允许 NULL 的存在。

2）改造用户获取题目接口，返回本次生成的 serialNumber ，利用 UUID 随机生成。

QuestionVO 新增流水号字段：

```java
/**
 * 答题流水号
 */
private String serialNumber;
```

QuestionServiceImpl#getQuestionVO 改造：

```java
@Override
public QuestionVO getQuestionVO(Question question, HttpServletRequest request) {
    // 对象转封装类
    QuestionVO questionVO = QsuestionVO.objToVo(question);

    // 返回流水号
    questionVO.setSerialNumber(UUID.randomUUID().toString());
    
    // 可以根据需要为封装对象补充值，不需要的内容可以删除
    // region 可选
    // 1. 关联查询用户信息
    Long userId = question.getUserId();
    User user = null;
    if (userId != null && userId > 0) {
        user = userService.getById(userId);
    }
    UserVO userVO = userService.getUserVO(user);
    questionVO.setUser(userVO);
    // endregion

    return questionVO;
}
```

3）改造用户提交答案接口，接收 serialNumber，并利用数据库唯一索引校验防止重复提交

UserAnswerAddRequest 新增流水号字段：

```java
    /**
     * 答题流水号
     */
    private String serialNumber;
```

UserAnswer 新增流水号字段：

```java
   /**
     * 答题流水号
     */
    private String serialNumber;
```

修改校验数据代码 UserAnswerServiceImpl#validUserAnswer，serialNumber 为必填

```java
    @Override
    public void validUserAnswer(UserAnswer userAnswer, boolean add) {
        ThrowUtils.throwIf(userAnswer == null, ErrorCode.PARAMS_ERROR);
        // 从对象中取值
        Long appId = userAnswer.getAppId();
        String serialNumber = userAnswer.getSerialNumber();
        
        // 创建数据时，参数不能为空
        if (add) {
            // 补充校验规则
            ThrowUtils.throwIf(appId == null || appId <= 0, ErrorCode.PARAMS_ERROR, "appId 非法");
            // 幂等判断，serialNumber 必填
            ThrowUtils.throwIf(StrUtil.isBlank(serialNumber) , ErrorCode.PARAMS_ERROR, "serialNumber 不存在");
        }
        // 修改数据时，有参数则校验
        // 补充校验规则
        if (appId != null) {
            App app = appService.getById(appId);
            ThrowUtils.throwIf(app == null, ErrorCode.PARAMS_ERROR, "应用不存在");
        }
    }
```

修改创建用户答案接口 UserAnswerController#addUserAnswer

```java
@PostMapping("/add")
public BaseResponse<Long> addUserAnswer(@RequestBody UserAnswerAddRequest userAnswerAddRequest, HttpServletRequest request) {
    ThrowUtils.throwIf(userAnswerAddRequest == null, ErrorCode.PARAMS_ERROR);
    // 在此处将实体类和 DTO 进行转换
    UserAnswer userAnswer = new UserAnswer();
    BeanUtils.copyProperties(userAnswerAddRequest, userAnswer);
    List<String> choices = userAnswerAddRequest.getChoices();
    userAnswer.setChoices(JSONUtil.toJsonStr(choices));
    // 数据校验
    userAnswerService.validUserAnswer(userAnswer, true);
    // 判断 app 是否存在
    Long appId = userAnswerAddRequest.getAppId();
    App app = appService.getById(appId);
    ThrowUtils.throwIf(app == null, ErrorCode.NOT_FOUND_ERROR);
    if (!ReviewStatusEnum.PASS.equals(ReviewStatusEnum.getEnumByValue(app.getReviewStatus()))) {
        throw new BusinessException(ErrorCode.NO_AUTH_ERROR, "应用未通过审核，无法答题");
    }
    // 填充默认值
    User loginUser = userService.getLoginUser(request);
    userAnswer.setUserId(loginUser.getId());

    long newUserAnswerId;

    // 写入数据库
    try {
        boolean result = userAnswerService.save(userAnswer);
        ThrowUtils.throwIf(!result, ErrorCode.OPERATION_ERROR);
        // 返回新写入的数据 id
        newUserAnswerId = userAnswer.getId();
    } catch (DuplicateKeyException e) {
        //ignore error 获取之前数据 id
        UserAnswer answer = userAnswerService.getOne(Wrappers.lambdaQuery(UserAnswer.class)
                .select(UserAnswer::getId)
                .eq(UserAnswer::getSerialNumber, userAnswer.getSerialNumber())
                .eq(UserAnswer::getAppId, userAnswer.getAppId())
                .eq(UserAnswer::getUserId, userAnswer.getUserId()));
        newUserAnswerId = answer.getId();
    }

    // 调用评分模块
    try {
        UserAnswer userAnswerWithResult = scoringStrategyExecutor.doScore(choices, app);
        userAnswerWithResult.setId(newUserAnswerId);
        userAnswerService.updateById(userAnswerWithResult);
    } catch (Exception e) {
        e.printStackTrace();
        throw new BusinessException(ErrorCode.OPERATION_ERROR, "评分错误");
    }
    return ResultUtils.success(newUserAnswerId);
}
```

---

---
url: /daily/面试专栏/面试专栏.md
---

# 面试专栏

https://juejin.cn/post/7278245697174093858

https://blog.csdn.net/sinat\_40770656/article/details/132895641

http://wed.xjx100.cn/news/204272.html

---

---
url: /daily/软件设计师/07_面向对象技术.md
---

# 面向对象技术

## 一、面向对象基础

面向对象的编程范式在上世纪六十年代末和七十年代初逐渐形成，并在八十年代得到了广泛应用。它的背景可以追溯到软件开发领域的一些问题和挑战。

在早期，软件开发主要采用的是过程式编程，这种编程方式将程序分解为一系列的过程或函数来完成特定的任务。然而，随着软件系统的不断增大和复杂化，过程式编程面临着一些挑战。

面向对象的编程范式的出现正是为了解决这些问题。它将程序中的数据和行为封装为对象，通过对象之间的交互来完成任务。

| 缺点                                                         | 解决方法                                                     |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| 缺乏模块化和可重用性：过程式编程往往将功能代码组织为一系列过程和函数，但随着系统的复杂性增加，这些过程和函数之间的关系变得难以管理。代码的重复编写也导致了效率低下和维护困难。 | 面向对象编程通过将功能封装在类中，使得代码更模块化，每个类负责特定的功能。通过类之间的关联和交互，实现代码的可重用性。同时，继承和多态的机制还进一步提供了代码重用的能力。 |
| 难以处理复杂关系和状态：传统的过程式编程很难处理对象之间的复杂关系和状态变化。对于一些复杂的业务问题，过程式编程往往需要大量的代码来维护对象之间的状态，导致代码的可读性和可维护性降低。 | 面向对象编程通过将对象的数据和行为封装在一起，保证了对象状态的一致性和完整性。通过类之间的关系和交互，可以更好地处理对象之间复杂的关系和状态变化。 |
| 缺乏抽象和封装：过程式编程往往缺乏对问题领域的抽象，代码的可读性差。同时，过程式编程也缺乏对数据和行为进行封装的机制，导致代码容易受到外部的影响。 | 面向对象编程通过类的定义和对象的创建，提供了对问题领域的抽象和建模。同时，封装机制将数据和行为封装在类中，隐藏了内部实现细节，提高了代码的可读性和可维护性。 |

### 1.相关概念

#### 🦋1.1 对象

对象是基本的运行实体，它是类的一个实例。一个对象封装了数据和行为的整体，代表了现实世界中的具体事物，例如学生、汽车等。对象具有明确的边界，定义了自己的行为，并且可以根据需要进行扩展。

举例来说，我们可以创建一个名为"Student"的类，然后通过该类来实例化不同的学生对象。每个学生对象都会有自己的属性（例如姓名、年龄、学号等）和行为（例如上课、考试、提交作业等）。这些属性和行为的封装在对象内部，外部的用户只能通过暴露的接口来访问和操作。这样，每个学生对象都具有清晰的边界和良好定义的行为。

另一个例子是汽车。我们可以创建一个名为"Car"的类，然后通过该类来实例化不同的汽车对象。每辆汽车对象都会有自己的属性（例如品牌、型号、颜色等）和行为（例如启动、加速、刹车等）。这些属性和行为的封装在对象内部，外部的用户只能通过暴露的接口来与汽车对象进行交互。每辆汽车对象都具有清晰的边界和良好定义的行为。

通过对象的封装和可扩展性，我们可以更好地管理和操作这些真实存在的实体，使程序代码更具可读性、可维护性和可扩展性。

#### 🦋1.2 消息

消息是对象之间进行通信的一种构造。类是对象的抽象，它定义了一组大体相似的对象结构，包括实体类、边界类和控制类。实体类用于对必须存储的信息和相关行为建模，它是需要长久保存且一直存在的类。边界类用于系统内部与系统外部的业务主角之间进行交互建模。控制类用于对一个或几个用例所特有的控制行为进行建模，它表示在用例执行过程中被动出现的特定行为。

举例说明：

假设我们要设计一个图书馆管理系统，可以有以下类别：

* 实体类：Book（书籍类，包括属性如书名、作者和出版日期等，以及行为如借阅和归还）、Library（图书馆类，包括属性如馆藏书籍和开放时间等，以及行为如添加书籍和借出书籍）；
* 边界类：User（用户类，包括属性如用户名和密码等，以及行为如登录和查看借阅历史）、UI（用户界面类，用于与用户进行交互，包括显示图书馆的书籍列表和接收用户的操作）；
* 控制类：Loan（借阅类，用于处理借阅过程中的控制行为，如检查是否有库存、记录借阅历史）、Authentication（认证类，用于处理用户登录的控制行为）。

这些类别之间可以通过消息进行通信。例如，当用户登录时，UI类会向Authentication类发送登录请求消息；当图书馆添加新书时，Library类会向Book类发送添加书籍消息。这样，不同类之间的通信构成了整个图书馆管理系统的功能。

#### 🦋1.3 继承

继承是一种机制，它允许父类和子类之间共享数据和方法。继承是类之间的一种关系，其中子类可以继承（获得）父类的属性和方法，同时可以添加自己独有的属性和方法。

举例说明：假设有一个父类Animal，它有属性name和方法sayHello()。现在有子类Cat和Dog，它们继承了Animal类。通过继承，Cat和Dog类可以拥有name属性和sayHello()方法，并且还可以在自己的类中添加额外的属性和方法。

#### 🦋1.4 多态

多态是指当不同的对象接收到同一个消息时，会产生完全不同的反应。它包括参数多态、包含多态、过载多态和强制多态这四种类型。多态的实现是通过继承机制来支持的。

具体来说，参数多态是指不同类型的参数可以有多种结构类型。例如，在一个图形绘制的程序中，可以有不同类型的图形对象（如圆形、矩形、三角形）作为参数传入一个绘制方法，每种类型的图形对象会通过自己的绘制方式进行绘制。

包含多态是指父类对象可以引用子类对象，通过父类的引用调用子类的方法。例如，有一个动物类作为父类，有猫类和狗类作为子类，可以通过动物类的引用调用子类特有的方法，如发出不同的叫声。

过载多态是指在同一个类中，可以有多个方法名相同但参数类型或个数不同的方法。例如，在一个计算器类中，可以有多个同名的加法方法，分别接收不同类型或个数的参数，实现不同类型的加法运算。

强制多态是指可以通过强制类型转换来实现多态。例如，将一个父类对象强制转换为子类对象，以调用子类特有的方法。

多态使得不同的对象能够根据自己的特性对同一个消息产生不同的反应，提高了代码的灵活性和可扩展性。

#### 🦋1.5 覆盖（重写）

子类通过重写父类的方法，可以在原有父类接口的基础上，用适合于自己要求的实现去替换父类中的相应实现。具体而言，在子类中可以重定义一个与父类同名同参数的方法，并在该方法中实现子类自己的逻辑。

举例来说，假设有一个父类Animal，其中有一个eat()方法用于描述动物的进食行为。现在有一个子类Dog，想要重写eat()方法并定义自己的进食行为。子类Dog可以通过重写父类的eat()方法，在自己的eat()方法中实现狗狗特有的进食行为。

#### 🦋1.6 重载

函数重载和函数覆盖是两个概念需要区分开来。函数重载是指在同一个类中，可以有多个同名函数，但它们的参数类型或个数必须不同。函数重载与子类和父类之间无关，只与函数本身的参数有关。例如，在一个计算器类中，可以有两个同名的add函数，一个接受两个整数参数，另一个接受两个浮点数参数。

函数覆盖（也称为函数重写或方法重写）则是指子类重写了父类中的同名函数。子类覆盖的函数必须与父类的函数具有相同的函数名、返回类型和参数列表。例如，有一个Animal类和它的子类Dog，Animal类中有一个makeSound函数，子类Dog可以重写该函数以发出不同的声音。

函数重载与函数签名有关，可以在同一个类中有多个同名函数，但参数类型或个数必须不同；而函数覆盖则是子类重写了父类中的同名函数，要求函数名、返回类型和参数列表都相同。

#### 🦋1.7 封装

封装是一种信息隐蔽技术，其目的是将对象的使用者和生产者分离。它允许其他开发人员无需了解所要使用的软件组件内部的工作机制，只需知道如何使用组件。封装的好处是可以隐藏实现细节，提高代码的可维护性、可重用性和安全性。

举个例子来说明封装的概念：假设有一个汽车类，内部包含了发动机、轮胎、方向盘等组件。使用封装的思想，我们可以将这些组件的内部工作机制隐藏起来，只提供一个公共接口，让其他开发人员只需要知道如何使用这些组件即可。比如，其他开发人员可以调用汽车类的加速、刹车、转向等方法来控制汽车的运行，而无需了解引擎是如何工作的、轮胎是如何转动的等细节。

通过封装，我们可以实现代码的模块化、屏蔽实现细节，从而提高了代码的可维护性。同时，封装还可以提高代码的可重用性，因为其他开发人员只需要关注如何使用组件，而无需重新编写相同的代码。另外，封装还可以提高代码的安全性，因为隐藏了内部实现细节，其他人无法直接访问和修改。

#### 🦋1.8 静态类型

静态类型是指一个对象的类型在编译时就确定的特性，而动态类型则是指对象类型在运行时才能确定。例如，对于静态类型，我们可以在编译时声明一个变量的类型，并且该变量的类型将在编译时就被确定，而无法在运行时改变。例如，使用静态类型的语言如Java中，我们可以声明一个整数变量int x = 5;，在编译时便确定了x的类型为整数。相比之下，动态类型允许类型在运行时才能确定，可以根据变量的赋值来推断其类型。例如，使用动态类型的语言如Python中，我们可以声明一个变量x = 5，类型会在运行时根据赋值自动确定为整数类型。这种灵活性使得动态类型语言更加适应于快速开发和灵活的编程需求。

#### 🦋1.9 静态绑定(静态分配)

静态绑定（静态分配）是基于静态类型的，这意味着在程序执行之前，方法已经被绑定。这意味着编译器可以根据变量的静态类型来确定调用的方法。例如，假设我们有一个基类Animal和两个子类Dog和Cat，它们都有一个名为"makeSound"的方法。我们创建一个Animal类型的变量a，并将其分别赋值为Dog和Cat的实例。当我们调用a.makeSound()时，由于静态类型是Animal，编译器将选择Animal类中的makeSound方法进行绑定，而不是具体子类的makeSound方法。这就是静态绑定的概念。

#### 🦋1.10 动态绑定

动态绑定是基于动态类型的，运行时根据变量实际引用的对象类型决定调用哪个方法。动态绑定支持多态，即同一个方法名可以有多个不同的实现，根据对象的类型来自动选择正确的方法。

举个例子，假设有一个父类Animal和两个子类Dog和Cat。它们都有一个名为"makeSound"的方法。我们定义一个Animal类型的变量animal，并将其引用指向Dog对象。当我们调用animal.makeSound()时，由于动态绑定的关系，实际上会调用Dog类中的makeSound方法。如果我们改变animal的引用，指向Cat对象，那么调用animal.makeSound()将会调用Cat类中的makeSound方法。这个例子展示了动态绑定的特性，方法的具体实现是在运行时根据对象的类型决定的。

### 2.面向对象分析

#### 🦋2.1 面向对象分析

面向对象分析是一种方法论，用于确定问题域并深入理解问题。它将问题领域分解为对象，通过审视对象之间的关系和行为，来分析问题的本质和要求。

举个例子，假设我们要设计一个图书管理系统。在面向对象分析过程中，我们会考虑系统中的各种对象，如图书、图书馆、读者等。我们会分析这些对象的属性和方法，了解它们之间的关系。比如，图书对象可能有属性包括书名、作者、出版日期等，方法包括借书、还书等操作。图书馆对象可能有属性包括馆名、地址等，方法包括添加图书、借书记录等操作。通过这样的分析，我们可以更好地理解问题域，从而更有效地设计系统。

#### 🦋2.2 五个活动

包含五个活动：认定对象(按自然存在的实体确定对象)、组织对象(分析对象关系，抽象成类)、对象间的相互作用(描述各对象在应用系统中的关系)、确定对象的操作(操作，如创建增加删除等)、定义对象的内部信息(属性)。

**认定对象**：通过观察自然存在的实体，确定需要在应用系统中考虑的对象。例如，在一个电商应用中，我们可以认定对象有商品、用户、订单等。

**组织对象**：分析对象之间的关系，将它们抽象成类。例如，根据商品和用户之间的交互关系，我们可以抽象出商品类和用户类。

**对象间的相互作用**：描述各对象在应用系统中的关系。例如，在电商应用中，商品和用户之间的相互作用可以是用户浏览商品、将商品添加到购物车等操作。

**确定对象的操作**：定义对象可以进行的操作，如创建、增加、删除等。例如，用户类可以有创建账号、登录、修改密码等操作。

**定义对象的内部信息**：定义对象的属性，即对象所具有的特征。例如，商品类可以有名称、价格、库存等属性。

综合例子：在一个社交媒体应用中，我们认定对象有用户、帖子、评论等。根据用户与帖子之间的关系，我们抽象出用户类和帖子类。用户可以浏览帖子、发表评论等，这些为对象间的相互作用。用户类可以有创建账号、修改个人信息等操作。帖子类可以有标题、内容、发布时间等属性。

#### 🦋2.3 面向对象设计

面向对象设计是一种方法论，旨在通过设计分析模型并实现相应的源代码，在目标代码环境中执行这些源代码，以解决特定的设计问题域。举个例子来说明，假设我们正在设计一个图书馆管理系统，我们可以使用面向对象设计来创建图书馆、图书、用户等对象，并定义它们之间的关系和行为。通过面向对象设计，我们可以设计出一个能够方便地管理图书、借还书籍、查询图书等功能的系统。在这个系统中，图书馆、图书、用户等就是面向对象设计中的类，它们的属性和方法就是相应的源代码，而系统的运行环境就是目标代码环境。通过面向对象设计，我们可以实现一个功能完善、易于维护和扩展的图书馆管理系统。

#### 🦋2.4 面向对象程序设计

面向对象程序设计是一种使用面向对象程序设计语言实现设计方案的方式。它的主要思想是将现实世界中的事物抽象为对象，通过定义对象的属性和方法来描述其特征和行为，并通过对象之间的交互来实现系统功能。

举例说明：假设我们要设计一个图书管理系统。面向对象程序设计的思想将图书抽象为一个对象，该对象具有属性（例如书名、作者、出版日期）和方法（例如借书、还书）。我们还可以定义一个用户对象，该对象具有属性（例如姓名、借阅记录）和方法（例如借书、归还书籍）。通过面向对象程序设计语言（如Java、C++）实现图书管理系统，我们可以方便地创建图书对象和用户对象，并通过对象之间的交互实现图书借阅、归还等功能。这样，我们可以在系统中使用面向对象的思想来组织和管理图书的相关操作，提高系统的可扩展性和可维护性。

#### 🦋2.5 面向对象测试

面向对象测试，与普通测试步骤并无不同，可分为四个层次。这四个层次分别是算法层、类层、模板层和系统层。

算法层是指在测试类中定义的每个方法，类似于单元测试。例如，对于一个图书管理系统的测试，可以对添加书籍、删除书籍等方法进行算法层的测试。

类层是指测试同一个类中所有方法与属性的相互作用，特有的模块测试。例如，在图书管理系统中，可以对图书类进行类层测试，测试其方法之间的相互调用以及属性的正确性。

模板层是指测试一组协同工作的类之间的相互作用，类似于集成测试。例如，在图书管理系统中，可以对图书类、图书馆类、读者类等多个类进行模板层测试，测试它们之间的交互是否正常。

系统层是指类似系统测试的测试层次。例如，在图书管理系统中，可以对整个系统进行系统层测试，测试其整体功能是否符合要求。

### 3.面向对象的设计原则

#### 🦋3.1 单一责任原则

这个原则就是让一个类只做一件事情，不要把太多的任务放在一个类里。这样做的好处是，当你需要修改某个功能时，只需要关注一个类，而不用担心影响其他功能。

举例：想象你正在开发一个学生管理系统。你有一个 Student 类，它负责存储学生的信息，比如姓名和年龄。你还有一个 StudentManager 类，它负责管理学生的添加、删除等操作。这样，每个类只负责一个特定的责任。

举例：想象你是一名学生。你每天要面对多门课程，每门课程都有不同的老师和作业。如果你把所有课程的笔记、作业和书都放在一个文件夹里，当你需要找到特定课程的资料时会变得非常混乱。相反，如果你为每门课程都准备一个专用的文件夹，你就能更轻松地管理和找到所需的信息。每个文件夹就代表了一个类，它们只负责一个特定的任务，即存储与该课程相关的资料。

#### 🦋3.2 开放封闭原则

这个原则意味着你可以扩展现有的代码，但不需要修改已有的代码。你应该允许新功能的添加，而不会影响到已经运行良好的功能。

举例：假设你正在编写一个图形绘制软件，你有一个 Shape 类，代表各种形状。现在，你想添加一个新的形状，比如三角形。你应该能够通过创建一个新的类（例如 Triangle 类），而不是修改已有的 Shape 类。

举例：想象你是一名家庭主妇，你正在准备一顿丰盛的晚餐。你已经在规划中有一些菜肴，但客人可能会有特殊的饮食要求。你可以轻松地加入一个新的菜肴或调整配方，而不会影响到你已经准备好的菜肴。这就是开放封闭原则，你的晚餐计划是“封闭”的，因为已经准备好了，但你可以“开放”地添加新的菜肴，以满足不同的需求。

#### 🦋3.3 里氏替换原则

这个原则强调子类应该能够替换父类而不会影响程序的正确性。换句话说，你应该能够使用子类的实例来替代父类的实例，而不引发错误。

举例：想象你有一个 Bird 类，代表鸟类，其中有一个 fly 方法。现在你派生了一个 企鹅类。根据里氏替换原则，你应该能够在不引发错误的情况下使用 企鹅对象来调用 fly 方法，即使实际上企鹅不会飞。

举例：想象你在一个家庭聚会上，有一个传统的糕点摊位。人们习惯了在那里购买各种类型的糕点。假设你去那里买了一个巧克力蛋糕，但是当你尝试吃它时，却发现它其实是一个水果蛋糕。这就违反了里氏替换原则，因为人们期望能够用巧克力蛋糕替代任何其他类型的蛋糕。但在这个例子中，水果蛋糕并不能真正替代巧克力蛋糕，因为它不是巧克力蛋糕的子类。

#### 🦋3.4 依赖倒置原则

这个原则强调抽象应该依赖于细节，而不是相反。高层模块不应该直接依赖于低层模块的细节，而应该通过抽象进行交互。

举例：假设你正在开发一个电子商务平台。你有一个 OrderProcessor 类负责处理订单。而这个类不应该直接依赖于具体的支付方式，而是依赖于一个抽象的 PaymentGateway 接口。这样，你可以轻松地更改支付方式，而不必修改 OrderProcessor。

举例：想象你是一名旅行者，你需要租一辆车去探索一个城市。你不需要亲自去了解车子的每个零件如何工作，你只需要知道如何使用它们。租车公司为你提供了一辆可用的车，而不是让你去修理引擎或更换轮胎。在这个例子中，你是高层模块，租车公司是低层模块，你依赖于租车公司提供的抽象服务，而不是直接与车辆细节打交道。

#### 🦋3.5 接口分离原则

这个原则强调客户端不应该被强制依赖它们不需要的方法。接口应该只包含客户端需要的方法，避免造成冗余和不必要的复杂性。

举例：想象你正在设计一个媒体播放器。你应该根据功能拆分成不同的接口，如 AudioPlayer 和 VideoPlayer。这样，如果你只需要一个音频播放器，你就不会被迫实现视频播放相关的方法，从而遵循了接口分离原则。

举例：假设你正在考虑加入一个运动俱乐部。你有多个选项可供选择，如游泳、篮球和瑜伽。不同的人有不同的兴趣，你可能只想参加其中一种活动。运动俱乐部应该将这些活动分开成不同的项目，以便每个人只关注他们感兴趣的部分。这样，你不需要强制自己参加所有的活动，而是可以选择与你有兴趣的活动接口。

#### 🦋类的设计原则总结

★ 1、开放-封闭原则：软件实体（类、模块、函数等）应该是可以扩展的，即开放的;但是不可修改的，即封闭的；
★ 2、里氏代换原则：要求子类型必须能够替换它们的基类型，所以在里氏代换原则中，任何可基类对象可以出现的地方，子类对象也一定可以出现，**子类可以替换父类**；
★ 3、依赖倒置原则：是指不应该强迫客户依赖于他们不用的方法，接口属于客户，不属于它所在的类层次结构。即**依赖于抽象，不依赖于具体**，同时，在抽象级别，不应该有对于细节的依赖；
★ 4、单一职责原则：实现类/方法要职责单一；
★ 5、接口隔离原则：在设计接口的时候要精简单一，不应该强迫客户依赖于它们不用的方法，即：依赖于抽象，不要依赖于具体；
★ 6、迪米特原则：一个对象应当对其他对象有尽可能少的了解；
★ 7、合成复用原则：要优先使用组合或者聚合关系复用，少用继承关系复用。

还有其他原则

* 共同封闭原则：包中的所有类对于同一类性质的变化应该是共同封闭的。一个变化若对一个包产生影响，则将对该包中的所有类产生影响，而对于其他的包不造成任何影响。
* 共同重用原则：一个包中的所有类应该是共同重用的。如果重用了包中的一个类，那么就要重用包中的所有类。

![在这里插入图片描述](/assets/1c8ba1e09be74609b7991f32477dd82b.CiNva4cg.png)

![在这里插入图片描述](/assets/dc31b031485344aea6359d741e2ec58b.DOohgK_a.png)

![在这里插入图片描述](/assets/73e8406dfca34829b45d1d2f9d1e26f3.CE4kkIid.png)

## 二、UML

UML（Unified Modeling Language）是一种用于软件系统设计的建模语言，它在面向对象技术中起着重要的作用。

UML提供了一套丰富的图形符号和标记，用于描述软件系统的结构、行为和交互。常用的UML图包括类图、对象图、序列图、活动图、状态图等。

在面向对象技术中，UML可以用来表示系统的静态结构，例如类的属性和方法、类之间的关系等。类图是最常用的UML图之一，用于表示类和类之间的关系，其中包括继承、关联、聚合、组合等。类图可以帮助开发人员理清系统中各个类的关系，从而更好地进行系统设计和开发。

UML还可以用来表示系统的行为和交互，例如序列图可以展示对象之间的交互流程，活动图可以展示一个系统中的业务流程等。这些图形化的表示方式使得开发人员更容易理解和沟通系统的设计和实现。

### UML的概念

UML（Unified Modeling Language）是一种统一建模语言，与程序设计语言并无直接关系。它是一种独立于编程语言的图形化表示技术，旨在帮助开发人员在软件开发过程中进行系统设计和建模。

与程序设计语言相比，UML更注重于系统的结构、行为和交互的可视化表示。它提供了一套丰富的建模图形，例如类图、对象图、序列图、活动图等，用于描述系统的各个方面。这些图形化的表示方式对于开发团队之间的沟通和理解非常重要，可以帮助开发人员更好地协同工作，并确保他们对系统的设计和实现有一个一致的理解。

与程序设计语言不同，在UML中并没有具体的语法规则和编译步骤。它更像是一种可视化的设计工具，用来辅助开发人员进行系统分析和设计。因此，UML可以与多种编程语言一起使用，例如Java、C++、C#等。开发人员可以根据UML图形表示的设计，使用合适的编程语言进行实现，并按照UML图中定义的结构和行为来开发系统。

UML的三个要素是：

| UML要素    | 描述                                                         |
| ---------- | ------------------------------------------------------------ |
| 基本构造块 | UML提供了一系列的基本构造块，用于描述系统中的各种元素和它们之间的关系。基本构造块包括类、对象、接口、关联、聚合、组合、继承、依赖、泛化等。 |
| 放置规则   | UML定义了一些规则，用于描述如何将基本构造块组合放置在一起，以形成更复杂的结构。例如，类和对象可以组合成包，包可以组合成子系统，子系统可以组合成系统等等。这些规则有助于组织和管理系统的各个部分。 |
| 公共机制   | UML提供了一些公共机制，用于增强语言的表达能力和扩展性。其中包括扩展机制，可以通过定义新的构造块、规则和关系来扩展UML语言。此外，UML还提供了一些标记、备注和注释等机制，用于解释和补充模型的信息。 |

### UML的基本构造

UML的基本构造块包括：事物(对模型中最具有代表性的成分的抽象)、关系(把事务结合在一起)、图(聚集了相关的事物)。

#### 事物

UML中有四种事物：结构事物、行为事物、分组事物、注释事物。

| 结构事物：模型的静态部分，如类、接口、用例、构件等； | ![在这里插入图片描述](/assets/500740ed076e4ef688d133bbc69fe90a.Ds-TKEVM.png) |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| 行为事物：模型的动态部分，如交互、活动、状态机               | ![在这里插入图片描述](/assets/750d112f731540c0b656755cdaf4326e.CpCjfpcl.png) |
| 分组事物：模型的组织部分，如包；                             | ![image-20240426004834171](/assets/image-20240426004834171.DBH8oQvB.png) |
| 注释事物：模型的解释部分，依附于一个元素或一组元素之上对其进行约束或解释的简单符号。 | ![image-20240426004844976](/assets/image-20240426004844976.ClNXo7dC.png) |

#### 关系

依赖：一个事物的语义依赖于另一个事物的语义的变化而变化

关联：是一种结构关系，描述了一组链，链是对象之间的连接。分为组合和聚合，都是部分和整体的关系，其中组合事物之间关系更强。两个类之间的关联，实际上是两个类所扮演角色的关联，因此，两个类之间可以有多个由不同角色标识的关联。

泛化：一般/特殊的关系，子类和父类之间的关系

实现：一个类元指定了另一个类元保证执行的契约。

![在这里插入图片描述](/assets/46aab7fe978a4a0c98f61370e5a17e69.C2LfkyJS.png)

### 图

#### 类图

类图：静态图，为系统的静态设计视图，展现一组**对象、接口、协作和它们之间的关系**。

多重度：指的是不同类之间的联系，类似于数据库设计的表与表的关系。

![在这里插入图片描述](/assets/59b1c71240c4422a935d3c751908d1b5.C-Evuz9o.png)

#### 对象图

对象图：静态图，展现**某一时刻一组对象及它们之间的关系**，为类图的某一快照。在没有类图的前提下，对象图就是静态设计视图。

![在这里插入图片描述](/assets/9e9c9d0b3f014e5dae901befa8185cbf.DnrSP8fk.png)

#### 用例图

用例图：静态图，展现了**一组用例、参与者以及它们之间的关系**。

用例图中的参与者是人、硬件或其他系统可以扮演的角色；用例是参与者完成的一系列操作。

用例之间的关系：包含（include）、扩展(extend)、泛化。

![在这里插入图片描述](/assets/04fea18270c542318c57144f263ab5a6.DT3LS1FB.png)

![在这里插入图片描述](/assets/ace78318726042f6874213ae2db5b014.Cwv4DM8Q.png)

#### 序列图

序列图：即顺序图，动态图，是场景的图形化表示，描述了以**时间顺序组织的对象之间的交互活动**。

有同步消息(进行阻塞调用，调用者中止执行，等待控制权返回，需要等待返回消息，用实心三角箭头表示)、异步消息(发出消息后继续执行，不引起调用者阻塞，也不等待返回消息，由空心箭头表示)、返回消息(由从右到左的虚线箭头表示)三种。

![在这里插入图片描述](/assets/25452d5b2f2242daa087a43c8f218168.BQa0R8fp.png)

#### 通信图

通信图：动态图，即协作图，是顺序图的另一种表示方法，也是由对象和消息组成的图，只不过不强调时间顺序，**只强调事件之间的通信**，而且也没有固定的画法规则，和顺序图统称为交互图。如下：

![在这里插入图片描述](/assets/5f0771dedeec452e9af3faaa6fbd92c2.D_jNzcQ6.png)

#### 状态图

状态图：动态图，展现了一个状态机，**描述单个对象在多个用例中的行为**，包括简单状态和组合状态。转换可以通过事件触发器触发，事件触发后相应的监护条件会进行检查。

状态图中转换和状态是两个独立的概念，如下：图中方框代表状态，箭头上的代表触发事件，实心圆点为起点和终点。下图描述的就是一个图书的状态变化

![在这里插入图片描述](/assets/45eb92032ba44ff4ab2f27db56cdd889.Bza6aA0s.png)

#### 活动图

活动图：动态图，是一种特殊的状态图，**展现了在系统内从一个活动到另二个活动的流程**。

活动的分岔和汇合线是一条水平粗线。

每个分岔的分支数代表了可同时运行的线程数。

活动图中能够并行执行的是在一个分岔粗线下的分支上的活动。

![在这里插入图片描述](/assets/06a952a1d25044c095e4136559bd984a.DJTwxme2.png)

#### 构件图

构件图(组件图):静态图，为系统静态**实现**视图，**展现了一组构件之间的组织和依赖**。

![在这里插入图片描述](/assets/c3a7a42b4ce4420480cc680d4aef73e6.3-MfLpWn.png)

#### 部署图

部署图：静态图，为系统静态部署视图，部署图描述的事**物理模块的节点分布**。它与构件图相关，通常一个结点包含一个或多个构件。其依赖关系类似于包依赖，因此部署组件之间的依赖是单向的类似于包含关系。

![在这里插入图片描述](/assets/1a3b73b9ebaf4105a75991616c08290d.uMxcsA-y.png)

![在这里插入图片描述](/assets/8b2adcd5c5f148af90c6aa4af7fdc2bf.BJ4xZrWR.png)

![在这里插入图片描述](/assets/f2d63e0469da41d8ac1d3a7da6e16111.C-3Tyx1M.png)

![在这里插入图片描述](/assets/5e0521ea27414df8a99ba614bebe1cde.B8glkWNS.png)

## 三、设计模式

### 设计模式概念

设计模式是在软件设计领域中，为解决常见问题而被反复使用、被广泛认可的一种设计思想。它是一种经过实践验证的，被认为是最佳解决方案的经验总结，可以帮助开发人员快速高效地解决软件设计中的一些固有问题。

每一个设计模式描述了一个在我们周围不断重复发生的问题，以及该问题的解决方案的核心。这样，你就能一次又一次地使用该方案而不必做重复劳动。设计模式的核心在于提供了相关问题的解决方案，使得人们可以更加简单方便的复用成功的设计和体系结构。

设计模式具有以下特点：

| 特点                   | 描述                                                         |
| ---------------------- | ------------------------------------------------------------ |
| 在特定情况下被使用     | 设计模式并不是适用于所有场景的通用解决方案，而是在特定问题领域中被广泛使用的解决方案。 |
| 涉及多种设计元素       | 设计模式涉及到多个设计元素，包括类，对象，接口，继承，组合等等。 |
| 以面向对象方式进行设计 | 设计模式主要是针对面向对象编程而提出的，尽管也可以用于其他编程范式。 |
| 遵循设计原则           | 设计模式往往遵循一些设计原则，如单一职责原则、开闭原则、里氏替换原则等，以提高设计的可维护 |

### 设计模式分类

设计模式的四个基本要素：模式名称、问题(应该在何时使用模式)、解决方案(设计的内容)、效果(模式应用的效果)。

分为三类，创建型模式主要是处理创建对象，结构型模式主要是处理类和对象的组合，行为型模式主要是描述类或者对象的交互行为，具体如下(红色粗体记忆关键字表示常考必须记住的)：

![在这里插入图片描述](/assets/ea92227c68bd48b79509ec32307b89a7.BnN1iI8a.png)

| 类型     | 描述                                                         |
| -------- | ------------------------------------------------------------ |
| 架构模式 | - 软件设计中的高层决策，如C/S结构 - 反映了开发软件系统过程中所作的基本设计决策 |
| 设计模式 | - 描述了一个在我们周围不断重复发生的问题和其解决方案的核心 - 提供了相关问题的解决方案，使得人们可以更加简单方便地复用成功的设计和体系结构 - 包含四个基本要素：模式名称、问题（应该在何时使用模式）、解决方案（设计的内容）、效果（模式应用的效果） |
| 惯用法   | - 最低层的模式，关注软件系统的设计与实现 - 通过特定的编程语言描述构件与构件之间的关系 - 每种编程语言都有自己的惯用法，即语言的特定模式（如C++中的引用计数） |

### 设计模式-创建型

#### 🔎1.工厂模式（Factory Pattern）

工厂模式就像是一家披萨店。你告诉披萨店你想要什么类型的披萨，它会根据你的需求为你制作披萨。在这里，披萨店就是一个工厂，披萨是工厂创建的产品。

举例说明：假设你想要定制一件特别款式的T恤，你可以去一家专门定制服装的工厂。你告诉工厂你需要一件红色的T恤，上面要印有你喜欢的图案。工厂根据你的需求，选择红色的布料，根据你提供的图案进行印花，然后制作出一件定制的T恤。在这个例子中，工厂根据你的需求创建了一件特定款式的产品（定制的T恤），实现了工厂模式。你作为客户只需要提供需求，而无需关心具体的制作过程和细节。

#### 🔎2.单例模式（Singleton Pattern）

单例模式确保一个类只有一个实例，就像一座城市只有一个市长。无论多少人住在这座城市，市长都是唯一的。在生活中，操作系统的任务管理器就是一个单例模式的应用。无论打开多少个任务管理器窗口，它们都指向同一个任务管理器实例。

例如，假设有一个名为"City"的类代表一座城市，其中有一个名为"Mayor"的静态变量用于保存唯一的市长实例。当创建第一个City实例时，该实例的Mayor变量将被设置为一个新的Mayor实例。当尝试创建更多的City实例时，它们将都引用到同一个Mayor实例。

在操作系统中，任务管理器是一个单例模式的应用。无论打开多少个任务管理器窗口，它们都指向同一个任务管理器实例。这意味着无论通过哪个窗口关闭一个进程或查看系统信息，都是在同一个任务管理器实例中进行操作。这种设计能够确保操作系统中的任务管理器始终是唯一和一致的。

#### 🔎3.建造者模式（Builder Pattern）

建造者模式类似于建造一座房子。你可以选择房子的类型、颜色、材料等，并由建筑工人按照你的选择来构建房子。

以工程师为例，假设有一个木制房子构建者和一个砖瓦房子构建者。工程师的任务是根据客户的要求建造不同类型的房子。

例如，客户A需要一座木制房子。工程师会选择木制房子构建者，并使用该构建者的方法来逐步构建木制房子。这包括设置木质墙壁、建造屋顶和安装窗户。

另一个例子是客户B需要一座砖瓦房子。工程师会选择砖瓦房子构建者，并使用该构建者的方法来逐步构建砖瓦房子。这包括砖瓦墙壁、瓦片屋顶和大窗户。

通过使用建造者模式，工程师可以根据客户的需求逐步构建不同类型的房子，而不必亲自处理所有细节。这使得构建过程灵活且可维护，同时允许创建不同风格和材质的房子。

#### 🔎4.原型模式（Prototype Pattern）

原型模式是通过复制现有对象来创建新对象的设计模式。这类似于使用3D打印机复制一件艺术品或零件，可以从一个原型创建出多个相同的物品。

生活中的一个例子是在电影制作中。制片人可以利用原型模式来复制特效道具，这样在不同的场景中就可以使用多个相同的道具。例如，如果电影中需要多个相同的怪物模型，制片人可以使用原型模式来复制并创建多个相同的怪物模型。这样可以节省时间和资源，同时确保每个怪物模型在外观和细节上都保持一致。

![在这里插入图片描述](/assets/384f77d7ade24df9b2a99016b3cf32f8.BBo4lcSg.png)

### 设计模式-结构型

结构型设计模式：关注如何将对象和类组合成更大的结构，以便更好地实现系统的整体结构。这些模式有助于解决对象之间的组合、接口和继承等关系，从而提高代码的可扩展性和可维护性。

#### 🔎1.适配器模式（Adapter Pattern）

适配器模式允许不兼容的接口协同工作，就像使用电源适配器将外国电器插头适配到国内插座一样。适配器模式可以使不同的类一起工作。

例如，假设你有一台笔记本电脑，只有一个USB-C接口，但你的打印机只支持USB-A接口。这两个接口是不兼容的，无法直接连接。但你可以使用USB-C到USB-A的适配器来使它们兼容，将适配器插入笔记本电脑的USB-C接口，然后将打印机连接到适配器的USB-A接口。这样，笔记本电脑和打印机就可以一起工作了。

同样地，适配器模式在软件开发中也非常有用。当需要使用一个类，但其接口与现有系统不兼容时，可以创建一个适配器类，将这个类的接口适配到系统所需的接口，从而实现两者的协同工作。

#### 🔎2.桥接模式（Bridge Pattern）

桥接模式分离了一个对象的抽象部分和具体部分，使它们可以独立地变化。这个模式就像一座桥，将两个独立的领域连接起来。

生活中的例子：

假设你想购买一辆汽车，汽车的品牌和颜色是两个独立的变化维度。桥接模式允许你将品牌和颜色抽象出来，使你可以轻松地组合不同品牌和颜色，例如创建一个红色的奥迪或蓝色的宝马。

举例说明：
当你购买一辆汽车时，你可以选择奥迪或宝马作为品牌，也可以选择红色或蓝色作为颜色。使用桥接模式，你可以将品牌和颜色分离出来，形成两个独立的维度。然后，你可以通过组合品牌和颜色选项，创建出不同的汽车组合，比如红色的奥迪或蓝色的宝马。这样，你可以根据自己的需求和喜好，创建出各种不同的汽车组合，而不需要为每个组合都定义一个具体的子类。

这个例子展示了桥接模式的优点，通过将变化的部分抽象出来，并通过桥接模式进行组合，使得对象的抽象和具体部分可以独立地进行变化。这样一来，你可以根据需要扩展品牌和颜色的选项，而不需要修改已有的代码。同时，这也避免了类的爆炸式增长，因为不同的品牌和颜色组合可以通过桥接模式进行动态创建，而不需要在类的继承体系中定义每个组合的具体子类。

#### 🔎3.组合模式（Composite Pattern）

组合模式允许将对象组织成树状结构，使单个对象和组合对象都可以一致地对待。生活中的一个例子是在操作系统中，文件夹可以包含文件和其他文件夹，从而创建了一个树状的组织结构。在这个树状结构中，你可以对文件夹和文件执行相似的操作，例如复制、删除等。

例如，在Windows操作系统中，你可以创建一个名为"根文件夹"的文件夹，然后在它下面创建两个文件夹，分别命名为"文件夹A"和"文件夹B"。在"文件夹A"下，你可以创建两个文件，分别是"文件1"和"文件2"。同样地，在"文件夹B"下，你也可以创建两个文件，分别是"文件3"和"文件4"。

这样，你就创建了一个树状的组织结构，其中"根文件夹"是顶层容器，"文件夹A"和"文件夹B"是其子文件夹，“文件1”、“文件2”、“文件3"和"文件4"是其文件。在这个组织结构中，你可以对"根文件夹”、“文件夹A”、“文件夹B”、“文件1”、“文件2”、“文件3”、"文件4"执行相似的操作，例如复制、删除等。

通过组合模式，我们可以将多个文件夹和文件组织成一个文件系统树，从而方便地管理和操作这些对象。每个对象都可以被视为一个节点，可以通过遍历树的方式来处理整个文件系统。这种设计模式使得单个对象和组合对象具有了一致的接口，提高了代码的复用性和可扩展性。

#### 🔎4.装饰器模式（Decorator Pattern）

装饰器模式允许动态地为对象添加新的功能，而无需修改其源代码。这种方式类似于家中不断添加新的家具或装饰来改善其外观和功能。

例如，假设你有一台智能音响，它可以播放音乐和接收语音指令。现在你想要扩展音响的功能，让它可以查询天气情况和进行音乐播放。使用装饰器模式，你可以添加新的语音助手技能来实现这些功能，而不需要修改音响的核心设计。换句话说，你可以在音响上添加一个天气查询装饰器，用于获取天气信息，并在音响上添加一个音乐播放装饰器，用于播放音乐。这样一来，你可以根据需要随时添加、删除或替换这些装饰器，而不会影响到音响的基本功能。

同样地，可以将这个概念应用到家居装饰中。你可以随时根据需要更换家具或添加新的装饰品来改善家居的外观和功能，而不需要对房屋结构进行修改。例如，你可以添加一个书架来增加存储空间，或者添加一幅画来增加艺术氛围。这些装饰品可以随时更换或移除，而不会对房屋的基本结构产生影响。

#### 🔎5.外观模式（Facade Pattern）

外观模式提供了一个简化的接口，用于访问复杂系统中的一组接口。类比于建筑物的外立面，外观模式隐藏了系统内部的复杂结构，使得用户可以轻松地访问并使用系统。

举个生活中的例子来说明，我们可以看看计算机操作系统中的图形用户界面（GUI）。GUI为用户提供了一个易于使用的外观，隐藏了底层操作系统的复杂性。通过点击图标、打开应用程序等操作，用户可以在不了解操作系统内部工作原理的情况下轻松地使用电脑。就像在一座建筑物中，我们只需要看到外立面上的窗户、门等部分，而不需要了解建筑物内部的复杂结构。

#### 🔎6.享元模式（Flyweight Pattern）

享元模式的目的是通过共享尽可能多的相似对象来最小化内存或计算开销。可以将其类比为在共享办公空间中租用一个工作区，多个人可以共享同一空间，从而减少资源浪费。

举个生活中的例子来说明：在图像编辑软件中，当多个图像元素共享相同的颜色或图案时，可以使用享元模式来减少内存占用。比如若多个图像元素需要使用相同的红色，那么可以创建一个红色的享元对象，并让这些图像元素引用同一个红色的享元对象。这样一来，就不需要为每个图像元素都存储一份相同的红色数据，从而减少了内存占用。

通过使用享元模式，可以有效地减少重复的数据存储，提高系统的性能和效率。

#### 🔎7.代理模式（Proxy Pattern）

代理模式允许一个对象代表另一个对象进行控制访问。类似于聘请一个房产经纪人代表你购买房产的情况，代理模式可以控制对另一个对象的访问。

生活中的例子：

计算机网络中的代理服务器充当客户端和目标服务器之间的中间层，用于缓存、过滤或加速请求，以提供更好的访问控制和性能。例如，你想访问某个网站，但是该网站在你所在地区的访问速度很慢。这时，你可以通过设置代理服务器，让代理服务器帮助你从外地访问该网站，并将请求结果缓存下来，以提高访问速度。这样，你就可以通过代理服务器实现对该网站的更快访问，而不必直接与目标服务器进行通信。

![在这里插入图片描述](/assets/98849acfe4ab4ee89c2df6f8dc51b890.CWwd6tZV.png)

### 设计模式-行为型

行为型设计模式：关注对象之间的通信、职责分配和算法的交互方式。它们帮助我们更好地管理对象之间的关系，使系统更具灵活性和可维护性

#### 🔎1.责任链模式（Chain of Responsibility Pattern）

责任链模式就如同传递请求一样，多个对象按顺序尝试处理请求，直到有一个对象能够处理为止。

生活中的例子：在一个公司中，员工请假申请可能需要经过多个级别的审批，例如部门主管、部门经理和总经理。每个级别的主管都有权决定是否批准员工的请假申请。当员工提交请假申请后，首先会由部门主管进行审批，如果部门主管批准了请假申请，那么流程结束；如果部门主管拒绝了请假申请，那么请假申请会被传递给部门经理进行审批，同样如果部门经理批准了请假申请，流程结束；如果部门经理也拒绝了请假申请，那么请假申请会被传递给总经理进行审批。总经理是最后一个级别的审批者，他的决定是最终决定。这个过程中，如果任何一个级别的主管批准了请假申请，那么就会结束审批流程，否则，请求会一直被传递下去，直到有一个对象能够处理请求为止。

#### 🔎2.命令模式（Command Pattern）

命令模式就像使用遥控器来控制设备。你将命令封装在遥控器按钮中，然后可以随时执行命令。生活中的一个例子是电视遥控器。

通过电视遥控器，你可以执行各种命令，例如打开电视、切换频道或调整音量。当你按下遥控器上的按钮时，电视就会接收到对应的命令，并执行相应的操作。这样，你不需要亲自操作电视机上的按钮或控制面板，只需使用遥控器就可以方便地控制电视。

举个例子，当你想要打开电视时，你可以按下遥控器上的电源按钮。这个按钮上封装了“打开电视”的命令，当你按下按钮时，遥控器会发送这个命令给电视机，电视机就会打开。同样地，如果你想要切换频道，你可以按下对应的频道按钮，这个按钮上封装了相应的“切换频道”的命令，电视就会切换到你想要的频道。

通过命令模式，我们可以将具体的命令与执行命令的对象（比如电视）解耦合，从而实现更灵活的控制。我们可以将各种命令封装在遥控器的按钮中，并在需要时执行这些命令，而无需关心具体是如何执行的。

除了电视遥控器，命令模式还可以应用于其他场景，比如智能家居系统中的遥控器、电梯控制面板等。无论是哪种情况，命令模式都提供了一种方便的方式来控制设备，并将操作命令与具体操作解耦合，提高了系统的可扩展性和灵活性。

#### 🔎3.解释器模式（Interpreter Pattern）

解释器模式用于处理语言解释和编译器等领域。它定义了一种语言的语法表示，并提供了解释器来解释这种语法。在编程中，正则表达式是解释器模式的一个实例，用于匹配和解释字符串模式。

举例来说，假设我们要编写一个程序来验证一个邮箱地址是否合法。我们可以使用正则表达式作为解释器来解释邮箱地址的语法规则。我们可以定义邮箱地址的语法规则，例如：一个合法的邮箱地址应该包含一个@符号和一个域名，而域名又由一个或多个单词组成，每个单词之间用点号(.)分隔。通过编写一个解释器来解释这个语法规则，我们可以对任意的邮箱地址进行验证，判断其是否合法。

例如，我们使用正则表达式解释器来解释邮箱地址的规则：

* 规则1：一个合法的邮箱地址应该包含一个@符号
* 规则2：@符号之前可以有一个或多个字符
* 规则3：@符号之后应该是一个或多个单词，每个单词之间用点号(.)分隔

通过使用解释器模式和正则表达式，我们可以创建一个邮箱地址验证器，输入一个邮箱地址，程序会根据定义的规则进行解释和验证，返回是否合法的结果。

#### 🔎4.迭代器模式（Iterator Pattern）

迭代器模式是一种类似于遍历集合的设计模式。它提供了一种按顺序访问集合元素的方法，而无需直接暴露集合的内部结构。

举个生活中的例子来说明，假设你想要看电视节目表上的所有节目。你可以选择按照时间顺序一个一个地查看节目，这就是使用迭代器模式的一种示例。你无需知道节目表的内部结构，只需按照顺序逐一访问节目即可。

在编程中，迭代器模式同样适用。假设你有一个存储各种数据的集合，例如数组、列表或集合。使用迭代器模式，你可以通过创建一个迭代器对象来遍历集合中的元素，而无需了解底层数据结构。迭代器会按照一定的顺序返回集合中的每个元素，你可以根据需要对每个元素执行特定的操作。这样，你就可以方便地处理集合中的数据，而不用关心具体的数据结构。

#### 🔎5.中介者模式（Mediator Pattern）

中介者模式就像是一个中间人，在多个对象之间协调交互。通过使用中介者模式，可以减少对象之间的直接通信，从而降低耦合度。

一个生活中的例子是在一个团队中，项目经理可以充当中介者的角色。项目经理负责协调团队成员之间的合作和沟通，以确保项目的顺利进行。项目经理作为中介者，可以处理团队成员之间的冲突和协调不同的需求和意见。通过项目经理的介入，团队成员可以专注于自己的任务，而不必直接与其他成员沟通和协调。这样，中介者模式帮助提高了团队的效率和协作能力。

#### 🔎6.备忘录模式（Memento Pattern）

备忘录模式是一种保存和还原状态的设计模式。它允许你保存对象的状态，以便将来可以还原到先前的状态。

生活中的例子可以是在文本编辑器中使用撤销和重做功能。当我们在编辑文本时，可以通过撤销操作回到之前的状态，然后再次使用重做操作恢复到之前的修改。这就是备忘录模式的应用。

#### 🔎7.观察者模式（Observer Pattern）

观察者模式类似于订阅通知。多个观察者（订阅者）订阅主题（发布者），当主题有新信息时，观察者会自动接收通知。

这种模式在生活中有许多例子，比如社交媒体平台。当你关注某个用户或主题时，系统会将他们的更新信息发送给你。举个例子，假设你在Twitter上关注了某个名人，每当这个名人发布新推文时，你会在你的主页上看到他的推文。在这里，你就是观察者，名人是主题，Twitter系统负责将新推文通知给你。

#### 🔎8.状态模式（State Pattern）

状态模式类似于人的不同情绪状态。就像一个人可以处于高兴、生气或伤心等不同的状态一样，每种状态下的行为可能不同。

举个生活中的例子：在自动售货机中，售货机可能有不同的状态，如待机、售卖中、缺货等。每种状态下，售货机的行为都不同。例如，在待机状态下，售货机可能会显示一个欢迎界面，并等待用户选择商品；在售卖中状态下，售货机会接受用户的投币或刷卡，并出货商品；而在缺货状态下，售货机可能会显示一个提示信息，告诉用户该商品暂时缺货。

通过状态模式，售货机可以根据当前的状态灵活地执行不同的行为，使操作更加简便和智能。

#### 🔎9.策略模式（Strategy Pattern）

策略模式类似于根据不同情况选择不同解决方案来解决同一个问题。通过选择不同的策略，我们可以实现相同的目标。

生活中的例子是在旅行规划中。在旅行规划中，我们希望到达一个目的地。然而，我们可以选择使用不同的交通工具作为我们的策略来达到目的地。例如，我们可以选择开车、乘坐火车或乘坐飞机。这些交通工具都是解决相同问题的不同策略。根据不同的情况，我们可以选择适合我们需要的策略来实现我们的目标。

举个例子，假设我们想去一个离我们家很远的城市旅行。如果我们有自己的汽车，那么我们可以选择开车去这个城市。这个策略可以使我们自由地掌控旅行的时间和路线。另一方面，如果我们没有汽车，但是有高速铁路可以直接到达目的地，那么我们可以选择乘坐火车作为我们的策略。如果我们非常着急，希望快速到达目的地，可能会选择乘坐飞机，这样可以更快地到达。无论我们选择哪种交通工具作为策略，我们的目标都是到达目的地，只是选择了不同的策略来实现相同的目标。

这个例子展示了策略模式的思想，通过选择不同的策略来解决相同的问题，我们可以根据不同的情况选择最适合的策略来实现我们的目标。

#### 🔎10.模板方法模式（Template Method Pattern）

模板方法模式就像是定义了一个算法的框架，但允许子类实现其中的一些步骤。在生活中的例子中，烹饪可以是一个使用模板方法模式的场景。假设我们有一个烹饪系统，其中有一个抽象的Cook类，它定义了一个烹饪方法cooking()。这个烹饪方法包含了一些固定的步骤，如准备、烹饪和装盘。

具体的烹饪步骤和材料可能因每道菜而异。因此，我们可以创建具体的子类，如SteakCook和PastaCook，来实现这些具体的步骤。子类可以重写cooking()方法中的一些步骤，以符合它们具体的烹饪需求。

举个例子，假设我们有一个SteakCook子类。在它的cooking()方法中，它可以重写准备步骤，使用牛排所需的材料和方法。然后，它可以继续使用Cook类的烹饪和装盘步骤，因为这些步骤在制作任何菜肴时都是相同的。

另一方面，如果我们有一个PastaCook子类，它可以重写烹饪步骤，使用面条和调味料。然后，它可以继续使用Cook类的准备和装盘步骤。

通过使用模板方法模式，我们可以在一个抽象类中定义共享的算法框架，但允许子类根据自己的需要实现和定制其中的一些步骤。这样，我们可以避免重复的代码，并提供灵活性和可扩展性。

#### 🔎11.访问者模式（Visitor Pattern）

访问者模式类似于访问不同类型的元素。你可以定义不同的访问者来执行不同类型元素的操作。

举一个生活中的例子来说明：在一个图书馆里，可能有各种不同类型的书籍，比如小说、科普书和艺术书。不同类型的书籍可能需要不同类型的读者来浏览和理解。

在这个例子中，访问者可以是不同类型的读者，例如一个文学爱好者可以是访问者，他对小说感兴趣并懂得如何欣赏文学作品。另外，一个科学爱好者也可以是访问者，他对科普书感兴趣并能够理解科学知识。还有一个艺术爱好者，他可以是访问者，他喜欢欣赏艺术书中的绘画和设计。

通过定义不同类型的访问者，我们可以针对不同类型的书籍执行不同的操作。比如，文学爱好者可以读取小说中的故事情节和对话，并欣赏其中的文学技巧。科学爱好者可以阅读科普书中的科学原理和实验，并从中获取科学知识。艺术爱好者可以欣赏艺术书中的绘画作品，并了解艺术家的创作思想和技巧。

通过访问者模式，我们可以让不同类型的访问者来访问不同类型的书籍，从而实现对元素的不同操作。这样，每个访问者可以专注于自己擅长的领域，并且可以灵活地添加新的访问者和元素，以适应不断变化的需求。

![在这里插入图片描述](/assets/eddc56b7ddd6454e910c14c50c5cead2.BFPENcqW.png)
![在这里插入图片描述](/assets/f33eccfa8a674194b35453c0bb0c6691.BP7V8dmC.png)

### 例题

![在这里插入图片描述](/assets/9538db15be9d4255b8b6845eff23dd51.BGpNeZVP.png)
![在这里插入图片描述](/assets/256519a37221488f9765f25633657da7.C1uRVHPW.png)
![在这里插入图片描述](/assets/ebe982a25fcb4740894ab2140ce3c885.DRDpJeva.png)
![在这里插入图片描述](/assets/085be25402e048c0a813e17541c38bda.Dj1a2k0G.png)

---

---
url: /Java/解决方案/秒杀/秒杀.md
---
# 秒杀

---

---
url: /01.指南/20.相关/99.鸣谢.md
---

# 鸣谢

本文记录 Teek 在成长过程中帮忙测试、开发的小伙伴们，以及 Teek 参考的其他优质 VitePress、VuePress 主题，感谢你们让 Teek 更加优秀。

## 主题

Teek 的灵感主要来自于 [vuepress-theme-vdoing](https://doc.xugaoyi.com/)，在 VitePress 没有出来之前，Teek 使用 `vuepress-theme-vdoing` 搭建的博客站，因此 Teek 含有 `vuepress-theme-vdoing` 的绝大部分功能。

::: shareCard

```yaml
- name: vuepress-theme-vdoing
  desc: 🚀一款简洁高效的VuePress 知识管理&博客 主题
  avatar: https://doc.xugaoyi.com/img/logo.png
  link: https://doc.xugaoyi.com/
  bgColor: "#B9D59C"
  textColor: "#3B551F"

- name: 粥里有勺糖
  desc: 简约风的 VitePress 博客主题
  avatar: https://theme.sugarat.top/logo.png
  link: https://theme.sugarat.top/

- name: VitePress 快速上手中文教程
  desc: 如果你也想搭建它，那跟我一起做吧
  avatar: https://avatars.githubusercontent.com/u/90893790?v=4
  link: https://vitepress.yiov.top/
  bgColor: "#CBEAFA"
  textColor: "#6854A1"

- name: 友人A
  desc: おとといは兎をみたの，昨日は鹿，今日はあなた
  avatar: http://niubin.site/logo.jpg
  link: http://niubin.site/

- name: Lumen
  desc: ✨ 集成 Vue 功能组件和主题美化的 VitePress 插件
  avatar: https://lumen.theojs.cn/Logo.webp
  link: https://lumen.theojs.cn/

- name: Nólëbase 集成
  desc: 多元化的文档工程工具合集
  avatar: https://nolebase-integrations.ayaka.io/assets/obsidian-logo.Cz_vZo3r.svg
  link: https://nolebase-integrations.ayaka.io/
```

:::

## 成员

::: shareCard

```yaml
- name: One
  desc: 明心静性，爱自己
  avatar: https://onedayxyy.cn/img/xyy-touxiang.png
  link: https://onedayxyy.cn/
  bgColor: "#FFE5B4"
  textColor: "#A05F2C"

- name: Hyde Blog
  desc: 人心中的成见是一座大山
  avatar: https://teek.seasir.top/avatar/avatar.webp
  link: https://teek.seasir.top/
  bgColor: "#FFB6C1"
  textColor: "#621529"

- name: 二丫讲梵
  desc: 💻学习📝记录🔗分享
  avatar: https://wiki.eryajf.net/img/logo.png
  link: https://wiki.eryajf.net/
```

:::

---

---
url: /StableDiffusion/Midjourney/1_Midjourney命令参数用途列表.md
---

# 命令参数用途列表

## 一、Midjourney命令列表🎨

可以通过输入命令与Discord上的Midjourney Bot (MJ机器人)进行交互。命令用于创建图像、更改默认设置、监视用户信息以及执行其他有用的任务。

Midjourney命令可以在任何机器人频道中使用，在允许Midjourney Bot运行的个人Discord服务器上使用，或者在与Midjourney Bot的直接消息中使用。

### 常用的命令：

| Midjourney命令 | 解释                                                         |
| ------------------------------ | ------------------------------------------------------------ |
| /imagine prompt                | 只能在这个命令prompt里面输入描述语才能生成图像               |
| /setting                       | 打开偏好设置，里面可以预设一些命令                           |
| /blend                         | 轻松地将多个图像混合在一起                                   |
| /show                          | 输入/show在jobid框里输入您的图库中所生成图像的作业ID后，你可以再次召唤和恢复自己的任何生成图像 |
| /fast                          | 切换快速模式，在快速模式下，您的生成图像将按增量计费。这模式生成图像较快 (订阅会员才能使用的模式) |
| /relax                         | 切换放松模式，在快速模式下，您生成图像可以无限使用，但这模式生成图像较慢。(30天和企业会员可无限使用这个模式) |
| /prefer suffix                 | 重置你的偏好设置（有时生成图片时会出现一些自己明明未添加的指令，却提示错误，可以执行一下这个命令） |
| /remix                         | 切换混音模式 (可以让你在重生成或者变化图片时修改描述语)      |
| /stealth                       | 对于专业计划(60美元/月)的用户切换到隐身模式                  |
| /public                        | 切换公共模式，在公共模式下，你生成的图像在画廊中对任何人都是可见的 |
| /subscribe                     | 创建一个指引您当前Discord账户会员订阅页面链接 (无需登录会员) |
| /prefer option                 | 创建或管理自定义选项。                                       |
| /prefer option list            | 查看当前自定义选项                                           |
| /help                          | 显示有关Midjourney机器人相关的信息和提示                     |

### 不常用的命令：

| Midjourney命令 | 解释                                                 |
| ------------------------------ | ---------------------------------------------------- |
| /ask                           | 在question框里输入你的问题，你会得到一个问题的答案。 |
| /daily\_theme                   | 切换#每日主题频道更新的通知ping                      |

### 弃用的命令：

|                           |                                                              |
| ------------------------- | ------------------------------------------------------------ |
| /private (替换为/stealth) | 切换私人模式，在私人模式下，你生成的图像仅你可见，使用私人模式需开通每月60美元的会员 |
| /pixels                   |                                                              |
| /idea                     |                                                              |

### 指令和参数列表

指令参数可更改图像的生成方式。指令参数可以更改图像的纵横比、在 Midjourney模型版本之间切换、更改使用的偏好设置等等。

指令参数是添加到描述语后面的末尾。您可以在描述语中添加多个指令参数。

![img](/assets/AgAABadAeiYMuN9ynDZMjISRb6gD4BkU.UUHlCEOe.jpeg)

注：

1. 需要指令格式、符号和空格，错了会失效。
2. 许多 Apple 设备会自动将双连字符 (--) 更改为破折号 (—)。Midjourney 两者都支持！

## 二、Midjourney后辍参数用途🎨

\--ar调整图片的长宽比例  如:--ar3:2表示生成的图片比例为3(长):2(宽)

\--q+数值   生成图片的质量，数值范围0.25-5，数值越大细节质量越丰富，也需要更长的生成时间如

\--q2表示生成更高质量的图片

\--s+数值  生成图片的个性化强度，数值范围0-1000，数值越大生成图像个性化越强

如:--s750表示生成的图像个性化较强

\--iw+数值    设置图片参数权重，数值范围0.25-2   数值越大 生成图片与参考图越相似

版本切换

\--version或--v 算法：现在默认是新算法--v5，表示用的什么版本如:（--v 5）表示使用V5版本

Niji

\--niji：把midjourney切换为niji・journey，这是新的专注于动漫风格的图像模型。

测试算法test

\--test：和--upbeta比较类似，Midjourney特殊测试模型，应用--test后，生成的图片会由四张变为一张

测试算法testp

\--testp：和--upbeta比较类似，Midjourney 特殊的以摄影为重点的测试模型，应用--testp后，生成的图片会由四张变为一张

老版本的HD算法

\--hd：该算法可能更适合较大的图像，生成更高清的图，四图的构图不一致性。适合用于风景画、抽象画适合用于风景画、抽象画。这也可以生成更高分辨率的图像，而无需放大。

基本指令：

纵横比

-aspect n:m或--ar n:m：n是长，m是宽，例如--ar 16:9，可以输出长宽比为16:9的图片。注意冒号必须使用英文的冒号，中文冒号会不生效，ar后面必须空一 格再输入，否则可能不生效。

V4风格切换

\--style 4a, 4b或4c：在Midjourney模型版本V4风格之间切换

无缝图案

\--tile：用于生成四方连续无缝图案，结合--upbeta一起使用。

seed种子

\--seed 种子：获取了自己生成图像的seed种子后，可以参考图像风格和内容生成新的图像 (获取图像的seed种子，可在这个图像内容右上角发\[信封]图标，就可以收到图像信息的消息)

风格化

\--stylize或--s 数值：让你生成的图像更具风格化和想象力，你设置的数值越高，它就越天马行空。默认值为2500，值范围625-60000

风格各异

\--chaos 数值：生成四张风格各异的图像，数值必须在0-100之间。较高的值让生成的图像风格更不同

质量

\--quality或--q 数值：生成图片的质量和时间。默认值为1，值越高，生成时间越长，质量越高。只能输入0.25、0.5、1、2、5这五个数值。（如果使用默认值，就无需加入这个指令）

移除对象

\--no 对象：--no 加具体物品，出图将不包含该物品，例如，输入--no grass 画面会尝试移除草。这就像给它赋予-0.5的权重 如:--no red表示图像中不要出现红色

停止生成百分比

\--stop 数值：按生成完成度百分比停止生成图像。必须在10-100之间。这目前只对四图有效，该模式不适用于升级大图

升级器指令：

Midjourney首先为每个作业生成一个低分辨率图像选项网格。您可以在任何网格图像上使用Midjourney upscaler来增加尺寸并添加更多细节。有多种可用于放大图像的放大模型。

beta升级器

\--upbeta：选择U按钮时，使用替代的 beta 升频器。结果更接近原始网格图像。放大后的图像添加的细节明显更少。不适用于--hd和--stylize和--q 5

轻量升级器

\--uplight：选择U按钮时，使用"较轻"升级器进行升级。结果更接近原始网格图像，放大后的图像细节更少，更平滑，适用于面部和光滑的表面。

其他指令：

这些参数仅适用于特定的早期Midjourney模型版本

创造力

\--creative：修改test和testp模型更加多样化和创造性。

参考图权重

\--iw 权重值：对参考图片相似程度，默认值为0.25，最大值为5。数值越高对参考图的相似度越高，反之相似度越低

生成视频

\--video：该视频发送\[信封]图标会收到图像信息，在信息中可看到视频链接，该模式不适用于升级大图

同种子

\--sameseed 种子：创建一个大的随机噪声场，应用于初始网格中的所有图像。当指定 --sameseed 时，初始网格中的所有图像都使用相同的起始噪声，并将生成非常相似的生成图像。如果不使用，每张图像的效果更多样性

弃用指令：

\--w或--h (替换为--aspect)

设置图片分辨率：分别设置生成图像的分辨率w宽度和n高度。值应设置在256和2034之间，最大分辨率为3兆像素。这些值作为64或128的倍数效果更好，如--w 1920 --h 1024

\--fast (替换为--quality)

更快的图像，更少的一致性，更便宜。你也可以使用--q 0.5和--q 0.25来获得类似效果

\--beta

实验算法：该模型需要更长的生成时间，仅同时生成两张图像 (非默认分辨率只生成一张图像)。不适用与--hd和--stylize和--q 5

\--vibe (现在称为V1)

\--hq

\--newclip

\--nostretch

\--old

### 设置和预设

设置命令为模型版本、样式值、质量值和升级器版本等常用选项提供切换按钮。设置也有/stealth和/public命令的切换按钮。

使用/settings命令，可以打开偏好设置，这指令将为你提供一些按钮来查看和更改你当前的偏好，例如/prefer suffix指令，非常直观。每组偏好都可以打开和关闭

![1716613075478.jpg](/assets/1716613075478.BY2uqyoz.jpg)

设置说明：

添加到描述语末尾的指令参数将覆盖偏好的设置，如你在描述语末尾加上--v6，你的偏好要设置是v6

设置说明：

添加到描述语末尾的指令参数将覆盖偏好的设置，如你在描述语末尾加上--v6，你的偏好要设置是v6

### 设置模型版本：

![image.png](1_Midjourney命令参数用途列表.assets/imagev6.png)

指令--v6使用原始的Midjourney算法

MIDJOURNEY

YMODELV5.2

![image.png](1_Midjourney命令参数用途列表.assets/imagev5.png)

指令--v 5.2使用原始的Midjourney算法

![img](1_Midjourney命令参数用途列表.assets/AgAABadAeiavBKHdTRFFrpGXuezEOEIR.png)

指令--v 1使用原始的Midjourney算法 (更抽象)

![img](1_Midjourney命令参数用途列表.assets/AgAABadAeiZpBdgn5k9N_LIPfB2qUsP2.jpeg)

指令--v 2，2022年7月25日之前使用的原始Midjourney算法

![img](1_Midjourney命令参数用途列表.assets/AgAABadAeibal6InAWZFBawkfncYpj1z.jpeg)

指令--v 3这是比较很有创意和艺术感的Midjourney算法

![img](1_Midjourney命令参数用途列表.assets/AgAABadAeiZoCvKTdK5J3aP3f_iusGtB.jpeg)

指令--v 4这是当前默认Midjourney算法

![img](1_Midjourney命令参数用途列表.assets/AgAABadAeiYDGrsjBjJMarvUYJ1utg2M.jpeg)

指令--niji，专出动漫二次元画风的算法

![img](1_Midjourney命令参数用途列表.assets/AgAABadAeiZWVMc0lUxFwopFgNSa5XNq.png)

指令--test，使用测试性算法，和--upbeta比较类似，可生成质量更高或者风格更奇特的图，应用--test后，生成的图片会由四张变为一张。

![img](1_Midjourney命令参数用途列表.assets/AgAABadAeiYGQQpFyqhOhr9bbsKVcSDB.jpeg)

指令--testp，testp是整体的生成模式 更偏照片真实风格，应用--test后，生成的图片会由四张变为一张

### 设置质量：

![img](1_Midjourney命令参数用途列表.assets/AgAABadAeiZmXhjkaxhBlYUfcaem4BQ-.jpeg)

指令--q 0.5，不太详细，1/2

![img](1_Midjourney命令参数用途列表.assets/AgAABadAeiZsmZ3KftBGqKMIX_8o2CwG.jpeg)

指令--q 1，默认值

![img](1_Midjourney命令参数用途列表.assets/AgAABadAeia1WUpuGK9ESZQrAL3w7WSV.jpeg)

指令--q 2，非常详细，贵2倍

### 设置风格化：

![img](1_Midjourney命令参数用途列表.assets/AgAABadAeiZsX36Plx5AL42GBqkXmgih.jpeg)

指令--s 50，低风格化

![img](1_Midjourney命令参数用途列表.assets/AgAABadAeiYAodwzc1BHMYj-HQJQwxym.jpeg)

指令--s 100，默认值

![img](1_Midjourney命令参数用途列表.assets/AgAABadAeiYlMaCRmpJJQ6Rq63A_r2Wr.jpeg)

指令--s 250，高风格化

![img](1_Midjourney命令参数用途列表.assets/AgAABadAeibH8mpCCd5PKJ_Gtj4ZHYUe.jpeg)

指令--s 750，非常高风格化

### 设置升级器：

![img](1_Midjourney命令参数用途列表.assets/AgAABadAeiZmiquGj3ZC87-jtQ3jqp4a.jpeg)

默认值，常规放大图像

![img](1_Midjourney命令参数用途列表.assets/AgAABadAeia6NdlLTYVNOLR05ZN5Jq0V.jpeg)

指令--uplight，这会导致U按钮放大图像同时保持原始细节

![img](1_Midjourney命令参数用途列表.assets/AgAABadAeiaJZ89051BPB6mLBlARAjnz.jpeg)

测试版放大图像

### 公共和隐身模式切换

![img](1_Midjourney命令参数用途列表.assets/AgAABadAeiaHOtAui6BH5LwhiYzofV3O.jpeg)

命令/prublic，公共模式

![img](1_Midjourney命令参数用途列表.assets/AgAABadAeiZfxIUwPo9I9atwrgjofgUd.jpeg)

命令/stealth，私有模式

![img](1_Midjourney命令参数用途列表.assets/AgAABadAeiYUdtkX4_9BvLRGY3-k_64Y.jpeg)

开启Remix mode，生成图片时可以用V和Make Vriations重新改变关键词再继续生成

### Fast和Relaxed模式切换

![img](1_Midjourney命令参数用途列表.assets/AgAABadAeiYvUiAp_7pNo62edLWKgvN0.jpeg)

命令/fast，切换快速模式

![img](1_Midjourney命令参数用途列表.assets/AgAABadAeiZt3wB6SOBLbIcWNlswL8ne.jpeg)

命令/relax，切换放松模式

### 自定义首选项

使用\[/prefer]命令创建自定义选项，可以将常用的指令参数添加到描述语末尾。

/prefer auto\_dm：

完成的工作会自动发送到直接消息

/prefer option：

创建或管理自定义选项。

/prefer option list：

查看您当前的自定义选项。

/prefer suffix：

指定要添加到每个提示末尾的后缀。

### 自定义首选项（prefer option）：

/prefer option set \[name] \[value]

创建可用于将多个参数快速添加到提示末尾的自定义参数。

![img](/assets/AgAABadAeiZjE5TVwxlATYmZMqOMb1dN.DB1aYFIb.jpeg)

/prefer option set mine --hd --ar 7:4

创建一个名为\[mine(我的)]的选项，转换为--hd --ar 7:4

![img](/assets/AgAABadAeiaYBpDrrKREl5jZguwMtQHY.CxIJzEpW.jpeg)

使用

/imagine prompt vibrant California poppies --mine

, 被解释为

/imagine prompt vibrant California poppies --hd --ar 7:4

/prefer option list

列出使用创建的所有选项prefer option set. 用户最多可以有20个自定义选项。

![img](/assets/AgAABadAeiYYhxRln_VEPq5UkWuidYVN.Bj4pJevo.jpeg)

要删除自定义选项，请使用

/prefer option set

值字段并将其留空。

### 自定义首选项（prefer suffix）：

/prefer suffix

在所有提示后自动附加指定的后缀。使用不带值的命令进行复位。

命令示例：

```
/prefer suffix --uplight --video
a. Only Parameters可以与/prefer suffix一起使用
b. 支持：prefer suffix --no orangeis
c. 不支持：prefer suffix orange::-1
```

### 表情图标的功能

用指定的表情图标会触发常用功能

![img](/assets/AgAABadAeiaxRHU06qdCC4kb0Hb-sP9C.DUFjt2EJ.gif)

![img](1_Midjourney命令参数用途列表.assets/AgAABadAeibdtl47FIFADqlIfUigKmzL.png)

envelop：

信封表情符号可以将带有seed种子、生成ID和生成视频的图像发送给你的DM

![img](1_Midjourney命令参数用途列表.assets/AgAABadAeiYwJtXXzI5HproYXWH-FJU9.jpeg)

star：

星星表情符号可以将图像标记为“收藏”，收藏的图像会显示在画廊中，并将图像发送到#favorites频道

![img](1_Midjourney命令参数用途列表.assets/AgAABadAeiafZiPsu_ZAso4pbTNXv6J0.jpeg)

X：

X表情符号可以取消和删除图像，同时也会从画廊中输出

## 参考资料

[👉命令参数用途列表----Midjourney (yuque.com)](https://www.yuque.com/u32937722/qb6y63/lpolvt5kotz8qkmw)

---

---
url: /StableDiffusion/StableDiffusion/2_模型使用.md
---

# 模型下载与使用

## 模型是什么

1、大模型

AI绘画中的“大模型”，一般指Checkpoint。

定义：AI训练的“数据集”，用来支持AI出图作画。

Checkpoint概念的来源：大部分模型会进行不断的训练、微调、迭代，过程中的“储存点”就是一个大模型。

基本属性：大小（1-7G）；常见格式（.ckpt/.safetensor）

2、除了“大模型”以外，还有“小模型”，例如：LoRA、Embeddings、Hypernetwork等。

3、VAE：变分自编码器

简单理解为“色彩滤镜”。

如果模型没有自带的VAE，需要正确配置VAE（最好根据模型作者的推荐），否则画面发灰发白，生成质量堪忧。几个目前比较主流的VAE（kl-f8-anime、vae-ft-mse-840000等）。

![image-20240229000234313](/assets/image-20240229000234313.D7xKtWEY.png)

VAE一般去Huggingface，大部分都可以找到；Civitai上没有专门的VAE分类，但通过搜索也可以找到部分VAE。

模型作者一般会推荐的VAE，是基于训练过程或者试验下来评估出效果最好的一个；使用其他VAE会有一定差别，但不至于“出不了图”。

WebUI左上角没有VAE的选项，需要进入“设置”标签，找到“Stable Diffusion”选项，里面有VAE的相关设置项。如果想把它放到左上角，转到下方“用户界面”选项；在其中的快捷设置列表中添加“sd\_vae”的字段（输入会自动补全），再通过最上方按钮重启前端即可。

## 模型下载渠道

### 官方模型和私炉模型的区别

官方模型：花费了非常大的力气训练出来的基础模型，支持AI作画的根源，但出图效果一般。

私炉模型：在官方模型的基础上“微调”出来的具有风格化特点的模型，由个人创作者训练。

训练模型，也被称为“炼丹”。

### 下载各类模型的渠道

#### Hugging Face（抱脸）

链接：<https://huggingface.co/models>

深度学习和人工智能网站，全世界AI研究与模型共享的前沿阵地，但专业性较强，检索起来不是很直观。

![image-20240301235640449](/assets/image-20240301235640449.BOnGVWgp.png)

#### Civitai（C站）

链接：<https://civitai.com>

全世界最受欢迎的AI绘画模型分享网站，除了模型还有跟多优秀作品展示

![image-20240301235022078](/assets/image-20240301235022078.-odaDVgY.png)

#### LibLibAI（哩布哩布）

链接：<https://www.liblib.art>

国内起步较早的模型网站之一，目前拥有丰富的模型库存与完善的作品展示社区

![image-20240301233912566](/assets/image-20240301233912566.CwIfOs7e.png)

#### eSheep（电子羊）

链接：<https://www.esheep.com>

国内新锐模型网站，有作品热榜、站内私信互动等多种创新性功能，支持在线ComfyUI工作流。

![image-20240301234031521](/assets/image-20240301234031521.CTK1jSKx.png)

### 模型的筛选方式

训练模型：从基础模型训练得来。

融合模型：多个训练模型混合得到的新模型。

模型标签：模型“擅长”的领域，如真人照片、动漫、插画、建筑、卡通、3D等。

### 学会使用模型

查看版本：有些模型可能存在不同的迭代版本，一般选用最新的。

查看ModelCard与模型说明：作者一般会提供使用说明，推荐采样器、VAE等。

查看例图提示词：在模型网站上，一般都可以直接复制作者或其他创作者上传图片的提示词并加以应用。

## 模型风格分类与推荐

### 二次元模型

偏漫画/插画风，具有较鲜明的绘画笔触质感

推荐模型：AbyssOrangeMix、Counterfeit（精致度满满，室内外场景优秀）、Anything（最受欢迎的二次元模型）、Dreamlike Diffusion（魔幻感十足）

### 真实系模型

偏真实系的，拟真化程度高，对现实世界还原强

推荐模型：Deliberate2.5D（精细的写实风格）、Realistic Vision（真实朴素）、LOFI（照片级）

### 2.5D模型

介于前两者之间，接近目前观众对一些游戏和3D动画的想象

推荐模型：NeverEndingDream（NED，动漫角色的二次创作，即真实又二次元）、Protogen x3.4（超现实的画面）、国风3（国风、小人书、水墨风）

### 其他特化风格模型

建筑设计、平面设计等

推荐模型：dvArch - Multi-Prompt Archittecture Tuned Model（富有现代感的建筑）、Cheese Daddy's Landscapes mix（富有魔幻感的场景）、Graphic design\_2.0（富有高级感的平面设计）、Logo生成器（Logo.Redmond）

## 参考资料

模型的使用

[AI绘画模型新手包！“画风”自由切换，有哪些你不知道的模型使用技巧？ | 零基础入门Stable Diffusion的保姆级新手教程 | SD模型下载方式与推荐\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1Us4y117Rg/)

模型推荐

[AI再进化，这次竟然学会摄影了！一秒生成胶片风、拍立得，还能智能“修脸”！Stable Diffusion AI绘画真实系人像模型+LoRA推荐\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1DP41167bZ/)

---

---
url: /15.主题开发/20.目录结构.md
---

# 目录结构

Teek 的目录结构如下：

```sh
packages.
├─ components   # 组件目录，具体内容请看「组件布局的目录结构」
├─ config       # 配置文件目录，在 `.vitepress/config.mts` 文件中引入
├─ helper       # 工具类目录
├─ composables  # composables 目录
├─ locale       # 国际化文件目录
├─ markdown     # markdown 插件目录
├─ static       # 静态资源目录
├─ teek         # Teek 入口文件
├─ theme-chalk   # 样式目录，具体内容请看「样式布局的样式目录」
```

`helper` 目录结构如下：

```sh
helper.
|  ├─ analytics
|  |  ├─ baiduAnalytics.ts    # 百度统计函数
|  |  ├─ googleAnalytics.ts   # 谷歌统计函数
|  |  ├─ umamiAnalytics.ts    # umami 统计函数
├─ color.ts                   # 颜色计算函数
├─ date.ts                    # 日期格式化函数
├─ index.ts                   # 工具函数入口文件，导出了所有的工具函数
├─ is.ts                      # 判断类型函数，如 isString、isFunction 等
├─ types.ts                   # 常用的 TS 类型
├─ util.ts                    # 基础工具函数
```

`composables` 目录结构如下：

```sh
composables.
├─ index.ts                   # composables 入口文件，导出了所有的 composables 函数
├─ onClickOutside.ts          # 监听鼠标点击外部元素函数
├─ useAnchorScroll.ts         # 锚点滚动函数
├─ useUvPv.ts                 # 访问量统计函数
├─ useClipboard.ts            # 文本复制函数
├─ useDebounce.ts             # 防抖函数
├─ useElementHover.ts         # 监听鼠标悬停指定元素函数
├─ useEventListener.ts        # 使用事件监听函数
├─ useLocale.ts               # 多语言读取函数
├─ useMediaQuery.ts           # 媒体查询函数，常用于获取 max-width、min-width 来判断是否为移动端
├─ useMounted.ts              # 监听元素全部挂载完成函数，使用了 Vue 的 onMounted 生命周期
├─ useNamespace.ts            # 命名空间函数，具体使用请看「样式布局的组件元素使用 BEM」
├─ usePopoverSize.ts          # 计算 Popover 出现的位置
├─ useScopeDispose.ts         # 父作用域销毁函数，概念等于 Vue 的 onUnmounted 生命周期
├─ useScrollData.ts           # 数据滚动函数，用于友情链接卡片自动向下滚动
├─ useStorage.ts              # 管理存储的函数，根据传入的存储类型（sessionStorage 或 localStorage）返回相应的操作函数
├─ useSwitchData.ts           # 数据定时切换函数，用于 Body、Banner 的图片切换
├─ useTextTypes.ts            # 文本打印函数，用于 Banner 的详细描述打印效果
├─ useThemeColor.ts           # 主题色计算函数，自动根据主题色计算其他的颜色
├─ useViewTransition.ts       # 切换动画效果函数，用于深色、浅色模式切换
├─ useVpRouter.ts             # 绑定自定义函数到 Router 的钩子里，为了防止覆盖掉其他人已添加在 Router 钩子的逻辑，useVpRouter 不是直接覆盖，而是追加
├─ useWindowSize.ts           # 窗口大小监听函数，用于实时监听窗口的 width、height
├─ useZIndex.ts               # z-index 管理函数
```

* `components` 的目录结构请看 [组件目录结构](/develop/components#目录结构)
* `theme-chalk` 的目录结构请看 [样式目录结构](/develop/styles#目录结构)。

其他目录结构内容较少，从命名可以看出效果，因此暂不进行详细说明。

---

---
url: /10.配置/20.目录页配置.md
---

# 目录页配置

什么是目录页？可以在本文章最上方的面包屑里点击 配置 查看效果。

::: warning
目录页数据来源于 `vitepress-plugin-catalogue` 插件实现，如果禁用了该插件，目录页将不会生效。
:::

目录页本质上是一个 Markdown 文档，因此可以与其他文档一起放到任意目录下，如：

::: code-group

```sh [当前文件夹] {4,10,15}
.
│
├─ 01.指南
│  │  00.目录.md
│  ├─ 01.指南 - 使用
│  │  │   00.目录.md
│  │  ├── 04.使用 - 登录认证.md
│  │  ├── 07.使用 - 权限认证.md
│  ├─ 05.指南 - 环境集成
│  │  │   00.目录.md
│  │  ├── 04.环境集成 - Spring Boot.md
│  │  ├── 07.环境集成 - Spring WebFlux.md
│  │  ├── 99.环境集成 - 上下文组件开发指南.md
├─ 05.设计
│  │  00.目录.md
│  ├─ 01.设计 - 思路
│  │  │  01.设计 - 思路设计.md
│  ├─ 03.设计 - Helpers
│  │  ├── 01.设计 - Helpers 说明.md
```

```sh [专门创建目录页文件夹] {3-7}
.
│
├─ 00.目录页
│  │  01.指南 - 目录.md
│  │  05.使用 - 目录.md
│  │  10.环境集成 - 目录.md
│  │  15.设计 - 目录.md
├─ 01.指南
│  ├─ 01.指南 - 使用
│  │  ├── 04.使用 - 登录认证.md
│  │  ├── 07.使用 - 权限认证.md
│  ├─ 05.指南 - 环境集成
│  │  ├── 04.环境集成 - Spring Boot.md
│  │  ├── 07.环境集成 - Spring WebFlux.md
│  │  ├── 99.环境集成 - 上下文组件开发指南.md
├─ 05.设计
│  ├─ 01.设计 - 思路
│  │  │  01.设计 - 思路设计.md
│  ├─ 03.设计 - Helpers
│  │  ├── 01.设计 - Helpers 说明.md
```

:::

有两种方式可以开启目录页：

1. 在 `frontmatter` 配置 `catalogue: true` 和 `layout: page` 来开启目录页
2. 在 `frontmatter` 配置 `catalogue: true` 和 `layout: TkCataloguePage` 来开启目录页

目录页的 `frontmatter` 配置如下：

::: code-group

```yaml [方式 1]
---
catalogue: true
layout: page
path: 05.设计
desc: Hd Security 设计思路
pageTitle: 设计体系目录
sidebar: false
article: false
---
```

```yaml [方式 1 带注释]
---
catalogue: true # 目录页（必填）
layout: page # page 布局（必填）
path: 05.设计 # 设置为根目录下的某个文件夹相对路径（必填）
desc: Hd Security 设计思路 # 目录描述
pageTitle: 设计体系目录 # 页面标题，默认为目录
sidebar: false # 不显示侧边栏
article: false # 不显示在首页的文章列表和归档页
---
```

```yaml [方式 2]
---
catalogue: true
layout: TkCataloguePage
path: 05.设计
desc: Hd Security 设计思路
pageTitle: 设计体系目录
sidebar: false
article: false
---
```

```yaml [方式 2 带注释]
---
catalogue: true # 目录页（必填）
layout: TkCataloguePage # TkCataloguePage 布局（必填）
path: 05.设计 # 设置为根目录下的某个文件夹相对路径（必填）
desc: Hd Security 设计思路 # 目录描述
pageTitle: 设计体系目录 # 页面标题，默认为目录
sidebar: false # 不显示侧边栏
article: false # 不显示在首页的文章列表和归档页
---
```

:::

::: tip
配置好目录页之后，点击文章页的面包屑将会跳转到目录页。

当然，您也可以在导航栏里添加目录页的链接。
:::

---

---
url: /Redis/Redis基础/2_内存淘汰策略.md
---

# 内存淘汰策略

## 一、Redis过期删除策略

Redis对于过期的key，有两种删除策略：

1. 定时删除
2. 定期删除
3. 惰性删除

### 定时删除

在设置键的过期时间的同时，设置一个定时器，当键过期了，定时器马上把该键删除。

定时删除对内存来说是友好的，因为它可以及时清理过期键；但对CPU是不友好的，如果过期键太多，删除操作会消耗过多的资源。

### 定期删除

redis 会将每个设置了过期时间的 key 放入到一个独立的字典中，以后会定期遍历这个字典来删除到期key。

Redis 默认会每秒进行十次过期扫描（100ms一次），过期扫描不会遍历过期字典中所有的 key，而是采用了一种简单的**贪心策略**。

1. 从过期字典中随机 20 个 key；
2. 删除这 20 个 key 中已经过期的 key；
3. 如果过期的 key 比率超过 1/4，那就重复步骤 1；

redis默认是每隔 100ms就随机抽取一些设置了过期时间的key，检查其是否过期，如果过期就删除。

注意这里是随机抽取的。为什么要随机呢？你想一想假如 redis 存了几十万个 key ，每隔100ms就遍历所有的设置过期时间的 key 的话，就会给 CPU 带来很大的负载。

定期删除是定时删除和惰性删除的一个折中方案。每隔一段时间来删除过期键，可以根据实际场景自定义这个间隔时间，在CPU资源和内存资源上作出权衡。

### 惰性删除

所谓惰性策略就是在客户端访问这个key的时候，redis对key的过期时间进行检查，如果过期了就立即删除，不会给你返回任何东西。

为啥需要两种删除策略呢？

定期删除可能会导致很多过期key到了时间并没有被删除掉。所以就有了惰性删除。假如你的过期 key，靠定期删除没有被删除掉，还停留在内存里，除非你的系统去查一下那个 key，才会被redis给删除掉。这就是所谓的惰性删除，即当你主动去查过期的key时，如果发现key过期了，就立即进行删除，不返回任何东西。

总结：定期删除是集中处理，惰性删除是零散处理。

## 二、配置Redis内存

Redis是基于内存key-value键值对的内存数据库，我们安装完数据库之后，内存往往会受到系统内存大小的限制，我们也可以配置redis能使用的最大的内存大小。

两种方式配置redis的内存：

* 通过配置文件修改
* 通过客户端修改

### 通过配置文件修改

我们可以通过配置文件修改，安装完redis之后我们可以在redis根目录中找到redis.conf文件,在配置文件中添加一下参数就可以设置redis的内存大小了

```sh
# 设置 Redis 最大使用内存大小为100M
maxmemory 100mb    //指定最大内存为100mb
```

上面的配置设置了，当 Redis 使用的内存超过 100Mb 时,就开始对数据进行淘汰。

下面是更加详细的例子：

```sh
#配置文件
maxmemory <bytes>
下面的写法均合法：
maxmemory 1024000
maxmemory 1GB
maxmemory 1G
maxmemory 1024KB
maxmemory 1024K
maxmemory 1024MB
...
```

### 通过客户端修改

在服务器上输入redis-cli之后进入redis客户端，通过命令动态修改redis内存大小

```sh
//设置Redis最大占用内存大小为100M
127.0.0.1:6379> config set maxmemory 100mb
//获取设置的Redis能使用的最大内存大小
127.0.0.1:6379> config get maxmemory
```

如果不设置，或者设置最大内存大小为0，在64位操作系统下，Redis不限制内存大小，在32位操作系统，Redis最多使用3GB内存。

下面是更加详细的例子：

```sh
#命令行
127.0.0.1:6379> config get maxmemory
1) "maxmemory"
2) "0"
127.0.0.1:6379> config set maxmemory 1GB
OK
127.0.0.1:6379> config get maxmemory
1) "maxmemory"
2) "1073741824"
```

## 三、Redis内存淘汰策略

我们设置完redis内存之后，我们就像里面放数据，但是内存总有满的时候，满的时候redis又是怎么处理的呢？

每进行一次redis操作的时候，redis都会检测可用内存，判断是否要进行内存淘汰，当超过可用内存的时候，redids 就会使用对应淘汰策略。

### 八种内存淘汰策略

redis内存淘汰策略，具体如下：

1. **no-envicition**
   该策略对于写请求不再提供服务，会直接返回错误，当然排除del等特殊操作，redis默认是no-envicition策略。
2. **allkeys-random**
   从redis中随机选取key进行淘汰。
3. **allkeys-lru**
   使用LRU（Least Recently Used，最近最少使用）算法，从redis中选取使用最少的key进行淘汰。
4. **volatile-random**
   从redis中设置过过期时间的key，进行随机淘汰。
5. **volatile-ttl**
   从redis中选取即将过期的key，进行淘汰。
6. **volatile-lru**
   使用LRU（Least Recently Used，最近最少使用）算法，从redis中设置过过期时间的key中，选取最少使用的进行淘汰。
7. **volatile-lfu**
   使用LFU（Least Frequently Used，最不经常使用），从设置了过期时间的键中选择某段时间之内使用频次最小的键值对清除掉。
8. **allkeys-lfu**
   使用LFU（Least Frequently Used，最不经常使用），从所有的键中选择某段时间之内使用频次最少的键值对清除。

主要是4种算法，针对不同的key，形成的策略。

算法：

1. random：随机淘汰。
2. ttl：快要过期的先淘汰。
3. lru（Least Recently Used）：最近很少的使用的key（根据时间，最不常用的淘汰）。
4. lfu（Least Frequently Used）：最近很少的使用的key (根据计数器，用的次数最少的key淘汰)。

key ：

1. volatile：有过期的时间的那些key。
2. allkeys：所有的key。

### 内存淘汰算法的具体工作原理

* 客户端执行一条新命令，导致数据库需要增加数据（比如set key value）
* Redis会检查内存使用，如果内存使用超过 maxmemory，就会按照置换策略删除一些 key
* 新的命令执行成功

### 如何设置淘汰策略

可以命令查看当前redis使用的淘汰策略

```
config get maxmemory-policy    //获取当前内存淘汰策略
config set maxmemory-policy valatile-lru  //通过命令修改淘汰策略
```

## 四、Redis 内存淘汰算法

Redis 的淘汰算法有多种，如下：

1. random
2. TTL
3. LRU（Least Recently Used，最近最少使用）
4. LFU（Least Frequently Used，最不经常使用）

random随机算法很好理解，就是从数据库中随机淘汰一些 Keys。

TTL 算法就是从设置了过期时间的 Keys 中获取最早过期的 一批 Keys，然后淘汰这些 Keys。

而 LRU 和 LFU 这两种算法在名字上比较容易混淆，所以这里介绍一下这两种算法的区别。

在缓存的内存淘汰策略中有FIFO、LRU、LFU三种，其中LRU和LFU是Redis在使用的。FIFO是最简单的淘汰策略，遵循着先进先出的原则，这里简单提一下：

![img](/assets/20200707212146582.DjGzyPC8.png)

### LRU算法

LRU（Least Recently Used，最近最少使用），主要是通过 Key 的最后访问时间来判定哪些 Key 更适合被淘汰，如下图所示：

![在这里插入图片描述](/assets/2021033121023937.DIDPccie.png)

如上图所示，所有的 Keys 都根据最后被访问的时间来进行排序的，所以在淘汰时只需要按照所有 Keys 的最后被访问时间，由小到大来进行即可。

![在这里插入图片描述](/assets/2021033121042516.Du0mx8Vf.png)

**LRU算法的特点：**

1. 新增key value的时候首先在链表结尾添加Node节点，如果超过LRU设置的阈值就淘汰队头的节点并删除掉HashMap中对应的节点。

2. 修改key对应的值的时候先修改对应的Node中的值，然后把Node节点移动队尾。

3. 访问key对应的值的时候把访问的Node节点移动到队尾即可。

**Redis的LRU实现：**

Redis维护了一个24位时钟，可以简单理解为当前系统的时间戳，每隔一定时间会更新这个时钟。每个key对象内部同样维护了一个24位的时钟，当新增key对象的时候会把系统的时钟赋值到这个内部对象时钟。比如我现在要进行LRU，那么首先拿到当前的全局时钟，然后再找到内部时钟与全局时钟距离时间最久的（差最大）进行淘汰，这里值得注意的是全局时钟只有24位，按秒为单位来表示才能存储194天，所以可能会出现key的时钟大于全局时钟的情况，如果这种情况出现那么就两个相加而不是相减来求最久的key。

```c
struct redisServer {
       pid_t pid; 
       char *configfile; 
       //全局时钟
       unsigned lruclock:LRU_BITS; 
       ...
};
typedef struct redisObject {
    unsigned type:4;
    unsigned encoding:4;
    /* key对象内部时钟 */
    unsigned lru:LRU_BITS;
    int refcount;
    void *ptr;
} robj;
```

Redis中的LRU与常规的LRU实现并不相同，常规LRU会准确的淘汰掉队头的元素，但是Redis的LRU并不维护队列，只是根据配置的策略要么从所有的key中随机选择N个（N可以配置）要么从所有的设置了过期时间的key中选出N个键，然后再从这N个键中选出最久没有使用的一个key进行淘汰。

下图是常规LRU淘汰策略与Redis随机样本取一键淘汰策略的对比，浅灰色表示已经删除的键，深灰色表示没有被删除的键，绿色表示新加入的键，越往上表示键加入的时间越久。从图中可以看出，在redis 3中，设置样本数为10的时候能够很准确的淘汰掉最久没有使用的键，与常规LRU基本持平。

![在这里插入图片描述](/assets/20210331210734240.CapWf8Hy.png)

**为什么要使用近似LRU？**

1、性能问题，由于近似LRU算法只是最多随机采样N个key并对其进行排序，如果精准需要对所有key进行排序，这样近似LRU性能更高

2、内存占用问题，redis对内存要求很高，会尽量降低内存使用率，如果是抽样排序可以有效降低内存的占用

3、实际效果基本相等，如果请求符合长尾法则，那么真实LRU与Redis LRU之间表现基本无差异

4、在近似情况下提供可自配置的取样率来提升精准度，例如通过 CONFIG SET maxmemory-samples 指令可以设置取样数，取样数越高越精准，如果你的CPU和内存有足够，可以提高取样数看命中率来探测最佳的采样比例。

### LFU算法

LFU（Least Frequently Used）表示最不经常使用，它是根据数据的历史访问频率来淘汰数据，其核心思想是“如果数据过去被访问多次，那么将来被访问的频率也更高”。

LFU算法反映了一个key的热度情况，不会因LRU算法的偶尔一次被访问被误认为是热点数据。

LFU算法的常见实现方式为链表：

新数据放在链表尾部 ，链表中的数据按照被访问次数降序排列，访问次数相同的按最近访问时间降序排列，链表满的时候从链表尾部移出数据。

![img](/assets/20200707183732233.B4Go38W-.png)

## 参考资料

https://blog.csdn.net/crazymakercircle/article/details/115360829

https://blog.csdn.net/weixin\_40980639/article/details/125446002

---

---
url: /Java/JVM性能调优/02.JVM性能监控与调优/5_内存泄露几种情况及案例分析.md
---

# 内存泄露几种情况及案例分析

## Java中内存泄露的几种情况

### 静态集合类

静态集合类，如HashMap、LinkedList等等。如果这些容器为静态的，那么它们的生命周期与JVM程序一致，则容器中的对象在程序结束之前将不能被释放，从而造成内存泄漏。简单而言，长生命周期的对象持有短生命周期对象的引用，尽管短生命周期的对象不再使用，但是因为长生命周期对象持有它的引用而导致不能被回收。

```java
public class MemoryLeak {
    static List list = new ArrayList();
    public void oomTests() {
        Object obj ＝ new Object(); // 局部变量
        list.add(obj);
    }
}
```

### 单例模式

单例模式，和静态集合导致内存泄露的原因类似，因为单例的静态特性，它的生命周期和 JVM 的生命周期一样长，所以如果单例对象如果持有外部对象的引用，那么这个外部对象也不会被回收，那么就会造成内存泄漏。

### 内部类持有外部类

内部类持有外部类，如果一个外部类的实例对象的方法返回了一个内部类的实例对象。这个内部类对象被长期引用了，即使那个外部类实例对象不再被使用，但由于内部类持有外部类的实例对象，这个外部类对象将不会被垃圾回收，这也会造成内存泄漏。

### 各种连接，如数据库连接、网络连接和IO连接等

在对数据库进行操作的过程中，首先需要建立与数据库的连接，当不再使用时，需要调用close方法来释放与数据库的连接。只有连接被关闭后，垃圾回收器才会回收对应的对象。否则，如果在访问数据库的过程中，对Connection、Statement或ResultSet不显性地关闭，将会造成大量的对象无法被回收，从而引起内存泄漏。

```sh
public static void main(String[] args) {
    try{
        Connection conn =null;
        Class.forName("com.mysql.jdbc.Driver");
        conn =DriverManager.getConnection("url","","");
        Statement stmt =conn.createStatement();
        ResultSet rs =stmt.executeQuery("....");
    } catch（Exception e）{//异常日志
    } finally {
        // 1．关闭结果集 Statement
        // 2．关闭声明的对象 ResultSet
        // 3．关闭连接 Connection
    }
}
```

### 变量不合理的作用域

变量不合理的作用域。一般而言，一个变量的定义的作用范围大于其使用范围，很有可能会造成内存泄漏。另一方面，如果没有及时地把对象设置为null，很有可能导致内存泄漏的发生。

```java
public class UsingRandom {
    private String msg;
    public void receiveMsg(){
        readFromNet(); // 从网络中接受数据保存到msg中
        saveDB(); // 把msg保存到数据库中
    }
}
```

如上面这个伪代码，通过readFromNet方法把接受的消息保存在变量msg中，然后调用saveDB方法把msg的内容保存到数据库中，此时msg已经就没用了，由于msg的生命周期与对象的生命周期相同，此时msg还不能回收，因此造成了内存泄漏。实际上这个msg变量可以放在receiveMsg方法内部，当方法使用完，那么msg的生命周期也就结束，此时就可以回收了。还有一种方法，在使用完msg后，把msg设置为null，这样垃圾回收器也会回收msg的内存空间。

### 改变哈希值

改变哈希值，当一个对象被存储进HashSet集合中以后，就不能修改这个对象中的那些参与计算哈希值的字段了。

否则，对象修改后的哈希值与最初存储进HashSet集合中时的哈希值就不同了，在这种情况下，即使在contains方法使用该对象的当前引用作为的参数去HashSet集合中检索对象，也将返回找不到对象的结果，这也会导致无法从HashSet集合中单独删除当前对象，造成内存泄漏。

这也是 String 为什么被设置成了不可变类型，我们可以放心地把 String 存入 HashSet，或者把String 当做 HashMap 的 key 值；

当我们想把自己定义的类保存到散列表的时候，需要保证对象的 hashCode 不可变。

```java
/**
 * 例1
 */
public class ChangeHashCode {
    public static void main(String[] args) {
        HashSet set = new HashSet();
        Person p1 = new Person(1001, "AA");
        Person p2 = new Person(1002, "BB");

        set.add(p1);
        set.add(p2);

        p1.name = "CC";//导致了内存的泄漏
        set.remove(p1); //删除失败

        System.out.println(set);

        set.add(new Person(1001, "CC"));
        System.out.println(set);

        set.add(new Person(1001, "AA"));
        System.out.println(set);

    }
}

class Person {
    int id;
    String name;

    public Person(int id, String name) {
        this.id = id;
        this.name = name;
    }

    @Override
    public boolean equals(Object o) {
        if (this == o) return true;
        if (!(o instanceof Person)) return false;

        Person person = (Person) o;

        if (id != person.id) return false;
        return name != null ? name.equals(person.name) : person.name == null;
    }

    @Override
    public int hashCode() {
        int result = id;
        result = 31 * result + (name != null ? name.hashCode() : 0);
        return result;
    }

    @Override
    public String toString() {
        return "Person{" +
                "id=" + id +
                ", name='" + name + '\'' +
                '}';
    }
}
```

```java
/**
 * 例2
 */
public class ChangeHashCode1 {
    public static void main(String[] args) {
        HashSet<Point> hs = new HashSet<Point>();
        Point cc = new Point();
        cc.setX(10);//hashCode = 41
        hs.add(cc);

        cc.setX(20);//hashCode = 51  此行为导致了内存的泄漏

        System.out.println("hs.remove = " + hs.remove(cc));//false
        hs.add(cc);
        System.out.println("hs.size = " + hs.size());//size = 2

        System.out.println(hs);
    }

}

class Point {
    int x;

    public int getX() {
        return x;
    }

    public void setX(int x) {
        this.x = x;
    }

    @Override
    public int hashCode() {
        final int prime = 31;
        int result = 1;
        result = prime * result + x;
        return result;
    }

    @Override
    public boolean equals(Object obj) {
        if (this == obj) return true;
        if (obj == null) return false;
        if (getClass() != obj.getClass()) return false;
        Point other = (Point) obj;
        if (x != other.x) return false;
        return true;
    }

    @Override
    public String toString() {
        return "Point{" +
                "x=" + x +
                '}';
    }
}
```

### 缓存泄露

内存泄漏的另一个常见来源是缓存，一旦你把对象引用放入到缓存中，他就很容易遗忘。比如：之前项目在一次上线的时候，应用启动奇慢直到夯死，就是因为代码中会加载一个表中的数据到缓存（内存）中，测试环境只有几百条数据，但是生产环境有几百万的数据。

对于这个问题，可以使用WeakHashMap代表缓存，此种Map的特点是，当除了自身有对key的引用外，此key没有其他引用那么此map会自动丢弃此值。

```java
public class MapTest {
    static Map wMap = new WeakHashMap();
    static Map map = new HashMap();

    public static void main(String[] args) {
        init();
        testWeakHashMap();
        testHashMap();
    }

    public static void init() {
        String ref1 = new String("obejct1");
        String ref2 = new String("obejct2");
        String ref3 = new String("obejct3");
        String ref4 = new String("obejct4");
        wMap.put(ref1, "cacheObject1");
        wMap.put(ref2, "cacheObject2");
        map.put(ref3, "cacheObject3");
        map.put(ref4, "cacheObject4");
        System.out.println("String引用ref1，ref2，ref3，ref4 消失");

    }

    public static void testWeakHashMap() {
        System.out.println("WeakHashMap GC之前");
        for (Object o : wMap.entrySet()) {
            System.out.println(o);
        }
        try {
            System.gc();
            TimeUnit.SECONDS.sleep(5);
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
        System.out.println("WeakHashMap GC之后");
        for (Object o : wMap.entrySet()) {
            System.out.println(o);
        }
    }

    public static void testHashMap() {
        System.out.println("HashMap GC之前");
        for (Object o : map.entrySet()) {
            System.out.println(o);
        }
        try {
            System.gc();
            TimeUnit.SECONDS.sleep(5);
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
        System.out.println("HashMap GC之后");
        for (Object o : map.entrySet()) {
            System.out.println(o);
        }
    }
}
```

上面代码和图示主演演示WeakHashMap如何自动释放缓存对象，当init函数执行完成后，局部变量字符串引用weakd1，weakd2，d1，d2都会消失，此时只有静态map中保存中对字符串对象的引用，可以看到，调用gc之后，HashMap的没有被回收，而WeakHashMap里面的缓存被回收了。

### 监听器和其他回调

内存泄漏第三个常见来源是监听器和其他回调，如果客户端在你实现的API中注册回调，却没有显示的取消，那么就会积聚。

需要确保回调立即被当作垃圾回收的最佳方法是只保存它的弱引用，例如将他们保存成为WeakHashMap中的键。

---

---
url: /Python/爬虫/1_爬虫的流程.md
---

# 爬虫的流程

爬虫是一种通过代码自动获取网页数据的技术，过程简单且应用广泛，但需在合法合规的前提下使用。以下是爬虫的核心步骤、注意事项及学习要点：

### 一、爬虫的基本步骤

无论目标是单个网页还是批量数据，爬虫的核心流程可分为三步：

#### 1. 获取网页内容

* 原理：通过代码向网站服务器发送 HTTP 请求，服务器返回网页的原始数据（如 HTML、JSON 等）。
  * 与浏览器访问的区别：浏览器会渲染数据为可视化页面，而爬虫获取的是未渲染的原始内容。
* **工具**：常用 Python 的`requests`库发送请求，简洁高效。

#### 2. 解析网页内容

* **目的**：从海量原始数据中提取目标信息（如商品价格、文章标题等）。
* **基础**：需了解 HTML 网页结构（标签、属性等），因为多数网页内容以 HTML 格式返回。
* **工具**：使用`Beautiful Soup`库解析 HTML，快速定位并提取所需数据。

#### 3. 储存或分析数据

* 根据需求处理：
  * 若需长期使用，可存入数据库（如 MySQL、MongoDB）；
  * 若需分析趋势，可生成可视化图表（如用`matplotlib`、`pandas`）；
  * 若需舆情监控，可结合 AI 进行文本情感分析等。
* **扩展**：支持批量爬取（如遍历多个 URL）或深度爬取（从一个网页链接跳转至其他相关页面）。

### 二、爬虫的合法合规性：红线与原则

技术本身中立，但需严格遵守规则，避免法律风险：

#### 1. 绝对禁止爬取的内容

* 公民隐私数据（如个人信息、账号密码）；
* 受著作权保护的内容（如付费文章、原创作品，未经授权不得爬取）；
* 涉及国家事务、国防建设、尖端科技等敏感领域的计算机系统数据。

#### 2. 做 “温和的爬虫”

* **控制请求频率**：避免高频、海量请求，防止给服务器造成负担（类似 DDoS 攻击）；
* **尊重反爬机制**：不强行突破网站的限制（如登录验证、验证码、IP 封锁等）；
* **遵守`robots.txt`协议**：访问网站根目录下的`robots.txt`文件（如`https://example.com/robots.txt`），了解网站允许爬取的范围和限制（部分网站会明确禁止爬虫访问某些路径）。

### 三、学习爬虫需掌握的知识

1. **HTTP 请求基础**：理解请求方法（GET/POST）、 headers 等，是发送有效请求的前提；
2. **`requests`库**：Python 中发送 HTTP 请求的核心工具，需掌握其基本用法；
3. **HTML 结构**：了解标签、属性、层级关系，才能准确解析网页内容；
4. **`Beautiful Soup`库**：解析 HTML 的利器，需掌握如何定位和提取数据。

---

---
url: /Python/爬虫/4_爬取豆瓣电影Top250.md
---

# 爬取豆瓣电影Top250

通过 Beautifulsoup 解析 HTML，结合循环遍历多页链接，可完整提取豆瓣电影 Top250 的所有中文标题。以下是具体步骤：

## 一、准备工作：安装库与引入模块

1. **安装 Beautiful Soup**：在终端执行

   ```sh
   pip install bs4  # Windows
   # 或 pip3 install bs4 （macOS/Linux）
   ```

   显示`Successfully installed bs4`即安装成功。

2. **引入所需库**：在 Python 文件（如`scrape_douban.py`）中引入

   ```python
   import requests
   from bs4 import BeautifulSoup
   ```

## 二、核心步骤：解析单页标题

### 1. 发送请求并获取 HTML

发送带`User-Agent`的请求，获取豆瓣电影 Top250 第一页的 HTML：

```python
# 定义请求头（伪装浏览器）
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
}

# 第一页URL（后续扩展为多页）
url = "https://movie.douban.com/top250"
response = requests.get(url, headers=headers)
html = response.text  # 存储HTML内容
```

### 2. 解析 HTML 并提取标题

通过浏览器开发者工具分析：电影标题位于`class="title"`的\`\`标签内，且中文标题不含斜杠`/`（区分于原版标题）。

解析代码：

```python
# 创建Beautiful Soup对象
soup = BeautifulSoup(html, "html.parser")

# 查找所有class="title"的<span>标签
all_titles = soup.find_all("span", attrs={"class": "title"})

# 提取中文标题（过滤含斜杠的原版标题）
for title_tag in all_titles:
    title = title_tag.string  # 获取标签内文本
    if "/" not in title:  # 仅保留中文标题
        print(title)
```

## 三、扩展：爬取全部 10 页内容

豆瓣电影 Top250 分 10 页展示，每页 25 部电影，URL 通过`start`参数区分（如`start=0`为第 1 页，`start=25`为第 2 页，直至`start=225`为第 10 页）。

### 循环遍历多页

```python
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
}

# 遍历10页（start参数：0,25,50,...,225）
for start_num in range(0, 250, 25):
    # 构造每页URL
    url = f"https://movie.douban.com/top250?start={start_num}"
    response = requests.get(url, headers=headers)
    html = response.text
    
    # 解析当前页
    soup = BeautifulSoup(html, "html.parser")
    all_titles = soup.find_all("span", attrs={"class": "title"})
    
    # 提取并打印中文标题
    for title_tag in all_titles:
        title = title_tag.string
        if "/" not in title:
            print(title)
```

## 四、关键说明

1. **多页逻辑**：通过`range(0, 250, 25)`生成`start`参数（0 到 225，步长 25），覆盖全部 10 页。
2. **标题过滤**：利用`if "/" not in title`排除含斜杠的原版标题（如 “肖申克的救赎 / The Shawshank Redemption” 仅保留 “肖申克的救赎”）。
3. **反爬处理**：始终携带`User-Agent`请求头，避免被豆瓣识别为爬虫。

---

---
url: /Java/数据结构/排序算法.md
---

# 排序算法

## 简单排序算法

### 直接插入排序

基本思想：在一个已排好序的记录子集的基础上，每一步将下一个待排序的记录有序插入到已排好序的记录子集中，直到将所有待排记录全部插入为止。

具体过程为：将第个记录的关键字Ki顺次与其前面记录的关键字Ki-1,ki-2,...ki进行比较，将所有关键字大于K的记录依次向后移动一个位置，直到遇见一个关键字小于或者等于Ki的记录Kj，此时K后面必为空位置，将第i个记录插入空放置即可。

![img](/assets/image.BMw5v_YR.png)

```c
#include <stdio.h>

void insertion_sort(int arr[], int n) {
    // 遍历数组，从第二个元素开始
    for (int i = 1; i < n; ++i) {
        // 将数组中的元素赋值给key
        int key = arr[i];
        // 将key的前一个元素赋值给j
        int j = i - 1;
        // 当j大于等于0且arr[j]大于key时，从j的前一个元素开始，将arr[j]放到arr[j+1]中
        while (j >= 0 && arr[j] > key) {
            arr[j + 1] = arr[j];
            j = j - 1;
        }
        // 将key放到arr[j+1]中
        arr[j + 1] = key;
    }
}

int main() {
    //定义一个数组，数组长度为8
    int arr[] = {64, 34, 25, 12, 22, 11, 90};
    //获取数组的长度
    int n = sizeof(arr) / sizeof(arr[0]);
    //调用insertion_sort函数，对数组进行插入排序
    insertion_sort(arr, n);
    //打印排序后的数组
    printf("Sorted array is: \n");
    for (int i = 0; i < n; ++i) {
        printf("%d ", arr[i]);
    }
    printf("\n");
    return 0;
}
```

该算法的要点是：

1. 使用监视哨key（即arr\[0]）临时保存待插入的记录。
2. 从后往前查找应插入的位置。
3. 查找与移动用同一循环完成。

直接插入排序算法分析：

1. 从空间角度来看，它只需要一个辅助空间key（即arr\[0]）。
2. 从时间耗费角度来看，主要时间耗费在关键字比较和移动元素上。

直接插入排序方法是稳定的排序方法。

### 简单选择排序

```c
void simple_selection_sort(int arr[], int n) {
    // 循环遍历数组
    for (int i = 0; i < n - 1; i++) {
        // 记录最小值的索引
        int min_idx = i;
        // 遍历下一个元素
        for (int j = i + 1; j < n; j++) {
            // 如果当前元素小于最小值，则更新最小值索引
            if (arr[j] < arr[min_idx]) {
                min_idx = j;
            }
        }
        // 如果最小值索引不等于当前元素索引，则交换两个元素
        if (min_idx!= i) {
            int temp = arr[i];
            arr[i] = arr[min_idx];
            arr[min_idx] = temp;
        }
    }
}
```

### 起泡排序

```c
void bubble_sort(int arr[], int n) {
    // 定义一个变量i，用于循环
    int i, j, temp;
    // 循环n-1次
    for (i = 0; i < n - 1; i++) {
        // 循环n-i-1次
        for (j = 0; j < n - i - 1; j++) {
            // 如果arr[j]大于arr[j+1]，则交换
            if (arr[j] > arr[j + 1]) {
                temp = arr[j];
                arr[j] = arr[j + 1];
                arr[j + 1] = temp;
            }
        }
    }
}
```

## 复杂排序算法

### 希尔排序

先取一个小于n的整数d1作为第一个增量，把文件的全部记录分组。所有距离为d1的倍数的记录放在同一个组

中。即R\[1],R\[1+d1],R\[1+2d1],..为第一组，R\[2],R\[2+d1],R\[2+2d1]....为第二组，先在各组内进行直接插入排序;

然后，取第二个增量d2\<d1重复上述的分组和排序，直至所取的增量dt=1(dt\<dt-1<...\<d2\<d1)，即所有记录放

在同一组中进行直接插入排序为止。

用增量来划分子序列的方法，达到减少关键字移动次数的目的。

![img](/assets/image.BMw5v_YR.png)

```c
void ShellPass(SeqList R, int d)
{//希尔排序中的一趟排序，d为当前增量
    int i, j;
    for (i = d + 1; i <= n; i++) {//将R[d+1．．n]分别插入各组当前的有序区
        if (R[i].key < R[i - d].key) {
            R[0] = R[i];//R[0]只是暂存单元
            j = i - d;
            while (j > 0 && R[0].key < R[j].key) {//查找R的插入位置
                R[j + d] = R[j];//后移记录
                j = j - d;//查找前一记录
            }
            R[j + d] = R[0];//插入R到正确的位置上
        }
    }
}
```

### 堆排序

堆排序是在排序过程中，将数据R\[1..n]看成一棵完全二叉树，利用完全二叉树中双亲结点和孩子结点之间的内在关系来选择关键字最小的记录。

具体做法：

​    把待排序的记录的关键字存放在数组r\[1..n]之中将r 看成是一棵完全二叉树的顺序表示，每个结点表示一个记录，第一个记录r\[1]作为二叉树的根。以下各记录r\[2...n]依次逐层从左到右顺序排列，任意结点r间的左孩子是r\[2i]，右孩子是r\[2i+1]，双亲是r\[Li/2]]。对这棵完全二叉树进行调整，使各结点的关键字值满足下列条件：

​    r\[i].keyar\[2i].key并且r\[i].keyzr\[2i+1].key(i=1,2, ... Ln/2]) .

​    满足这个条件的完全二叉树为大根堆。

反之，如果这棵完全二叉树中任意结点的关键字小于或者等于其左孩子和右孩子的关键字（当有左孩子或右孩子时），对应的堆为小根堆。

---

---
url: /10.配置/目录.md
---


---

---
url: /常用框架/SpringBoot/SpringBoot与Web应用/4_配置错误页.md
---

# 配置错误页

---

---
url: /常用框架/SpringBoot/SpringBoot程序开发/4_配置访问路径.md
---

# 配置访问路径

## 一、控制器模式

在实际的项目开发中，控制器的路径可能会有许多个。在进行控制器编写的时候，也会有以下两种运行模式。

1. 控制器跳转模式：可以使用@Controller注解定义，如果要实现Restful显示，也可以联合@ResponseBody注解一起使用。
2. Restful显示：可以使用@RestController注解，里面所有路径访问的信息都以Restful形式展示。

## 二、使用Restful风格

```java
@RestController
public class MessageController {

    @RequestMapping("/")
    public String home(){
        return "www.xxl.cn";
    }
    
}
```

此时的控制器程序类上使用了@RestController注解，这样就可以避免在方法上使用@ResponseBody注解。此时，MessageController类中的所有映射路径都会以Restful形式展示。

> **提示：Restful是SpringCloud技术的实现核心。**
>
> 在控制器里面一旦使用了@RestController注解，则意味着所有方法都将以Restful风格展示。这种做法未必适合于SpringBoot项目，因为在很多时候需要通过控制器跳转到显示层页面，而Restful是SpringCloud技术的实现关键。

## 三、地址传递参数

也可以进行参数的接收处理。传递参数到控制器中最简单的做法是使用地址重写传递“`访问路径?参数名称=内容`”（`只支持GET请求`），在MessageController控制器程序类之中扩充一个新的echo()方法。

```java
@GetMapping("/echo")
public String echo(String msg) {
    return "【echo】" + msg;
}
```

此时如果要进行该路径的访问，则可以直接通过地址栏传递参数（http://localhost:8080/echo?msg=www.xxlcn），并且参数的名称应该默认使用msg。

## 四、@PathVariable

由于SpringBoot支持Restful风格处理，所以参数的接收可以采用路径参数的形式完成，但是需要在控制器方法的参数声明上使用@PathVariable注解与访问路径的参数进行关联。（`只支持GET请求`）

```java
@GetMapping("/echo/{message}")
public String echo2(@PathVariable("message") String msg) {
    return "【echo】" + msg;
}
```

通过地址传递参数，地址设置为http://localhost:8080/echo/www.xxl.cn

---

---
url: /常用框架/SpringBoot/SpringBoot程序开发/8_配置环境属性.md
---

# 配置环境属性

## 一、properties配置

SpringBoot提倡的是一种“零配置”的设计框架，所以提供有许多默认的配置项。例如，SpringBoot项目默认运行的8080端口就是一种默认配置。如果开发者需要修改SpringBoot的这种默认配置，可以在项目所在的CLASSPATH下添加`application.properties`配置文件。

建立一个新的源文件目录`src/main/resources`

在`src/main/resources`源文件目录中建立`application.properties`配置文件

> **注意：配置文件名称要相同。**
>
> SpringBoot开发框架对一些结构（子包扫描）和配置文件（application.properties）做出了限定，这样开发者在使用框架开发的时候可以减少配置。如果开发者定义的配置文件名称不是application.properties，那么SpringBoot将无法加载。

## 二、端口修改

在配置中进行SpringBoot项目默认端口的变更，将其修改为80端口运行。

```properties
# 设置运行服务所在端口
server.port=80
```

修改完成后重新启动SpringBoot项目（使用的是Tomcat容器），可以看到提示信息：Tomcat started on port(s): 80 (http)，表示当前的项目可以直接运行在80端口上。

## 三、配置上下文路径

SpringBoot项目默认情况下会将程序发布在根目录下，如果有需要，也可以配置上下文路径（ContextPath）。

```properties
# 设置运行服务所在端口
server.port=80
# 配置ContextPath访问路径，实际开发中一般不进行配置
server.servlet.context-path=/xxl
```

追加了一个`context-path`配置，所以项目的访问路径为

http://localhost/xxl/（追加了/xxl的路径前缀）

## 四、yml配置

在SpringBoot中可以使用的配置文件类型有两种：`application.properties`和`application.yml`，这两种配置文件都可以实现对SpringBoot环境的修改。下面将`application. properties`配置替换为`application.yml`，内容如下：

```yaml
server:
  port: 80 # 设置运行服务所在端口
  servlet:
    context-path: /xxl # 定义ContextPath访问路径
```

使用`application.yml`配置文件的结构要比使用`application.properties`更加清晰

> **提示：关于yml配置文件说明。**
>
> yml实际上是YAML（Yet Another Markup Languange，一种标记语言）文件，这是一种结构化的数据文件，大量应用在各种开源项目之中，如Apache Storm。
>
> Spring官方推荐使用application.yml来进行SpringBoot或SpringCloud框架的配置定义。如果项目中同时存在application.yml与application.properties配置文件并且配置冲突，将以application.properties文件中的配置为参考。

---

---
url: /10.配置/01.主题配置/01.配置简介.md
---

# 配置简介 推荐

::: tip
Teek 内置了大量的主题配置，但不会影响 vitepress 原来的配置，两者是独立生效的。
:::

主题的配置通常添加在 `.vitepress/config.mts` 文件中。

如下是一份简单的模板：

```ts
// .vitepress/config.mts
import { defineConfig } from "vitepress";
import { defineTeekConfig } from "vitepress-theme-teek/config";

// Teek 主题配置
const teekConfig = defineTeekConfig({
  // ...
});

// VitePress 配置
export default defineConfig({
  extends: teekConfig,
  // ...
});
```

在 VitePress 配置中通过 `extends` 可以将主题配置合并到 VitePress 配置里，也就是说完全可以在主题配置里添加 VitePress 的 `themeConfig` 配置项，但是不能反过来，如：

::: code-group

```ts [各自配置]
// .vitepress/config.mts
import { defineConfig } from "vitepress";
import { defineTeekConfig } from "vitepress-theme-teek/config";

// Teek 主题配置
const teekConfig = defineTeekConfig({
  teekTheme: true,
});

// VitePress 配置
export default defineConfig({
  extends: teekConfig,
  themeConfig: {
    logo: "/teek-logo-mini.svg",
  },
});
```

```ts [统一配置]
// .vitepress/config.mts
import { defineConfig } from "vitepress";
import { defineTeekConfig } from "vitepress-theme-teek/config";

// Teek 主题配置 + VitePress 的 themeConfig 配置
const teekConfig = defineTeekConfig({
  teekTheme: true,
  logo: "/teek-logo-mini.svg",
});

export default defineConfig({
  extends: teekConfig,
});
```

:::

Teek 的所有主题配置支持 4 种方式：

1. `provide`：在 `.vitepress/theme/index.ts` 通过 provide 函数注入配置项
2. `frontmatter.tk`：在 Markdown 文档的 `frontmatter` 配置
3. `frontmatter`：在 Markdown 文档的 `frontmatter` 配置
4. `config`：在 `.vitepress/config.mts` 配置

::: warning
函数式和 Node 环境的配置项无法在 `frontmatter` 中配置。
:::

优先级依次从高到低排列，如 `frontmatter` 的配置会覆盖 `config` 的配置，因此您可以在 `config` 全局配置，然后在部分 Markdown 文档的 `frontmatter` 进行局部配置覆盖。

::: tip `frontmatter.tk` 和 `frontmatter` 配置的区别

* `frontmatter.tk` 建议在首页 `index.md` 配置，目的是为防止和 VitePress 的冲突，而文章页建议使用 `frontmatter` 配置。
* 如果部分配置项与第三方插件的配置有冲突，也可以使用 `frontmatter.tk` 配置。

:::

举个例子：

::: code-group

```ts [provide] {11}
// .vitepress/theme/index.ts
import Teek, { teekConfigContext } from "vitepress-theme-teek";
import "vitepress-theme-teek/index.css";
import { defineComponent, h, provide } from "vue";

export default {
  extends: Teek,
  Layout: defineComponent({
    name: "TeekProvider",
    setup() {
      provide(teekConfigContext, { author: { name: "Teeker" } });
      return () => h(Teek.Layout);
    },
  }),
};
```

```yaml [frontmatter.tk]
---
tk:
  author:
    name: Teeker
---
```

```yaml [frontmatter]
---
author:
  name: Teeker
---
```

```ts [config]
// .vitepress/config.mts
import { defineConfig } from "vitepress";
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  author: {
    name: "Teeker",
  },
});

export default defineConfig({
  extends: teekConfig,
});
```

:::

Teek 的所有主题配置请阅读 [Types](https://github.com/Kele-Bingtang/vitepress-theme-teek/blob/master/packages/config/types.ts) 文件。

侧边栏的主题配置专题已经按照模块进行划分，如：

* 全局配置讲述对全局生效的配置说明
* Banner 配置讲述对博客风格首页 Banner 模块配置
* 文章列表配置讲述对博客风格首页的文章列表模块配置
* ...

**接下来请在侧边栏选择自己要配置的模块文章进行阅读。**

---

---
url: /daily/面试专栏/微服务相关/3_配置中心.md
---

# 配置中心

---

---
url: /常用框架/SpringBoot/SpringBoot与Web应用/1_配置Tomcat运行.md
---

# 配置Tomcat运行

---

---
url: /Java/系统优化/系统优化/5_平台智能化.md
---

# 平台智能化

---

---
url: /10.配置/01.主题配置/35.评论配置.md
---

# 评论配置

## comment

评论配置，目前内置 `Giscus`、`Twikoo`、`Waline`、`Artalk` 四种评论插件。

::: tip
支持每个文章页配置不同的在评论区提供者 `provider`。
:::

::: code-group

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  comment: {
    provider: "giscus", // 评论区提供者
    // 评论区配置项，根据 provider 不同而不同，具体看对应官网的使用介绍
    options: {
      // twikoo 配置，官网：https://twikoo.js.org/
      // envId: "your envId",

      // waline 配置，官网：https://waline.js.org/
      // serverURL: "your serverURL",
      // jsLink: "https://unpkg.com/@waline/client@v3/dist/waline.js",
      // cssLink: "https://unpkg.com/@waline/client@v3/dist/waline.css",

      // giscus 配置，官网：https://giscus.app/zh-CN
      repo: "your repo",
      repoId: "your repoId",
      category: "your category",
      categoryId: "your categoryId",

      // artalk 配置，官网：https://artalk.js.org/
      // server: "your server",
      // site: "site",
    },
  },
});
```

```yaml [文章页 xxx.md]
---
tk:
  comment:
    provider: giscus
    options:
      repo: your repo
      repoId: your repoId
      category: your category
      categoryId: your categoryId
---
```

```ts [更多配置项]
interface TeekConfig {
  /**
   * 评论配置
   */
  comment?:
    | CommentConfig<"twikoo">
    | CommentConfig<"waline">
    | CommentConfig<"giscus">
    | CommentConfig<"artalk">
    | CommentConfig<"render">;
}

type CommentConfig<T extends keyof CommentProvider = "twikoo" | "waline" | "giscus" | "artalk" | "render"> = {
  /**
   * 评论区提供者
   * twikoo 官网：https://twikoo.js.org/
   * waline 官网：https://waline.js.org/
   * giscus 官网：https://giscus.app/zh-CN
   * artalk 官网：https://artalk.js.org/
   * render 需要自定义评论区组件，并通过 comment 插槽传入
   */
  provider: T;
  /**
   * 评论区配置项，根据 provider 不同而不同，具体看对应官网的使用介绍
   */
  options?: CommentProvider[T];
};

export type CommentProvider = {
  /**
   * twikoo 评论区配置项
   */
  twikoo: {
    /**
     * 官网其他配置项
     */
    [key: string]: any;
    envId: string;
    /**
     * twikoo.js 在线链接
     *
     * @default 'https://cdn.jsdelivr.net/npm/twikoo@{version}/dist/twikoo.nocss.js'
     */
    jsLink?: string;
    /**
     * twikoo.css 在线链接
     *
     * @default 'https://cdn.jsdelivr.net/npm/twikoo@{version}/dist/twikoo.css'
     */
    cssLink?: string;
    /**
     * twikoo 版本号，不定期更新为最新版
     *
     * @default '1.6.42'
     */
    version?: string;
    /**
     * twikoo 的 css、js 的 integrity
     */
    jsIntegrity?: string;
    /**
     * 页面渲染后多少毫秒开始渲染 twikoo，如果设置太短，可能获取的 DOM 还没加载完成
     *
     * @default 700 (0.7秒)
     */
    timeout?: number;
    /**
     * katex 配置项，如果设置，则加载 katex
     */
    katex?: {
      /**
       * katex 的 css、core、render 的在线链接
       */
      cssLink: string;
      coreJsLink: string;
      renderJsLink: string;
      /**
       * katex 的 css、core、render 的 integrity
       */
      cssIntegrity?: string;
      coreJsIntegrity?: string;
      renderJsIntegrity?: string;
    };
  };
  /**
   * waline 评论区配置项
   */
  waline: {
    /**
     * 官网其他配置项
     */
    [key: string]: any;
    /**
     * waline 后台服务器地址
     */
    serverURL: string;
    /**
     * waline.js 在线链接
     */
    jsLink?: string;
    /**
     * waline.css 在线链接
     */
    cssLink?: string;
    /**
     * waline.css 的 integrity
     */
    cssIntegrity?: string;
    /**
     * 暗黑模式，具体使用请看 waline 官网
     *
     * @default "html[class='dark']"
     */
    dark?: string;
  };
  /**
   * giscus 评论区配置项
   */
  giscus: {
    [key: string]: any;
    repo: `${string}/${string}`;
    repoId: string;
    category: string;
    categoryId: string;
    mapping?: "url" | "title" | "og:title" | "specific" | "number" | "pathname";
    strict?: "0" | "1";
    reactionsEnabled?: "0" | "1";
    emitMetadata?: "0" | "1";
    inputPosition?: "top" | "bottom";
    lang?: string;
    theme?: string;
    loading?: "lazy" | "eager";
    /**
     * 是否使用在线链接
     *
     * @default true
     */
    useOnline?: boolean;
    /**
     * giscus.js 在线链接，useOnline 为 true 时生效
     *
     * @default 'https://giscus.app/client.js'
     */
    link?: string;
    /**
     * giscus.js 的 integrity
     */
    integrity?: string;
  };
  /**
   * artalk 评论区配置项
   */
  artalk: {
    [key: string]: any;
    /**
     * artalk 后台服务器地址
     */
    server: string;
    /**
     * artalk 站点名称
     */
    site: string;
  };
  /**
   * 自定义评论组件
   */
  render: Record<string, any>;
};
```

:::

如果您使用 Twikoo，你只需要传入 `version` 版本号即可，Teek 会根据版本号去分别请求 Twikoo 的 JS、CSS 文件，其请求地址分别为：

* JS：`https://cdn.jsdelivr.net/npm/twikoo@{version}/dist/twikoo.nocss.js`
* CSS：`https://cdn.jsdelivr.net/npm/twikoo@{version}/dist/twikoo.css`

::: tip
如果您无法访问 `cdn.jsdelivr.net`，则可以通过 `jsLink` 和 `cssLink` 配置项来手动传入链接地址，但是 Teek 建议不要传入 `twikoo.all.min.js` 或 `twikoo.min.js` 的在线链接，而是传入 `twikoo.nocss.js` 和 `twikoo.css` 在线链接。

原因：`twikoo.all.min.js` 或 `twikoo.min.js` 内部会自动引入 Twikoo 的 CSS 文件，该 CSS 文件会全局影响 Teek 的样式，因此请手动传入 `twikoo.nocss.js` 和 `twikoo.css` 在线链接，Teek 会让其样式局部生效。
:::

## 评论区实例注入

在 `comment` 配置项里，评论区的实例都是通过传入在线 JS、CSS 链接来实现，如果网速不好或者在线链接无法访问，那么评论区会无法正常加载。

Teek 支持手动传入评论区的实例，因此您可以安装评论区的依赖，然后按照官方的 API 初始化实例后传给 Teek。

首先安装评论插件依赖（按需安装）：

```sh
# 安装 Waline 依赖
pnpm add -D @waline/client

# 安装 Giscus 依赖
pnpm add -D @giscus/vue

# 安装 Artalk 依赖
pnpm add -D artalk
```

然后在 `.vitepress/theme/index.ts` 里面注入评论区的实例。

```ts {2,5-9,20-31}
// .vitepress/theme/index.ts
import Teek, { artalkContext, giscusContext, walineContext } from "vitepress-theme-teek";
import "vitepress-theme-teek/index.css";
import { useData, useRoute } from "vitepress";
import { init } from "@waline/client";
import "@waline/client/style";
import Giscus from "@giscus/vue";
import Artalk from "artalk";
import "artalk/Artalk.css";

export default {
  extends: Teek,
  Layout: defineComponent({
    name: "LayoutProvider",
    setup() {
      const { isDark, page } = useData();
      const route = useRoute();

      // 注入评论区实例
      provide(walineContext, (el, options) => init({ serverURL: options.serverURL!, dark: options.dark, el }));
      provide(giscusContext, () => Giscus);
      provide(artalkContext, (el, options) =>
        Artalk.init({
          el,
          darkMode: isDark.value,
          pageKey: route.path,
          pageTitle: page.value.title,
          server: options.server,
          site: options.site,
        })
      );

      return () => h(Teek.Layout, null, {});
    },
  }),
};
```

::: tip
这些依赖都是评论插件官方文档提供的，如果无法安装/注入成功，请前往对应官方文档阅读如何安装依赖、初始化实例。
:::

最后可以把 `config` 里的在线链接配置项删除，当然您也可以保留，当两者同时存在，以评论区实例注入为主。

## 自定义评论区

如果这四个评论区提供者都不符合需求，可以自己实现评论区，然后传入进来。

先把 `provider` 必须指定为 `render`。

```ts
// .vitepress/config.mts
const teekConfig = defineTeekConfig({
  comment: {
    provider: "render", // 自定义评论区必须指定 render
  },
});
```

最后通过 `teek-comment` 插槽传入自定义评论区组件。

```ts
// .vitepress/theme/index.ts
import Teek from "vitepress-theme-teek";
import "vitepress-theme-teek/index.css";
import MyCommentComponent from "./components/MyCommentComponent.vue";
import { h } from "vue";

export default {
  extends: Teek,
  Layout() {
    return h(Teek.Layout, null, {
      "teek-comment": () => h(MyCommentComponent),
    });
  },
};
```

---

---
url: /10.配置/01.主题配置/05.全局配置.md
---

# 全局配置

全局配置是影响范围较广的配置。

## teekTheme

* 类型：`boolean`
* 默认值：`true`

是否启用主题，如果为 false，则不会启用主题的 99% 功能，只保留如下功能：

* 自动添加侧边栏
* 自动添加一级标题
* 自动添加永久链接
* 文档内容分析（作者、创建时间、文章字数、预计阅读时间等信息）
* 锚点滚动
* 深色/浅色模式过渡动画

::: tip
如果您仍然想要关闭保留的部分功能，Teek 也提供了相关配置项来关闭，请继续往下阅读保留功能的配置项。
:::

配置如下：

```ts
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  teekTheme: true,
});
```

::: tip
如果想全部清除 Teek 的功能，那么在 `.vitepress/theme/index.ts` 文件里去掉 Teek 引用。
:::

## teekHome

* 类型：`boolean`
* 默认值：`true`

是否启用 Teek 的首页风格，如果为 false，则还原到 VitePress 的默认首页，其他功能不变。

::: code-group

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  teekHome: true,
});
```

```yaml [index.md]
---
tk:
  teekHome: true
---
```

:::

## vpHome

* 类型：`boolean`
* 默认值：`true`

是否启用 VitePress 首页风格，支持 `teekHome` 和 `vpHome` 同时存在。

::: code-group

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  vpHome: true,
});
```

```yaml [index.md]
---
tk:
  vpHome: true
---
```

:::

## features&#x20;

站点特性列表，在文档风格的首页展渲染，与 VitePress 的 `features` 配置模板一致，但是不会与 VitePress 的 `features` 配置渲染冲突。

该配置项 Teek 建议在首页 `index.md` 文档里的 `frontmatter.tk.features` 进行配置。

每一个 `feature` 的 `icon` 为 [TkIcon](/guide/icon-use) Props 的 `icon` 属性，支持传入 `iconfont`、`SVG`、图片等图标。

::: code-group

```yaml [index.md]
tk:
  teekHome: false
  features:
    - title: 快速开发
      details: 提供了完整版参考代码和精简版开发代码
      image: /feature/ui.svg
      highlights:
        - title: 从零安装：运行 <code>pnpm add vitepress-theme-teek vitepress</code> 以从 NPM 下载 Teek 主题。
        - title: 现有模板：运行 <code>git clone https://github.com/Kele-Bingtang/vitepress-theme-teek-docs-template.git</code> 以下载当前文档模板。

    - title: 拥有丰富的 Features，并持续更新
      details: 满足大部分开发场景。
      image: /feature/features.svg
      features:
        - title: 最新流行稳定技术栈
          icon: icon-github
          details: 基于 Vue3.2、TypeScript、Vite4、Pinia、Element-Plus 等最新技术栈开发
          link: /guide/intro

        - title: 简单上手 & 学习
          icon: <svg viewBox="0 0 24 24" width="1.2em" height="1.2em"><path fill="currentColor" d="m23 12l-7.071 7.071l-1.414-1.414L20.172 12l-5.657-5.657l1.414-1.414L23 12zM3.828 12l5.657 5.657l-1.414 1.414L1 12l7.071-7.071l1.414 1.414L3.828 12z"></path></svg>
          details: 项目结构清晰，代码简单、易读。

        - title: 规范工程化工作流
          icon: /teek-logo-mini.svg
          details: 配置 Eslint、Prettier、Husky、Commitlint、Lint-staged 规范前端工程代码规范。

        - title: 完善的打包优化方案
          icon: icon-github
          details: 内置规范的打包目录，提供打包压缩功能，减少打包体积。

        - title: 丰富的组件
          icon: /teek-logo-mini.svg
          details: 提供丰富的通用组件、业务组件。
          link: /ecosystem/components

        - title: 常用 Hook 函数
          icon: icon-gitee
          details: 提供丰富的组件、常用 Hooks 封装，实现复用思想，减少重复开发，提高效率。

        - title: 个性化主题配置
          icon: icon-xiangce
          details: 提供主题颜色配置，暗黑、灰色、色弱等模式切换。
          link: /guide/theme-enhance

        - title: 多种布局配置
          icon: /teek-logo-mini.svg
          details: 提供多种布局、标签栏切换，布局显隐，满足大部分场景。

        - title: 项目权限管控
          icon: /teek-logo-mini.svg
          details: 采用 RBAC 权限管控，提供菜单、路由及按钮粗细粒度权限管理方案

        - title: 国际化
          icon: /teek-logo-mini.svg
          details: 内置常用国际化转换函数，支持自定义国际化切换，

        - title: IFrame 嵌入
          icon: /teek-logo-mini.svg
          details: 提供 IFrame 嵌入、缓存功能，支持门户 Portal 布局。

        - title: 自定义指令
          icon: /teek-logo-mini.svg
          details: 内置多种 Vue 自定义指令，提供傻瓜式指令一键注册功能。

        - title: Axios 封装
          icon: /teek-logo-mini.svg
          details: 基于 Axios 封装常用请求模块，内置业务拦截器、异常拦截器。

        - title: 多种图标类型
          icon: /teek-logo-mini.svg
          details: 支持 IconFont、SVG、Iconify 等多种图标类型渲染。
          link: /guide/icon-use

    - title: 布局
      details: 多种布局、标签栏切换，布局组件显隐
      image: /feature/layout.svg
      highlights:
        - title: 六大布局
          icon: /teek-logo-mini.svg
          details: 内置纵向、经典、横向、分栏、混合、子系统六大布局切换

        - title: 深色模式
          icon: /teek-logo-mini.svg
          details: 可以自由切换浅色模式与深色模式

        - title: 主题色切换
          icon: /teek-logo-mini.svg
          details: 支持自定义主题色并允许用户在预设的主题颜色之间切换

        - title: 布局组件
          icon: /teek-logo-mini.svg
          details: 支持图标、面包屑、导航栏等组件显隐，内置缓存功能，记住用户的布局配置
```

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  features: [
    {
      title: "快速开发",
      details: "提供了完整版参考代码和精简版开发代码",
      image: "/feature/ui.svg",
      highlights: [
        {
          title: "从零安装：运行 <code>pnpm add vitepress-theme-teek vitepress</code> 以从 NPM 下载 Teek 主题。",
        },
        {
          title:
            "现有模板：运行 <code>git clone https://github.com/Kele-Bingtang/vitepress-theme-teek-docs-template.git</code> 以下载当前文档模板。",
        },
      ],
    },
    {
      title: "拥有丰富的 Features，并持续更新",
      details: "满足大部分开发场景。",
      image: "/feature/features.svg",
      features: [
        {
          title: "最新流行稳定技术栈",
          icon: "icon-github",
          details: "基于 Vue3.2、TypeScript、Vite4、Pinia、Element-Plus 等最新技术栈开发",
          link: "/guide/intro",
        },
        {
          title: "简单上手 & 学习",
          icon: '<svg viewBox="0 0 24 24" width="1.2em" height="1.2em"><path fill="currentColor" d="m23 12l-7.071 7.071l-1.414-1.414L20.172 12l-5.657-5.657l1.414-1.414L23 12zM3.828 12l5.657 5.657l-1.414 1.414L1 12l7.071-7.071l1.414 1.414L3.828 12z"></path></svg>',
          details: "项目结构清晰，代码简单、易读。",
        },
        {
          title: "规范工程化工作流",
          icon: "/teek-logo-mini.svg",
          details: "配置 Eslint、Prettier、Husky、Commitlint、Lint-staged 规范前端工程代码规范。",
        },
        {
          title: "完善的打包优化方案",
          icon: "icon-github",
          details: "内置规范的打包目录，提供打包压缩功能，减少打包体积。",
        },
        {
          title: "丰富的组件",
          icon: "/teek-logo-mini.svg",
          details: "提供丰富的通用组件、业务组件。",
          link: "/ecosystem/components",
        },
        {
          title: "常用 Hook 函数",
          icon: "icon-gitee",
          details: "提供丰富的组件、常用 Hooks 封装，实现复用思想，减少重复开发，提高效率。",
        },
        {
          title: "个性化主题配置",
          icon: "icon-xiangce",
          details: "提供主题颜色配置，暗黑、灰色、色弱等模式切换。",
          link: "/guide/theme-enhance",
        },
        {
          title: "多种布局配置",
          icon: "/teek-logo-mini.svg",
          details: "提供多种布局、标签栏切换，布局显隐，满足大部分场景。",
        },
        {
          title: "项目权限管控",
          icon: "/teek-logo-mini.svg",
          details: "采用 RBAC 权限管控，提供菜单、路由及按钮粗细粒度权限管理方案",
        },
        {
          title: "国际化",
          icon: "/teek-logo-mini.svg",
          details: "内置常用国际化转换函数，支持自定义国际化切换，",
        },
        {
          title: "IFrame 嵌入",
          icon: "/teek-logo-mini.svg",
          details: "提供 IFrame 嵌入、缓存功能，支持门户 Portal 布局。",
        },
        {
          title: "自定义指令",
          icon: "/teek-logo-mini.svg",
          details: "内置多种 Vue 自定义指令，提供傻瓜式指令一键注册功能。",
        },
        {
          title: "Axios 封装",
          icon: "/teek-logo-mini.svg",
          details: "基于 Axios 封装常用请求模块，内置业务拦截器、异常拦截器。",
        },
        {
          title: "多种图标类型",
          icon: "/teek-logo-mini.svg",
          details: "支持 IconFont、SVG、Iconify 等多种图标类型渲染。",
          link: "/guide/icon-use",
        },
      ],
    },
    {
      title: "布局",
      details: "多种布局、标签栏切换，布局组件显隐",
      image: "/feature/layout.svg",
      highlights: [
        {
          title: "六大布局",
          icon: "/teek-logo-mini.svg",
          details: "内置纵向、经典、横向、分栏、混合、子系统六大布局切换",
        },
        {
          title: "深色模式",
          icon: "/teek-logo-mini.svg",
          details: "可以自由切换浅色模式与深色模式",
        },
        {
          title: "主题色切换",
          icon: "/teek-logo-mini.svg",
          details: "支持自定义主题色并允许用户在预设的主题颜色之间切换",
        },
        {
          title: "布局组件",
          icon: "/teek-logo-mini.svg",
          details: "支持图标、面包屑、导航栏等组件显隐，内置缓存功能，记住用户的布局配置",
        },
      ],
    },
  ],
});
```

```ts [更多配置项]
import type { IconProps } from "@teek/components/common/Icon/src/icon";

interface Feature {
  /**
   * 标题
   */
  title: string;
  /**
   * 描述
   */
  details?: string;
  /**
   * 图片地址
   */
  image?: string;
  /**
   * Features 数据
   */
  features?: FeatureItem[];
  /**
   * Highlights 数据
   */
  highlights?: FeatureItem[];
}

interface FeatureItem {
  /**
   * 标题
   */
  title: string;
  /**
   * 描述
   */
  details?: string;
  /**
   * 图标地址
   */
  icon?: IconProps["icon"];
  /**
   * 点击跳转链接
   */
  link?: string;
}
```

:::

## loading&#x20;

* 类型：`boolean` | `string`
* 默认值：`false`

页面加载 Loading 动画配置，如果为 `boolean`，则控制是否启用，如果为字符串，则指定加载 Loading 动画的文案。

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  loading: true, // 启用 Loading 动画，为 false 则关闭 Loading 动画
  // loading: "正在加载中...", // 修改 Loading 文案
});
```

## homeCardListPosition&#x20;

* 类型：`left` | `right`
* 默认值：`right`

首页卡片栏列表位置，当为 `left` 则在文章列表左侧，当为 `right` 则在文章列表右侧。

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  homeCardListPosition: "right",
});
```

## anchorScroll

* 类型：`boolean`
* 默认值：`true`

是否启用锚点滚动功能，即阅读文章时，自动将 `h1 ~ h6` 标题添加到地址栏 `#` 后面。

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  anchorScroll: true,
});
```

## viewTransition

* 类型：`boolean`
* 默认值：`true`

深色、浅色模式切换时是否开启过渡动画。

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  viewTransition: true,
});
```

## themeSize

* 类型：`small` | `default` | `large` | `wide`
* 默认值：`default`

配置主题尺寸。只影响 Teek 主题首页和功能页，不影响 VitePress 默认主题。

```ts
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  themeSize: "default",
});
```

## backTop&#x20;

右下角回到顶部配置。

如果希望将实时数字换成一个火箭图标，则将 `backTop.content` 设置为 `icon`。

::: code-group

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  backTop: {
    enabled: true, // 是否启动回到顶部功能
    content: "progress", // 回到顶部按钮的显示内容，可选配置 progress | icon
    done: TkMessage => TkMessage.success("返回顶部成功"), // 回到顶部后的回调
  },
});
```

```ts [更多配置项]
import { Message } from "@teek/components/common/Message/src/message";

interface BackTop {
  /**
   * 是否启动回到顶部功能
   *
   * @default true
   */
  enabled?: boolean;
  /**
   * 回到顶部按钮的显示内容
   *
   * @default 'progress'
   */
  content?: "progress" | "icon";
  /**
   * 回到顶部后的回调
   */
  done?: (TkMessage: Message) => void;
}
```

:::

如果想重写回到顶部的组件，则使用 [teek-back-top](/guide/slot#全局插槽) 插槽。

## toComment&#x20;

右下角滚动滚动到评论区配置。

::: code-group

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  toComment: {
    enabled: true, // 是否启动滚动到评论区功能
    done: TkMessage => TkMessage.success("已抵达评论区"), // 滚动到评论区后的回调
  },
});
```

```ts [更多配置项]
import { Message } from "@teek/components/common/Message/src/message";

interface ToComment {
  /**
   * 是否启动滚动到评论区功能
   *
   * @default true
   */
  enabled?: boolean;
  /**
   * 滚动到评论区后的回调
   */
  done?: (TkMessage: Message) => void;
}
```

:::

如果想重写滚动滚动到评论区的组件，则使用 [teek-to-comment](/guide/slot#全局插槽) 插槽。

## codeBlock

新版代码块配置，您现在看到的代码块是新版代码块。

::: tip
在 `details` 容器下或父元素的 class 为 `tk-vp-code` 时，恢复为 VitePress 的默认代码块风格。
:::

::: code-group

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  codeBlock: {
    disabled: false, // 是否禁用新版代码块
    collapseHeight: 700, // 超出高度后自动折叠，设置 true 则默认折叠，false 则默认不折叠
    overlay: false, // 代码块底部是否显示展开/折叠遮罩层
    overlayHeight: 400, // 当出现遮罩层时，指定代码块显示高度，当 overlay 为 true 时生效
    copiedDone: (TkMessage: Message) => TkMessage.success("复制成功！"),
  },
});
```

```yaml [文章页 xxx.md]
---
codeBlock:
  disabled: false
  collapseHeight: 700
---
```

```ts [更多配置项]
interface CodeBlock {
  /**
   * 是否禁用新版代码块
   *
   * @default false
   */
  disabled?: boolean;
  /**
   * 超出高度后自动折叠，设置 true 则默认折叠，false 则默认不折叠
   *
   * @default 700
   */
  collapseHeight?: number | boolean;
  /**
   * 复制代码完成后的回调
   */
  copiedDone?: (TkMessage: Message) => void;
  /**
   * 代码块底部是否显示展开/折叠遮罩层
   *
   * @default false
   * @version 1.4.0
   */
  overlay?: boolean;
  /**
   * 当出现遮罩层时，指定代码块显示高度，当 overlay 为 true 时生效
   *
   * @default 400
   * @version 1.4.0
   */
  overlayHeight?: number;
}
```

:::

新版代码块的语言默认为大写，如果希望为小写或者首字母大写，通过修改 `css var` 的 `tk-code-block-lang-transform` 来达成目标。

::: tip
`tk-code-block-lang-transform` 的值等于 CSS 中 `text-transform` 的值。
:::

先定义一个 `css` 文件：

```css [tk-code-style.css]
/* .vitepress/theme/style/tk-code-style.css */
:root {
  /*
   * none：文本中的单词保持默认风格
   * capitalize：文本中的每个单词以大写字母开头
   * lowercase：文本中的每个单词全部转为小写
   * uppercase：定文本中的单次全部转为大写
   */
  --tk-code-block-lang-transform: lowercase;
}
```

在 `.vitepress/theme/index.ts` 文件引入该 `css` 文件：

```ts {4}
// .vitepress/theme/index.ts
import Teek from "vitepress-theme-teek";
import "vitepress-theme-teek/index.css";
import "./style/tk-code-style.css";

export default {
  extends: Teek,
};
```

## sidebarTrigger&#x20;

* 类型：`boolean`
* 默认值：`false`

是否启用侧边栏展开/折叠触发器，点击触发器可以展开/折叠侧边栏。

```ts
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  sidebarTrigger: false,
});
```

如果想重写侧边栏展开/折叠触发器组件，则使用 [teek-sidebar-trigger](/guide/slot#全局插槽) 插槽。

## windowTransition&#x20;

* 类型：`boolean` / `object`
* 默认值：`true`

是否全局给部分元素启用视图渐入过渡效果，当为 `boolean` 类型，则控制全局是否启用，当为 `object` 类型，则控制部分元素是否启用。

`object` 为 `WindowTransition` 类型，请看下方代码块的 更多配置项。

::: tip 什么是视图渐入过渡效果
当第一次进入博客风格的首页或者归档页时，向下滚动，会看到每一个元素的从下方向上移动的过渡效果。
:::

::: code-group

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  windowTransition: true,
});
```

```ts [更多配置项]
interface TeekConfig {
  /**
   * 是否全局启用视图渐入过渡效果
   *
   * @default true
   */
  windowTransition?: boolean | WindowTransition;
}

interface WindowTransition {
  /**
   * 是否开启首页文章列表过渡效果
   *
   * @default false
   */
  post?: boolean;
  /**
   * 是否开启首页卡片列表过渡效果
   *
   * @default false
   */
  card?: boolean;
  /**
   * 是否开启归档页过渡效果
   *
   * @default false
   */
  archives?: boolean;
}
```

:::

## bodyBgImg

body 背景图片配置，将整个网站的背景色改为图片。

::: code-group

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  bodyBgImg: {
    imgSrc: ["/img/bg1.jpg", "/img/bg2.png"], // body 背景图片链接。单张图片 string | 多张图片 string[], 多张图片时每隔 imgInterval 秒换一张
    imgOpacity: 1, // body 背景图透明度，选值 0.1 ~ 1.0
    imgInterval: 15000, //  body 当多张背景图时（imgSrc 为数组），设置切换时间，单位：毫秒
    imgShuffle: false, // body 背景图是否随机切换，为 false 时按顺序切换
    mask: false, // body 背景图遮罩
    maskBg: "rgba(0, 0, 0, 0.2)", // body 背景图遮罩颜色，如果为数字，则是 rgba(0, 0, 0, ${maskBg})，如果为字符串，则作为背景色。mask 为 true 时生效
  };
});
```

```yaml [文章页 xx.md]
---
bodyBgImg:
  imgSrc:
    - /img/bg1.jpg
    - /img/bg2.png
  imgOpacity: 1
  imgInterval: 15000
  imgShuffle: false
  mask: false
  maskBg: "rgba(0, 0, 0, 0.2)"
---
```

```ts [更多配置项]
interface BodyBgImg {
  /**
   * body 背景图片链接。单张图片 string | 多张图片 string[], 多张图片时每隔 imgInterval 秒换一张
   */
  imgSrc?: string | string[];
  /**
   * body 背景图透明度，选值 0.1 ~ 1.0
   *
   * @default 1
   */
  imgOpacity?: 0.1 | 0.2 | 0.3 | 0.4 | 0.5 | 0.6 | 0.7 | 0.8 | 0.9 | 1;
  /**
   * body 当多张背景图时（imgSrc 为数组），设置切换时间，单位：毫秒
   *
   * @default 15000 (15秒)
   */
  imgInterval?: number;
  /**
   * body 背景图是否随机切换，为 false 时按顺序切换
   *
   * @default false
   */
  imgShuffle?: boolean;
  /**
   * body 背景图遮罩
   *
   * @default false
   */
  mask?: boolean;
  /**
   * body 背景图遮罩颜色，如果为数字，则是 rgba(0, 0, 0, ${maskBg})，如果为字符串，则作为背景色。mask 为 true 时生效
   *
   * @default 'rgba(0, 0, 0, 0.2)'
   */
  maskBg?: string | number;
}
```

:::

::: tip
`bodyBgImg` 设置了 `imgSrc` 后，`banner` 的图片风格会失效。
:::

## themeEnhance&#x20;

主题增强配置，当开启后，右上角将有主题增强面板出现。

关于主题增强详细的介绍请看 [主题增强](/guide/theme-enhance)。

::: code-group

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  themeEnhance: {
    enabled: true, // 启用主题增强功能
    position: "top", // 位置，top 为导航栏右侧，bottom 为右下角
    // 布局切换配置
    layoutSwitch: {
      disabled: false,
      defaultMode: "original"
    },
    // 布局主题色配置
    themeColor: {
      disabled: false,
      defaultColorName: "vp-default",
      defaultSpread: false
    },
    // 聚光灯配置
    spotlight: {
      disabled: false,
      defaultStyle: "aside",
      defaultValue: true
    }
  };
});
```

```ts [更多配置项]
import type { ThemeColorName, LayoutMode, SpotlightStyle } from "vitepress-theme-teek";

interface ThemeEnhance {
  /**
   * 启用主题增强功能
   *
   * @default true
   */
  enabled?: boolean;
  /**
   * 位置，top 为导航栏右侧，bottom 为右下角
   *
   * @default 'top'
   */
  position?: "top" | "bottom";
  /**
   * 布局切换配置
   */
  layoutSwitch?: {
    /**
     * 禁用布局切换
     */
    disabled?: boolean;
    /**
     * 布局切换的默认模式
     *
     * @default LayoutMode.Original
     */
    defaultMode?: LayoutMode | "fullWidth" | "sidebarWidthAdjustableOnly" | "bothWidthAdjustable" | "original";
    /**
     * 切换布局成功后的回调
     *
     * @since 1.3.2
     */
    switchModeDone?: (
      mode: LayoutMode | "fullWidth" | "sidebarWidthAdjustableOnly" | "bothWidthAdjustable" | "original"
    ) => void;
    /**
     * 禁用帮助提示
     *
     * @default false
     */
    disableHelp?: boolean;
    /**
     * 禁用布局切换动画
     */
    disableAnimation?: boolean;
    /**
     * 内容布局最大宽度的默认百分比，仅限 0-100
     *
     * @default 90 (90%)
     */
    defaultDocMaxWidth?: number;
    /**
     * 禁用帮助提示
     *
     * @default false
     */
    disableDocMaxWidthHelp?: boolean;
    /**
     * 页面布局最大宽度的默认百分比，仅限 0-100
     *
     * @default 95 (95%)
     */
    defaultPageMaxWidth?: number;
    /**
     * 禁用帮助提示
     *
     * @default false
     */
    disablePageMaxWidthHelp?: boolean;
  };
  /**
   * 布局主题色配置
   */
  themeColor?: {
    /**
     * 禁用布局主题色切换
     *
     * @default false
     */
    disabled?: boolean;
    /**
     * 从 0 完全自定义布局主题色，不使用内置主题色
     *
     * @default false
     * @version 1.4.1
     */
    customize?:
      | boolean
      | {
          /**
           * 是否使用 vitepress 的主题色
           *
           * @default true
           */
          vitepressTheme?: boolean;
          /**
           * 是否使用 elementPlus 的主题色
           *
           * @default true
           */
          elementPlusTheme?: boolean;
        };
    /**
     * 布局默认主题色
     *
     * @default ThemeColorName.vpDefault
     */
    defaultColorName?:
      | ThemeColorName
      | "vp-default"
      | "vp-green"
      | "vp-yellow"
      | "vp-red"
      | "ep-blue"
      | "ep-green"
      | "ep-yellow"
      | "ep-red";
    /**
     * 切换布局成功后的回调
     *
     * @since 1.3.2
     */
    switchColorDone?: (color: string) => void;
    /**
     * 是否将主题色扩散到其他元素（根据主题色计算其他元素需要的颜色）
     *
     * @default false
     */
    defaultSpread?: boolean;
    /**
     * 禁用帮助提示
     *
     * @default false
     */
    disableHelp?: boolean;
    /**
     * 是否在移动端禁用
     *
     * @default false
     */
    disabledInMobile?: boolean;
    /**
     * 自定义主题色，将会追加到内置主题色后面
     */
    append?: {
      /**
       * 主题组名称
       */
      label: string;
      /**
       * 主题组提示信息，鼠标悬停时显示
       */
      tip?: string;
      /**
       * 主题组内容
       */
      options: {
        /**
         * 主题名称，用于页面文字渲染
         */
        label: string;
        /**
         * 主题标识，在 html 标签的 theme 属性添加该标识
         */
        value: string;
      }[];
    }[];
  };
  /**
   * 聚光灯配置
   */
  spotlight?: {
    /**
     * 禁用聚光灯
     *
     * @default false
     */
    disabled?: boolean;
    /**
     * 聚光灯默认样式
     *
     * @default SpotlightStyle.Aside
     */
    defaultStyle?: SpotlightStyle | "aside" | "under";
    /**
     * 禁用帮助提示
     *
     * @default false
     */
    disableHelp?: boolean;
    /**
     * 聚光灯默认开关状态
     *
     * @default true
     */
    defaultValue?: boolean;
  };
}
```

:::

&#x20;如果想去掉主题增强面板主题色的 VitePress 主题色或者 ElementPlus 主题色，可以使用 `themeEnhance.themeColor.customize` 配置项。

* 该配置项为 `false` 时，将关闭 Teek 所有内置的主题色，此时你可以通过 `themeEnhance.themeColor.append` 自定义添加自己的主题色。
* 该配置项为 `object` 时，可以选择关闭 VitePress 主题色或者 ElementPlus 主题色

```ts {7,9-12}
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  themeEnhance: {
    themeColor: {
      customize: false, // 关闭所有内置主题色

      // 或选择性开启/关闭部分内置主题色
      // customize: {
      //   vitepressTheme: true,
      //   elementPlusTheme: true,
      // },
    },
  },
});
```

如果想拓展自己的主题色，请看 [主题增强拓展](/reference/theme-enhance)。

## author

文章默认的作者信息。

在首页的文章列表和文章页使用该功能。

::: code-group

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  author: {
    name: "Teeker", // 作者名称
    link: "https://github.com/Kele-Bingtang", // 点击作者名称后跳转的链接
  };
});
```

```yaml [文章页 xx.md]
---
author:
  name: "Teeker"
  link: "https://github.com/Kele-Bingtang",
---
```

```ts [更多配置项]
interface Author {
  /**
   * 作者名称，作用在首页的 PostItem 和文章页
   */
  name: string;
  /**
   * 点击作者名称后跳转的链接
   */
  link?: string;
}
```

:::

## notice

公告配置。

公告组件只提供基础功能，不提供任何内容的渲染，需要您自定义组件，然后在 `.vitepress/theme/index.ts` 中通过 `teek-notice-content` 插槽传进来。

使用如下：

```ts [插槽]
// .vitepress/theme/index.ts
import Teek from "vitepress-theme-teek";
import "vitepress-theme-teek/index.css";
import MyNoticeContent from "./components/MyNoticeContent.vue";
import { h } from "vue";

export default {
  extends: Teek,
  Layout() {
    return h(Teek.Layout, null, {
      "teek-notice-content": () => h(MyNoticeContent),
    });
  },
};
```

配置如下：

::: code-group

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  notice: {
    enabled: true, // 是否启用公告功能
    title: "公告", // 公告标题，支持函数式：需要和国际化搭配使用，根据不同语言环境返回不同标题
    initOpen: true,
    duration: 0, // 弹框定时自动关闭，0 不自动消失
    mobileMinify: false, // 移动端自动最小化
    reopen: true,
    useStorage: true, // 是是否使用 localStorage 存储公告状态，如：当打开公告弹框后，下次进来则自动打开弹框
    twinkle: false, // 公告图标是否打开闪烁提示
    position: "top", // 公告弹框出现位置
    // ...
  },
});
```

```yaml [文章页 xx.md]
---
notice:
  enabled: true
  title: "公告"
  initOpen: true
  duration: 0
  mobileMinify: false
  reopen: true
  useStorage: true
  twinkle: false
  position: "top"
---
```

````ts [更多配置项]
import type { Route } from "vitepress";
import type { IconProps } from "vitepress-theme-teek";

interface Notice {
  /**
   * 是否启用公告功能
   *
   * @default false
   */
  enabled?: boolean;
  /**
   * 公告自定义全局样式
   *
   * @example
   * ```css
   * .tk-notice a {
   *    color: var(--vp-c-brand-2);
   * }
   * ```
   */
  noticeStyle?: string;
  /**
   * 公告图标样式
   */
  iconStyle?: Record<string, any>;
  /**
   * 公告弹窗样式
   */
  popoverStyle?: Record<string, any>;
  /**
   * 公告标题，函数式需要和国际化搭配使用，根据不同语言环境返回不同标题
   *
   * @default '公告'
   */
  title?: string | ((localeIndex: string) => string);
  /**
   * 第一次进入页面，是否默认打开公告弹框
   *
   * @default true
   */
  initOpen?: boolean;
  /**
   * 弹框定时自动关闭，0 不自动消失
   *
   * @default 0
   */
  duration?: number;
  /**
   * 移动端自动最小化
   *
   * @default false
   */
  mobileMinify?: boolean;
  /**
   * 关闭公告弹框后，是否支持重新打开，如果为 false，则代表公告只显示一次
   *
   * @default true
   */
  reopen?: boolean;
  /**
   * 是否使用 localStorage 存储公告状态，如：当打开公告弹框后，下次进来则自动打开弹框
   */
  useStorage?: boolean;
  /**
   * 公告图标是否打开闪烁提示
   *
   * @default false
   */
  twinkle?: boolean;
  /**
   * 公告弹框出现位置
   *
   * @default top
   */
  position?: "top" | "center";
  /**
   * 公告图标地址
   *
   * @remark 与 noticeIconType 配合使用
   */
  noticeIcon?: IconProps["icon"];
  /**
   * 公告弹框关闭图标地址，与 noticeIcon 配置一致
   */
  closeIcon?: IconProps["icon"];
  /**
   * 路由切换后的自定义回调
   *
   * @param to 切换到的目标路由
   */
  onAfterRouteChange?: (to: Route, noticeShow: boolean, showPopover: boolean) => void;
}
````

:::

## siteAnalytics

站点分析配置，目前集成了三种常见的站点统计工具：

* 百度分析 `Baidu Analytics`
* 谷歌分析 `Google Analytics`
* `Umami` 分析

具体使用说明请看 [站点统计](/guide/statistics)。

使用如下：

::: code-group

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  siteAnalytics: [
    {
      provider: "google",
      options: {
        id: "******",
      },
    },
    {
      provider: "baidu",
      options: {
        id: "******",
      },
    },
    {
      provider: "umami",
      options: {
        id: "******",
        src: "**",
      },
    },
  ],
});
```

```ts [更多配置项]
import type { BaiduAnalyticsOptions, GoogleAnalyticsOptions, UmamiAnalyticsOptions } from "vitepress-theme-teek";

type SiteAnalytics<T extends keyof SiteAnalyticsProvider = ""> = {
  /**
   * 赞赏位置
   */
  provider: T;
  /**
   * 赞赏配置
   */
  options?: SiteAnalyticsProvider[T];
};

type SiteAnalyticsProvider = {
  "": object;
  baidu: BaiduAnalyticsOptions;
  google: GoogleAnalyticsOptions;
  umami: UmamiAnalyticsOptions;
};
```

:::

---

---
url: /常用框架/SpringBoot/SpringBoot程序开发/11_统一返回结果对象.md
---

# 全局统一响应格式、参数校验、异常处理

## 一、统一返回数据格式

项目中我们会将响应封装成json返回，一般我们会将所有接口的数据格式统一， 使前端(iOS Android, Web)对数据的操作更一致、轻松。

一般情况下，统一返回数据格式没有固定的格式，只要能描述清楚返回的数据状态以及要返回的具体数据就可以。但是一般会包含状态码、返回消息、数据这几部分内容。

例如，我们的系统要求返回的基本数据格式如下：

**列表：**

```json
{
  "success": true,
  "code": 20000,
  "message": "成功",
  "data": {
    "items": [
      {
        "id": "1",
        "name": "刘德华",
        "intro": "毕业于师范大学数学系，热爱教育事业，执教数学思维6年有余"
      }
    ]
  }
}
```

**分页：**

```json
{
  "success": true,
  "code": 20000,
  "message": "成功",
  "data": {
    "total": 17,
    "rows": [
      {
        "id": "1",
        "name": "刘德华",
        "intro": "毕业于师范大学数学系，热爱教育事业，执教数学思维6年有余"
      }
    ]
  }
}
```

**没有返回数据：**

```json
{
  "success": true,
  "code": 20000,
  "message": "成功",
  "data": {}
}
```

**失败：**

```json
{
  "success": false,
  "code": 20001,
  "message": "失败",
  "data": {}
}
```

因此，我们定义统一结果

```json
{
  "success": 布尔, //响应是否成功
  "code": 数字, //响应码
  "message": 字符串, //返回消息
  "data": HashMap //返回数据，放在键值对中
}
```

## 二、定义统一返回结果

### 1、创建返回码定义枚举类

在项目中创建包`com.xxl.common.constants`，创建枚举类`ResultCodeEnum.java`

```java
package com.xxl.common.constants;
import lombok.Getter;

@Getter
public enum ResultCodeEnum {
    
    SUCCESS(true, 20000,"成功"),
    UNKNOWN_REASON(false, 20001, "未知错误"),
    BAD_SQL_GRAMMAR(false, 21001, "sql语法错误"),
    JSON_PARSE_ERROR(false, 21002, "json解析异常"),
    PARAM_ERROR(false, 21003, "参数不正确"),
    FILE_UPLOAD_ERROR(false, 21004, "文件上传错误"),
    EXCEL_DATA_IMPORT_ERROR(false, 21005, "Excel数据导入错误");
    
    private Boolean success;
    
    private Integer code;
    
    private String message;
    
    private ResultCodeEnum(Boolean success, Integer code, String message) {
        this.success = success;
        this.code = code;
        this.message = message;
    }
}
```

### **2、创建结果类**

创建包`com.xxl.common.vo`，创建类`R.java`

vo：View Object

```java
package com.xxl.common.vo;

/**
 * 全局统一返回结果
 * 
 * @author xxl
 * @date 2025/1/5 17:54
 */
@Data
public class R {
    
 	/**
     * 是否成功
     */
    private Boolean success;

    /**
     * 返回码
     */
    private Integer code;

    /**
     * 返回消息
     */
    private String message;

    /**
     * 返回数据
     */
    private Map<String, Object> data = new HashMap<String, Object>();
    
    private R(){}
    
    public static R ok(){
        R r = new R();
        r.setSuccess(ResultCodeEnum.SUCCESS.getSuccess());
        r.setCode(ResultCodeEnum.SUCCESS.getCode());
        r.setMessage(ResultCodeEnum.SUCCESS.getMessage());
        return r;
    }
    
    public static R error(){
        R r = new R();
        r.setSuccess(ResultCodeEnum.UNKNOWN_REASON.getSuccess());
        r.setCode(ResultCodeEnum.UNKNOWN_REASON.getCode());
        r.setMessage(ResultCodeEnum.UNKNOWN_REASON.getMessage());
        return r;
    }
    
    public static R setResult(ResultCodeEnum resultCodeEnum){
        R r = new R();
        r.setSuccess(resultCodeEnum.getSuccess());
        r.setCode(resultCodeEnum.getCode());
        r.setMessage(resultCodeEnum.getMessage());
        return r;
    }
    
    public R success(Boolean success){
        this.setSuccess(success);
        return this;
    }
    
    public R message(String message){
        this.setMessage(message);
        return this;
    }
    
    public R code(Integer code){
        this.setCode(code);
        return this;
    }
    
    public R data(String key, Object value){
        this.data.put(key, value);
        return this;
    }
    
    public R data(Map<String, Object> map){
        this.setData(map);
        return this;
    }
}
```

## **二、测试统一返回结果**

### 1、修改Controller中的返回结果

修改项目中的接口返回值

列表

```java
@ApiOperation(value = "所有人员列表")
@GetMapping
public R list(){
    List<User> list = userService.list(null);
    return R.ok().data("items", list);
}
```

删除

```java
@ApiOperation(value = "根据ID删除人员")
@DeleteMapping("{id}")
public R removeById(
    @ApiParam(name = "id", value = "人员ID", required = true)
    @PathVariable String id){
    userService.removeById(id);
    return R.ok();
}
```

### 2、重启测试

---

---
url: /常用框架/SpringBoot/SpringBoot与Web应用/5_全局异常处理.md
---

# 全局异常处理

---

---
url: /15.主题开发/60.容器自定义.md
---

# 容器自定义

Teek 提供了两种创建容器的 API，这两种容器 Teek 分别命名为 Simple 容器、Card 容器。

Teek 容器都有哪些？请看 [Markdown 拓展](/guide/markdown)。

容器 API 请看 [Markdown 插件工具](/ecosystem/md-plugin-utils)

## Simple 容器

VitePress 的 `info`、`tip`、`warning`、`danger` 容器都是 Simple 容器，其原理是添加 `div` 来包裹 Markdown 文本，然后通过 CSS 来实现样式。

举个例子（并非实际）

```markdown
::: tip 提示
测试 TIP
:::
```

最终渲染为：

```html
<div class="tip">
  <p class="title">提示</p>
  <p>测试 TIP</p>
</div>
```

此时就可以在 CSS 文件中通过 `.tip` 和 `.title` 来添加样式，如添加一个背景色，给 title 加大字号等。

Teek 的 Simple 容器 API 请看 [simpleContainer.ts](https://github.com/Kele-Bingtang/vitepress-theme-teek/blob/master/packages/markdown/helper/simpleContainer.ts) 文件，该文件参考自 VitePress 官方项目，并修改了些许内容。

`simpleContainer.ts` 文件中真正干活的 API 为 `createContainerThenGet` 函数，该函数提供两个 HTML 模板：

::: code-group

```html [开启标题]
<div class="${type} ${className}">
  <p class="title ${type}-title ${className}-title">${defaultTitle || 传入标题}</p>
  <p>${输入的内容}</p>
</div
```

```html [不开启标题]
<div class="${type} ${className}">
  <p>${输入的内容}</p>
</div
```

:::

举个例子，使用 `createContainerThenUse` 创建 Simple 容器，该函数内部会调用 `createContainerThenGet` 来生成 HTML：

```ts
import { createContainerThenUse } from "vitepress-theme-teek";

createContainerThenUse(md, { type: "demo1", useTitle: true, defaultTitle: "demo1", className: "demo1-container" });
createContainerThenUse(md, { type: "demo2", useTitle: false, className: "demo2-container" });
```

当需要定义非常多的 Simple 容器时，可以使用 `createContainersThenUse` 函数：

```ts
import { createContainersThenUse } from "vitepress-theme-teek";

createContainersThenUse(md, [
  { type: "demo1", useTitle: true, defaultTitle: "demo1", className: "demo1-container" },
  { type: "demo2", useTitle: false, className: "demo2-container" },
]);
```

::: tip
第一个参数 `md` 为 `markdown-it` 实例，VitePress 已经在 `markdown.config` 回调函数的第一个参数提供出来。
:::

使用 Simple 容器：

```markdown
::: demo1
测试 demo1 容器
:::

::: demo1 容器标题
测试 demo1 容器
:::

::: demo2
测试 demo2 容器
:::
```

生成的 HTML 结构如下：

```html

<div class="demo1 demo1-container">
  <p class="title demo1-title demo1-container-title">demo1</p>
  <p>测试 demo1 容器</p>
</div

<div class="demo1 demo1-container">
  <p class="title demo1-title demo1-container-title">容器标题</p>
  <p>测试 demo1 容器</p>
</div

<div class="demo2 demo2-container">
  <p>测试 demo2 容器</p>
</div

```

此时需要您针对不同容器的 class 来添加样式，如：

```scss
// .vitepress/theme/style/container.scss
.demo1-container {
  font-size: 16px;
  .title {
    font-size: 18px;
  }
}

.demo2-container {
  font-size: 12px;
}
```

然后引入样式文件：

```ts
// .vitepress/theme/index.ts
import "./style/container.scss";
```

Teek 已经支持在 `.vitepress/config.mts` 中提供配置项 `markdown.container.config` 函数来构建 Simple 容器，请看 [自定义容器](/reference/plugin-config#自定义容器)。

## Card 容器

Teek 的 `shareCard`、`imgCard`、`navCard` 都是 Card 容器，其原理是通过 `js-yaml` 依赖解析 `yaml` 代码块，然后循环生成 HTML，最后通过 CSS 来实现样式。

Card 容器 API 请看 [cardContainer.ts](https://github.com/Kele-Bingtang/vitepress-theme-teek/blob/master/packages/markdown/helper/cardContainer.ts) 文件。

Card 容器 API 需要您在 `htmlRender` 函数里传入 HTML 标签，您可以参考 [shareCard.ts](https://github.com/Kele-Bingtang/vitepress-theme-teek/blob/master/packages/markdown/plugins/shareCard.ts) 文件来实现自定义 Card 容器。

---

---
url: /daily/软件设计师/05_软件工程基础知识.md
---

# 软件工程基础知识

软件工程是一门研究和应用如何以系统化、规范化、可量化的方法开发和维护软件的学科。它涉及到软件开发的全过程，包括需求分析、设计、编码、测试、部署和维护等阶段。

软件工程的目标是以最大限度地提高软件的质量、可靠性、可维护性和可重用性，同时控制软件开发的成本、进度和风险。

软件工程包括许多技术和工具的应用，如需求工程、面向对象的分析和设计、软件测试、软件项目管理等。同时，它还涉及到与用户和其他相关人员的沟通和协作，以确保软件能够满足用户的需求。

随着计算机技术的发展和软件在各个领域的广泛应用，软件工程的重要性日益增强。良好的软件工程实践可以提高软件的质量和可靠性，降低软件开发和维护的成本，并促进软件的创新和协同开发。因此，软件工程已成为计算机科学和软件工程专业的重要组成部分，并受到越来越多的关注和重视。

## 一、软件工程概述

### :cactus:软件工程诞生原因

早期的软件主要是通过个别开发者采用个体工作方式来实现的程序。随着软件项目规模和复杂性的不断增加，单纯依靠个人的能力已经无法满足需求。因此，人们开始探索一种更为系统化和规范化的方法来开发和管理软件，以应对软件危机和提高开发效率。这就是软件工程诞生的原因之一。软件工程作为一门综合性学科，整合了计算机科学、工程学和管理学的知识，为软件开发提供了更加科学和有效的方法。

第一次软件危机是发生在20世纪60年代中期的一段时间，表现为许多软件项目在质量、进度和预算等方面出现了严重问题。软件质量低下、项目无法按时完成、项目严重超支等情况屡见不鲜。更为严重的是，由于软件错误而导致的重大事故也时有发生。

这次危机的主要原因是，软件开发过程缺乏规范和组织性，很多项目仅依赖个别开发者的个人能力和经验，缺乏系统化的方法和控制。软件规模和复杂性也在不断增加，但开发工具和技术的支持有限。这导致了软件项目的风险和不确定性增加，使得项目管理变得困难，并且软件质量的控制变得迫切与困难。

为了解决这一危机，软件工程作为一门学科逐渐兴起。软件工程的诞生：1968年在NATO会议上提出对软件危机的解决方法。

软件工程的目标是提供一套科学化和规范化的方法，以有效地管理软件开发过程，确保软件质量和项目成功。软件工程引入了一系列的原则、方法和技术，如需求分析、软件设计、编码和测试等，逐步建立了一套完整的软件开发生命周期。此外，软件工程还重视软件项目管理，包括进度控制、资源分配、风险管理等。通过软件工程的实践，软件行业逐渐走上了一条更为科学和可靠的发展道路。

软件工程的基本要素：方法、工具、过程：

| 软件工程基本要素 | 定义和说明                                                   |
| :--------------- | :----------------------------------------------------------- |
| 方法（Method）   | 软件工程方法指的是经过验证和证明能够规范和指导软件开发过程的一系列步骤、技术和技巧。不同的软件工程方法适用于不同的开发场景和需求，例如瀑布模型、敏捷开发、迭代开发等。软件工程方法的目标是提高开发效率、质量和可维护性。 |
| 工具（Tools）    | 软件工程工具是指用于支持软件开发、测试和管理的各种软件和系统。这些工具可以帮助开发团队进行代码编写、测试自动化、版本控制、项目管理等任务。常见的软件工程工具包括集成开发环境（IDE）、测试工具、版本控制系统、项目管理工具等。工具的使用可以提高开发团队的效率、协作和质量。 |
| 过程（Process）  | 软件工程过程是指规划、组织、控制和评估软件开发活动的一系列步骤和方法。软件工程过程通常包括需求分析、设计、编码、测试、部署和维护等阶段。每个过程阶段都有特定的输入、输出和活动，以确保软件开发按照计划进行并产生高质量的结果。软件工程过程的管理可以帮助团队实现项目目标，并不断改进开发过程。 |

### :cactus:软件工程基本原理

为了应对第一次软件危机，软件工程引入了一系列的原则、方法和技术，以提高软件开发过程的效率和质量。

#### :shell:用分阶段的生命周期计划严格管理

软件工程通过将软件开发过程划分为不同的阶段，并为每个阶段制定详细的计划和目标，以确保项目按照规定的时间表和预算进行。每个阶段的完成情况都需要进行评估和记录，以便及时调整和纠正。

#### :shell:坚持进行阶段评审

在每个阶段结束时，对已完成的工作进行评审是非常重要的。评审的目的是检查每个阶段的成果是否符合预期，并发现和解决潜在的问题和风险。通过定期的阶段评审，开发团队可以及时调整和改进，避免问题的蔓延和扩大。

#### :shell:实现严格的产品控制

软件工程强调对软件产品进行严格的控制和管理。这包括对需求的管理、配置管理、版本控制、变更管理等。通过建立适当的控制措施，可以确保软件产品的稳定性和一致性，避免不必要的错误和问题。

#### :shell:采用现代程序设计技术

软件工程鼓励开发团队采用现代的程序设计技术和工具，以提高开发效率和软件质量。这包括使用结构化编程、面向对象编程、设计模式等技术，以及使用自动化测试和集成工具等。通过利用现代技术，开发团队可以更好地组织和管理代码，降低出错概率，并提高可维护性。

#### :shell:结果应能清楚地审查

软件工程强调结果的可审查性。这意味着软件开发过程中的每个阶段和成果都应该能够被审查和评估。开发团队应该建立清晰的文档和记录，以便他人能够理解和审查工作的内容和质量。

#### :shell:开发小组的人员应少而精

在软件工程中，开发小组的人员数量应该适度控制，以保证团队的高效工作和良好的沟通。小团队成员之间可以更好地协作和合作，并更容易管理和调整。此外，人员的选择应该考虑到其专业技能和经验，以确保团队成员能够胜任和完成工作。

#### :shell:承认不断改进软件工程实践的必要性

软件工程是一个不断发展和改进的领域。新的技术和方法不断涌现，需要不断地学习和应用。软件工程的实践需要持续地反思和改进，以适应不断变化的需求和环境。开发团队应该保持对新想法和最佳实践的开放态度，并根据实际情况不断调整和优化自己的工作方式。

### :cactus:软件生存周期

#### :shell:可行性分析与项目开发计划

在软件生命周期中，进行可行性分析和制定项目开发计划非常重要。

可行性分析是对项目的可行性进行评估，包括技术可行性、经济可行性、市场可行性等方面的考虑。通过可行性分析，可以评估项目是否有足够的资源支持、是否满足用户需求、是否能够实现预期的经济效益等，从而确定项目是否值得投入开发。

项目开发计划是根据可行性分析的结果来制定的，它包括项目的时间安排、资源分配、里程碑设定等。一个合理的项目开发计划可以确保项目按时交付、高质量完成。在制定项目开发计划时，需要考虑项目的复杂性、开发人员的技术能力、项目的风险等因素。

在软件生命周期中，可行性分析和项目开发计划是紧密相关的。只有通过可行性分析，确定项目的可行性后，才能制定出合理的项目开发计划。同时，在项目开发过程中，还需要不断进行可行性分析的更新和调整，以保证项目的顺利进行。

```csharp
 - 目标：确定软件开发目标及其可行性
 - 输出：可行性分析报告、项目开发计划
```

#### :shell:需求分析

软件生存周期中的需求分析是指在软件开发过程中，开发团队与客户共同合作，通过对客户需求的收集、整理和分析，确定软件系统的功能、性能、界面、安全等方面的需求。需求分析是软件开发过程中的关键环节，它直接影响着软件系统的质量和成功与否。

需求分析的主要目标是确定软件系统的功能需求和非功能需求。功能需求指的是软件系统需要具备的功能，包括输入、处理和输出等功能。非功能需求则是软件系统的性能、界面、安全等方面的需求，如系统的响应时间、用户界面的友好性、数据的安全性等。

需求分析的过程包括需求收集、需求分析和需求确认三个阶段。需求收集是指通过与客户沟通、参观现场、观察现有系统等方式，获取客户的需求信息。需求分析是在收集到需求信息后，对需求进行整理、分类和分析，确定软件系统的功能和非功能需求。需求确认是指与客户共同确认需求的正确性和完整性，以确保需求与客户的期望一致。

需求分析的方法包括面谈法、[问卷调查](https://cloud.tencent.com/product/survey?from_column=20065\&from=20065)法、原型法等。面谈法是通过与客户进行面对面的交流，了解客户的需求和期望。问卷调查法是通过编制问卷，向客户发放，收集客户的需求信息。原型法是通过制作软件原型，向客户展示软件的功能和界面，以获取客户的反馈和需求信息。

需求分析的结果是需求规格说明书，它包括了系统需求的详细描述、用例图、功能需求、非功能需求等内容。需求规格说明书是开发团队与客户之间的合同，对于软件开发过程的后续阶段起到了重要的指导作用。

```csharp
 - 目标：确定软件系统要做什么，以及它的功能、性能、数据和界面等要求，从而确定系统的逻辑模型。
 - 输出：软件需求说明书
```

#### :shell:概要设计

软件工程中的概要设计是在需求分析阶段之后进行的一项重要工作。概要设计是将需求分析的结果转化为软件系统的高层结构和模块化设计的过程。

概要设计的主要目标是定义软件系统的整体结构和模块之间的关系，以及模块的功能和接口。概要设计的结果通常以图形化的方式展现，如UML类图、流程图等。

在概要设计阶段，软件工程师需要考虑以下几个方面：

1. 确定软件系统的整体结构：根据需求分析的结果，确定软件系统的模块和子系统，以及它们之间的层次结构和关系。
2. 定义模块的功能和接口：对每个模块进行细化设计，明确模块的功能和它们之间的接口。这包括定义模块的输入和输出，以及模块的内部逻辑和数据结构。
3. 根据软件开发的原则进行设计：概要设计需要遵循软件工程的原则和最佳实践，如高内聚低耦合、模块化设计、可扩展性、可维护性等。
4. 考虑软件系统的性能和可靠性：在概要设计阶段，软件工程师需要考虑软件系统的性能和可靠性要求，并相应地进行设计。
5. 进行评审和修改：概要设计完成后，需要进行评审和修改，确保设计符合需求，并满足软件工程的要求。

```csharp
 - 目标：明确软件中的模块、模块的层次结构、模块的调用关系和功能。明确数据库结构
 - 输出：概要设计说明书
```

#### :shell:详细设计

软件工程中的详细设计是软件开发过程中的一个重要阶段，它是在需求分析和系统设计之后进行的。

在详细设计阶段，软件工程师将系统设计的高级抽象转化为实际的软件结构和算法。详细设计的主要目标是定义系统的内部组件和模块，确定它们之间的接口和交互方式，以及定义实现这些组件和模块所需的算法和数据结构。

在详细设计阶段，工程师通常会使用一些工具和技术来辅助设计，例如UML（统一建模语言），流程图，数据流图等。这些工具可以帮助工程师更好地理解系统的结构和功能，并以可视化的方式进行设计和沟通。

详细设计的输出通常包括以下内容：

1. 系统的模块和组件：详细设计阶段将系统拆分为多个模块和组件，并定义它们的职责和功能。
2. 接口定义：详细设计阶段确定每个模块和组件之间的接口，并定义它们之间的通信方式和数据格式。
3. 数据结构和算法：详细设计阶段定义实现系统功能所需的数据结构和算法，包括数据的存储和处理方式。
4. 错误处理和异常处理：详细设计阶段考虑系统可能出现的错误和异常情况，并定义相应的处理策略和机制。
5. 性能优化：在详细设计阶段，工程师可以对系统进行性能分析和优化，以确保系统能够满足性能要求。

```csharp
 - 目标：明确模块的控制结构
 - 输出：详细设计说明书
```

#### :shell:编码

在软件工程中，编码是指将设计好的软件系统规划转化为计算机程序代码的过程。编码是软件开发的核心部分之一，它是将设计好的逻辑和功能转化成机器可执行的指令集的过程。

在编码过程中，开发人员会根据设计文档和业务需求，使用特定的编程语言和技术，按照一定的编码规范和最佳实践，将逻辑和功能逐步转换为计算机程序的代码。编码涉及到语法、语义、算法、数据结构等方面的知识。

编码的目标是实现软件的功能需求，并保证代码的可读性、可维护性、可扩展性和性能等方面的优化。良好的编码实践可以提高代码的质量，减少错误和缺陷，并提高开发效率和团队协作。

在编码过程中，开发人员通常会使用集成开发环境（IDE）或文本编辑器来编写代码，并经过编译、运行和调试等步骤来验证代码的正确性和效果。编码后的代码会进行版本控制，并通过软件测试和代码评审等方式来确保代码的质量和正确性。

```csharp
 - 目标：将过程描述转变为程序代码
 - 输出：某种特定语言的源代码
```

#### :shell:测试

软件工程中的测试是指在软件开发过程中对软件进行验证和验证的活动。测试的目的是发现错误和缺陷，以确保软件的质量和功能的正确性。以下是软件工程中的几种常见的测试方法：

1. 单元测试：对软件中的最小功能单元进行测试，如函数、方法或模块。它通常由开发人员自行进行，并使用测试框架来编写和执行测试用例。
2. 集成测试：测试不同模块或组件之间的集成和交互。它旨在确保不同部分之间的集成正常工作，并验证系统的整体功能和性能。
3. 系统测试：对整个软件系统进行全面的测试，以验证其是否符合需求规格和用户期望。这种测试通常由专门的测试团队进行，并使用各种测试技术和方法。
4. 验收测试：由最终用户或客户进行的测试，以确认软件是否满足其需求和期望。它通常在软件开发的最后阶段进行，以确保软件已经达到可交付和可用的状态。

除了这些基本的测试方法外，还有一些其他的测试技术，如性能测试、[安全测试](https://cloud.tencent.com/product/sr?from_column=20065\&from=20065)、可靠性测试和兼容性测试等，用于验证软件在不同方面的质量和性能。

```csharp
 - 目标：保证软件质量
 - 输出：软件测试计划、测试用例、软件测试报告
```

#### :shell:维护

软件工程中的维护是指对已经开发完成的软件系统进行修改、优化和修复工作。维护是软件开发过程中非常重要的一环，因为维护工作的质量直接影响到软件系统的稳定性、可靠性和可用性。

维护包括以下几个方面：

1. 纠正性维护：指修复软件系统中的错误和缺陷，确保软件系统的正常运行。
2. 适应性维护：指根据外部环境和需求的变化，对软件系统进行修改和扩展。
3. 完善性维护：指对软件系统的性能、可用性和用户体验进行改进和优化。
4. 预防性维护：指对软件系统进行预防性的检查和调整，以保证系统的稳定性和可靠性。

维护工作通常包括以下几个阶段：

1. 问题诊断和分析：对软件系统中出现的问题进行诊断和分析，确定问题的原因和解决方案。
2. 问题修复和验证：根据问题的解决方案对软件系统进行修复，并进行验证和测试，确保修复后的系统正常运行。
3. 修改和扩展：根据需求变更，对软件系统进行修改和扩展，以满足新的需求和功能。
4. 性能优化：对软件系统的性能进行分析和优化，提升系统的响应速度和效率。
5. 文档更新：对软件系统的文档进行更新和维护，确保文档与实际系统保持一致和最新。

维护工作需要软件工程师具备扎实的编程和调试能力，以及对软件系统的深入理解和熟悉。同时，维护工作也需要与用户和需求方进行有效的沟通和协作，以确保维护工作的准确性和及时性。

### :cactus:能力成熟度模型-CMM

能力成熟度模型（Capability Maturity Model，简称CMM）是一种软件工程评估模型，用于评估和提高组织的软件开发和维护过程的成熟度。

CMM最初由美国软件工程研究所（SEI）于1987年开发，后来发展成为CMMI（Capability Maturity Model Integration）。

CMM是一个五阶段模型，每个阶段描述了组织的软件工程能力水平和过程成熟度。

![img](/assets/40f0cf473232b36e38589ee900016fe1.BIyypHAQ.png)

### :cactus:能力成熟度模型集成-CMMI

是若干过程模型的综合和改进，不仅仅是软件，而是支持多个工程学科和领域的、系统的、一致的过程改进框架，能适应现代工程的特点和需要，能提高过程的质量和工作效率。

CMMI两种表示方法：

1. 阶段式模型：类似于CMM，它关注组织的成熟度。
2. 连续式模型：关注每个过程域的能力，一个组织对不同的过程域可以达到不同的过程域能力等级。

阶段式模型成熟度等级描述如下：

![img](/assets/786a9696c1efe9d84e646773c7b8fc0d.OvZy5g0z.png)

![img](/assets/abf72c29f7af83d4cd8bbe6a48fa8706.CVc1i9sj.png)

![img](/assets/c3270ed4fa65040ce3c4cc234676bb34.BsgF2_Dw.png)

## 二、软件过程模型

软件工程的过程模型是指开发软件的过程中所采用的一种规范化方法或框架。常见的软件工程过程模型包括瀑布模型、迭代开发模型、喷泉模型、敏捷开发模型等。

1. 瀑布模型：软件开发按照线性顺序依次进行，包括需求分析、设计、编码、测试和部署等阶段，每个阶段的完成都依赖于上一个阶段的输入。适用于需求稳定、项目较小的情况。
2. 迭代开发模型：将软件开发划分为多个迭代周期，每个周期都包括需求分析、设计、编码、测试和部署等阶段。每个迭代周期产生一个可交付的软件部分，迭代的次数和周期可以根据实际情况调整。适用于需求变化较快、项目较大的情况。
3. 喷泉模型：将软件开发看作是一个持续的过程，包括需求分析、设计、编码、测试等阶段。不同于瀑布模型的是，各个阶段可以交叉进行，灵活调整开发的优先级和顺序。适用于需求变化频繁、项目较复杂的情况。
4. 敏捷开发模型：以迭代的方式进行软件开发，强调团队合作和灵活性。将需求分解为小的用户故事，通过短暂的迭代周期快速交付部分功能，并根据实际反馈进行迭代和调整。适用于需求变化频繁、项目较灵活的情况。

软件工程的过程模型是一种规范和指导软件开发过程的框架或方法。它将软件开发过程分解为一系列阶段和活动，并定义了每个阶段的输入、输出和相应的工作内容。

### :cactus:瀑布模型

#### :shell:瀑布模型（S D L C）

瀑布模型（Waterfall Model）是软件开发生命周期（SDLC）中的一种经典开发模型。它是一种顺序性的开发模型，由上往下依次进行，各个阶段依赖于前一阶段的结果。

瀑布模型是一个经典的生命周期模型，一般将软件开发分为：可行性分析（计划）、需求分析、软件设计（概要设计、详细设计）、编码、测试、运行维护几个阶段。在每个阶段完成后，才能进行下一个阶段。这种顺序性的开发模型适用于需求变化较少且具体明确的项目。

瀑布模型是以文档作为驱动、适合于软件需求很明确的软件项目的模型。

#### :shell:瀑布模型特点

① 从上一项开发活动接收该项活动的工作对象作为输入。

② 接收输入后，实施该项活动应完成的工作内容。

③ 给出该项活动的工作成果，作为输出传给下一项开发活动。

④ 对该项活动的实施工作成果进行评审 。 若评审通过 ， 则进入下一项开发活动；否则返回前一项甚至更前项的活动。

![img](/assets/d88e2a3890795d12c7fdf0a85789e4c8.D7fpneUe.png)

#### :shell:瀑布模型的优缺点

瀑布模型的优点：明确的项目计划和清晰的开发流程；适用于较小规模的项目；开发过程中轻松追踪和控制进度。

瀑布模型的缺点：由于各个阶段的依赖关系，需求变更较大时，会导致整个项目的延迟；测试阶段在开发结束后才进行，可能导致问题的发现和修复较晚；客户参与程度较低，可能导致最终产品与客户需求有较大差距。

### :cactus:瀑布模型变种-V模型

#### :shell:V模型

V模型是瀑布模型的一种变种，它将瀑布模型的开发过程与质量保证过程相互关联，以确保产品的质量和符合用户需求。

在V模型中，开发过程和测试过程是相互平行的，如同字母“V”一样。下面是V模型的几个阶段：

1. 需求分析阶段：在这个阶段，需求分析师与客户共同定义和理解项目需求，并将其转化为需求规格说明书。
2. 系统设计阶段：在这个阶段，系统设计师根据需求规格说明书，设计系统架构和功能描述，以确保满足特定需求。
3. 模块设计阶段：在这个阶段，软件设计师将系统设计拆解为模块，并定义每个模块的详细设计规范。
4. 编码阶段：在这个阶段，开发人员根据模块设计规范，编写源代码。
5. 单元测试阶段：在这个阶段，开发人员对每个编写的模块进行测试，以确保其功能正常。
6. 集成测试阶段：在这个阶段，将已经测试通过的模块进行集成，并对整个系统进行测试，以确保模块之间的互操作性和功能一致性。
7. 系统测试阶段：在这个阶段，将整个系统进行全面测试，以验证系统是否满足用户需求和规格。
8. 验收测试阶段：在这个阶段，将系统交付给用户或客户，由用户执行测试，以确认系统是否符合需求。

V模型强调测试在整个开发过程中的重要性，通过将测试过程纳入开发过程中，可以及早发现和解决问题，提高产品质量。同时，V模型也能够提供一个明确的开发和测试过程的框架，提高开发团队的工作效率。

#### :shell:V模型特点

① 单元测试主要针对编码过程中可能存在的各种错误进行验证。

② 集成测试主要针对详细设计中可能存在的各种错误进行验证。

③ 系统测试主要针对概要设计，检查系统作为一个整体是否可以有效地运行。

④ 验收测试主要是针对需求建模、需求分析，通常由业务专家或者用户进行，以确认产品能真正符合用户业务上的需要。

⑤ V模型用于需求明确和需求变更不频繁的情形。

⑥ V模型中的突出的是测试和开发生命周期各阶段的结合。

![img](/assets/3f783fc7a1639aa7481e30bc161f424d.DlVadosI.png)

### :cactus:演化模型-原型模型

#### :shell:原型模型

在项目开始阶段，进行与项目干系人的有效交流，确保对需求的准确理解。然后，创建一个快速原型，以便项目干系人与未来用户之间可以通过原型进行交互，从而更好地了解需求和反馈。随后，通过与相关干系人进行充分的讨论和分析，以深入了解当前系统的需求。最后，在基于原型的基础上，进行开发过程，以确保最终交付的产品能够满足用户的期望和需求。

#### :shell:原型模型的特点

① 实际可行。

② 具有最终系统的基本特征。

③ 构造方便、快速，造价低。

④ 它对用户需求是动态的、逐步纳入的。

![img](/assets/73ded5bc2d8c96a3e4a96a279d91b778.lTsU_OPv.png)

### :cactus:增量模型

#### :shell:增量模型

增量模型是软件开发中的一种开发方法，也称为增量开发。它是一种迭代式的开发方法，将软件系统划分成多个增量，每个增量分别开发、测试和部署，然后按顺序进行整合。每个增量都是一个完整的软件系统的一部分，具有一定的功能和价值。

#### :shell:增量模型的特点

1. 增量划分：将软件系统分为多个增量，每个增量都有独立的功能和价值。
2. 迭代开发：各个增量按照顺序进行开发、测试和部署，每个增量都是前一个增量的基础。
3. 重复循环：每个增量都经历开发、测试和部署的循环，直到整个软件系统完成。
4. 增量交付：每个增量都能够交付给用户使用，用户可以在每个增量的基础上进行反馈和指导。

![img](/assets/b293fd6e12e87a9c3430cddcbe79b860.CuOF0_3e.png)

#### :shell:增量模型的优缺点

增量模型的优点包括：

1. 提高反馈效率：每个增量都可以及时交付给用户使用，用户可以提供实际的反馈和指导，有助于及时纠正和改进。
2. 降低风险：由于每个增量都是独立的，因此在开发过程中发现的问题不会对整个系统造成重大影响。
3. 提高可维护性：每个增量都有独立的设计和文档，方便后续的维护和升级操作。

增量模型的缺点包括：

1. 需要额外的成本和时间：由于需要迭代开发和多次测试，增量模型需要额外的成本和时间来进行。
2. 需要用户的积极参与：增量模型需要用户的积极参与，包括对每个增量的评估和反馈，对某些项目来说，可能用户的参与程度较低。

### :cactus:演化模型-螺旋模型

#### :shell:螺旋模型

在螺旋模型中，软件开发过程被表示为一个螺旋形状的曲线，其沿着时间和成本两个维度进行演化。螺旋模型的基本思想是通过不断进行风险分析和迭代来逐步开发和改进软件系统。

螺旋模型的核心是不断循环的风险分析和迭代过程。在每个迭代中，开发团队会分析和评估当前系统的风险，然后制定相应的开发计划和策略。接下来，团队会根据计划进行开发和测试，并将结果反馈给用户。通过不断的迭代，系统逐渐完善和改进。

螺旋模型将瀑布模型和演化模型结合起来，强调了风险分析，特别适用于大型、复杂度高、风险高的系统。

#### :shell:螺旋模型的特点

螺旋模型将开发过程分为几个螺旋周期，具有周期性重复的螺旋线状，每个螺旋周期大致和瀑布模型相符合 。 每个螺旋周期分为如下4个工作步骤：制定计划、风险分析、实施工程和客户评估。

![img](/assets/96643d15ffc54dd6e1520e7066847250.DiNftqtU.png)

#### :shell:螺旋模型的优缺点

螺旋模型的优点：是可以根据项目的实际情况进行灵活调整，并且有助于及早发现和解决问题。它可以帮助开发团队在开发过程中不断学习和改进，从而提高整体的开发效率和质量。

螺旋模型的确定：首先，它需要较高的技术和管理能力，因为风险分析和迭代过程需要对项目进行全面的评估和规划。此外，螺旋模型通常用于大型和复杂的项目，对于小型项目可能过于繁琐。

### :cactus:喷泉模型

#### :shell:喷泉模型

喷泉模型是一种软件开发模型，它将用户需求作为主要动力，并以对象为驱动。它适用于面向对象的开发方法，并且具有迭代性和无间隙性。

在喷泉模型中，整个开发过程被视为一个喷泉，代表了用户需求的源泉。开发团队通过不断迭代和改进，将用户需求转化为最终的软件产品。

喷泉模型的开发过程是连续的，没有明确的阶段划分。开发团队不断进行需求分析、设计、编码和测试等活动，以不断改进软件的功能和质量。

#### :shell:喷泉模型的特点

* 喷泉模型的各个阶段没有明显的界线，开发人员可以同步进行。
* 其优点是可以提高软件项目的开发效率，节省开发时间。
* 由于喷泉模型在各个开发阶段是重叠的，在开发过程中需要大量的开发人员， 不利于项目的管理。此外，这种模型要求严格管理文档，使得审核的难度加大。

![img](/assets/4f3769a90075f0daeecf8be1c2bc2847.ClwCE2kL.png)

#### :shell:喷泉模型的优缺点

喷泉模型的优点：是能够及时响应用户需求的变化，保持开发过程的灵活性和敏捷性。与传统的瀑布模型相比，喷泉模型更加适合快速迭代和开发周期短的项目。

喷泉模型的缺点：由于没有明确的阶段划分，开发团队可能会在需求分析和设计阶段出现混乱，导致项目进展缓慢。此外，喷泉模型对开发团队的能力和协作要求较高，需要具备良好的沟通和协调能力。

### :cactus:基于构件的开发模型

#### :shell:基于构件的开发模型

基于构件的开发模型（Component-based Development Model）是一种软件开发方法，它将软件系统划分为独立的、可重用的构件，并通过构件之间的链接和组合来构建软件系统。

#### :shell:基于构件的开发模型的特点

基于构件的开发模型的主要特点包括：

1. 构件的独立性：构件是可以独立开发、测试和维护的，每个构件都具有明确定义的功能和接口。
2. 构件的可重用性：构件可以被多个软件系统所共享和重用，从而提高软件开发的效率和质量。
3. 构件的组合和链接：通过构件之间的链接和组合，可以灵活地构建符合需求的软件系统。
4. 构件的管理和维护：通过构件的管理和维护，可以实现对软件系统的快速更新和扩展。

特点是增强了复用性，在系统开发过程中，会构建一个构件库，供其他系统复用，因此可以提高可靠性，节省时间和成本。

![img](/assets/ce86a1913ed27dc445131aefa05e35b6.CqUqCZGw.png)

### :cactus:形式化方法模型

#### :shell:形式化方法模型

形式化方法模型的核心思想是使用形式化语言来描述系统的规范和要求。形式化语言通常是符号逻辑或数学语言，如集合论、谓词逻辑、时序逻辑等。通过将系统规范和要求转化为形式化语言的形式规范，开发人员可以使用形式化方法来进行系统分析、模型检查、定理证明等。

形式化方法模型可以有不同的形式，如有限状态自动机、Petri网、时序逻辑模型等。这些模型可以用于描述系统的结构、行为和性质。开发人员可以使用形式化方法模型进行系统建模、属性验证、错误检测等，以提高系统的可靠性和正确性。

#### :shell:形式化方法模型的优缺点

形式化方法模型具有形式化和精确性的优点，可以帮助开发人员在设计和开发过程中发现和解决潜在的问题。它还可以提供系统的形式证明，以确保系统的正确性和安全性。但是，形式化方法模型通常需要较高的技术水平和时间成本，因此在实际应用中可能有一定的限制。

### :cactus:统一过程模型

#### :shell:统一过程模型

统一过程（UP）模型是一种软件开发过程，**以用例和风险为驱动，以架构为中心，采用迭代和增量的方法**。UP模型通过将整个软件开发项目划分为多个小型项目，每个项目都包含计划、分析和设计、构造、集成和测试、内部和外部发布等阶段。

UP模型使用UML（统一建模语言）方法和工具来支持开发过程。UML是一种用于描述、构造、可视化和文档化软件系统的标准语言。它提供了一种统一的可视化表示方法，用于描述系统的需求、结构、行为和交互。

UP模型的核心理念是迭代和增量开发。迭代意味着将整个开发过程划分为多个迭代周期，每个周期都包含完整的开发阶段。每个迭代周期都会产生一个可执行的软件部分，可以进行测试和评审。增量指的是每个迭代周期都会向软件系统中添加新的功能，逐步增加系统的功能和性能。

UP模型的另一个重要特点是以用例和风险为驱动。用例是对系统功能和用户需求的描述，它们被用作需求分析和设计的基础。风险是可能对项目成功产生负面影响的因素，UP模型将风险识别和风险管理作为开发过程的重要组成部分。

#### :shell:统一过程模型的特点

统一过程模型包括4个阶段：初始阶段、精化阶段、构建阶段、移交阶段。

① 初始阶段：生命周期目标。

② 精化阶段：生命周期架构。

③ 构建阶段：初始运作功能。

④ 移交阶段：产品发布。

每次迭代产生包括最终系统的部分完成的版本和任何相关的项目文档的基线，通过逐步迭代基线之间相互构建，直到完成最终系统。在每个迭代中有5个核心工作流：捕获系统应该做什么的需求工作流，精化和结构化需求的分析工作流，在系统构架内实现需求的设计工作流， 构造软件的实现工作流，验证实现是否如期望那样工作的测试工作流。

#### :shell:RUP

RUP（Rational Unified Process）是一种软件开发过程框架，被认为是统一过程（UP）的商业扩展。它在UP的基础上进行了完善和详细化，为软件开发团队提供了一套全面的方法论和工具，以管理和实施软件项目的全生命周期。

RUP的特点包括：

1. 迭代开发：RUP将软件开发过程划分为多个迭代周期，每个周期包含一系列可交付成果，增量地构建系统。
2. 体系结构驱动：RUP强调对软件系统的体系结构进行设计和评审，确保系统的稳定性、可扩展性和可维护性。
3. 风险管理：RUP强调风险管理的重要性，通过早期风险评估和风险驱动的开发来最大限度地降低项目失败的风险。
4. 适应性：RUP可以根据不同的项目需求和特点进行定制，灵活适应各种规模和复杂度的软件开发项目。

RUP通过提供一系列指导文档、模板、工具和最佳实践，帮助开发团队规范和管理软件开发过程。它强调团队合作、迭代开发和[持续集成](https://cloud.tencent.com/product/coding-ci?from_column=20065\&from=20065)，旨在提高开发效率、降低风险并交付高质量的软件产品。

### :cactus:敏捷方法

#### :shell:敏捷方法

敏捷方法（Agile methods）是一种项目管理和开发方法论，旨在通过迭代、协作和快速响应变化来提高团队的效率和灵活性。敏捷方法的核心原则包括：

1. 个体和互动胜过流程和工具：强调团队成员之间的沟通和合作，以及快速解决问题。
2. 可工作的软件胜过详尽的文档：重视开发出可用的软件，而不是花费过多时间编写详细的文档。
3. 客户合作胜过合同谈判：鼓励与客户保持密切合作和沟通，以便快速了解需求变化，并提供满足客户需求的解决方案。
4. 响应变化胜过遵循计划：接受需求变化，并灵活地调整开发计划和优先级，以应对市场变化和客户需求的变化。

敏捷方法通常采用迭代开发的方式，将项目划分为一系列短期的开发周期，每个周期称为一个迭代（Iteration），通常持续2-4周。每个迭代结束时，团队会交付一部分可用的软件功能，然后根据客户反馈和需求变化进行调整和优化。

敏捷过程的典型方法有很多，每一种方法基于一套原则，这些原则实现了敏捷方法所宣称的理念(敏捷宣言)：极限编程（XP）、水晶法（C r y s t a l）、并列争求法（Sc rum）、自适应软件开发（AS D）、敏捷统一过程（AUP）

#### :shell:敏捷方法的特点

敏捷开发是一种以人为核心、迭代、循序渐进的开发方法，相对于传统软件开发方法的“非敏捷”，更强调程序员团队与业务专家之间的紧密协作、面对面的沟通（认为比书面文档更有效）、频繁交付新的软件版本、紧凑而自我组织型的团队、能够很好地适应需求变化的代码编写和团队组织方法，也更注重软件开发中人的作用。

敏捷软件开发宣言：

* 个体和交互胜过过程和工具
* 可以工作的软件胜过面面俱到的文档
* 客户合作胜过合同谈判
* 响应变化胜过遵循计划

### :cactus:敏捷方法-极限编程

XP是一种轻量级（敏捷）、高效、低风险、柔性、可预测的、科学的软件开发方式。它由价值观、原则、实践和行为四个部分组成，彼此相互依赖、关联，并通过行为贯穿于整个生命周期。

4大价值观：沟通、简单性、反馈和勇气。

5个原则：快速反馈、简单性假设、逐步修改、提倡更改和优质工作。

12个最佳实践：

1. 计划游戏：快速制定计划、随着细节的不断变化而完善。
2. 小型发布：系统的设计要能够尽可能早地交付。
3. 隐喻：找到合适的比喻传达信息。
4. 简单设计：只处理当前的需求，使设计保持简单。
5. 测试先行：先写测试代码，然后再编写程序。
6. 重构：重新审视需求和设计，重新明确地描述它们以符合新的和现有的需求。
7. 结对编程。
8. 集体代码所有制。
9. 持续集成：可以按日甚至按小时为客户提供可运行的版本。
10. 每周工作40个小时。
11. 现场客户。
12. 编码标准。

### :cactus:敏捷方法-其他方法

| 实践       | 描述                                                         |
| :--------- | :----------------------------------------------------------- |
| 结对编程   | 一个程序员开发，另一个程序员在一旁观察审查代码，提高代码质量 |
| 自适应开发 | 强调开发方法的适应性，为软件的重要性提供基础，适应组织和管理层次 |
| 水晶方法   | 不同项目需要不同的策略、约定和方法论                         |
| SCRUM      | 迭代的增量化工程方法，按需求的优先级别实现产品               |

### :cactus:总结

![img](/assets/9080b88987a1bbc1aef71a8f66595ee8._RVcgK53.png)

![img](/assets/2e6ad2ca37eedb3202b348a955a544cd.xx1uD21x.png)

![img](/assets/e66884b41c93efb77f4ec96513273c9b.Ie03_h8w.png)

## 三、需求分析

软件工程需求分析是软件开发过程中的重要环节之一，它主要是通过收集、分析和规范用户的需求，为软件开发团队提供明确的需求指导，确保软件开发的目标和方向与用户需求一致。

在软件工程需求分析过程中，一般包括以下几个主要步骤：

1. 需求收集：通过与用户沟通、访谈、调查等方式，收集用户对软件的需求和期望。收集的需求可以包括功能需求、性能需求、安全需求等。
2. 需求分析：对收集到的需求进行分析和整理，将其转化为可操作的需求规范。需求分析一般包括需求描述、需求分类、需求优先级确定等步骤。
3. 需求验证：验证需求是否符合用户的实际需求，通常包括需求审查、原型验证、用户测试等方式。通过需求验证，可以发现和纠正需求规格中的问题，确保需求的准确性和完整性。
4. 需求管理：管理和维护需求规格，包括需求变更管理、需求跟踪管理、需求优先级调整等。在软件开发过程中，需求可能会随着时间和用户需求变化而发生变更，需求管理可以帮助开发团队及时响应和处理需求变更。

需求分析是软件开发过程中的关键环节，它对于软件开发的成功与否起着至关重要的作用。只有通过需求分析，开发团队才能了解用户的真实需求，设计出符合用户期望的软件系统。

### :cactus:软件需求

#### :shell:定义

软件需求是指用户或相关利益相关方对软件系统所提出的期望或要求。它涵盖了系统的`功能、行为、性能、设计约束`等方面。

* `功能需求`描述了系统应该具有的功能和特性。这些功能需求可以是用户期望的基本功能，也可以是系统必须具有的关键功能。
* `行为需求`描述了系统在不同情况下的行为和响应。这些行为需求可以包括用户界面的交互过程、数据处理过程、错误处理过程等。
* `性能需求`描述了系统在处理各种输入情况下的性能要求。这些性能需求可以包括系统的响应时间、吞吐量、可靠性等。
* `设计约束`描述了系统设计和实现时需要遵循的约束条件。这些约束条件可以包括操作系统和硬件平台的要求、技术限制、安全需求等。

IEEE中的定义是：软件需求指用户解决问题或达到目标所需要的条件或能力，是系统或系统部件要满足合同、标准、规范或其他正式规定文档所需具有的条件或能力，以及反映这些条件或能力的文档说明。

#### :shell:需求来源

* 可以来自于用户（实际的潜在的）、用户的规约、应用领域的专家、相关的技术标准和法规。
* 可以来自于原有的系统、原有系统的用户、新系统的潜在用户。
* 甚至还可以来自于竞争对手的产品。🦋1.3 需求分类

软件需求的分类：软件需求就是软件必须完成的事以及必须具备的品质。需求是多层次的，包括业务需求、用户需求和系统需求，这三者是从目标到具体，从整体到局部，从概念到细节。

| 需求分类      | 描述                                                         |
| :------------ | :----------------------------------------------------------- |
| 业务需求      | 反映企业或客户对系统高层级的目标要求（高层级需求）           |
| 用户需求      | 描述用户的具体目标，用户想要什么，以及要这些做什么（用户需求） |
| 系统需求      | 从系统的角度说明软件的需求，包括功能需求、非功能需求、设计约束等 |
| -- 功能需求   | 系统必须完成的那些事，即为了向其用户提供有用的功能，产品必须执行的动作 |
| -- 非功能需求 | 产品必须具备的性能或品质，例如，可靠性、容错性等             |
| -- 设计约束   | 也称为限制条件、补充规约，通常是对解决方案的一些约束说明，例如，某系统必须采用国有自主知识版权的数据库，必须运行在UNIX系统之下，等等 |

![img](/assets/3c09499284354d61bb94a58eb1009d6b.BjsdPIb7.png)

质量功能部署（QFD）是一种技术，旨在将用户要求转化为软件需求，以最大限度提高用户满意度。它通过多层次的演绎分析，将用户对产品的需求转化为产品设计需求、工程部件特征、工艺要求和生产要求，以指导产品设计和保证产品质量。由于QFD使用的图形类似于房屋，因此也被称为"质量屋"。

![img](/assets/9fc4fd9e4cfa6009f2beee1dec9a7444.B-mPC8tS.png)

QDF将软件需求分为三类：

| 软件需求分类 | 描述                                                         |
| :----------- | :----------------------------------------------------------- |
| 常规需求     | 系统应该做到的功能或性能，实现的越多，用户越满意             |
| 期望需求     | 用户想当然认为系统应该做到，但是不能正确表达的功能，没有实现会让用户不满意 |
| 意外需求     | 用户要求范围外的功能或性能，开发人员控制，实现会高兴，不实现也没关系 |

![img](/assets/da32309db87c349ece1f8afe7ecde35c.aGBTKSlm.png)

### :cactus:需求工程

需求工程：是一个不断反复的需求定义、文档记录、需求演进的过程，并最终在验证的基础上冻结需求。需求工程可以细分为需求获取、需求分析（包括系统建模）、需求规约、需求验证以及需求管理5个阶段。

* 需求开发：包括需求获取、需求分析、编写规约（系统规格说明书）和需求验证4个阶段。
* 需求管理：通常包括定义需求基线、处理需求变更及需求跟踪等方面的工作。

需求开发的4个阶段：

在需求开发阶段需要确定产品所期望的用户类型、获取每种用户类型的需求、了解实际的用户任务和目标，以及这些任务所支持的业务需求。

同时还包括分析源于用户的信息、对需求进行优先级分类、将所收集的需求编写成为软件规格说明书和需求分析模型，以及对需求进行评审等工作。

#### :shell:需求获取

需求获取是一个确定和理解不同项目干系人的需求和约束的过程。在需求获取的过程中，主要解决需求调查的问题。为了做好需求调查，必须清楚地了解以下三个问题：

1. What：应该搜集什么信息。 需要明确搜集的信息内容，包括功能需求、性能需求、用户体验需求、安全需求等。
2. Where：从什么地方搜集这些信息。 需要确定搜集信息的来源，可以通过用户访谈、[问卷调查](https://cloud.tencent.com/product/survey?from_column=20065\&from=20065)、采样、情节串联板、联合需求计划等方式来获取需求信息。
3. How：用什么机制或技术来搜集这些信息。 需要选择合适的机制或技术来进行需求获取，例如使用结构化和非结构化的用户访谈，设计用户调查问卷，采用统计分析技术进行采样，使用情节串联板来讲述故事，以及通过联合讨论会来组织会议等。需求记录技术，如任务卡片、场景说明、用户故事等，也可以用来搜集需求信息。

常见的需求获取方式有：

| 需求获取方式 | 描述                                                         |
| :----------- | :----------------------------------------------------------- |
| 用户访谈     | 一对一进行访谈，适合于针对有代表性的用户。形式分为结构化和非结构化两种。 |
| 问卷调查     | 设计问题、制作成用户调查问卷、下发填写、整理分析。适合用户面广、用户需要灵活时间进行回馈。 |
| 采样         | 采用统计分析技术，从目标总体中选择出样本集的过程。可以是随机抽样，也可以是非随机抽样。适合用于大量用户信息或者数据信息采集分析，最终得出需求的场景。 |
| 情节串联板   | 一系列图片，通过这些图片来讲述故事，从而帮助理解用户需求和场景。 |
| 联合讨论会   | 通过联合关键用户代表、系统分析师、开发团队代表等进行组织的会议，讨论需求，以促进有效的交流和共识形成。 |
| 需求记录技术 | 使用任务卡片、场景说明、用户故事等方式来记录和描述用户需求，以便后续分析和编写需求规格书。 |

![img](/assets/726782f229538ca7225500c723d890a9.CfBOQQo1.png)

#### :shell:需求分析

##### ☀️2.2.1 需求分析定义

需求分析是将用户的杂乱无章的要求和期望转化为清晰、准确、可测试、可跟踪的系统需求，并形成最终的需求规约。一个好的需求应具有无二义性、完整性、一致性、可测试性、确定性、可跟踪性、正确性和必要性等特性。

在需求分析过程中，需求分析人员逐步细化软件功能，找出系统各元素之间的联系、接口特性和设计上的限制，并分析它们是否满足需求。不合理的部分被剔除，需要的部分被增加。最终，综合成系统的解决方案，并给出详细的逻辑模型，描述系统的功能和实现方式。

##### ☀️2.2.2 需求分析任务

需求分析的任务包括：

* 绘制系统上下文范围关系图，以帮助理解系统的范围和边界。
* 创建用户界面原型，用于展示系统的外观和交互方式，以便用户和开发团队进行讨论和确认。
* 分析需求的可行性，评估各项需求在技术、资源和时间等方面的可行性和可实现性。
* 确定需求的优先级，将各个需求进行排序，以确保关键需求得到优先满足。
* 为需求建立模型，使用各种符号和图形来描述需求，以便更好地理解和传达需求信息。
* 创建数据字典，对系统中使用的数据进行定义和描述，确保数据的一致性和准确性。
* 使用QFD（质量功能部署），将用户需求与系统设计和开发过程相连接，以确保系统能够满足用户的期望和需求。

##### ☀️2.2.3 需求分析方法

目前，已存在的多种需求分析方法引用了不同的分析策略，常用的分析方法有以下两种：

* 面向数据流的结构化分析方法 (SA)。
* 面向对象的分析方法 (OOA)。

###### 🌈2.2.3.1 面向数据流的结构化分析方法

结构化分析方法的特点是：自顶向下、逐步分解、分析的核心是数据字典。

结构化分析应该建立三种模型：数据模型、功能模型、行为模型：

| 模型类型 | 描述                                                   | 常用图示        |
| :------- | :----------------------------------------------------- | :-------------- |
| 数据模型 | 描述实体、属性和实体间的关系                           | 实体关系(E-R)图 |
| 功能模型 | 描述数据在系统间的传递情况                             | 数据流图(DFD)   |
| 行为模型 | 描述系统状态和状态转换的事件，指出系统行为和执行的动作 | 状态转换图(STD) |

![img](/assets/47012ea6ab8a3a34d54b04f4457e651c.DqKLQaVf.png)

###### 🍬2.2.3.1.1 状态转换图(STD)

![img](/assets/574c7a9590c97a4257607649b0002d7c.Cb8Zznn2.png)

![img](/assets/bd2ac0e09f886a40b33b875b5f92a82b.B66r3a3c.png)

###### 🍬2.2.3.1.2 数据流图(DFD)

描述数据在系统中如何被传送或变换，以及如何对数据流进行变换的功能或子功能，用于对功能建模，数据流图相关概念示例如下：

![img](/assets/38e75a428c4343cd814d495d7739bc07._O8LLsHz.png)

数据图是一种可以分层的图表，根据层级可以分为顶层数据流图、中层数据流图和底层数据流图。顶层数据流图是最高层的图表，其他数据流图从零开始编号。

| 数据流图级别 | 描述                                                         |
| :----------- | :----------------------------------------------------------- |
| 顶层数据流图 | 只有一个加工，代表整个系统。包括输入数据流和输出数据流，表示系统的范围和与外部环境的数据交换关系。 |
| 中层数据流图 | 对顶层数据流图中的某个加工进行细化，可以再次细化形成子图。中间层次的多少取决于系统的复杂程度。 |
| 底层数据流图 | 不能再分解的数据流图，其加工称为"原子加工"。                 |

![img](/assets/6c3bb4ba8d4f161a8fce6f343b7d2b1b.CEM_60_i.png)

![img](/assets/2799ff06aa1316c9565d417e37e8935d.BCA4NDgM.png)

![img](/assets/12cce88f5029343519cb3945054bb342.CCpa0HFm.png)

![img](/assets/fcb878b9711c0d2e333acfa325d4d23c._bzqcZOO.png)

#### :shell:需求规约

##### ☀️2.3.1 需求规约定义

软件需求规约（SRS）是需求分析任务的最终产物，也被称为需求定义或软件需求规格说明书。它通过建立完整的信息描述、详细的功能和行为描述、性能需求和设计约束的说明，以及合适的验收标准，来提供对目标软件的各种需求。作为用户和开发者之间的一个协议，需求规约在软件工程的各个阶段发挥重要作用。

##### ☀️2.3.2 需求规约内容

| 内容     | 描述                                                         |
| :------- | :----------------------------------------------------------- |
| 引言     | 描述软件目标，以计算机系统为背景进行阐述                     |
| 信息描述 | 描述问题的详细说明、信息内容、信息流和信息结构               |
| 功能描述 | 为每个功能提供处理过程说明，描述设计约束和性能特征，使用图形表示软件的整体结构和与其他系统元素的相互影响 |
| 行为描述 | 描述软件操作的外部事件和内部控制特征                         |
| 校验标准 | 描述系统成功的测试标准，即哪些测试和结果表示系统已成功实现   |
| 参考书目 | 引用与软件相关的文档，包括其他软件工程文档、技术参考文献、厂商文献和标准 |
| 附录     | 包含补充信息、表格数据、算法描述、图表和其他资料等           |

##### ☀️2.3.3 需求规约方法

1. 严格定义（又称预先定义）是需求建立在以下基础假设之上：所有需求都能被预先定义；开发人员与用户能准确交流；采用图形（或文字）充分体现最终系统。
2. 原型方法是一种迭代循环的开发方式，但需注意并非所有需求都能在系统开发前准确说明。原型提供了克服交流困难的手段，需要实际可供用户参与的系统模型和合适的系统开发环境。反复是完全需要和值得提倡的，一旦需求确定，就应遵从严格的方法。🦋2.4 需求验证☀️2.4.1 需求验证定义

需求验证，也称为需求确认，旨在与用户一起确认需求的准确性。这个过程包括两个步骤：需求评审和需求测试。需求评审是对需求规约进行正式或非正式的评审。需求测试则是设计概念测试用例来验证需求。

需求验证作为需求开发阶段的一项工作，旨在复查需求的正确性、完整性和清晰性，以确保它能够准确反映用户的意愿。由于需求的变化可能会导致系统设计和实现的变更，因此，由需求引起的系统变更成本往往比修改设计或代码错误的成本要高得多。为了确保软件需求定义的质量，评审应由专门的人员负责，并按照规程严格进行。除了分析人员外，还应有用户、开发部门的管理者以及软件设计、实现和测试人员参加评审。

需求验证通过后，需要请用户签字确认，作为验收标准之一。此时，需求规格说明书作为需求基线，不可再随意更新。如果需要更改，必须按需求变更流程进行。需要注意的是，需求验证无法发现所有的需求问题。因此，在需求验证之后，无法避免地会对遗漏的需求进行补充，对错误理解进行更正，需求管理是必要的。

##### ☀️2.4.2 需求验证内容

1. 确保系统定义的目标与用户的要求一致。
2. 检查系统需求分析阶段提供的文档资料是否齐全，以及文档中的描述是否完整、清晰、准确地反映了用户要求。
3. 确认被开发项目的数据流与数据结构是否确定且充足。
4. 检查主要功能是否已包括在规定的软件范围之内，并且是否都已充分说明。
5. 确认设计的约束条件或限制条件是否符合实际。
6. 评估开发的技术风险。
7. 确保详细制定了检验标准，并且这些标准能够对系统定义进行确认。

#### :shell:需求管理

##### ☀️2.5.1 需求管理定义

软件需求管理是指在软件项目进行过程中，通过一系列的活动来识别、控制和跟踪需求。它涉及需求工程的规划和控制，以获取、组织和记录系统需求，并使用户和项目团队在不断变更的需求上达成一致。需求管理是确保软件开发过程中需求的正确性、一致性和有效性的关键过程。它帮助项目团队确定和理解用户需求，并在开发过程中追踪和管理这些需求的变化。

在需求管理中，每个需求被赋予唯一的标识符，这种需求的唯一标识符通常被称为需求ID，它可以是一个数字、字母或组合的代码。通过为需求建立跟踪表，可以清楚地记录需求与其他相关文档或代码的关系，方便跟踪和管理需求的变更和演化过程。

特征跟踪表：

| 需求ID | 特征1 | 特征2 | 特征3 | ...  |
| :----- | :---- | :---- | :---- | :--- |
| REQ001 | 是    | 否    | 是    | ...  |
| REQ002 | 否    | 是    | 否    | ...  |
| REQ003 | 是    | 是    | 是    | ...  |
| ...    | ...   | ...   | ...   | ...  |

来源跟踪表：

| 需求ID | 来源1    | 来源2    | 来源3    | ...  |
| :----- | :------- | :------- | :------- | :--- |
| REQ001 | 用户反馈 | 业务需求 | ...      | ...  |
| REQ002 | 业务需求 | ...      | ...      | ...  |
| REQ003 | 用户反馈 | 业务需求 | 用户调研 | ...  |
| ...    | ...      | ...      | ...      | ...  |

依赖跟踪表：

| 需求ID | 依赖需求1 | 依赖需求2 | 依赖需求3 | ...  |
| :----- | :-------- | :-------- | :-------- | :--- |
| REQ001 | REQ002    | REQ003    | ...       | ...  |
| REQ002 | REQ004    | ...       | ...       | ...  |
| REQ003 | REQ002    | REQ005    | ...       | ...  |
| ...    | ...       | ...       | ...       | ...  |

跟踪表可以包含以下信息：

1. 需求ID：每个需求的唯一标识符。
2. 需求描述：对需求的详细描述。
3. 需求状态：需求的当前状态，如提出、评审中、已实现等。
4. 需求优先级：需求的优先级，用于确定开发和测试的顺序。
5. 需求来源：需求的来源，如业务需求、用户反馈等。
6. 需求关系：需求与其他需求或文档的关系，可以是包含关系、依赖关系等。
7. 需求变更历史：需求的变更记录，包括变更原因、变更内容和变更日期等。

这些跟踪表可以用于需求跟踪。在整个开发过程中，进行需求跟踪的目的是为了建立和维护从用户需求开始到测试之间的一致性与完整性，确保所有的实现是以用户需求为基础，所有的输出符合用户需求，并且全面覆盖了用户需求。

##### ☀️2.5.2 版本控制

是的，版本控制在需求管理中起到非常重要的作用。通过版本控制，可以确保团队和用户之间对需求的共识，并定义需求的基线。一旦需求规约经过评审并形成了需求基线，下次如果有需求变更，则需要按照一定的流程进行处理。

版本控制可以通过以下步骤来进行：

1. 需求规约评审：一旦完成了需求规约的编写，团队需要进行评审，确保需求规约的准确性和完整性。只有通过了评审的需求规约才能被视为需求基线。
2. 变更请求：如果有任何需求变更的请求，用户或团队可以提交变更请求。变更请求应该包含详细的变更描述和原因。
3. 变更评估：变更请求被接收后，团队需要对变更进行评估。评估包括分析变更的影响范围、资源需求、时间计划等。
4. 变更批准：根据变更评估结果，团队确定是否批准变更请求。如果批准，团队将进行相应的变更实施。
5. 变更实施：团队根据批准的变更请求进行相应的变更实施，更新需求规约或其他相关文档。

![img](/assets/2955ff75f6aacc253dac1e53bf863ec4.CRmzT0qO.png)

##### ☀️2.5.3 需求跟踪

1、 需求跟踪

需求跟踪是确保软件开发过程中满足用户需求的一种方法。正向跟踪和逆向跟踪是两种常用的需求跟踪方式。

正向跟踪是从用户需求出发，通过检查需求规约中的每个需求是否在后继工作产品中找到对应点来进行跟踪。简单来说，就是核对用户原始需求是否都已实现。

逆向跟踪则是从工作产品出发，检查设计文档、代码、测试用例等产品是否能在需求规约中找到对应的来源。简单来说，就是检查软件实现是否符合用户需求。

2、状态跟踪

整个项目过程中，要始终跟踪需求状态即变更情况

##### ☀️2.5.4 变更控制

###### 🌈2.5.4.1 变更控制的风险

变更控制是[项目管理](https://cloud.tencent.com/product/coding-pm?from_column=20065\&from=20065)中关键的一环，其主要目标是管理需求的变更，确保变更的合理性和有效性。在需求变更过程中，存在一些潜在的风险，需要进行风险管理。

以下是一些带有风险的做法：

| 风险因素           | 描述                                                         |
| :----------------- | :----------------------------------------------------------- |
| 无足够用户参与     | 如果项目团队缺乏与最终用户的充分沟通和合作，可能导致对用户需求的误解和遗漏，最终影响项目的成功。 |
| 忽略用户分类       | 不同用户群体有不同的需求和优先级，忽略了用户分类可能导致无法满足某些重要用户群体的需求，降低项目价值。 |
| 用户需求的不断增加 | 如果在项目进行过程中，不断有新的需求被提出并接受，可能导致项目进度延迟和成本增加。 |
| 模棱两可的需求     | 如果需求描述模糊不清或存在歧义，可能导致开发过程中产生错误理解，最终交付的产品不符合预期。 |
| 不必要的特性       | 如果项目团队在需求变更过程中不加以筛选和评估，可能导致添加不必要的特性，增加开发成本和复杂性。 |
| 过于精简的SRS      | 如果需求文档(SRS)过于简单和不完整，可能导致对需求的理解不准确或遗漏，影响项目的规划和实施。 |
| 不准确的估算       | 如果在需求变更过程中对成本、时间和资源的估算不准确，可能导致项目延期、超支和资源不足。 |

###### 🌈2.5.4.2 变更控制的原因

| 变更原因           | 描述                                                         |
| :----------------- | :----------------------------------------------------------- |
| 外部环境的变化     | 市场需求的变化、竞争对手的新产品推出、法规政策的调整等，都可能导致业务流程和需求发生变化，从而需要进行变更。 |
| 需求和设计不够完整 | 在项目开始阶段，需求和设计可能未能考虑到所有情况，或者在实施过程中发现了一些问题，需要进行相应的调整和变更。 |
| 新技术的出现       | 随着科技的不断发展，新的技术可能提供了更好的解决方案，或者能够提高效率和质量，因此需要对原有系统进行升级或变更。 |
| 公司机构重组       | 公司的组织结构发生调整，业务流程可能会有所变化，需要调整相应的系统和流程来适应新的组织架构。 |

###### 🌈2.5.4.3 变更控制委员会

变更控制委员会（Change Control Board, CCB）是一个负责评价、审批和监督变更的组织机构。它也被称为配置控制委员会。CCB通常由项目经理、技术专家、利益相关者和其他相关人员组成。它的任务包括：

| CCB职责          | 描述                                                         |
| :--------------- | :----------------------------------------------------------- |
| 评价变更建议     | CCB会对提出的配置项变更建议进行评估，包括对其目的、范围、影响和风险等方面进行分析和评估。 |
| 审批变更请求     | CCB对变更请求进行审批，决定是否将其纳入项目的变更控制流程中。审批过程可能包括讨论、投票和达成共识。 |
| 监督已批准的变更 | 一旦变更请求得到批准，CCB负责监督变更的实施过程，确保按照批准的计划和要求进行变更。 |
| 跟踪和报告变更   | CCB跟踪所有已批准的变更，并定期向相关人员报告变更的状态和结果。 |
| 风险管理         | CCB也需要评估变更可能带来的风险，并提出相应的控制措施，以最大程度地减少对项目的负面影响。 |

###### 🌈2.5.4.4 变更控制的内容

* 需求变更需要经过正式评估来决定是否批准：变更控制的核心是确保对需求变更进行评估和决策，以避免不必要的变更带来的影响和风险。在变更控制过程中，需求变更将经过严格的评估，包括对变更的必要性、影响和可行性进行分析和讨论，以决定是否批准变更。
* 保持项目计划与需求的同步：变更控制的一个重要目标是确保项目计划与需求的一致性和同步。当需求发生变更时，需要及时修改项目计划，并确保所有相关方了解和同意这些变更。这有助于确保项目在变更发生时能够及时做出相应的调整，以保证项目的进度和质量。
* 以可控制的方式将需求变更融入项目中：变更控制的另一个重要目标是确保需求变更能够被有效地管理和融入项目中。这包括定义变更的优先级和紧迫性，制定变更实施计划，确保变更的可控性和可追踪性，以及对变更进行适当的测试和验证。
* 估计变更需求产生的影响，并协商新的约定：在变更控制过程中，需要对变更需求产生的影响进行估计，并与相关方进行协商，制定新的约定和规划。这包括对项目进度、成本、资源和风险等方面的影响进行评估，以及与相关方就变更的实施和后续工作进行协商和达成一致。这有助于确保变更的顺利实施，并最大程度地减少变更造成的不利影响。

![img](/assets/307c0c2075caf4cd69c74f77babf9117.DmoXKEkI.png)

## 四、系统设计

软件工程中的系统设计是指在需求分析的基础上，对软件系统进行整体架构和各个模块的设计。系统设计的目标是将需求转化为具体的实现方案，明确软件的结构和功能，并考虑系统的可维护性、可扩展性、可重用性等方面的要求。

系统设计包括以下几个主要阶段：

| 阶段         | 描述                                                         | 常用方法               |
| :----------- | :----------------------------------------------------------- | :--------------------- |
| 系统结构设计 | 确定软件系统的整体结构，包括模块之间的关系、数据的流动等。   | 层次结构设计、模块划分 |
| 模块设计     | 对每个模块进行详细设计，包括模块的功能、接口、数据结构等。   | 模块图、数据流图       |
| 数据设计     | 对系统中涉及到的数据进行设计，包括数据的存储结构、数据库设计等。 | 数据流图、实体关系图   |
| 接口设计     | 设计模块之间的接口，明确各模块之间的通信方式和数据传递方式。 |                        |
| 安全性设计   | 考虑系统的安全性需求，包括用户认证、数据加密等。             |                        |
| 可维护性设计 | 考虑系统的可维护性需求，包括代码的可读性、可测试性等。       |                        |
| 性能设计     | 考虑系统的性能要求，包括响应时间、吞吐量等。                 |                        |

系统设计需要综合考虑多个因素，包括功能需求、可行性、技术可行性、资源约束等。在设计过程中，需要与需求分析、系统测试、实施等其他阶段进行紧密的沟通和协作，确保设计的准确性和可行性。

### :cactus:概述

#### :shell:主要目的

系统设计的主要目的就是为系统制定蓝图，在各种技术和实施方法中权衡利弊，精心设计，合理地使用各种资源，最终勾画出新系统的详细设计方案。

#### :shell:主要内容

系统设计的主要内容包括新系统总体结构设计、代码设计、输出设计、输入设计、处理过程设计、数据存储设计、用户界面设计和安全控制设计等。

| 设计内容           | 描述                                                         |
| :----------------- | :----------------------------------------------------------- |
| 新系统总体结构设计 | 确定系统的整体结构，包括模块之间的关系、数据流程、功能划分等，以确保系统的合理性和可扩展性。 |
| 代码设计           | 设计系统的具体实现方式，包括选择适当的编程语言和技术框架，定义类、函数、方法等的结构和功能，以实现系统的各项功能和业务需求。 |
| 输出设计           | 设计系统生成的各种输出，如报表、文件、图像等，以满足用户的需求和展示信息的可读性和易用性。 |
| 输入设计           | 设计系统接受的各种输入方式，如用户界面输入、文件导入等，以确保用户能够方便地输入所需的数据和信息。 |
| 处理过程设计       | 设计系统的处理逻辑和流程，包括数据处理、计算、判断、决策等，以实现系统的各项功能和业务逻辑。 |
| 数据存储设计       | 设计系统的数据存储方式和结构，包括数据库设计、文件存储等，以确保数据的安全性、完整性和可访问性。 |
| 用户界面设计       | 设计系统的用户界面，包括界面的布局、颜色、字体、图标等，以提供良好的用户体验和易用性。 |
| 安全控制设计       | 设计系统的安全机制和控制策略，包括用户认证、权限管理、数据加密等，以保护系统和用户的数据安全。 |

#### :shell:设计方法

| 系统设计方法                    | 描述                                                         |
| :------------------------------ | :----------------------------------------------------------- |
| 面向数据流的结构化设计方法 (SD) | 该方法侧重于系统的数据流动和转换过程，通过分析系统的输入、处理和输出，将系统划分为不同的模块，以实现功能的分解和模块的协作。这种方法注重系统的逻辑结构和数据流程，以达到系统可维护、可测试和可扩展的设计目标。 |
| 面向对象的设计方法 (OOD)        | 该方法以对象为中心，将系统的功能和数据封装成对象，并定义对象之间的关系和交互方式。通过面向对象的思维方式，可以更好地实现系统的模块化和重用性，提高系统的灵活性和可维护性。这种方法注重系统的结构和行为，以达到系统的可重用、可扩展和易于理解的设计目标。 |

#### :shell:基本原理

系统设计基本原理是指在设计系统时应该遵循的一些基本原则和准则，以确保系统具有良好的可扩展性、可维护性和可重用性。

| 基本原理                   | 内容                                                         |
| :------------------------- | :----------------------------------------------------------- |
| 抽象化                     | 将系统分解为多个层次或模块，每个层次或模块负责不同的功能。通过抽象化，可以降低系统的复杂度，提高系统的可理解性和可重用性。 |
| 自顶而下，逐步求精         | 系统设计应该从整体上考虑，从高层次抽象开始，逐步细化系统的设计。这样可以确保系统的各个部分符合整体设计思路，便于系统的管理和维护。 |
| 信息隐蔽                   | 模块间应该通过接口进行通信，而不直接访问对方的内部实现细节。这样可以隐藏模块的内部实现，提高模块的独立性和可重用性。 |
| 模块独立（高内聚、低耦合） | 每个模块应该负责一个明确的功能，模块的功能应该尽可能独立。模块之间应该松散耦合，即模块之间的依赖关系应该尽量减少，使得系统的修改和扩展更加灵活和容易。 |

#### :shell:原则

| 系统设计原则               | 内容                                                         |
| :------------------------- | :----------------------------------------------------------- |
| 保持模块的大小适中         | 模块的大小应该适中，既不过于庞大也不过于微小。庞大的模块难以理解和维护，微小的模块难以复用和管理。 |
| 尽可能减少调用的深度       | 模块之间的调用应该尽可能减少深度，即减少调用链的层次。这样可以提高系统的执行效率和代码的可维护性。 |
| 多扇入、少扇出             | 模块应该有多个调用者，即多扇入。但是模块本身调用其他模块的数量应该尽量减少，即少扇出。这样可以降低模块之间的耦合度，提高模块的独立性和复用性。 |
| 单入口、单出口             | 模块应该只有一个入口和一个出口。这样可以提高模块的可读性和可维护性，减少模块之间的依赖关系。 |
| 模块的作用域应该在模块之内 | 模块的作用域应该尽量限制在模块之内，不影响其他模块的状态和数据。这样可以降低模块之间的耦合度，提高模块的独立性和可维护性。 |
| 功能应该是可预测的         | 模块的功能应该是可预测的，即根据输入产生确定的输出。这样可以提高系统的可靠性和可测试性，减少错误和异常情况的发生。 |

### :cactus:概要设计

#### :shell:设计软件系统总体结构

① 概要设计的基本任务就是软件系统总体结构，是将系统的功能需求分配给软件模块，确定每个模块的功能和调用关系，形成软件的模块结构图，即系统结构图。

② 其基本任务是采用某种设计方法，将一个复杂的系统按功能划分成模块；确定每个模块的功能；确定模块之间的调用关系；确定模块之间的接口，即模块之间传递的信息；评价模块结构的质量。

③ 软件系统总体结构的设计是概要设计关键的一步，直接影响到下一个阶段详细设计与编码的工作。软件系统的质量及一些整体特性都在软件系统总体结构的设计中决定。

#### :shell:数据结构及数据库设计

1、数据结构的设计

逐步细化的方法也适用于数据结构的设计。在需求分析阶段，已经通过数据字典对数据的组成、操作约束和数据之间的关系等方面进行了描述，确定了数据的结构特性，在概要设计阶段要加以细化，详细设计阶段则规定具体的实现细节。在概要设计阶段，宜使用抽象的数据类型。

2、数据库的设计

数据库的设计是指数据存储文件的设计，主要进行以下几方面设计。

* 概念设计。在数据分析的基础上，采用自底向上的方法从用户角度进行视图设计， 一般用E-R 模型来表述数据模型。 E-R 模型既是设计数据库的基础，也是设计数据结构的基础。
* 逻辑设计。E-R 模型是独立于[数据库管理](https://cloud.tencent.com/product/dbbrain?from_column=20065\&from=20065)系统 ( D BMS) 的，要结合具体的 D BMS 特征来建立数据库的逻辑结构。
* 物理设计。对于不同的 D BMS, 物理环境不同，提供的存储结构与存取方法各不相同。物理设计就是设计数据模式的一些物理细节，如数据项存储要求、存取方法和索引的建立等。

#### :shell:编写概要设计文档

文档主要有概要设计说明书、数据库设计说明书、用户手册以及修订测试计划。

`概要设计说明书：`概要设计说明书是在需求分析阶段的基础上，对系统的整体架构和模块进行设计的文档。其中会包括系统的结构、模块的功能和相互关系、所使用的技术和工具等。

`数据库设计说明书：`数据库设计说明书是对系统中所使用的数据库进行设计的文档。其中会包括数据库的结构、表的设计、字段的定义、索引的创建等。

`用户手册：`用户手册是面向系统的最终用户，用于指导用户如何正确使用系统的文档。其中会包括系统的安装步骤、系统的功能介绍、操作指南等。

`修订测试计划：`修订测试计划是在系统开发过程中，对已经实施的测试计划进行修订和更新的文档。其中会包括修订的原因、修订的内容、修订后的测试方法和测试步骤等。

#### :shell:评审

对设计部分是否完整地实现了需求中规定的功能、性能等要求，设计方法的可行性，关键的处理及内外部接口定义的正确性、有效性、各部分之间的一致性等都一一进行评审。

详细设计的基本任务：

1. 对每个模块进行详细的算法设计，使用图形、表格和语言等工具描述每个模块处理过程的详细算法。
2. 设计模块内的数据结构。
3. 进行数据库的物理设计，确定数据库的物理结构。
4. 进行其他设计，根据软件系统的类型，可能还需要进行以下设计： a) 代码设计：优化数据的输入、分类、存储和检索等操作，节约内存空间，对数据库中某些数据项的值进行代码设计。 b) 输入/输出格式设计。 c) 用户界面设计。
5. 编写详细设计说明书。
6. 进行评审，对处理过程的算法和数据库的物理结构进行评审。

系统设计的结果是一系列的系统设计文件，这些文件是实现一个信息系统（包括硬件设备和编制软件程序）的重要基础。

![img](/assets/eaef5f69d8719dea5e0647774ff1666d.Dtmfys0_.png)

## 五、系统测试

系统测试是一种测试方法，用于确定计算机系统或软件是否满足所需的功能和需求。在系统测试中，测试人员会执行一系列测试用例和场景，以验证系统的各个部分和功能是否正常工作。系统测试通常包括功能测试、性能测试、[安全测试](https://cloud.tencent.com/product/sr?from_column=20065\&from=20065)、兼容性测试等。这种测试方法旨在发现系统错误和问题，并为解决这些问题提供反馈和改进建议。系统测试是软件开发生命周期中的一个重要步骤，可以确保系统在投入使用之前是可靠和高质量的。

### :cactus:意义和目的

#### :shell:意义

系统测试的意义在于验证系统是否符合预期的功能需求和性能要求，以及发现系统中的缺陷和风险。系统测试通过模拟真实的使用环境和场景，对系统进行全面的测试和评估，以确保系统能够稳定运行并满足用户的期望。

#### :shell:目的

系统测试的目的：就是希望能以最少的人力和时间发现潜在的各种错误和缺陷。

信息系统测试应包括软件测试、硬件测试和网络测试。硬件测试、网络测试可以根据具体的性能指标进行，此处所说的测试更多的是指软件测试。

软件测试是对软件系统进行验证和验证的过程。它旨在确保软件满足预先确定的需求和规范，并且能够按照预期的方式运行。软件测试的主要目的是发现软件中的错误、缺陷和问题，并提供修复和改进的机会。

软件测试的过程通常包括以下步骤：

1. 需求分析和测试计划：确定软件的需求和测试目标，并制定详细的测试计划。
2. 测试设计：根据测试计划，设计测试用例和测试脚本，用于执行不同的测试场景和情况。
3. 测试执行：根据测试设计，执行测试用例和脚本，记录测试结果和问题。
4. 问题追踪和修复：对发现的问题进行跟踪和管理，通知开发人员进行修复。
5. 测试报告和总结：生成测试报告，记录测试的结果和发现，总结整个测试过程。

在软件测试中，可以使用不同的测试方法和技术，如黑盒测试、白盒测试、灰盒测试、功能测试、性能测试、安全测试等。每种方法都有其特定的目的和适用范围，旨在发现不同类型的错误和问题。

#### :shell:原则

系统测试是保证系统质量和可靠性的关键步骤，是对系统开发过程中的系统分析、系统设计和实施的最后复查。根据测试的概念和目的，在进行信息系统测试时应遵循以下基本原则：

| 原则                                     | 描述                                                         |
| :--------------------------------------- | :----------------------------------------------------------- |
| 尽早进行测试                             | 测试应贯穿在开发的各个阶段，应尽早纠正错误，消除隐患。       |
| 委托专门人员进行测试                     | 测试工作应由专门人员来进行，避免由原开发软件的人或小组承担。 |
| 设计测试方案时要确定预期输出结果         | 在设计测试方案时，不仅要确定输入数据，还要根据系统功能确定预期输出结果。 |
| 设计包含不合理、失效的输入条件的测试用例 | 在设计测试用例时，不仅要设计有效、合理的输入条件，也要包含不合理、失效的输入条件。 |
| 检验程序做了该做的事和不该做的事         | 在测试程序时，不仅要检验程序是否做了该做的事，还要检验程序是否做了不该做的事。 |
| 严格按照测试计划进行测试                 | 严格按照测试计划来进行测试，避免测试的随意性。               |
| 妥善保存测试计划和测试用例               | 妥善保存测试计划、测试用例，作为软件文档的组成部分，为维护提供方便。 |
| 设计可重复使用的测试例子                 | 测试例子都是精心设计出来的，可以为重新测试或追加测试提供方便。 |

### :cactus:测试过程

| 步骤               | 描述                                                         |
| :----------------- | :----------------------------------------------------------- |
| 制订测试计划       | 考虑项目开发时间、进度和人为因素，确定测试内容、进度安排、测试环境和条件、测试培训安排等 |
| 编制测试大纲       | 规定针对系统每项功能或特性必须完成的基本测试项目和测试完成标准 |
| 设计和生成测试用例 | 根据测试大纲，确定被测项目、输入数据、测试过程和预期输出结果 |
| 实施测试           | 将预先编制的测试大纲和测试用例应用于被测软件或设备，进行完整的测试 |
| 生成测试报告       | 对测试进行概要说明，列出测试结论，指出缺陷和错误，并提出修改建议，包括修改方法、工作量和责任人 |

### :cactus:测试策略

#### :shell:单元测试

单元测试（unit testing）是软件开发过程中的一项测试活动，用于验证一个软件系统的最小可测试单元（即单元）是否按照预期功能进行工作。单元测试的目的是对系统的各个独立部分进行测试，以确保其功能正确性。

在软件开发中，一个单元可以是一个函数、一个方法、一个类或一个模块。单元测试通常由开发人员编写，并在代码编写过程中进行。它们是自动化的测试，以确保代码的功能正常运行，并且可以方便地进行重复测试。

单元测试的主要特点是独立性、封闭性和重复性。单元测试应该独立于系统的其他部分，只关注被测试单元的功能。它们应该是封闭的，即不依赖于外部资源或其他单元的状态。而且，单元测试应该可以重复执行，确保测试结果的一致性。

通过编写单元测试，开发人员可以更早地发现和纠正代码中的错误和缺陷。单元测试可以帮助提高代码质量、可维护性和可重复性。它们还能够提供文档化的测试用例，以便将来维护和优化代码时使用。

常用的单元测试框架和工具有JUnit、PyTest、NUnit等。这些工具提供了方便的断言库和测试运行环境，使开发人员可以更容易地编写、运行和管理单元测试。

单元测试依据是软件详细设计说明书。

#### :shell:集成测试

集成测试（Integration testing）是软件测试的一种方法，旨在测试软件系统中各个模块之间的交互和集成。集成测试的目标是**确保不同模块之间的接口正常工作**，并且整个系统在集成后能够正常运行。

集成测试通常在单元测试之后进行，它可以对多个模块的功能进行整体测试，以确保系统在不同模块集成后能够协调工作。集成测试可以模拟实际环境中的各种情况和交互，包括输入和输出数据的正确性、各模块之间的调用关系、数据传递和处理等。

集成测试可以帮助发现模块之间的接口问题、数据传输错误、模块之间的依赖关系问题等，从而提高软件系统的质量和稳定性。它可以帮助发现由于模块集成引起的错误，减少在系统上线后出现的问题。

集成测试可以使用自动化测试工具和手动测试的方法进行。在进行集成测试时，需要先确定测试的范围和测试策略，然后编写测试用例并执行测试，最后对测试结果进行评估和分析。

集成测试目的是检查模块之间，以及模块和已集成的软件之间的接口关系，并验证已集成的软件是否符合设计要求

```csharp
1、自顶向下集成测试。自顶向下集成测试是一种构造软件体系结构的增量方法。
2、自底向上集成测试。自底向上集成测试就是从原子模块(程序结构的最底层构件)开始进行构造和测试。
3、回归测试。回归测试有助于保证变更不引入无意识行为或额外的错误。回归测试可以手工进行，方法 是重新执行所有测试用例的子集，或者利用捕捉/回放工具自动执行。
4、冒烟测试。冒烟测试是一种常用的集成测试方法，是时间关键项目的决定性机制，它让软件团队频繁地对项目进行评估。
```

集成测试依据是软件概要设计文档。

#### :shell:确认测试

确认测试（Confirmation testing）是软件测试中的一种测试技术，用于**确认先前发现的缺陷是否已经修复或解决**。确认测试通常在开发团队修复了一个或多个缺陷后进行，以确保这些缺陷已经被成功修复，且不会引入新的问题。确认测试的目的是验证修复操作的正确性和有效性，以确保软件在修复后能够正常工作。

确认测试主要用于**验证软件的功能、性能和其他特性是否与用户需求一致**。根据用户的参与程度，通常包括以下类型：

1. 内部确认测试：由软件开发组织内部按照SRS进行测试。
2. α测试：由有代表性的最终用户在开发者的场所进行测试，在受控的环境下进行。用户在开发环境下测试软件。
3. β测试：用户在实际使用环境下进行测试，这是软件在不被开发者控制的环境下的真实应用。接收到β测试问题报告后，开发人员会对软件进行修复，并准备向最终用户发布软件产品。
4. 客户验收测试：针对需求规约，在交付前以用户为主进行测试，测试对象为完整的、集成的计算机系统。验收测试的目的是在真实的用户工作环境下，检验软件系统是否满足开发技术合同或SRS。验收测试的结论是用户确定是否接收该软件的主要依据。在进行验收测试之前，需要确认被测软件系统已通过系统测试，并满足一般测试的准入条件。

#### :shell:系统测试

系统测试是**验证完成的软件配置项是否与系统正确连接**，并满足系统/子系统设计文档和软件开发合同规定的要求的一种测试。测试依据是用户需求或开发合同，并包括以下主要内容：

a) 恢复测试：通过各种方式强制系统发生故障，并验证系统能否按要求从故障中恢复，并在约定的时间内开始事务处理，不对系统造成任何伤害。

b) 安全性测试：验证系统内建立的保护机制是否能够实际保护系统免受非法入侵。

c) 压力测试：以非正常的数量、频率或容量等方式对系统进行测试。

d) 性能测试：测试软件在集成环境中的运行性能，可以在测试过程中的任何步骤进行性能测试。

e) 部署测试（也称为配置项测试）：测试对象是软件配置项，测试目的是检验软件配置项与系统需求规范的一致性。在进行该测试之前，应确认被测软件配置项已通过单元测试和集成测试。

### :cactus:测试方法

软件测试方法分为静态测试和动态测试。

#### :shell:静态测试

静态测试是指对程序进行检测的一种方法，不需要在机器上运行被测试程序，而是通过人工检测和计算机辅助静态分析来发现逻辑设计和编码错误。静态测试包括对文档和代码的测试。对文档的静态测试主要以检查单的形式进行，通过人工审查程序或评审软件来检查文档的准确性和完整性。

对代码的静态测试包括以下方式：

1. 人工检测：不依靠计算机，而是通过人工审查程序或评审软件进行检查。这包括代码检查、静态结构分析和代码质量度量等方法。
2. 计算机辅助静态分析：利用静态分析工具对被测试程序进行特性分析，从程序中提取信息，以便检查程序逻辑的各种缺陷和可疑的程序构造。

使用静态测试能够有效地发现程序的逻辑设计和编码错误，估计可以发现30%-70%的错误。通过结合人工检测和计算机辅助静态分析，可以提高测试的效率和准确性。

#### :shell:动态测试

动态测试是指通过运行程序，发现并纠正错误。在对软件产品进行动态测试时，可以使用黑盒测试法和白盒测试法。

黑盒测试也被称为功能测试，是在不考虑软件的内部结构和特性的情况下，测试软件的外部特性。常用的黑盒测试技术包括等价类划分、边界值分析、错误推测和因果图等。

白盒测试也被称为结构测试，根据程序的内部结构和逻辑来设计测试用例，对程序的路径和过程进行测试，以检查是否满足设计需求。白盒测试常用的技术有逻辑覆盖、循环覆盖和基本路径测试。

![img](/assets/51b44632f049108889bc752ea712ecfc.CYucxNZn.png)

### :cactus:黑盒测试

黑盒测试是一种软件测试的方法，其中测试人员只关注测试对象的输入和输出，而不考虑内部的代码和结构。在黑盒测试中，测试人员不了解被测试的软件系统的内部实现细节，而只是根据软件的规格说明书和功能需求来设计测试用例。测试人员通过输入特定的测试数据，观察系统的输出结果，并分析其是否符合预期。黑盒测试主要关注软件的功能、性能和安全等方面的验证，以确保软件在各种情况下都能正确运行。

黑盒测试可以帮助发现软件系统中的错误、缺陷和漏洞等问题，以及确认软件是否按照客户需求和设计规范进行开发。它可以增加软件的可靠性和稳定性，并提高软件的质量和用户体验。

与白盒测试相比，黑盒测试更加注重用户的角度，通过模拟用户的使用场景和操作行为，验证软件系统是否能够正常运行。同时，黑盒测试也可以提供一些优化建议和改进方案，以提升软件系统的性能和安全性。

常用的黑盒测试技术有等价类划分、边界值分析、错误推测和因果图

#### :shell:等价类划分

##### ☀️5.1.1 等价类划分规则

等价类划分是一种测试设计技术，主要用于确定测试用例。

下面是等价类划分的步骤：

1. 识别输入域：首先，确定程序的输入域，也就是程序接受的所有可能输入的范围。
2. 划分等价类：将输入域划分为若干个等价类，每个等价类包含具有相同特征和行为的输入。等价类应该被选取以揭示潜在的错误或异常条件。
3. 选择代表性数据：从每个等价类中选择一个代表性数据作为测试用例。这些代表性数据应该能够有效地检测每个等价类的特征和行为。

举个例子，假设有一个程序接受一个数字作为输入，并根据数字的大小返回不同的结果。输入域可以是所有可能的数字。

等价类划分可以将输入域划分为三个等价类：负数、零和正数。这是因为程序对这三类输入数字的处理方式可能不同。

然后，从每个等价类中选择一个代表性数据作为测试用例。例如，选择-5作为负数的代表性数据，选择0作为零的代表性数据，选择5作为正数的代表性数据。

通过这种方式，我们可以有效地覆盖输入域，同时最大限度地减少重复测试的数量。

##### ☀️5.1.2 等价类划分情况

等价类划分有两种不同的情况：有效等价类和无效等价类。

有效等价类是指具有相同的功能需求和期望输出的测试用例组成的等价类，即这些测试用例应该产生相同的结果。无效等价类是指具有相同的功能需求但期望输出不同的测试用例组成的等价类，即这些测试用例应该产生不同的结果。

在等价类划分中，将输入域划分为若干互不相交的等价类，然后从每个等价类中选择一个测试用例进行测试。这样可以大大减少测试用例的数量，同时保证了测试用例的覆盖率。

在进行等价类划分时，需要考虑以下因素：

1. 有效等价类的划分：将输入域划分为可以产生相同结果的等价类，通常选择一些常见的典型输入，覆盖主要的功能需求。
2. 无效等价类的划分：将输入域划分为可以产生不同结果的等价类，通常选择一些边界值或异常情况的输入，覆盖非法的输入或错误的输入。

例如，对于一个用户登录系统，可以将用户名和密码作为输入，有效等价类可以是正确的用户名和密码组合，无效等价类可以是错误的用户名和密码组合。然后从每个等价类中选择一个测试用例进行测试，例如正确的用户名和密码、错误的用户名和密码等。

##### ☀️5.1.3 等价类划分设计原则

等价类测试用例的设计原则：

* 设计一个新的测试用例，使其尽可能多地覆盖尚未被覆盖的有效等价类， 重复这一步，直到所有的有效等价类都被覆盖为止；
* 设计一个新的测试用例，使其仅覆盖一个尚未被覆盖的无效等价类，重复这一步，直到所有的无效等价类都被覆盖为止 。🦋5.2 边界值分析

边界值分析是一种测试技术，用于选取测试用例的方法。它基于以下观点：在一些情况下，边界上的值和接近边界的值更有可能导致错误。因此，边界值分析旨在选择这些边界和接近边界的值作为测试用例。

具体而言，边界值分析的步骤如下：

1. 确定输入范围：首先，要明确待测程序的输入范围。例如，如果我们需要测试一个接收整数输入的函数，那么输入范围可能是-99到99之间的整数。
2. 确定边界：在输入范围内，边界是指范围的起始点和结束点。在上述的例子中，边界为-99和99。
3. 选择上点：选择边界上的点作为测试用例。在本例中，我们可以选择-99和99作为测试用例。
4. 选择离点：选择接近边界的值作为测试用例。在这种情况下，我们可以选择-100、100、-98、98作为测试用例。
5. 选择内点：选择范围内的值作为测试用例。在本例中，我们可以选择50作为测试用例。

判断输入的数据是否小于-99或者大于99，如果小于-99或大于99给出错误提示

![img](/assets/e18567115c290049611ac29579e54fa2.BG53NloI.png)

#### :shell:错误推测

错误推测：没有固定的方法，凭经验而言，来推测有可能产生问题的地方，作为测试用例进行测试。错误推测法的思想是根据经验列举出可能出现问题的清单，根据清单分享问题可能原因，推测发现缺陷。

适合的场景：

* 时间紧任务量大时，根据之前项目类似经验找出易出错的模块重点测试。

* 时间宽裕通过该方法列出之前出现问题较多的模块再次复测。

#### :shell:因果图

黑盒测试的因果图，是指通过一个结果来反推出导致该结果的原因。这种方法可以帮助测试人员分析系统的功能和逻辑，以确定可能导致问题的潜在原因。

在构建因果图时，可以考虑以下步骤：

1. 确定系统的输出结果：首先需要明确要测试的系统或功能的输出结果是什么。
2. 收集可能的因素：根据系统的特性和功能，收集可能对输出结果产生影响的因素。这些因素可以是输入值、系统配置、环境条件等。
3. 分析因果关系：根据收集到的可能因素，分析它们与输出结果之间的因果关系。考虑每个因素是否可能导致特定的输出结果，或者是否与其他因素存在依赖关系。
4. 构建因果图：根据分析的因果关系，将因素和结果绘制在因果图中。可以使用箭头表示因果关系，指向导致特定结果的因素。
5. 分析结果：通过观察因果图，可以根据输出结果来推测可能导致该结果的原因。这样可以帮助测试人员更有针对性地设计测试用例，以验证系统中可能存在的问题。

因果图的构建过程是根据具体的系统和功能来进行分析和推测的，并没有固定的方法或模板。

### :cactus:白盒测试

白盒测试是一种软件测试方法，其中测试人员具有对被测试软件的内部结构和代码的详细了解。与黑盒测试相比，白盒测试更加关注测试对象的内部逻辑和结构。

白盒测试的目的是验证软件的内部逻辑是否正确，并且最大限度地覆盖测试对象的代码路径。测试人员通常会使用静态分析和动态调试等技术来检查代码的正确性和执行路径的覆盖率。他们还可以在代码级别进行单元测试、集成测试和系统测试。

白盒测试的优点是可以发现代码层面的问题，如逻辑错误、边界条件错误和无效输入等。它还可以提供对程序性能的深入了解，并帮助改善代码质量和可维护性。

白盒测试也有一些限制。首先，测试人员需要具备深入的编程和代码理解能力。其次，白盒测试无法完全模拟真实环境中的所有情况，因此可能无法发现与外部系统和硬件交互相关的问题。

白盒测试常用的技术是逻辑覆盖、循环覆盖和基本路径测试

#### :shell:逻辑覆盖

逻辑覆盖是通过测试数据来检查被测程序对程序逻辑的覆盖程度的方法。主要有六种逻辑覆盖标准：语句覆盖、判定覆盖、条件覆盖、判定/条件覆盖、条件组合覆盖和路径覆盖。

| 覆盖标准      | 描述                                                         |
| :------------ | :----------------------------------------------------------- |
| 语句覆盖      | 选择足够的测试数据，使每条语句至少执行一次。语句覆盖对程序逻辑的覆盖程度较低，被视为较弱的逻辑覆盖。 |
| 判定覆盖      | 设计足够的测试用例，使得每个判定表达式条件的真假分支都要执行一次。判定覆盖也被称为分支覆盖，比语句覆盖更强。 |
| 条件覆盖      | 构造一组测试用例，使每个判定语句中每个逻辑条件的各种可能的值至少满足一次。 |
| 判定/条件覆盖 | 设计足够的测试用例，使得每个判定中每个条件的所有可能取值至少出现一次，并使每个判定本身的结果也至少出现一次。 |
| 条件组合覆盖  | 设计足够的测试用例，使得每个判定中条件的各种可能值的组合都至少出现一次。满足此覆盖的测试用例一定满足判定覆盖、条件覆盖和判定/条件覆盖。 |
| 路径覆盖      | 覆盖被测试程序中的所有可能路径。路径覆盖是最强大和全面的逻辑覆盖标准，能发现更多的错误和潜在问题，但测试用例数量较多，测试工作量较大。 |

#### :shell:循环覆盖

循环覆盖是白盒测试中的一种技术，用于确保被测试的软件中的循环结构被充分执行和覆盖。循环覆盖的目标是测试循环中的所有可能情况，包括循环条件为真、为假以及循环体被执行的不同次数等。

循环覆盖的基本原则是通过设计测试用例来驱动循环执行，以便覆盖循环的各个可能路径。具体的循环覆盖策略可以分为以下几种：

| 测试策略     | 目标                                     | 测试用例示例                                                 |
| :----------- | :--------------------------------------- | :----------------------------------------------------------- |
| 简单循环覆盖 | 保证循环至少被执行一次和至少不被执行一次 | 循环条件为真的情况下执行一次 2. 循环条件为假的情况下不执行   |
| 边界循环覆盖 | 关注循环的边界情况                       | 循环次数为最小值的情况下执行 2. 循环次数为最大值的情况下执行 3. 循环次数为中间值的情况下执行 |
| 全循环覆盖   | 覆盖循环的所有可能路径                   | 循环条件为真的情况下执行一次 2. 循环条件为假的情况下不执行 3. 循环体被执行0次 4. 循环体被执行1次 5. 循环体被执行多次 |

在进行循环覆盖时，需要结合其他白盒测试技术，如路径覆盖、条件覆盖和分支覆盖，来确保循环中的各个分支和条件也得到充分测试。循环覆盖的目的是找出可能存在的循环错误和效率问题。

#### :shell:基本路径测试

基本路径测试是白盒测试中的一种测试技术，旨在检查程序中所有可能的路径。它基于控制流图，通过选择测试用例来覆盖控制流图中的所有基本路径。

控制流图是一个图形化表示程序控制流程的图，其中节点表示程序的基本块（即一组连续的语句），边表示控制流转的可能路径。

基本路径是控制流图中的一条路径，从一个节点到另一个节点，期间经过的所有边都只经过一次。

基本路径测试的目标是选择测试用例来覆盖控制流图中的所有基本路径，以确保程序的所有路径都被测试到，并尽可能地发现潜在的错误。

基本路径测试的步骤如下：

| 步骤         | 目标                                         | 示例                                                         |
| :----------- | :------------------------------------------- | :----------------------------------------------------------- |
| 绘制控制流图 | 根据源代码绘制控制流图                       | 控制流图包含多个基本块，每个基本块标记为一个节点，用边连接各个基本块 |
| 确定基本路径 | 从控制流图中确定所有可能的基本路径           | 从起始节点到结束节点的路径 2. 经过特定条件节点的路径         |
| 选择测试用例 | 选择一组测试用例，以覆盖所有基本路径         | 选择测试用例来覆盖从起始节点到结束节点的路径 2. 选择测试用例来覆盖经过特定条件节点的路径 |
| 执行测试用例 | 根据选择的测试用例，执行测试，并记录测试结果 | 执行测试用例来验证从起始节点到结束节点的路径 2. 执行测试用例来验证经过特定条件节点的路径 |
| 分析结果     | 分析测试结果，检查程序的行为和潜在错误       | 检查程序是否按照预期路径执行 2. 检查是否存在潜在的错误       |

基本路径测试是一种比较全面的测试技术，可以有效地发现程序中的错误。它也有一些限制，比如在复杂的程序中，基本路径的数量可能很大，难以覆盖所有的基本路径。基本路径测试仅关注程序的控制流程，对于数据流和其他方面的问题可能无法完全覆盖。

![img](/assets/c6d89694beaa09fb5f8a041ecc87747a.BeYfL5CC.png)

![img](/assets/61578aafd9c901daff872929e272310a.FxVnopT7.png)

### :cactus:调试

测试是发现错误，调试是根据测试时所发现的错误找出原因和具体的位置，进行改正。

调试需要确定错误的准确位置，确定问题的原因并设法改正；改正后要进行回归测试。

调试的方法有：试探法、回溯法、原因排除法（对分查找法、归纳法、演绎法）。

| 方法       | 描述                                                         | 适用情况                               |
| :--------- | :----------------------------------------------------------- | :------------------------------------- |
| 试探法     | 分析错误症状，猜测问题位置，通过设置输出语句、分析寄存器和存储器等手段逐步试探和分析错误所在 | 结构比较简单的程序                     |
| 回溯法     | 从错误症状的位置开始，沿着程序的控制流程往回跟踪代码，直到找到错误根源 | 小型程序，大规模程序的路径较多时不适用 |
| 对分查找法 | 缩小错误的范围，通过逐步的二分查找来定位问题的位置           |                                        |
| 归纳法     | 从测试所暴露的问题出发，收集正确和不正确的数据，分析它们之间的关系，提出错误原因的假设，用数据证实或反驳来查出错误所在 |                                        |
| 演绎法     | 根据测试结果列出所有可能的错误原因，分析已有的数据排除不可能和矛盾的原因，选择可能性最大的假设来解释测试结果 |                                        |

### :cactus:软件度量

软件的两种属性：外部属性指面向管理者和用户的属性，可直接测量，一般为性能指标。内部属性指软件产品本身的属性，如可靠性，只能间接测量。

McCabe度量法：又称为环路复杂度，假设有向图中有向边数为m，节点数为n，则此有向图的环路复杂度为m-n+2p

注意m和n点的含义不能混淆，可以用一个最简单的环路来做特殊值记忆此公式，另外，针对一个程序流程图，每一个分支变（连线）就有一条有向边，每一条语句（语句框）就是一个顶点。

![img](/assets/8c2160ea8f3448d02f8781e6fccb709d.D71Lse3j.png)

## 六、运行与维护知识

运行软件的过程涉及将软件安装在用户的计算机、服务器或移动设备上，并确保其正常运行。这可能涉及到设置和配置软件，以适应特定的硬件和操作系统环境。运行软件还包括监控软件运行的性能和稳定性，以确保它能够满足用户需求。

维护软件的过程涉及对软件进行修复和改进，以纠正已经发现的错误或缺陷，并进行功能扩展或性能优化。维护工作可能包括错误修复、安全补丁、性能优化、软件更新等。这些维护工作可以通过与用户的反馈和需求进行交流来确定和优先处理。

运行和维护是一个持续的过程。软件开发人员需要密切关注用户的反馈和需求，以及技术的变化和创新。通过持续的运行和维护工作，软件可以持续地满足用户的需求，并保持与技术环境的兼容性。

### :cactus:系统转换

系统转换是指：新系统开发完毕，投入运行，取代现有系统的过程，需要考虑多方面的问题，以实现与老系统的交接，有一下三种转换计划：

| 转换计划       | 描述                                                         |
| :------------- | :----------------------------------------------------------- |
| 直接转换       | 在确定新系统运行无误时立刻启用新系统，终止旧系统运行。风险较大，但节省人员和设备费用。适用于处理过程不复杂、数据不重要的场合。 |
| 并行转换       | 新旧系统并行工作一段时间，经过考验后正式替代旧系统。风险较小，安全可靠，但费用和工作量较大。适用于复杂的大型系统，提供了与旧系统比较的机会，公正评价新旧系统的时间要求、出错次数和工作效率。消除了对新系统不安的感觉。 |
| 分段转换       | 分期分批逐步转换，结合了直接和并行转换的特点。将大型系统分为多个子系统，逐个试运行并成熟后转换。适用于大型项目，耗时较长，需要协调好接口等问题。现有系统和新系统间混合使用。 |
| 数据转换与迁移 | 将数据从旧数据库迁移到新数据库中。有三种方法：1. 系统切换前通过工具迁移；2. 系统切换前采用手工录入；3. 系统切换后通过新系统生成。 |

### :cactus:系统维护

#### :shell:系统的可维护性

系统的可维护性可以定义为维护人员理解、改正、改动和改进这个软件的难易程度，可维护性是所有软件都应具有的基本特点，必须在开发阶段和其他软件工程阶段保证软件具有可维护的特点。其评价指标如下：

| 评价指标 | 描述                                                         |
| :------- | :----------------------------------------------------------- |
| 可理解性 | 系统的结构、界面、功能和内部过程的难易程度，模块化、详细设计文档、结构化设计和良好的高级程序设计语言等有助于提高可理解性。 |
| 可测试性 | 诊断和测试的容易程度，取决于易理解的程度。开发人员应尽力将程序设计成易诊断和测试的，充分利用系统测试阶段保存的测试用例。 |
| 可修改性 | 诊断和测试的容易程度与系统设计的设计原则有关。模块的耦合、内聚、作用范围与控制范围的关系对可修改性会产生影响。 |

#### :shell:软件的可维护性

系统维护主要包括硬件维护、软件维护和数据维护。

| 维护类型   | 描述                                                         |
| :--------- | :----------------------------------------------------------- |
| 正确性维护 | 指改正在系统开发阶段已发生但在系统测试阶段尚未发现的错误。   |
| 适应性维护 | 指使应用软件适应信息技术变化和管理需求变化的修改。           |
| 完善性维护 | 对已有的软件系统增加一些在系统分析和设计阶段中没有规定的功能与性能特征。 |
| 预防性维护 | 为了改进应用软件的可靠性和可维护性，主动增加预防性的新功能，使应用系统适应各类变化而不被淘汰。 |

### :cactus:系统评价

#### :shell:系统评价分类

按评价的时间与信息系统所处的阶段的关系又可从总体上把广义的信息系统评价分成立项评价、中期评价和结项评价。

| 阶段                           | 评价类型 | 描述                                                         |
| :----------------------------- | :------- | :----------------------------------------------------------- |
| 立项评价                       | 预评价   | 在信息系统开发之前进行的评价，用于系统规划阶段中的可行性研究。 |
| 中期评价                       | 阶段评审 | 在项目开发的中期阶段进行的阶段评审，或在项目开发中遇到重大变故时评估是否继续开发。 |
| 结项评价                       | 综合评价 | 在系统投入正式运行后，对系统进行综合评价，判断系统是否达到预期目的和要求。 |
| 时间与信息系统所处的阶段的关系 | 参考     | 评价类型是根据信息系统所处的阶段和时间确定的，可以从总体上将广义的信息系统评价分为立项评价、中期评价和结项评价。 |

#### :shell:系统评价指标

1、从信息系统的组成部分出发，信息系统是一个由人机共同组成的系统，所以可以按照运行效果和用户需求(人)、系统质量和技术条件(机)这两条线索构造指标。

2、从信息系统的评价对象出发，对于开发方来说，他们所关心的是系统质量和技术水平； 对于用户方而言，关心的是用户需求和运行质量；系统外部环境则主要通过社会效益指标来反映。

3、从经济学角度出发，分别按系统成本、系统效益和财务指标3条线索建立指标。

![img](/assets/d178d3b2fe58e2db134514e8234defac.DN87Eyf2.png)

## 七、软件项目管理

## 八、软件质量

软件质量是指软件在满足用户需求的同时，具备一定的可靠性、可维护性、可测试性等特性。而软件度量是指通过对软件产物进行度量，来评估和衡量软件质量的一种方法。下面将分别介绍软件质量和软件度量的一些重要概念和方法。

1. 软件质量属性 软件质量属性是衡量软件质量的依据，包括以下几个方面：
2. 功能性：软件能否满足用户需求；
3. 可靠性：软件在给定环境下的稳定性和可靠性；
4. 易维护性：软件是否容易进行修改和扩展；
5. 可测试性：软件是否易于进行测试；
6. 可移植性：软件是否可以在不同环境中运行。
7. 软件度量方法

### :cactus:概念

软件质量是指软件系统或软件产品满足规定或隐含需求能力的特征和特性的总体表现。它反映了软件的可靠性、可用性、安全性、性能、可维护性和可扩展性等方面的表现。

软件质量管理是对软件开发过程进行独立的检查活动，旨在确保软件达到预期的质量标准。它涵盖了质量保证、质量规划和质量控制三个主要活动。质量保证包括制定质量策略、制定质量标准与规范、制定质量计划以及进行质量评审和审核等。质量规划包括确定质量目标、计划质量活动和资源以及建立质量保证体系等。质量控制包括执行质量活动、监控软件开发过程和产品质量、进行质量评估和改进等。

软件质量保证是为了确保软件系统或软件产品充分满足用户要求的质量而进行的有计划、有组织的活动。它包括识别和管理风险、执行质量活动、实施质量评估、建立质量测量和指标、持续改进等。其目的是生产高质量、稳定可靠的软件，提供优质的用户体验和满足用户需求的软件产品。

### :cactus:特性

讨论软件质量首先要了解软件的质量特性，目前已经有多种软件质量模型来描述软件质量特性，例如ISO/EC9126 软件质量模型和 McCall软件质量模型

#### :shell:ISO/EC9126 软件质量模型

ISO/IEC 9126是一个软件质量模型，旨在帮助组织评估和改进软件产品的质量。该模型定义了六个主要的软件质量特性，每个特性又包含一些子特性。

ISO/IEC 9126软件质量模型提供了一种综合和结构化的方法来评估软件质量，并可用于指导软件开发组织改进其软件产品的质量。该模型的使用可以帮助组织识别和解决软件质量问题，从而提高软件产品的可靠性、性能和用户满意度。

![img](/assets/611ca9ccf06bace65e6738fd31c7011e.CT6beR6I.png)

##### ☀️2.1.1 功能性(Functionality)

与一组功能及其指定的性质的存在有关的一组属性，功能是指满足规定或隐含需求的那些功能

| 软件属性 | 定义                                           |
| :------- | :--------------------------------------------- |
| 适应性   | 能否提供一组功能以及这组功能是否适合特定任务   |
| 准确性   | 能够得到正确或相符的结果或效果                 |
| 互用性   | 能够与其他指定系统进行交互操作                 |
| 依从性   | 能够使软件服从有关的标准、约定、法规及类似规定 |
| 安全性   | 能够避免对程序及数据的非授权故意或意外访问     |

##### ☀️2.1.2 可靠性(Reliability)

与在规定的一段时间内和规定的条件下软件维持在其性能水平有关的能力

| 软件属性 | 描述                                                         |
| :------- | :----------------------------------------------------------- |
| 成熟性   | 与由软件故障引起失效的频度有关的软件属性                     |
| 容错性   | 与在软件错误或违反指定接口的情况下维持指定的性能水平的能力有关的软件属性 |
| 易恢复性 | 与在故障发生后，重新建立其性能水平并恢复直接受影响数据的能力，以及为达到此目的所需的时间和努力有关的软件属性 |

##### ☀️2.1.3 易使用性(Usability)

与为使用所需的努力和由一组规定或隐含的用户对这样使用所做的个别评价有关的一组属性

| 属性     | 描述                                                         |
| :------- | :----------------------------------------------------------- |
| 易理解性 | 与用户为理解逻辑概念及其应用所付出的劳动有关的软件属性       |
| 易学性   | 与用户为学习其应用（例如操作控制、输入、输出）所付出的努力相关的软件属性 |
| 易操作性 | 与用户为进行操作和操作控制所付出的努力有关的软件属性         |

##### ☀️2.1.4 效率(Efficiency)

在规定条件下，与软件的性能水平与所用资源量之间的关系有关的软件属性

| 属性类型          | 时间特性                                                   | 资源特性                                                     |
| :---------------- | :--------------------------------------------------------- | :----------------------------------------------------------- |
| 相关说明          | 与响应和处理时间以及软件执行功能时的吞吐量有关的软件属性。 | 与软件执行功能时所使用的资源量和使用资源的持续时间有关的软件属性。 |
| 相关软件属性      | 响应时间、处理时间、吞吐量                                 | 资源使用量、资源持续时间                                     |
| 相关软件特征      | 软件的快速性和反应能力                                     | 软件的资源效率和资源管理能力                                 |
| 相关影响/问题     | 较长的响应时间和处理时间可能导致用户不满意                 | 资源消耗过大可能导致系统崩溃或性能下降                       |
| 相关解决方案/优化 | 优化算法和数据结构、增加硬件资源                           | 资源管理策略和优化、调整系统配置                             |

##### ☀️2.1.5 可维护性(Maintainability)

与进行规定的修改所需要的努力有关的一组属性

| 属性     | 描述                                                         |
| :------- | :----------------------------------------------------------- |
| 易分析性 | 与诊断缺陷或失效原因，或为判定待修改的部分所需努力有关的软件属性 |
| 易改变性 | 与进行修改、排错或适应环境变换所需努力有关的软件属性         |
| 稳定性   | 与修改造成未预料效果的风险有关的软件属性                     |
| 易测试性 | 为确认经修改软件所需努力有关的软件属性                       |

##### ☀️2.1.6 可移植性(Portability)

与软件可从某一环境转移到另一环境的能力有关的一组属性

| 软件属性 | 定义                                                         |
| :------- | :----------------------------------------------------------- |
| 适应性   | 与软件转移到不同环境时的处理或手段有关的软件属性             |
| 易安装性 | 与在指定环境下安装软件所需努力有关的软件属性                 |
| 一致性   | 使软件服从与可移植性有关的标准或约定的软件属性               |
| 易替换性 | 与一软件在该软件环境中用来替代指定的其他软件的可能和努力有关的软件属性 |

![img](/assets/404f572d078b58e8ee80259675defaf8.JqqEXSzW.png)

#### :shell:Mc Call质量模型

McCall质量模型是一种用于评估软件质量的模型，于1977年由McCall等人提出。该模型将从软件产品的运行、修正和转移3个方面确定了11个质量特性，并为每个因素定义了指标，通过对这些指标进行评估，可以对软件的质量进行综合评价。

这11个因素包括：

| 质量因素   | 描述                                                   |
| :--------- | :----------------------------------------------------- |
| 基本操作性 | 测试软件的基本功能是否正常运行                         |
| 准确性     | 软件输出的准确性，是否符合用户的预期                   |
| 可靠性     | 软件在给定条件下的可靠性，是否能够持续稳定运行         |
| 效率       | 软件在给定条件下的执行效率，包括响应时间、资源利用率等 |
| 可维护性   | 软件的易维护性，包括易理解、易修改、易测试等           |
| 灵活性     | 软件的适应性，是否能够满足用户的不同需求               |
| 可测试性   | 软件的易测试性，是否容易进行测试和验证                 |
| 可理解性   | 软件的易理解性，包括文档、注释、命名等                 |
| 可用性     | 软件的易用性，是否容易上手和操作                       |
| 可移植性   | 软件的可移植性，是否能够在不同的环境和平台上运行       |
| 可互操作性 | 软件的互操作性，是否能够与其他软件和系统进行交互       |

通过对这些因素进行评估，可以得出软件的质量评分，从而指导软件开发和维护的工作。McCall质量模型提供了一个系统化的评估方法，可以帮助开发团队和项目经理在软件开发过程中关注和优化不同的质量因素。

![img](/assets/7b2c075d988a57a0a5ba31c4c5145f82.IxUdkNkS.png)

### :cactus:软件质量保证

软件质量保证是指为保证软件系统或软件产品充分满足用户要求的质量而进行的有计划、有组织的活动，其目的是生产高质量的软件。主要包含3个要点和7个任务。

3个要点：

| 要点         | 描述                                                         |
| :----------- | :----------------------------------------------------------- |
| 用户需求满足 | 软件必须满足用户需求，与用户需求不一致的软件无质量可言。     |
| 开发标准遵循 | 软件应遵循规定的一系列开发标准，不遵循这些准则的软件，其质量难以得到保证。 |
| 隐含需求满足 | 软件还应满足某些隐含的需求（如可理解性、可维护性等），未明确写在用户需求中。 |

7个任务：

| 任务           | 描述                                 |
| :------------- | :----------------------------------- |
| 应用技术方法   | 把软件质量设计到产品中而非事后保证。 |
| 正式的技术评审 | 进行正式的技术评审。                 |
| 测试软件       | 进行软件测试。                       |
| 标准的实施     | 遵循标准进行软件开发。               |
| 控制变更       | 控制软件变更。                       |
| 度量           | 收集软件度量。                       |
| 记录保存和报告 | 记录保存和报告软件质量相关信息。     |

### :cactus:软件评审

软件评审：是指对软件开发过程中的文档、设计、代码、测试用例等进行系统性的检查和审查的过程。评审的目的是发现潜在的问题、错误和改进的机会，以提高软件质量和有效性。

通常，把“质量”理解为“用户满意程度”。为了使得用户满意，有以下两个必要条件。

(1)设计的规格说明书符合用户的要求，这称为设计质量。

(2)程序按照设计规格说明所规定的情况正确执行，这称为程序质量。

软件的评审主要包含以下两种评审：

* 设计质量的评审：设计质量评审的对象是在需求分析阶段产生的软件需求规格说明、数据需求规格说明，以及在软件概要设计阶段产生的软件概要设计说明书等
* 程序质量的评审：程序质量评审通常是从开发者的角度进行评审，与开发技术直接相关。它是着眼于软件本身的结构、与运行环境的接口以及变更带来的影响而进行的评审活动

### :cactus:软件容错技术

提高软件质量和可靠性的技术大致可分为两类，一类是避开错误，即在开发的过程中不让差错潜入软件的技术；另一类是容错技术，即对某些无法避开的差错，使其影响减至最小的技术。

软件容错技术：容错就是软件遇到错误的处理能力，实现容错的手段主要是冗余，冗余是指对于实现系统规定功能是多余的那部分资源，包括硬件、软件、信息和时间。由于加入了这些资源，有可能使系统的可靠性得到较大的提高，包括四种冗余技术：结构冗余、信息冗余、时间冗余、冗余附加技术

#### :shell:结构冗余

结构冗余是通过在系统中添加额外的硬件或软件组件来提高系统的可靠性和容错能力，这种冗余可以分为静态、动态和混合冗余三种类型：

| 冗余类型 | 定义                                                         | 示例                       |
| :------- | :----------------------------------------------------------- | :------------------------- |
| 静态冗余 | 在系统中添加多个相同的组件，通过表决和比较来选择正确的输出   | 航空航天领域的多发动机系统 |
| 动态冗余 | 在系统中添加多个相同的组件，但只有一个组件在工作，其他处于待机状态 | 服务器领域的 RAID 技术     |
| 混合冗余 | 将静态和动态冗余技术结合起来使用，提高系统的可靠性和容错能力 | 核电站中的反应堆控制系统   |

#### :shell:信息冗余

信息冗余是通过在数据中添加额外的信息来提高数据的检错和纠错能力。这种冗余通常采用校验码原理，即通过对数据进行某种运算来生成校验码，并将校验码附加到数据中。当数据传输或存储时，如果校验码与数据不匹配，则说明数据可能发生了错误。

校验码原理：校验码原理是指通过对数据进行某种运算来生成校验码，并将校验码附加到数据中。常见的校验码包括循环冗余校验码（CRC）、汉明码等。例如，在 USB 接口中，数据传输时会使用 CRC 校验码来检测数据是否发生了错误。

#### :shell:时间冗余

时间冗余是通过在系统中添加额外的时间延迟来提高系统的容错能力。这种冗余通常采用重复执行的方式，即当系统出现错误时，会重复执行相同的操作，直到操作成功为止。如果重复执行多次仍然失败，则说明系统可能出现了严重的故障。

| 名词     | 定义                                                         | 示例                                                         |
| :------- | :----------------------------------------------------------- | :----------------------------------------------------------- |
| 重复执行 | 当系统出现错误时，重复执行相同的操作，直到操作成功为止。     | 在计算机系统中，当硬盘读取数据时出现错误时，操作系统会尝试多次读取数据，直到读取成功为止。 |
| 回滚     | 当系统出现错误时，将系统状态恢复到之前的某个时间点，以避免错误的影响。 | 在数据库系统中，如果某个事务执行失败，可以使用回滚操作将数据库状态恢复到事务开始之前的状态。 |

#### :shell:冗余附加技术

冗余附加技术是指为实现结构、信息和时间冗余技术所需的资源和技术，包括程序、指令、数据、存放和调动它们的空间和通道等。

## 九、软件度量

软件度量是通过对软件产物进行定量的测量和评估，来衡量软件质量的方法。常用的软件度量方法包括：

1. 功能点分析（Function Point Analysis，FPA）：通过对软件的功能进行统计，来评估软件规模和复杂度；
2. 代码行数度量：通过对软件代码行数进行统计，来评估软件大小和复杂度；
3. 缺陷密度度量：通过对软件缺陷的数量和严重程度进行统计，来评估软件的质量；
4. 可靠性度量：通过对软件的故障率和可用性进行统计，来评估软件的可靠性。

### :cactus:属性

软件度量是用于对软件产品及其开发过程进行度量的一种方法。软件可以具有两种属性，即外部属性和内部属性。

外部属性是指面向管理者和用户的属性，可以直接进行测量。一般来说，外部属性主要关注软件的性能指标，例如成本、效益、以及开发人员的生产率等。

内部属性是指软件产品本身的属性，只能通过间接的方式进行测量。内部属性主要关注软件的可靠性、可维护性等方面的特性。这些属性无法直接测量，但可以通过对软件的设计、开发和测试等过程进行分析和评估来间接获取相关信息。

### :cactus:分类方法

软件度量可以按两种分类方法进行划分：

1. 面向规模的度量、面向功能的度量和面向人的度量。
2. 生产率度量、质量度量和技术度量。🔎3.复杂度的度量方法

软件复杂度的度量方法：McCabe度量法：又称为环路复杂度，假设有向图中有向边数为m,节点数为n,则此有向图的环路复杂度为m-n+2

注意m和n代表的含义不能混淆，可以用一个最简单的环路来做特殊值记忆此公式，另外，针对一个程序流程图，每一个分支边（连线）就是一条有向边，每一条语句(语句框)就是一个顶点。

还有一种更加简单的算法：封闭空间数量+1

![img](/assets/98ce2f7c7c2704da32c990d7548f8386.DFlzbHlc.png)

![img](/assets/d3eb59326b80d1484a402b245bae14fd.C6pjcU1l.png)

![img](/assets/725aeebd64d88c347efbf55026e34efd.BVwWy1zs.png)

## 十、软件工具与软件开发环境

---

---
url: /daily/软件设计师/00_软件设计师考点.md
---

# 软件设计师考点

### 一、计算机系统概论

计算机硬件基本组成

中央处理单元CPU

数据表示

校验码

指令系统

存储系统

输入输出技术

总线

加密技术

认证技术

计算机可靠性

### 二、程序设计语言基础

程序设计语言基本概念

程序设计语言基本成分

编译程序基本原理

### 三、操作系统

操作系统特征与功能

操作系统分类

进程的通信

死锁

存储管理方案

设备管理技术

磁盘调度

文件结构

文件目录

文件存储空间管理

### 四、网络与信息安全

网络分类

网络体系结构

物理层互联设备

数据链路层互联设备

网络层互联设备

应用层协议

传输层协议

网络层协议

防火墙技术

### 五、数据库基础

数据库的三级模式

基本概念

ER模型

关系模型

关系的相关名称

完整性约束

专门的关系运算

函数依赖

规范化

模式分解

事务管理

需求分析

逻辑设计

### 六、软件工程

软件生存周期

软件开发模型

敏捷方法

系统设计

系统测试与运行维护

项目管理

软件质量

软件度量

概述

结构化分析

结构化设计

面向对象基本概念

面向对象分析

面向对象设计

UML

设计模式

### 七、数据结构与算法

线性表

栈

队列

二维数组

矩阵

树

图

查找算法

排序算法

基本概念

算法的基础

### 八、知识产权

著作权

专利权法实施细则

商标法

### 九、案例分析

### 十、专业英语

放弃

### 学习资料

希赛网视频：https://www.bilibili.com/video/BV13U4y1E7oA

https://blog.csdn.net/chengsw1993/article/details/125040967

https://bbs.huaweicloud.com/community/usersnew/id\_1628242069124339

https://cloud.tencent.com/developer/column/101804

历年真题

<https://ebook.qicoder.com/软件设计师>

---

---
url: /daily/软件设计师/12_软件系统分析与设计.md
---

# 软件系统分析与设计

---

---
url: /daily/开发文档/扫码登录.md
---

# 扫码登录

流程：https://blog.csdn.net/csdnsevenn/article/details/105383127

https://blog.csdn.net/hehe\_www2012/article/details/116855457

https://blog.csdn.net/xiaoliangtx/article/details/117162566

原理

https://www.cnblogs.com/maolan/p/6015431.html

简单示例

https://www.freesion.com/article/93501295470/

https://www.bilibili.com/video/BV1WC4y1s7wR

http://www.lmk.limuke.top/2020/04/13/qrcode-demo/

js生成

https://blog.csdn.net/gentlu/article/details/78592571

带源码

https://blog.csdn.net/Tracker\_/article/details/98851335

---

---
url: /Java/解决方案/支付解决方案/支付宝支付API对接指南/1_商家基本信息获取.md
---

# 商家基本信息获取

相对于微信支付，支付宝支付要简单许多，不得不说阿里家产品的文档做的都很棒，首先[登录](https://www.alipay.com/)进入支付宝开放平台，然后进入[控制台](https://open.alipay.com/develop/manage)获取appid及公私钥。

参考资料

<https://www.cnblogs.com/DevinZhang1990/p/12876532.html>

<https://blog.csdn.net/JinglongSource/article/details/114194485>

---

---
url: /Java/设计模式/01.基本概念/设计模式.md
---

# 设计模式

24大设计模式和7个原则

## 7个原则单一职责原则

【SINGLE RESPONSIBILITY PRINCIPLE】: 一个类负责一项职责。里氏替换原则

【LISKOV SUBSTITUTION PRINCIPLE】: 继承与派生的规则。依赖倒置原则

【DEPENDENCE INVERSION PRINCIPLE】: 高层模块不应该依赖低层模块，二者都应该依赖其抽象；抽象不应该依赖细节；细节应该依赖抽象。即针对接口编程，不要针对实现编程。接口隔离原则

【INTERFACE SEGREGATION PRINCIPLE】: 建立单一接口，不要建立庞大臃肿的接口，尽量细化接口，接口中的方法尽量少。迪米特法则

【LOW OF DEMETER】: 低耦合，高内聚。开闭原则

【OPEN CLOSE PRINCIPLE】: 一个软件实体如类、模块和函数应该对扩展开放，对修改关闭。组合/聚合复用原则

【Composition/Aggregation Reuse Principle(CARP) 】: 尽量使用组合和聚合少使用继承的关系来达到复用的原则。

## 24大设计模式

> **第一步：创建型设计模式**

* 创建型 - 单例模式(Singleton pattern)
  * 单例模式(Singleton pattern): 确保一个类只有一个实例，并提供该实例的全局访问点, 本文介绍6中常用的实现方式
* 创建型 - 简单工厂(Simple Factory)
  * 简单工厂(Simple Factory)，它把实例化的操作单独放到一个类中，这个类就成为简单工厂类，让简单工厂类来决定应该用哪个具体子类来实例化，这样做能把客户类和具体子类的实现解耦，客户类不再需要知道有哪些子类以及应当实例化哪个子类
* 创建型 - 工厂方法(Factory Method)
  * 工厂方法(Factory Method)，它定义了一个创建对象的接口，但由子类决定要实例化哪个类。工厂方法把实例化操作推迟到子类
* 创建型 - 抽象工厂(Abstract Factory)
  * 抽象工厂(Abstract Factory)，抽象工厂模式创建的是对象家族，也就是很多对象而不是一个对象，并且这些对象是相关的，也就是说必须一起创建出来。而工厂方法模式只是用于创建一个对象，这和抽象工厂模式有很大不同
* 创建型 - 生成器(Builder)
  * 生成器(Builder)，封装一个对象的构造过程，并允许按步骤构造
* 创建型 - 原型模式(Prototype)
  * 原型模式(Prototype)，使用原型实例指定要创建对象的类型，通过复制这个原型来创建新对象

> **第二步：结构型设计模式**

* 结构型 - 外观(Facade)
  * 外观模式(Facade pattern)，它提供了一个统一的接口，用来访问子系统中的一群接口，从而让子系统更容易使用
* 结构型 - 适配器(Adapter)
  * 适配器模式(Adapter pattern): 将一个类的接口, 转换成客户期望的另一个接口。 适配器让原本接口不兼容的类可以合作无间。 对象适配器使用组合, 类适配器使用多重继承
* 结构型 - 桥接(Bridge)
  * 桥接模式(Bridge pattern): 使用桥接模式通过将实现和抽象放在两个不同的类层次中而使它们可以独立改变
* 结构型 - 组合(Composite)
  * 组合模式(composite pattern): 允许你将对象组合成树形结构来表现"整体/部分"层次结构. 组合能让客户以一致的方式处理个别对象以及对象组合
* 结构型 - 装饰(Decorator)
  * 装饰者模式(decorator pattern): 动态地将责任附加到对象上, 若要扩展功能, 装饰者提供了比继承更有弹性的替代方案
* 结构型 - 享元(Flyweight)
  * 享元模式(Flyweight Pattern): 利用共享的方式来支持大量细粒度的对象，这些对象一部分内部状态是相同的。 它让某个类的一个实例能用来提供许多"虚拟实例"
* 结构型 - 代理(Proxy)
  * 代理模式(Proxy pattern): 为另一个对象提供一个替身或占位符以控制对这个对象的访问

> **第三步：行为型设计模式**

* 行为型 - 责任链(Chain Of Responsibility)
  * 责任链模式(Chain of responsibility pattern): 通过责任链模式, 你可以为某个请求创建一个对象链. 每个对象依序检查此请求并对其进行处理或者将它传给链中的下一个对象
* 行为型 - 策略(Strategy)
  * 策略模式(strategy pattern): 定义了算法族, 分别封闭起来, 让它们之间可以互相替换, 此模式让算法的变化独立于使用算法的客户
* 行为型 - 模板方法(Template Method)
  * 模板方法模式(Template pattern): 在一个方法中定义一个算法的骨架, 而将一些步骤延迟到子类中. 模板方法使得子类可以在不改变算法结构的情况下, 重新定义算法中的某些步骤
* 行为型 - 命令模式(Command)
  * 命令模式(Command pattern): 将"请求"封闭成对象, 以便使用不同的请求,队列或者日志来参数化其他对象. 命令模式也支持可撤销的操作
* 行为型 - 观察者(Observer)
  * 观察者模式(observer pattern): 在对象之间定义一对多的依赖, 这样一来, 当一个对象改变状态, 依赖它的对象都会收到通知, 并自动更新
* 行为型 - 访问者(Visitor)
  * 访问者模式(visitor pattern): 当你想要为一个对象的组合增加新的能力, 且封装并不重要时, 就使用访问者模式
* 行为型 - 状态(State)
  * 状态模式(State pattern): 允许对象在内部状态改变时改变它的行为, 对象看起来好象改了它的类
* 行为型 - 解释器(Interpreter)
  * 解释器模式(Interpreter pattern): 使用解释器模式为语言创建解释器，通常由语言的语法和语法分析来定义
* 行为型 - 迭代器(Iterator)
  * 迭代器模式(iterator pattern): 提供一种方法顺序访问一个聚合对象中的各个元素, 而又不暴露其内部的表示
* 行为型 - 中介者(Mediator)
  * 中介者模式(Mediator pattern) : 使用中介者模式来集中相关对象之间复杂的沟通和控制方式
* 行为型 - 备忘录(Memento)
  * 备忘录模式(Memento pattern): 当你需要让对象返回之前的状态时(例如, 你的用户请求"撤销"), 你使用备忘录模式

---

---
url: /Java/系统优化/性能优化/2_实时通讯方案.md
---

# 实时通讯方案

## 一、需求分析

原先 AI 生成题目的场景响应较慢，如果题目数过多，容易产生请求超时；并且界面上没有响应，用户体验不佳。

需要 **流式化改造** AI 生成题目接口，一道一道地实时返回已生成题目给前端，而不是让前端请求一直阻塞等待，最后一起返回，提升用户体验且避免请求超时。

首先智谱 AI 为我们提供了流式响应的支持，数据已经可以一点一点地返回给后端了，那么我们要思考的问题是如何让后端接收到的一点一点的内容实时返回给前端？

需要进行一些调研，来了解前后端实时通讯的方案。

## 二、前后端实时通讯方案

几种主流的实现方案：

1. 1）轮询（前端主动去要）

   前端间隔一定时间就调用后端提供的结果接口，比如 200ms 一次，后端处理一些结果就累加放置在缓存中。

2. 2）SSE（后端推送给前端）

   前端发送请求并和后端建立连接后，后端可以实时推送数据给前端，无需前端自主轮询。

3. 3）WebSocket

   全双工协议，前端能实时推送数据给后端（或者从后端缓存拿数据），后端也可以实时推送数据给前端。

如果对 SSE 技术比较陌生，下面重点讲解。

## 三、SSE 技术

### 基本概念

服务器发送事件（Server-Sent Events）是一种用于从服务器到客户端的 **单向、实时** 数据传输技术，基于 HTTP协议实现。

它有几个重要的特点：

1. 单向通信：SSE 只支持服务器向客户端的单向通信，客户端不能向服务器发送数据。
2. 文本格式：SSE 使用 **纯文本格式** 传输数据，使用 HTTP 响应的 `text/event-stream` MIME 类型。
3. 保持连接：SSE 通过保持一个持久的 HTTP 连接，实现服务器向客户端推送更新，而不需要客户端频繁轮询。
4. 自动重连：如果连接中断，浏览器会自动尝试重新连接，确保数据流的连续性。

### SSE 数据格式

SSE 数据流的格式非常简单，每个事件使用 `data` 字段，事件以两个换行符结束。还可以使用 `id` 字段来标识事件，并且 `retry` 字段可以设置重新连接的时间间隔。

示例格式如下：

```plain
data: First message\n
\n
data: Second message\n
\n
data: Third message\n
id: 3\n
\n
retry: 10000\n
data: Fourth message\n
\n
```

### 自主实现 SSE

实现 SSE 非常简单，无论是 Java 服务端还是前端 HTML5 都支持了 SSE，以下内容仅作了解。

1）服务器端

需要生成符合 SSE 格式的响应，并设置合适的 HTTP 头。使用 Servlet 来实现 SSE 示例：

```java
import java.io.IOException;
import java.io.PrintWriter;
import javax.servlet.ServletException;
import javax.servlet.annotation.WebServlet;
import javax.servlet.http.HttpServlet;
import javax.servlet.http.HttpServletRequest;
import javax.servlet.http.HttpServletResponse;

@WebServlet("/sse")
public class SseServlet extends HttpServlet {
    protected void doGet(HttpServletRequest request, HttpServletResponse response)
            throws ServletException, IOException {
        response.setContentType("text/event-stream");
        response.setCharacterEncoding("UTF-8");
        PrintWriter writer = response.getWriter();
        for (int i = 0; i < 10; i++) {
            writer.write("data: Message " + i + "\n\n");
            writer.flush();
            try {
                Thread.sleep(1000);
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
        }
        writer.close();
    }
}
```

当然，如果是 Spring 项目，还有更简单的实现方式，直接给请求返回 SseEmitter 对象即可。

**注意，一定要使用 Get 请求！！！**

示例代码：

```java
@GetMapping("/sse")
public SseEmitter testSSE() {
    // 建立 SSE 连接对象，0 表示不超时
    SseEmitter emitter = new SseEmitter(0L);   
    ... 业务逻辑处理
    return emitter;
}
```

2）Web 前端

可以使用 JavaScript 的 `EventSource` 对象来连接和处理服务器发送的事件。示例代码：

```javascript
// 创建 SSE 请求
const eventSource = new EventSource(
  "http://localhost:8080/sse"
);
// 接收消息
eventSource.onmessage = function (event) {
  console.log(event.data);
};
// 生成结束，关闭连接
eventSource.onerror = function (event) {
  if (event.eventPhase === EventSource.CLOSED) {
    eventSource.close();
  }
};
```

### 应用场景

由于现代浏览器普遍支持 SSE，所以它的应用场景非常广泛，AI 对话就是 SSE 的一个典型的应用场景。

再举一些例子：

1. 实时更新：股票价格、体育比赛比分、新闻更新等需要实时推送的应用。
2. 日志监控：实时监控服务器日志或应用状态。
3. 通知系统：向客户端推送系统通知或消息。

## 四、方案设计

### 方案对比

熟悉了 SSE 技术后，对比上述前后端实时通讯方案。

1）主动轮询其实是一种伪实时，比如每 3 秒轮询请求一次，结果后端在 0.1 秒就返回了数据，还要再等 2.9 秒，存在延迟。

2）WebSocket 和 SSE 虽然都能实现服务端推送，但 Websocket 会更复杂些，且是二进制协议，调试不方便。AI 对话只需要服务器单向推送即可，不需要使用双向通信，所以选择文本格式的 SSE。

### 最终方案

回归到本项目，具体实现方案如下：

1）前端向后端发送普通 HTTP 请求

2）后端创建 SSE 连接对象，为后续的推送做准备

3）后端流式调用智谱 AI，获取到数据流，使用 RxJava 订阅数据流

4）以 SSE 的方式响应前端，至此接口主流程已执行完成

5）异步：基于 RxJava 实时获取到智谱 AI 的数据，并持续将数据拼接为字符串，当拼接出一道完整题目时，通过 SSE 推送给前端。

6）前端每获取一道题目，立刻插入到表单项中

明确方案后，下面进行开发。

## 五、后端开发

### 1、封装通用的流式调用 AI 接口

跟之前的请求方法不同的是：

* 将请求的 stream 参数为 true，表示开始流式调用。
* 返回结果为 Flowable 类型，为流式结果。

代码如下：

```java
/**
 * 通用流式请求（简化消息传递）
 *
 * @param systemMessage
 * @param userMessage
 * @param temperature
 * @return
 */
public Flowable<ModelData> doStreamRequest(String systemMessage, String userMessage, Float temperature) {
    // 构造请求
    List<ChatMessage> messages = new ArrayList<>();
    ChatMessage systemChatMessage = new ChatMessage(ChatMessageRole.SYSTEM.value(), systemMessage);
    ChatMessage userChatMessage = new ChatMessage(ChatMessageRole.USER.value(), userMessage);
    messages.add(systemChatMessage);
    messages.add(userChatMessage);
    return doStreamRequest(messages, temperature);
}

/**
 * 通用流式请求
 *
 * @param messages
 * @param temperature
 * @return
 */
public Flowable<ModelData> doStreamRequest(List<ChatMessage> messages, Float temperature) {
    // 构造请求
    ChatCompletionRequest chatCompletionRequest = ChatCompletionRequest.builder()
    .model(Constants.ModelChatGLM4)
    .stream(Boolean.TRUE)
    .invokeMethod(Constants.invokeMethod)
    .temperature(temperature)
    .messages(messages)
    .build();
    ModelApiResponse invokeModelApiResp = clientV4.invokeModelApi(chatCompletionRequest);
    return invokeModelApiResp.getFlowable();
}
```

### 2、新建 AI 生成题目的 SSE 接口

1）首先定义接口，注意 SSE 必须是 get 请求：

```java
@GetMapping("/ai_generate/sse")
public SseEmitter aiGenerateQuestionSSE(
    AiGenerateQuestionRequest aiGenerateQuestionRequest) {
    ...
}
```

2）建立 SSE 连接对象，并返回：

```java
// 建立 SSE 连接对象，0 表示不超时
SseEmitter emitter = new SseEmitter(0L);
return emitter;
```

3）调用 AI 获取数据流：1671423090896154626\_0.4811694112607443

```java
// AI 生成，sse 流式返回
Flowable<ModelData> modelDataFlowable = aiManager.doStreamRequest(GENERATE_QUESTION_SYSTEM_MESSAGE, userMessage, null);
```

4）异步对流进行解析和转换，转为单个字符，便于处理：

```java
modelDataFlowable
// 异步线程池执行
.observeOn(Schedulers.io())
.map(chunk -> chunk.getChoices().get(0).getDelta().getContent())
.map(message -> message.replaceAll("\\s", ""))
.filter(StrUtil::isNotBlank)
.flatMap(message -> {
    // 将字符串转换为 List<Character>
    List<Character> charList = new ArrayList<>();
    for (char c : message.toCharArray()) {
        charList.add(c);
    }
    return Flowable.fromIterable(charList);
})
```

5）异步拼接 JSON 单题数据，并利用 SSE 推送至前端

括号匹配算法：

https://leetcode.cn/problems/valid-parentheses/description/1671423090896154626\_0.530474413034121

```java
// 左括号计数器，除了默认值外，当回归为 0 时，表示左括号等于右括号，可以截取
AtomicInteger counter = new AtomicInteger(0);
// 拼接完整题目
StringBuilder stringBuilder = new StringBuilder();

flowable.doOnNext(c -> {
    {
        // 识别第一个 [ 表示开始 AI 传输 json 数据，打开 flag 开始拼接 json 数组
        if (c == '{') {
            flag.addAndGet(1);
        }
        if (flag.get() > 0) {
            contentBuilder.append(c);
        }
        if (c == '}') {
            flag.addAndGet(-1);
            if (flag.get() == 0) {
                // 累积单套题目满足 json 格式后，sse 推送至前端
                // sse 需要压缩成当行 json，sse 无法识别换行
                emitter.send(JSONUtil.toJsonStr(contentBuilder.toString()));
                // 清空 StringBuilder
                contentBuilder.setLength(0);
            }
        }
    }
})
```

6）监听 flowable 完成事件，并开启订阅：

```java
flowable
.doOnComplete(emitter::complete)
.subscribe()
```

完整代码如下：

```java
@GetMapping("/ai_generate/sse")
public SseEmitter aiGenerateQuestionSSE(AiGenerateQuestionRequest aiGenerateQuestionRequest) {
    ThrowUtils.throwIf(aiGenerateQuestionRequest == null, ErrorCode.PARAMS_ERROR);
    // 获取参数
    Long appId = aiGenerateQuestionRequest.getAppId();
    int questionNumber = aiGenerateQuestionRequest.getQuestionNumber();
    int optionNumber = aiGenerateQuestionRequest.getOptionNumber();
    // 获取应用信息
    App app = appService.getById(appId);
    ThrowUtils.throwIf(app == null, ErrorCode.NOT_FOUND_ERROR);

    // 封装 Prompt
    String userMessage = getGenerateQuestionUserMessage(app, questionNumber, optionNumber);
    // 建立 SSE 连接对象，0 表示不超时
    SseEmitter emitter = new SseEmitter(0L);
    // AI 生成，sse 流式返回
    Flowable<ModelData> modelDataFlowable = aiManager.doStreamRequest(GENERATE_QUESTION_SYSTEM_MESSAGE, userMessage, null);
    StringBuilder contentBuilder = new StringBuilder();
    AtomicInteger flag = new AtomicInteger(0);
    modelDataFlowable
    // 异步线程池执行
    .observeOn(Schedulers.io())
    .map(chunk -> chunk.getChoices().get(0).getDelta().getContent())
    .map(message -> message.replaceAll("\\s", ""))
    .filter(StrUtil::isNotBlank)
    .flatMap(message -> {
        // 将字符串转换为 List<Character>
        List<Character> charList = new ArrayList<>();
        for (char c : message.toCharArray()) {
            charList.add(c);
        }
        return Flowable.fromIterable(charList);
    })
    .doOnNext(c -> {
        {
            // 识别第一个 [ 表示开始 AI 传输 json 数据，打开 flag 开始拼接 json 数组
            if (c == '{') {
                flag.addAndGet(1);
            }
            if (flag.get() > 0) {
                contentBuilder.append(c);
            }
            if (c == '}') {
                flag.addAndGet(-1);
                if (flag.get() == 0) {
                    // 累积单套题目满足 json 格式后，sse 推送至前端
                    // sse 需要压缩成当行 json，sse 无法识别换行
                    emitter.send(JSONUtil.toJsonStr(contentBuilder.toString()));
                    // 清空 StringBuilder
                    contentBuilder.setLength(0);
                }
            }
        }
    }).doOnComplete(emitter::complete).subscribe();
    return emitter;
}
```

## 六、前端开发

1）在 AI 生成抽屉表单中补充一个 AI 生成按钮，绑定提交事件。

```jsx
<a-form-item>
  <a-space>
    <a-button
      :loading="submitting"
    type="primary"
    html-type="submit"
    style="width: 120px"
    >
    {{ submitting ? "生成中" : "一键生成" }}
  </a-button>
  <a-button
:loading="submitting"
style="width: 120px"
@click="doSSESubmit"
  >
  {{ submitting ? "生成中" : "实时生成" }}
</a-button>
</a-space>
  </a-form-item>
```

2）编写提交函数，遵循前面提到的 SSE 前端实现方法，先能够打印出 SSE 推送的消息、生成开始、生成结束的信息。1671423090896154626\_0.4346634254510635

需要注意这里不能调用生成的 api，因为 axios 默认不支持 SSE。

3）当有内容生成时，向父页面插入题目（利用属性传递实现）

---

---
url: /Python/AI大模型应用开发/7_实战1：视频脚本一键生成器.md
---

# 实战1：视频脚本一键生成器

## 一、项目介绍

### 1、核心功能

* 实现视频脚本的一键生成，结合用户需求与外部信息，提升脚本的准确性和实时性。

### 2、界面与用户输入

1. **侧边栏**
   * 支持用户输入 API 密钥（如 OpenAI API 密钥），避免仅消耗开发者的 token。
   * 附官方网站链接，方便用户了解如何获取 API 密钥。
   * 可灵活适配其他大模型：若使用文心一言等，可替换输入提示语及后端模型；自用场景下，可移除输入框，直接在代码中嵌入密钥。
2. **主页面**
   * 用户需输入：视频主题（任意主题）、生成脚本的大致时长（任意数字）、创造力参数（范围 0-1，对应 AI 的 Temperature 参数：值越低，随机性和创造性越弱；值越高，随机性和创造性越强，可能出现意外回答）。

### 3、生成流程

1. **点击 “生成脚本” 后**
   * 先判断用户是否提供 API 密钥，若已提供，显示加载组件，提示用户 AI 正在生成。
   * 生成完成后，展示视频标题、脚本内容，以及维基百科搜索结果（用户可点击查看详情）。
2. **背后的增强机制**
   * 调用维基百科 API：在向 AI 模型发送请求前，先通过维基百科 API 获取相关内容，与用户需求结合后一并传给 AI。
   * 目的：弥补 AI 模型知识受训练日期限制的问题（例如 2024 年 2 月公开的 SORA 模型，此前训练的模型均不了解该内容），确保 AI 获得真实、实时的补充信息，避免胡编乱造。

## 二、创建项目及安装依赖项

### 1、创建项目文件夹

* 新建独立文件夹（如命名为`video-script-generator`），用于存放网站相关的所有代码、图片等资源，实现文件管理的规范化。

### 2、配置venv虚拟环境

1. **创建方式**：
   * PyCharm 创建项目时默认使用虚拟环境；其他编辑器可搜索对应方法（如`python -m venv 虚拟环境名`）。
2. **核心作用**：
   * **环境隔离**：为不同项目提供独立的开发环境，支持使用不同版本的 Python 解释器（如项目 1 用 3.8，项目 2 用 3.10）。
   * **依赖管理**：避免不同项目的依赖冲突（例如项目 1 需库 A 1.0 版本，项目 2 需库 A 1.1 版本时，不会相互影响）。
3. **注意事项**：
   * 缺点：每个虚拟环境会复制解释器和库，占用较多存储空间。
   * 例外：临时练习的小项目可省略虚拟环境。

### 3、处理依赖文件（requirements.txt）

1. **文件作用**：

   * 存储项目运行所需的所有依赖库及**精准版本号**，确保项目在不同环境中运行的一致性（避免因版本差异导致代码报错）。

2. **使用步骤**：

   * 从项目文件夹新建`requirements.txt`。

     ```txt
     aiofiles==23.2.1
     aiohttp==3.9.1
     aiosignal==1.3.1
     altair==5.2.0
     annotated-types==0.6.0
     anyio==3.7.1
     async-timeout==4.0.3
     attrs==23.1.0
     beautifulsoup4==4.12.2
     blinker==1.7.0
     Brotli==1.1.0
     cachetools==5.3.2
     certifi==2023.11.17
     charset-normalizer==3.3.2
     click==8.1.7
     dataclasses-json==0.6.3
     distro==1.8.0
     duckduckgo-search==3.9.9
     exceptiongroup==1.2.0
     frozenlist==1.4.0
     gitdb==4.0.11
     GitPython==3.1.40
     greenlet==3.0.1
     h11==0.14.0
     h2==4.1.0
     hpack==4.0.0
     httpcore==1.0.2
     httpx==0.25.2
     hyperframe==6.0.1
     idna==3.6
     importlib-metadata==6.8.0
     Jinja2==3.1.2
     jsonpatch==1.33
     jsonpointer==2.4
     jsonschema==4.20.0
     jsonschema-specifications==2023.11.2
     langchain==0.1.9
     langchain-community==0.0.24
     langchain-core==0.1.26
     langchain-openai==0.0.7
     langsmith==0.1.6
     lxml==4.9.3
     markdown-it-py==3.0.0
     MarkupSafe==2.1.3
     marshmallow==3.20.1
     mdurl==0.1.2
     multidict==6.0.4
     mypy-extensions==1.0.0
     numpy==1.26.2
     openai==1.12.0
     orjson==3.9.15
     packaging==23.2
     pandas==2.1.3
     Pillow==10.1.0
     protobuf==4.25.1
     pyarrow==14.0.1
     pydantic==2.5.2
     pydantic_core==2.14.5
     pydeck==0.8.1b0
     Pygments==2.17.2
     python-dateutil==2.8.2
     pytz==2023.3.post1
     PyYAML==6.0.1
     referencing==0.31.1
     regex==2023.12.25
     requests==2.31.0
     rich==13.7.0
     rpds-py==0.13.2
     six==1.16.0
     smmap==5.0.1
     sniffio==1.3.0
     socksio==1.0.0
     soupsieve==2.5
     SQLAlchemy==2.0.23
     streamlit==1.31.1
     tenacity==8.2.3
     tiktoken==0.6.0
     toml==0.10.2
     toolz==0.12.0
     tornado==6.4
     tqdm==4.66.1
     typing-inspect==0.9.0
     typing_extensions==4.8.0
     tzdata==2023.3
     tzlocal==5.2
     urllib3==2.1.0
     validators==0.22.0
     wikipedia==1.4.0
     yarl==1.9.3
     zipp==3.17.0
     ```

   * 在终端运行命令安装依赖：`pip install -r requirements.txt`，即可自动安装所有指定版本的库。

3. **文件生成方式**：

   * 若需自己生成，在虚拟环境中运行命令：`pip freeze > requirements.txt`，会将当前环境中所有安装包及版本号写入文件。

## 三、创建AI请求

### 1、文件与函数准备

1. 新建代码文件（如命名为`utils.py`），用于封装与 AI 大模型交互的代码。
2. 定义`generate_script`函数，接收参数：视频主题（`subject`）、时长（`duration`）、创造性（`creativity`）、API 密钥（`api_key`），用于生成视频标题和脚本。

### 2、提示模板定义

从`LangChain`导入`ChatPromptTemplate`，分别定义生成标题和脚本的提示模板：

* **标题模板（`title_template`）**：通过`from_messages`方法接收含变量（视频主题`subject`）的消息列表，用于生成视频标题。
* **脚本模板（`script_template`）**：同样用`from_messages`方法，消息列表包含多个变量（标题`title`、时长`duration`、维基百科搜索结果`wikipedia_search`），用于生成脚本内容。

### 3、模型配置

从`LangChain.OpenAI`导入聊天模型，配置`ChatOpenAI`：

* 传入用户提供的`api_key`。
* 将`creativity`参数值赋给模型的`temperature`（控制内容创造性，值越高创造性越强）。
* 若使用课程提供的 API 密钥，可补充`openai_api_base`参数。

### 4、链式调用与结果获取

1. **生成标题**：
   * 组装标题生成链（`TitleChain`），将`TitleTemplate`与模型结合。
   * 调用`TitleChain.invoke`，传入含`subject`（用户提供的视频主题）的字典，通过返回消息的`content`属性获取标题，存入`title`变量。
2. **获取维基百科搜索结果**：
   * 从`LangChain.Community.Utilities`导入`WikipediaAPIWrapper`，创建实例并设置`lang="zh"`（指定中文搜索，也可默认英文）。
   * 调用实例的`run`方法，传入视频主题作为搜索词，获取搜索结果摘要。
3. **生成脚本**：
   * 组装脚本生成链（`ScriptChain`），将`ScriptTemplate`与模型结合。
   * 调用`ScriptChain.invoke`，传入含`title`（生成的标题）、`duration`（用户提供的时长）、`wikipedia_search`（维基百科搜索结果）的字典，通过返回消息的`content`属性获取脚本内容。

### 5、结果返回与测试

* 函数最后返回视频标题、脚本内容、维基百科搜索结果。
* 测试时调用`GenerateScript`，传入测试用的视频主题、时长、创造性参数及 API 密钥，打印结果以验证逻辑（运行中若出现维基百科库的警告，一般可忽略，重点关注错误`error`）。

### 6、完整代码

````python
from langchain.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_community.utilities import WikipediaAPIWrapper

# import os

def generate_script(subject, video_length, creativity, api_key):
    title_template = ChatPromptTemplate.from_messages(
        [
            ("human", "请为'{subject}'这个主题的视频想一个吸引人的标题")
        ]
    )
    script_template = ChatPromptTemplate.from_messages(
        [
            ("human",
             """你是一位短视频频道的博主。根据以下标题和相关信息，为短视频频道写一个视频脚本。
             视频标题：{title}，视频时长：{duration}分钟，生成的脚本的长度尽量遵循视频时长的要求。
             要求开头抓住限球，中间提供干货内容，结尾有惊喜，脚本格式也请按照【开头、中间，结尾】分隔。
             整体内容的表达方式要尽量轻松有趣，吸引年轻人。
             脚本内容可以结合以下维基百科搜索出的信息，但仅作为参考，只结合相关的即可，对不相关的进行忽略：
             ```{wikipedia_search}```""")
        ]
    )

    model = ChatOpenAI(openai_api_key=api_key, temperature=creativity)

    title_chain = title_template | model
    script_chain = script_template | model

    title = title_chain.invoke({"subject": subject}).content

    search = WikipediaAPIWrapper(lang="zh")
    search_result = search.run(subject)

    script = script_chain.invoke({"title": title, "duration": video_length,
                                  "wikipedia_search": search_result}).content

    return search_result, title, script

# print(generate_script("sora模型", 1, 0.7, os.getenv("OPENAI_API_KEY")))
````

## 四、创建网站页面

使用 Streamlit 开发视频脚本生成器前端

### 1、前期准备

1. **文件与依赖**
   * 删除 / 注释上一节中测试`generate_script`的代码。
   * 新建代码文件`main.py`（作为网站主页），导入`streamlit`库和上一节创建的`generate_script`函数（无需再导入 LangChain 相关库）。
2. **运行网页**
   * 给网页添加标题（如 “视频脚本生成器”），通过命令`streamlit run 文件名.py`运行网页，实时查看效果。

### 2、侧边栏设计（用户 API 密钥相关）

通过`with streamlit.sidebar:`创建侧边栏，缩进内容均显示在侧边栏中：

* **API 密钥输入框**：使用`st.text_input`，提示文字为 “请输入 OpenAI API 密钥”，设置`type="password"`隐藏输入内容，用变量保存输入的密钥（如`api_key`）。
* **API 密钥获取链接**：用`st.markdown`添加 Markdown 格式链接（语法：`[链接标签](链接地址)`），直达 OpenAI 官方密钥创建页面，方便用户获取密钥。

### 3、主页面组件（用户输入相关）

主页面组件无需缩进（避免归入侧边栏），包括：

1. **视频主题输入**：`st.text_input`，提示文字为 “请输入视频的主题”，结果保存到`subject`变量。
2. **视频时长输入**：`st.number_input`，提示文字为 “请输入视频的大致时长”，设置最小值`0.1`、步长`0.1`（每次增减 0.1），结果保存到`video_length`变量。
3. **创造力调节滑块**：`st.slider`，控制 AI 的`temperature`参数，设置最小值`0.0`、最大值`1.0`（避免过高导致内容混乱）、默认值`0.2`、步长`0.1`，结果保存到`creativity`变量。
4. **生成按钮**：`st.button`，文字为 “生成脚本”，返回布尔值（点击后为`True`），用于触发后续逻辑。

### 4、用户交互逻辑（点击按钮后处理）

1. **输入验证**：
   * 若用户点击按钮但未提供`api_key`：用`st.info`提示 “请输入 API 密钥”，调用`st.stop()`终止后续代码。
   * 若未提供`subject`：提示 “请输入视频的主题” 并终止。
   * 若`video_length`小于 0.1：提示 “视频时长需不小于 0.1” 并终止。
2. **调用后端函数与加载状态**：
   * 验证通过后，用`with st.spinner("AI正在生成脚本..."):`包裹`generate_script`调用，显示加载动画（动画持续至函数执行完毕）。
   * 调用`generate_script(subject, video_length, creativity, api_key)`，获取返回结果：维基百科搜索结果、视频标题、脚本内容。
3. **结果展示**：
   * 用`st.success("视频脚本生成成功！")`提示成功。
   * 展示标题：`st.subheader("视频标题")` + 生成的标题内容。
   * 展示脚本：`st.subheader("视频脚本")` + 生成的脚本内容。
   * 展示维基百科结果：用`st.expander("维基百科搜索结果")`创建折叠组件，内容放入其中（用户可自主展开查看）。

### 5、完整代码

```python
import streamlit as st
from utils import generate_script

st.title("🎬 视频脚本生成器")

with st.sidebar:
    openai_api_key = st.text_input("请输入OpenAI API密钥：", type="password")
    st.markdown("[获取OpenAI API密钥](https://platform.openai.com/account/api-keys)")

subject = st.text_input("💡 请输入视频的主题")
video_length = st.number_input("⏱️ 请输入视频的大致时长（单位：分钟）", min_value=0.1, step=0.1)
creativity = st.slider("✨ 请输入视频脚本的创造力（数字小说明更严谨，数字大说明更多样）", min_value=0.0,
                       max_value=1.0, value=0.2, step=0.1)
submit = st.button("生成脚本")

if submit and not openai_api_key:
    st.info("请输入你的OpenAI API密钥")
    st.stop()
if submit and not subject:
    st.info("请输入视频的主题")
    st.stop()
if submit and not video_length >= 0.1:
    st.info("视频长度需要大于或等于0.1")
    st.stop()
if submit:
    with st.spinner("AI正在思考中，请稍等..."):
        search_result, title, script = generate_script(subject, video_length, creativity, openai_api_key)
    st.success("视频脚本已生成！")
    st.subheader("🔥 标题：")
    st.write(title)
    st.subheader("📝 视频脚本：")
    st.write(script)
    with st.expander("维基百科搜索结果 👀"):
        st.info(search_result)
```

### 6、页面效果展示

![image-20250820225827468](/assets/image-20250820225827468.s-5SZxb3.png)

---

---
url: /Python/AI大模型应用开发/8_实战2：爆款小红书文案生成器.md
---

# 实战2：爆款小红书文案生成器

## 一、项目介绍

### 1、核心功能

* 帮助用户生成适用于小红书平台的爆款文案，包括待选标题和正文内容，支持用户通过自有 API 密钥调用 AI 模型。

### 2、界面设计

1. **侧边栏**
   * 可收起 / 展开，支持用户输入 API 密钥（避免消耗开发者 token）。
   * 附 OpenAI 官方 API 密钥获取链接，方便用户跳转查看获取方法。
2. **主页面**
   * 设计简洁，包含：一个输入框（用于用户填写文案主题）、一个 “开始写作” 按钮。

### 3、用户交互流程

1. **输入验证**
   * 若用户未提供 API 密钥却点击 “开始写作”：提示用户输入密钥。
   * 若用户提供了 API 密钥但未填写主题：提示用户补充主题。
2. **生成过程**
   * 信息完整后点击按钮，触发 AI 生成流程，此时显示加载组件（告知用户 AI 正在生成内容）。
3. **结果展示**
   * 生成完成后，结果在分隔线下方展示，分为两列：
     * 左列：AI 生成的 5 个待选标题。
     * 右列：AI 生成的文案正文。

## 二、创建AI请求

### 1、项目初始化

1. 创建项目与依赖安装

   * 新建项目文件夹`xiaohongshu-generator`，目录下创建`requirements.txt`文件夹，粘贴以下内容。

     ```txt
     aiohttp==3.8.6
     aiosignal==1.3.1
     altair==5.1.2
     annotated-types==0.6.0
     anyio==3.7.1
     appnope==0.1.3
     argon2-cffi==23.1.0
     argon2-cffi-bindings==21.2.0
     arrow==1.3.0
     astroid==3.0.1
     asttokens==2.4.1
     async-lru==2.0.4
     async-timeout==4.0.3
     attrs==23.1.0
     Babel==2.13.1
     beautifulsoup4==4.12.2
     bleach==6.1.0
     blinker==1.7.0
     cachetools==5.3.2
     Cerberus==1.3.5
     certifi==2023.7.22
     cffi==1.15.1
     charset-normalizer==3.3.2
     click==8.1.7
     comm==0.2.0
     contourpy==1.2.0
     cycler==0.12.1
     dataclasses-json==0.6.2
     debugpy==1.8.0
     decorator==5.1.1
     defusedxml==0.7.1
     dill==0.3.7
     distlib==0.3.7
     distro==1.8.0
     docopt==0.6.2
     exceptiongroup==1.1.3
     executing==2.0.1
     fastjsonschema==2.19.0
     fonttools==4.46.0
     fqdn==1.5.1
     frozenlist==1.4.0
     gitdb==4.0.11
     GitPython==3.1.40
     greenlet==3.0.1
     h11==0.14.0
     httpcore==1.0.2
     httpx==0.25.1
     idna==3.4
     importlib-metadata==6.8.0
     ipykernel==6.27.1
     ipython==8.18.1
     ipywidgets==8.1.1
     isoduration==20.11.0
     isort==5.13.0
     jedi==0.19.1
     Jinja2==3.1.2
     json5==0.9.14
     jsonpatch==1.33
     jsonpath-ng==1.6.1
     jsonpointer==2.4
     jsonschema==4.19.2
     jsonschema-specifications==2023.7.1
     jupyter==1.0.0
     jupyter-console==6.6.3
     jupyter-events==0.9.0
     jupyter-lsp==2.2.1
     jupyter_ai_magics==2.10.0
     jupyter_client==8.6.0
     jupyter_core==5.5.0
     jupyter_server==2.12.1
     jupyter_server_terminals==0.4.4
     jupyterlab==4.0.9
     jupyterlab-widgets==3.0.9
     jupyterlab_pygments==0.3.0
     jupyterlab_server==2.25.2
     kiwisolver==1.4.5
     langchain==0.1.9
     langchain-community==0.0.24
     langchain-core==0.1.26
     langchain-openai==0.0.7
     langsmith==0.1.9
     markdown-it-py==3.0.0
     MarkupSafe==2.1.3
     marshmallow==3.20.1
     matplotlib==3.8.2
     matplotlib-inline==0.1.6
     mccabe==0.7.0
     mdurl==0.1.2
     mistune==3.0.2
     multidict==6.0.4
     mypy-extensions==1.0.0
     nbclient==0.9.0
     nbconvert==7.12.0
     nbformat==5.9.2
     nest-asyncio==1.5.8
     notebook==7.0.6
     notebook_shim==0.2.3
     numpy==1.26.2
     openai==1.12.0
     orjson==3.9.15
     overrides==7.4.0
     packaging==23.2
     pandas==2.1.3
     pandocfilters==1.5.0
     parso==0.8.3
     patsy==0.5.4
     pep517==0.13.1
     pexpect==4.9.0
     Pillow==10.1.0
     pip-api==0.0.30
     pipreqs==0.4.13
     platformdirs==4.1.0
     plette==0.4.4
     ply==3.11
     prometheus-client==0.19.0
     prompt-toolkit==3.0.41
     protobuf==4.25.0
     psutil==5.9.6
     ptyprocess==0.7.0
     pure-eval==0.2.2
     pyarrow==14.0.1
     pycparser==2.21
     pydantic==2.6.2
     pydantic_core==2.16.3
     pydeck==0.8.1b0
     Pygments==2.16.1
     pylint==3.0.2
     pyparsing==3.1.1
     python-dateutil==2.8.2
     python-json-logger==2.0.7
     pytz==2023.3.post1
     PyYAML==6.0.1
     pyzmq==25.1.2
     qtconsole==5.5.1
     QtPy==2.4.1
     referencing==0.30.2
     regex==2023.12.25
     requests==2.31.0
     requirementslib==3.0.0
     rfc3339-validator==0.1.4
     rfc3986-validator==0.1.1
     rich==13.6.0
     rpds-py==0.12.0
     scipy==1.11.4
     seaborn==0.13.0
     Send2Trash==1.8.2
     six==1.16.0
     smmap==5.0.1
     sniffio==1.3.0
     soupsieve==2.5
     SQLAlchemy==2.0.23
     stack-data==0.6.3
     statsmodels==0.14.0
     streamlit==1.28.2
     tenacity==8.2.3
     terminado==0.18.0
     tiktoken==0.6.0
     tinycss2==1.2.1
     toml==0.10.2
     tomlkit==0.12.3
     toolz==0.12.0
     tornado==6.3.3
     tqdm==4.66.1
     traitlets==5.14.0
     types-python-dateutil==2.8.19.14
     typing-inspect==0.9.0
     typing_extensions==4.8.0
     tzdata==2023.3
     tzlocal==5.2
     uri-template==1.3.0
     urllib3==2.1.0
     validators==0.22.0
     wcwidth==0.2.12
     webcolors==1.13
     webencodings==0.5.1
     websocket-client==1.7.0
     widgetsnbextension==4.0.9
     yarg==0.1.9
     yarl==1.9.2
     zipp==3.17.0
     ```

   * 终端运行命令安装依赖：`pip install -r requirements.txt`，核心依赖包括`streamlit`、`openai`、`langchain`、`langchain-openai`。

### 2、核心函数与组件设计

新建`utils.py`，定义`generate_xiaohongshu`函数，接收参数：小红书主题（`theme`）、用户提供的 API 密钥（`api_key`），用于串联 “提示模板→模型→输出解析器” 全链路，生成 5 个待选标题和正文。

```python
from prompt_template import system_template_text, user_template_text
from langchain_openai import ChatOpenAI
from langchain.output_parsers import PydanticOutputParser
from langchain.prompts import ChatPromptTemplate
from xiaohongshu_model import Xiaohongshu

# import os


def generate_xiaohongshu(theme, openai_api_key):
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_template_text),
        ("user", user_template_text)
    ])
    model = ChatOpenAI(model="gpt-3.5-turbo", api_key=openai_api_key)
    output_parser = PydanticOutputParser(pydantic_object=Xiaohongshu)
    chain = prompt | model | output_parser
    result = chain.invoke({
        "parser_instructions": output_parser.get_format_instructions(),
        "theme": theme
    })
    return result

# print(generate_xiaohongshu("大模型", os.getenv("OPENAI_API_KEY")))
```

### 3、提示模板配置

1. **模板文件创建**
   * 新建`prompt_template`文件，专门存放提示模板文本，包含：
     * 系统提示（`system_template_text`）：定义 AI 生成逻辑和风格（符合小红书平台特性）。
     * 用户提示（`user_template_text`）：引导用户输入主题。
     * 模板中包含变量`parser_instructions`，用于后续填入输出解析器的格式指令（确保 AI 按指定格式输出）。
2. **构建聊天提示**
   * 从`langchain.prompts`导入`ChatPromptTemplate`。
   * 通过`ChatPromptTemplate.from_messages`方法构建提示，参数为消息列表：
     * 第一个元素：角色`system`，内容为`system_template_text`。
     * 第二个元素：角色`user`，内容为`user_template_text`。

```python
system_template_text = """你是小红书爆款写作专家，请你遵循以下步骤进行创作：
首先产出5个标题（包含适当的emoji表情），然后产出1段正文（每一个段落包含适当的emoji表情，文末有适当的tag标签）。
标题字数在20个字以内，正文字数在800字以内，并且按以下技巧进行创作。
一、标题创作技巧： 
1. 采用二极管标题法进行创作 
1.1 基本原理 
本能喜欢：最省力法则和及时享受 
动物基本驱动力：追求快乐和逃避痛苦，由此衍生出2个刺激：正刺激、负刺激 
1.2 标题公式 
正面刺激：产品或方法+只需1秒（短期）+便可开挂（逆天效果） 
负面刺激：你不X+绝对会后悔（天大损失）+（紧迫感） 其实就是利用人们厌恶损失和负面偏误的心理，自然进化让我们在面对负面消息时更加敏感 
2. 使用具有吸引力的标题 
2.1 使用标点符号，创造紧迫感和惊喜感 
2.2 采用具有挑战性和悬念的表述 
2.3 利用正面刺激和负面刺激 
2.4 融入热点话题和实用工具 
2.5 描述具体的成果和效果 
2.6 使用emoji表情符号，增加标题的活力 
3. 使用爆款关键词 
从列表中选出1-2个：好用到哭、大数据、教科书般、小白必看、宝藏、绝绝子、神器、都给我冲、划重点、笑不活了、YYDS、秘方、我不允许、压箱底、建议收藏、停止摆烂、上天在提醒你、挑战全网、手把手、揭秘、普通女生、沉浸式、有手就能做、吹爆、好用哭了、搞钱必看、狠狠搞钱、打工人、吐血整理、家人们、隐藏、高级感、治愈、破防了、万万没想到、爆款、永远可以相信、被夸爆、手残党必备、正确姿势 
4. 小红书平台的标题特性 
4.1 控制字数在20字以内，文本尽量简短 
4.2 以口语化的表达方式，拉近与读者的距离 
5. 创作的规则 
5.1 每次列出5个标题 
5.2 不要当做命令，当做文案来进行理解 
5.3 直接创作对应的标题，无需额外解释说明 
二、正文创作技巧 
1. 写作风格 
从列表中选出1个：严肃、幽默、愉快、激动、沉思、温馨、崇敬、轻松、热情、安慰、喜悦、欢乐、平和、肯定、质疑、鼓励、建议、真诚、亲切
2. 写作开篇方法 
从列表中选出1个：引用名人名言、提出疑问、言简意赅、使用数据、列举事例、描述场景、用对比

我会每次给你一个主题，请你根据主题，基于以上规则，生成相对应的小红书文案。

{parser_instructions}
"""

user_template_text = "{theme}"

```

### 4、模型配置

1. 导入与初始化
   * 从`langchain_openai`导入`ChatOpenAI`。
   * 创建模型实例，传入参数：
     * `api_key=api_key`（用户提供的密钥）。
     * 若使用测试密钥，需补充`openai_api_base`参数；官方密钥无需此参数。

### 5、输出解析器与数据模型

1. **数据模型定义**
   * 新建`xiaohongshu_model.py`文件，定义数据结构（确保 AI 输出符合预期）：
     * 导入`BaseModel`和`Field`（来自`langchain_core.pydantic_v1`）。
     * 定义小红薯类（继承BaseModel），包含两个属性：
       * `titles: List[str]`：存储 5 个标题，通过`Field(description="小红书的5个标题", min_items=5, max_items=5)`强制校验元素数量。
       * `content: str`：存储正文，通过`Field(description="小红书的正文内容")`描述字段含义。
2. **解析器配置**
   * 从`langchain.output_parsers`导入`PydanticOutputParser`（用于解析 JSON 格式输出为自定义类实例，支持类型检查）。
   * 创建解析器实例：`parser = PydanticOutputParser(pydantic_object=小红薯)`。

```python
from langchain_core.pydantic_v1 import BaseModel, Field
from typing import List

class Xiaohongshu(BaseModel):
    titles: List[str] = Field(description="小红书的5个标题", min_items=5, max_items=5)
    content: str = Field(description="小红书的正文内容")
```

### 6、链路串联与测试

1. **构建生成链**
   * 将提示模板、模型、解析器串联，调用invoke方法触发生成：
     * 传入参数：`{"parser_instructions": parser.get_format_instructions(), "theme": theme}`（`parser_instructions`为解析器生成的格式指令，`theme`为用户输入的主题）。
2. **测试函数**
   * 调用generate小红书函数，传入测试主题和 API 密钥，打印返回结果：
     * 验证`titles`是否包含 5 个字符串，`content`是否为字符串，确保解析正确。

## 三、创建网站页面

### 1、前期准备

1. 文件与导入
   * 删除 / 注释上一节中测试`generate_xiaohongshu.py`的代码。
   * 新建代码文件作为网站主页，导入`streamlit`库和上一节创建的`generate小红书`函数。

### 2、界面基础配置

1. **标题与运行**
   * 给网页添加标题：`st.title("爆款小红书AI写作助手")`。
   * 终端运行命令启动网页：`streamlit run 文件名.py`，实时查看效果。
2. **侧边栏设计**
   * 通过`with st.sidebar`创建可收起 / 展开的侧边栏，缩进内容均显示在侧边栏中：
     * **API 密钥输入框**：`st.text_input("请输入OpenAI API密钥", type="password")`，用变量（如`api_key`）保存输入的密钥（隐藏显示，保障安全）。
     * **密钥获取链接**：`st.markdown("[获取OpenAI API密钥](官方链接)")`，支持用户跳转至官网获取密钥。

### 3、主页面组件与交互逻辑

1. **用户输入组件**
   * **主题输入框**：`theme = st.text_input("请输入小红书写作主题")`，获取用户输入的文案主题。
   * **生成按钮**：`generate_btn = st.button("开始写作")`，点击后触发生成逻辑（返回`True`）。
2. **输入验证**
   * 若点击按钮但未提供`api_key`：`st.info("请输入你的OpenAI API密钥")` + `st.stop()`（终止后续代码）。
   * 若提供密钥但未输入`theme`：`st.info("请输入生成内容的主题")` + `st.stop()`。

### 4、生成流程与结果展示

1. **加载状态与后端调用**

   * 验证通过后，用`with st.spinner("AI正在生成文案..."):`包裹`generate小红书`调用，显示加载动画（持续至函数执行完毕）。
   * 调用函数：`result = generate小红书(theme, api_key)`，获取返回结果（含`titles`列表和`content`字符串）。

2. **结果布局与展示**

   * **分割线**：`st.divider()`，分隔输入区与结果区。

   * **双列布局**：`left_column, right_column = st.columns(2)`，左侧展示标题，右侧展示正文。

   * **左侧列（标题）**：

     ```python
     with left_column:
         st.markdown("##### 小红书标题1")
         st.write(result.titles[0])
         st.markdown("##### 小红书标题2")
         st.write(result.titles[1])
         st.markdown("##### 小红书标题3")
         st.write(result.titles[2])
         st.markdown("##### 小红书标题4")
         st.write(result.titles[3])
         st.markdown("##### 小红书标题5")
         st.write(result.titles[4])
     ```

   * **右侧列（正文）**：

     ```python
     with right_column:
         st.markdown("##### 小红书正文")
         st.write(result.content)
     ```

### 5、完整代码

`main.py`

```python
import streamlit as st

from utils import generate_xiaohongshu


st.header("爆款小红书AI写作助手 ✏️")
with st.sidebar:
    openai_api_key = st.text_input("请输入OpenAI API密钥：", type="password")
    st.markdown("[获取OpenAI API密钥](https://platform.openai.com/account/api-keys)")

theme = st.text_input("主题")
submit = st.button("开始写作")

if submit and not openai_api_key:
    st.info("请输入你的OpenAI API密钥")
    st.stop()
if submit and not theme:
    st.info("请输入生成内容的主题")
    st.stop()
if submit:
    with st.spinner("AI正在努力创作中，请稍等..."):
        result = generate_xiaohongshu(theme, openai_api_key)
    st.divider()
    left_column, right_column = st.columns(2)
    with left_column:
        st.markdown("##### 小红书标题1")
        st.write(result.titles[0])
        st.markdown("##### 小红书标题2")
        st.write(result.titles[1])
        st.markdown("##### 小红书标题3")
        st.write(result.titles[2])
        st.markdown("##### 小红书标题4")
        st.write(result.titles[3])
        st.markdown("##### 小红书标题5")
        st.write(result.titles[4])
    with right_column:
        st.markdown("##### 小红书正文")
        st.write(result.content)
```

### 6、功能测试与扩展

* 测试流程：输入 API 密钥→填写主题→点击 “开始写作”，验证是否成功生成 5 个标题和正文。
* 扩展方向：基于此框架定制其他平台（如微博、抖音）的文案生成工具，调整提示模板和输出格式即可。

后续可根据需求优化界面样式或增加功能（如文案风格选择），进一步提升用户体验。

---

---
url: /Python/AI大模型应用开发/10_实战3：克隆AI聊天助手.md
---

# 实战3：克隆AI聊天助手

## 一、项目介绍

项目的目标是开发一个仿照 ChatGPT、文心一言的 AI 聊天助手网站，重点实现 “带上下文记忆的连续对话” 功能，以下从**界面结构、核心交互逻辑、关键功能亮点**三方面整理具体效果：

### 1、界面结构：侧边栏 + 主聊天区

网站界面分为两大核心区域，布局简洁且符合主流 AI 聊天工具的使用习惯：

1. 左侧侧边栏
   * 核心功能：供用户输入自定义 API 密钥（用于调用 AI 模型生成内容）。
   * 辅助功能：提供可跳转的**OpenAI 官方 API 密钥获取链接**，帮助用户快速了解如何获取密钥，降低使用门槛。
2. 主聊天区

### 2、核心交互逻辑：密钥验证→连续对话

整个助手的交互流程围绕 “密钥有效性” 和 “上下文记忆” 展开，步骤清晰：

#### 2.1、前置：API 密钥验证

* 若用户未输入 API 密钥：聊天功能无法开启，系统会主动提醒 “请输入 API 密钥”，避免因密钥缺失导致模型调用失败。
* 若用户输入有效 API 密钥：验证通过后，聊天功能解锁，用户可开始与 AI 助手交互。

#### 2.2、核心：带记忆的连续对话

这是项目三与前两个项目（仅支持一次性互动）的核心区别，具体效果如下：

* **多轮对话连贯性**：用户可发起多轮追问，所有消息（用户提问 + AI 回答）会按时间顺序依次展示在主聊天区（新消息自动追加在历史消息下方）。
* 上下文记忆能力：AI 能识别并关联历史对话信息。例如：
  1. 用户第一轮提问 “牛顿第二定律是什么？”，AI 给出解释；
  2. 用户第二轮追问 “它的公式如何推导？”，AI 会自动识别 “它” 指代 “牛顿第二定律”，基于上一轮内容给出推导过程，无需用户重复说明主语。

### 3、关键功能亮点

1. **用户自主性高**：支持自定义 API 密钥，用户可使用自己的账号资源调用模型，无需依赖项目方提供的 API 服务，灵活度更高。
2. **易用性设计**：通过 “官方链接指引”“无密钥提醒” 等细节，降低用户操作成本，尤其适合对 API 密钥获取不熟悉的新手。
3. **核心能力突破**：实现 “上下文记忆”，解决前序项目 “单次互动无关联” 的问题，让对话更贴近真实交流场景（如日常咨询、知识问答、任务协作等）。

## 二、创建AI请求

### 1、前期准备：环境搭建

1. **创建项目结构**：
   新建项目文件夹，将`requirements.txt`文件放入其中（包含项目所需依赖）。

   ```txt
   aiohttp==3.9.3
   aiosignal==1.3.1
   altair==5.2.0
   annotated-types==0.6.0
   anyio==4.3.0
   async-timeout==4.0.3
   attrs==23.2.0
   blinker==1.7.0
   cachetools==5.3.3
   certifi==2024.2.2
   charset-normalizer==3.3.2
   click==8.1.7
   dataclasses-json==0.6.4
   distro==1.9.0
   exceptiongroup==1.2.0
   frozenlist==1.4.1
   gitdb==4.0.11
   GitPython==3.1.42
   greenlet==3.0.3
   h11==0.14.0
   httpcore==1.0.4
   httpx==0.27.0
   idna==3.6
   Jinja2==3.1.3
   jsonpatch==1.33
   jsonpointer==2.4
   jsonschema==4.21.1
   jsonschema-specifications==2023.12.1
   langchain==0.1.12
   langchain-community==0.0.28
   langchain-core==0.1.31
   langchain-openai==0.0.8
   langchain-text-splitters==0.0.1
   langsmith==0.1.25
   markdown-it-py==3.0.0
   MarkupSafe==2.1.5
   marshmallow==3.21.1
   mdurl==0.1.2
   multidict==6.0.5
   mypy-extensions==1.0.0
   numpy==1.26.4
   openai==1.14.0
   orjson==3.9.15
   packaging==23.2
   pandas==2.2.1
   pillow==10.2.0
   protobuf==4.25.3
   pyarrow==15.0.1
   pydantic==2.6.4
   pydantic_core==2.16.3
   pydeck==0.8.1b0
   Pygments==2.17.2
   python-dateutil==2.9.0.post0
   pytz==2024.1
   PyYAML==6.0.1
   referencing==0.33.0
   regex==2023.12.25
   requests==2.31.0
   rich==13.7.1
   rpds-py==0.18.0
   six==1.16.0
   smmap==5.0.1
   sniffio==1.3.1
   SQLAlchemy==2.0.28
   streamlit==1.32.1
   tenacity==8.2.3
   tiktoken==0.6.0
   toml==0.10.2
   toolz==0.12.1
   tornado==6.4
   tqdm==4.66.2
   typing-inspect==0.9.0
   typing_extensions==4.10.0
   tzdata==2024.1
   urllib3==2.2.1
   yarl==1.9.4
   ```

2. **安装依赖**：
   在终端执行命令，安装所有依赖包：

   ```sh
   pip install -r requirements.txt
   ```

3. **新建代码文件**：
   创建一个 Python 文件（`utils.py`），用于编写与 AI 模型交互的核心逻辑。

### 2、核心函数

`utils.py`中定义`get_chat_response`函数，封装与 AI 模型的交互过程，关键是传入 “外部记忆” 以维持对话连贯性。

#### 函数参数设计

函数需接收 3 个核心参数：

* `prompt`：用户当前的输入提示（字符串）。
* `openai_api_key`：用户提供的 API 密钥（用于调用 AI 模型）。
* `memory`：外部传入的记忆实例（储存历史对话，确保跨函数调用时记忆不丢失）。

#### 函数内部逻辑

`utils.py`完整代码

```python
from langchain.chains import ConversationChain
from langchain_openai import ChatOpenAI

import os
from langchain.memory import ConversationBufferMemory


def get_chat_response(prompt, memory, openai_api_key):
    model = ChatOpenAI(model="gpt-3.5-turbo", openai_api_key=openai_api_key)
    chain = ConversationChain(llm=model, memory=memory)

    response = chain.invoke({"input": prompt})
    return response["response"]
```

#### 关键逻辑解析

1. **模型初始化**：
   使用`ChatOpenAI`创建模型实例，通过`openai_api_key`参数传入用户提供的密钥，确保模型调用权限。
2. **对话链与记忆关联**：
   `ConversationChain`自动处理 “加载历史记忆→生成回应→更新记忆” 的全流程，无需手动调用`load_memory_variables`或`save_context`，简化逻辑并减少出错概率。
3. **回应提取**：
   对话链的`invoke`方法返回字典（含`input`、`history`、`response`等字段），仅提取`response`字段（AI 的回答内容）作为函数返回值，符合实际需求。

### 3、功能测试：验证记忆有效性

为确保函数能维持对话记忆，需进行多轮调用测试，在`utils.py`中增加如下测试代码：

```python
memory = ConversationBufferMemory(return_messages=True)
print(get_chat_response("牛顿提出过哪些知名的定律？", memory, os.getenv("OPENAI_API_KEY")))
print(get_chat_response("我上一个问题是什么？", memory, os.getenv("OPENAI_API_KEY")))
```

#### 测试预期结果

* 第一轮：AI 正确回答牛顿第二定律的内容。
* 第二轮：AI 能识别 “它” 指代 “牛顿第二定律”，基于历史对话给出公式，证明记忆生效（对话连贯性成立）。

### 4、核心亮点

1. **记忆外部化**：记忆实例从外部传入函数，而非在函数内部初始化，确保多轮调用时历史对话不丢失，是实现连续对话的关键。
2. **流程自动化**：借助`ConversationChain`自动处理记忆加载与更新，避免手动操作可能的遗漏或错误。
3. **灵活性**：支持用户自定义 API 密钥，模型型号可调整，适配不同场景需求。

## 三、创建网站页面

通过 Streamlit 构建聊天助手的前端界面，实现 “API 密钥输入→对话展示→用户交互” 的完整功能，最终打造一个可部署、支持连贯对话的仿 ChatGPT 网站。以下是详细实现步骤与解析：

### 1、前期准备：代码结构与依赖

1. **清理测试代码**：
   删除或注释掉`utils.py`中`get_chat_response`函数的测试代码（避免干扰前端调用）。
2. **新建前端文件**：
   创建前端代码文件（如`main.py`），作为网站主页入口。
3. **导入核心库**：
   需导入 Streamlit（构建前端）和后端的`get_chat_response`函数，代码如下：

```python
import streamlit as st
from langchain.memory import ConversationBufferMemory

from utils import get_chat_response
```

### 2、前端界面搭建：分模块实现

#### 2.1、页面标题与基础配置

设置网站标题，提升视觉辨识度：

```python
st.title("💬 克隆ChatGPT")
```

#### 2.2、侧边栏：API 密钥输入

添加侧边栏用于用户输入 API 密钥（复用前序项目逻辑，确保安全性与自主性）：

```python
with st.sidebar:
    openai_api_key = st.text_input("请输入OpenAI API Key：", type="password")
    st.markdown("[获取OpenAI API key](https://platform.openai.com/account/api-keys)")
```

#### 2.3、会话状态初始化：解决 “记忆重置” 问题

Streamlit 有一个关键特性：**用户交互（如输入、点击按钮）或代码修改后，会从头重新运行代码**。若直接初始化记忆，会导致每轮对话后记忆被清空，无法维持连贯性。

解决方案：利用`st.session_state`（会话状态）储存记忆和对话历史，仅在首次加载时初始化，后续复用已有状态：

```python
if "memory" not in st.session_state:
    st.session_state["memory"] = ConversationBufferMemory(return_messages=True)
    st.session_state["messages"] = [{"role": "ai",
                                     "content": "你好，我是你的AI助手，有什么可以帮你的吗？"}]
```

#### 2.4、对话历史展示：区分角色与内容

通过循环迭代`st.session_state.messages`，利用 Streamlit 的`st.chat_message`函数展示对话，自动区分 “AI” 和 “人类” 角色（含默认图标）：

```python
for message in st.session_state["messages"]:
    st.chat_message(message["role"]).write(message["content"])
```

#### 2.5、用户输入框：接收消息并触发对话

使用`st.chat_input`创建用户输入框，实现 “输入→验证→调用后端→展示回应” 的流程：

```python
# 用户输入框：获取用户消息
prompt = st.chat_input()
if prompt:
    # 1. 验证API密钥：无密钥则提示并终止
    if not openai_api_key:
        st.info("请输入你的OpenAI API Key")
        st.stop()
	
    # 2. 储存并展示用户消息
    # 加入对话历史
    st.session_state["messages"].append({"role": "human", "content": prompt})
    # 前端展示用户消息
    st.chat_message("human").write(prompt)
    
	# 3. 调用后端函数获取AI回应（带加载提示）
    with st.spinner("AI正在思考中，请稍等..."):
        # 传入会话状态中的记忆，维持连贯性
        response = get_chat_response(prompt, st.session_state["memory"],
                                     openai_api_key)
    
    # 4. 储存并展示AI回应
    msg = {"role": "ai", "content": response}
    # 加入对话历史
    st.session_state["messages"].append(msg)
    # 前端展示AI回应
    st.chat_message("ai").write(response)
```

#### 2.6、完整代码

`main.py`

```python
import streamlit as st
from langchain.memory import ConversationBufferMemory

from utils import get_chat_response

st.title("💬 克隆ChatGPT")

with st.sidebar:
    openai_api_key = st.text_input("请输入OpenAI API Key：", type="password")
    st.markdown("[获取OpenAI API key](https://platform.openai.com/account/api-keys)")

if "memory" not in st.session_state:
    st.session_state["memory"] = ConversationBufferMemory(return_messages=True)
    st.session_state["messages"] = [{"role": "ai",
                                     "content": "你好，我是你的AI助手，有什么可以帮你的吗？"}]

for message in st.session_state["messages"]:
    st.chat_message(message["role"]).write(message["content"])

prompt = st.chat_input()
if prompt:
    if not openai_api_key:
        st.info("请输入你的OpenAI API Key")
        st.stop()
    st.session_state["messages"].append({"role": "human", "content": prompt})
    st.chat_message("human").write(prompt)

    with st.spinner("AI正在思考中，请稍等..."):
        response = get_chat_response(prompt, st.session_state["memory"],
                                     openai_api_key)
    msg = {"role": "ai", "content": response}
    st.session_state["messages"].append(msg)
    st.chat_message("ai").write(response)
```

### 3、功能测试与验证

1. **运行前端**：
   启动 Streamlit 服务
2. **测试流程**：
   * 步骤 1：在侧边栏输入 OpenAI API 密钥。
   * 步骤 2：在输入框发送问题（如 “牛顿第二定律是什么？”），观察 AI 回应是否展示。
   * 步骤 3：继续追问（如 “它的公式是什么？”），验证 AI 是否能关联历史对话（记忆生效）。
3. **预期效果**：
   * 对话按 “用户→AI” 顺序展示，角色气泡清晰区分。
   * 多轮追问时，AI 能理解上下文（如 “它” 指代前序问题中的 “牛顿第二定律”）。

### 4、可选功能：对话清空（挑战任务）

为提升用户体验，可添加 “清空对话” 按钮，实现 “重置记忆 + 清空历史” 的功能，代码示例如下：

```python
# 在侧边栏添加“清空对话”按钮
with st.sidebar:
    if st.button("清空对话"):
        # 重置记忆和对话历史
        st.session_state.memory = ConversationBufferMemory(return_messages=True)
        st.session_state.messages = [{"role": "ai", "content": "你好！我是你的AI助手，有什么可以帮你的吗？"}]
        st.rerun()  # 重新加载页面，生效清空操作
```

### 5、核心亮点

1. **会话状态管理**：通过`st.session_state`解决 Streamlit 代码重跑导致的 “记忆丢失” 问题，是实现连贯对话的关键。
2. **角色化展示**：利用`st.chat_message`自动区分 AI 与人类角色，界面贴近 ChatGPT 等主流工具，用户体验友好。
3. **流程完整性**：包含 “密钥验证→加载提示→对话储存→回应展示” 全流程，逻辑闭环且容错性强（如无密钥时提示）。

### 6、部署与扩展（后续方向）

* **部署**：可通过 Streamlit Community Cloud（免费）、Heroku 等平台部署网站，生成公开链接供他人使用。
* **扩展**：可添加 “模型选择”（如 GPT-3.5/GPT-4）、“对话导出”（保存为 TXT/CSV）等功能，提升工具实用性。

---

---
url: /Python/AI大模型应用开发/12_实战4：智能PDF问答工具.md
---

# 实战4：智能PDF问答工具

## 一、项目介绍

智能 PDF 问答工具是 RAG（检索增强生成）技术的典型应用，通过将用户上传的 PDF 内容转化为知识库，结合大语言模型实现 “基于 PDF 内容的精准问答”，帮助用户快速获取文档关键信息，无需通读全文。以下从**核心功能、使用流程、技术内核、扩展思路**四方面整理：

### 1、核心功能与使用流程

工具的核心目标是 “让用户通过提问快速获取 PDF 中的信息”，整体使用流程清晰直观：

#### 1. 前期准备：API 密钥配置与 PDF 上传

* **API 密钥输入**：左侧侧边栏提供 API 密钥输入框，用户需填写自己的 OpenAI（或其他 LLM）密钥，用于驱动模型生成回答（确保密钥权限正常，避免调用失败）。
* **PDF 格式限制**：仅支持`.pdf`后缀的文件上传，自动过滤非 PDF 格式（如 TXT、DOCX），避免因格式不兼容导致内容解析失败。

#### 2. 核心交互：基于 PDF 内容的问答

* **提问触发条件**：需先上传 PDF 文档，否则提问框不可输入（避免无上下文时的无效提问）。
* 问答示例（以 “AI 论文汇总 PDF” 为例）：
  * 场景 1：用户提问 “哪篇论文介绍了 Transformer 架构？”，工具自动检索 PDF 中相关片段，返回对应论文的标题、链接及摘要（与 PDF 原文一致）。
  * 场景 2：追问 “该论文的核心贡献是什么？”，工具基于历史对话记忆（识别 “该论文” 指代 Transformer 相关论文），结合 PDF 内容生成精准回答。

#### 3. 辅助功能：对话记忆与历史记录

* **对话记忆**：工具自动保存历史对话（用户提问 + AI 回答），支持多轮上下文关联（如 “它的作者是谁？” 中的 “它” 可正确指代前文提到的论文）。
* **历史记录查看**：通过底部组件可展开查看所有历史对话，便于回溯问答过程或验证信息连贯性。

### 2、技术内核：基于 RAG 的实现逻辑

工具背后的核心技术是 RAG 流程，将 “PDF 处理” 与 “智能问答” 衔接，具体步骤如下：

1. **PDF 内容加载与解析**：
   通过`PyPDFLoader`（或类似工具）提取 PDF 中的文本内容，转换为 LangChain 的`Document`对象（含文本段落与页码等元数据）。
2. **文本分割**：
   用`RecursiveCharacterTextSplitter`将长文本切分为短块（如 500 字符 / 块，含 40 字符重叠），确保语义完整且不超 LLM 上下文窗口。
3. **向量存储**：
   通过嵌入模型（如 OpenAI Embeddings）将文本块转为向量，存入向量数据库（如 FAISS），构建 PDF 专属知识库。
4. 检索增强问答：
   * 用户提问时，检索器从向量数据库中找到与问题最相关的文本块；
   * 通过`ConversationalRetrievalChain`将 “问题 + 相关文本块 + 历史对话” 传给 LLM，生成基于 PDF 内容的回答。

### 3、扩展思路：从 “上传式” 到 “本地知识库”

当前工具是 “用户上传 PDF 即时处理” 模式，若需构建**本地固定知识库问答工具**（如企业手册、产品文档问答），可调整如下：

1. **预处理知识库**：
   提前将本地文档（PDF/Word/ 网页等）加载、分割、嵌入并存入向量数据库，无需用户上传（减少用户操作步骤）。
2. **简化前端交互**：
   隐藏文件上传功能，用户直接提问即可（工具自动从预构建的知识库中检索信息）。
3. **保留核心逻辑**：
   检索、对话记忆、LLM 生成等核心流程与 PDF 工具一致，仅需修改 “文档来源”（从 “用户上传” 改为 “本地预加载”）。

### 4、工具价值与适用场景

* **效率提升**：用户无需通读长 PDF（如百页报告、论文集），通过提问快速定位关键信息，节省阅读时间。
* **准确性保障**：回答严格基于 PDF 内容（非模型固有知识），并可通过历史记录追溯来源，减少 AI 幻觉。
* **适用场景**：学术论文查询、企业文档检索、合同条款解读、说明书问答等需 “基于特定文档精准回答” 的场景。

该工具展示了 RAG 技术在实际应用中的落地方式，核心是通过 “文档处理→知识存储→检索增强” 的闭环，让 LLM 成为 “特定文档的智能解读器”。

## 二、创建AI请求

### 1、前期准备：项目初始化与依赖安装

#### 1. 项目结构

```
智能PDF问答工具/
├─ utils.py          # 核心逻辑：PDF处理与AI交互
└─ requirements.txt  # 依赖清单（需包含langchain、openai、pypdf等）
```

#### 2. 依赖安装

通过`requirements.txt`安装所需库（终端执行）：

```
pip install -r requirements.txt
```

* 关键依赖：`langchain`（RAG 流程框架）、`openai`（调用 GPT 模型）、`pypdf`（解析 PDF）、`faiss-cpu`（向量数据库）。

::: details requirements.txt 点我查看

```txt
aiohttp==3.9.3
aiosignal==1.3.1
altair==5.3.0
annotated-types==0.6.0
anyio==4.3.0
async-timeout==4.0.3
attrs==23.2.0
blinker==1.7.0
cachetools==5.3.3
certifi==2024.2.2
charset-normalizer==3.3.2
click==8.1.7
dataclasses-json==0.6.4
distro==1.9.0
exceptiongroup==1.2.0
faiss-cpu==1.8.0
frozenlist==1.4.1
gitdb==4.0.11
GitPython==3.1.43
greenlet==3.0.3
h11==0.14.0
httpcore==1.0.5
httpx==0.27.0
idna==3.6
Jinja2==3.1.3
jsonpatch==1.33
jsonpointer==2.4
jsonschema==4.21.1
jsonschema-specifications==2023.12.1
langchain==0.1.15
langchain-community==0.0.32
langchain-core==0.1.41
langchain-openai==0.1.2
langchain-text-splitters==0.0.1
langsmith==0.1.43
markdown-it-py==3.0.0
MarkupSafe==2.1.5
marshmallow==3.21.1
mdurl==0.1.2
multidict==6.0.5
mypy-extensions==1.0.0
numpy==1.26.4
openai==1.16.2
orjson==3.10.0
packaging==23.2
pandas==2.2.1
pillow==10.3.0
protobuf==4.25.3
pyarrow==15.0.2
pydantic==2.6.4
pydantic_core==2.16.3
pydeck==0.8.1b0
Pygments==2.17.2
pypdf==4.2.0
python-dateutil==2.9.0.post0
pytz==2024.1
PyYAML==6.0.1
referencing==0.34.0
regex==2023.12.25
requests==2.31.0
rich==13.7.1
rpds-py==0.18.0
six==1.16.0
smmap==5.0.1
sniffio==1.3.1
SQLAlchemy==2.0.29
streamlit==1.33.0
tenacity==8.2.3
tiktoken==0.6.0
toml==0.10.2
toolz==0.12.1
tornado==6.4
tqdm==4.66.2
typing-inspect==0.9.0
typing_extensions==4.11.0
tzdata==2024.1
urllib3==2.2.1
yarl==1.9.4
```

:::

### 2、核心函数`qa_agent`实现

该函数是工具的 “大脑”，接收用户 API 密钥、对话记忆、上传的 PDF 文件、用户问题，返回基于 PDF 内容的 AI 回答。完整步骤如下：

#### 1. 函数参数设计

```python
def qa_agent(openai_api_key, memory, pdf_file, user_question):
    # 参数说明：
    # openai_api_key：用户提供的OpenAI API密钥（驱动模型）
    # memory：对话记忆实例（储存历史对话，需从外部传入以保持连续性）
    # pdf_file：用户上传的PDF文件对象（内存中）
    # user_question：用户的提问（字符串）
    # 返回值：AI生成的回答（字符串）
```

* **记忆需外部传入**：若在函数内创建记忆，每次调用会重置历史对话，导致无法实现连续对话。

#### 2. 初始化大语言模型（LLM）

```python
from langchain_openai import ChatOpenAI

# 初始化GPT模型（以gpt-3.5-turbo为例）
llm = ChatOpenAI(
    model_name="gpt-3.5-turbo",
    openai_api_key=openai_api_key  # 使用用户提供的密钥
)
```

#### 3. 处理用户上传的 PDF（内存→临时文件→加载）

用户上传的 PDF 储存在内存中（无本地路径），需先写入临时文件再加载：

```python
from langchain_community.document_loaders import PyPDFLoader
import os

# 步骤1：读取内存中的PDF二进制内容
pdf_content = pdf_file.read()  # pdf_file为用户上传的文件对象（如Streamlit的st.file_uploader返回值）

# 步骤2：创建临时文件，写入PDF内容（便于PyPDFLoader加载）
temp_pdf_path = "temp.pdf"  # 临时文件路径（当前目录）
with open(temp_pdf_path, "wb") as f:  # "wb"：二进制写入模式
    f.write(pdf_content)

# 步骤3：用PyPDFLoader加载临时PDF文件
loader = PyPDFLoader(temp_pdf_path)
documents = loader.load()  # 返回按页码分割的Document列表

# （可选）删除临时文件，避免占用空间
os.remove(temp_pdf_path)
```

#### 4. 文本分割（长文本→短文本块）

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

# 初始化中文适配的文本分割器
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,          # 单个文本块最大长度（500字符）
    chunk_overlap=40,        # 相邻块重叠长度（40字符，保持语义连贯）
    separators=["\n\n", "\n", "。", "！", "？", "，", "、", ""]  # 中文优先分割符
)

# 分割文档：将Document列表切分为更小的文本块
split_docs = text_splitter.split_documents(documents)
```

#### 5. 文本嵌入与向量存储

```python
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS

# 初始化嵌入模型（将文本转为向量）
embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)

# 将分割后的文本块嵌入为向量，并存储到FAISS数据库
db = FAISS.from_documents(split_docs, embeddings)

# 创建检索器：用于从向量数据库中检索与问题相关的文本块
retriever = db.as_retriever(search_kwargs={"k": 3})  # 返回Top3最相关片段
```

#### 6. 创建带记忆的检索增强对话链

```python
from langchain.chains import ConversationalRetrievalChain

# 创建对话链：整合LLM、检索器、记忆
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,          # 大语言模型
    retriever=retriever,  # 文档检索器
    memory=memory     # 对话记忆（维持多轮上下文）
)
```

#### 7. 调用链生成回答

```python
# 传入用户问题，调用链生成回答
result = qa_chain.invoke({
    "question": user_question,  # 用户当前问题
    "chat_history": memory.load_memory_variables({})["chat_history"]  # 历史对话
})

# 返回AI的回答（result字典中"answer"键对应的值）
return result["answer"]
```

### 3、完整代码总结（utils.py）

```python
# 导入所需模块
from langchain.chains import ConversationalRetrievalChain  # 带记忆的检索增强对话链
from langchain_community.document_loaders import PyPDFLoader  # PDF文档加载器
from langchain_community.vectorstores import FAISS  # 向量数据库
from langchain_openai import OpenAIEmbeddings  # OpenAI嵌入模型
from langchain_openai import ChatOpenAI  # OpenAI聊天模型
from langchain_text_splitters import RecursiveCharacterTextSplitter  # 文本分割器


def qa_agent(openai_api_key, memory, uploaded_file, question):
    # 初始化GPT-3.5模型
    model = ChatOpenAI(model="gpt-3.5-turbo", openai_api_key=openai_api_key)
    
    # 读取上传的PDF文件内容（二进制）
    file_content = uploaded_file.read()
    
    # 将内存中的PDF内容写入临时文件
    temp_file_path = "temp.pdf"
    with open(temp_file_path, "wb") as temp_file:
        temp_file.write(file_content)
    
    # 加载临时PDF文件
    loader = PyPDFLoader(temp_file_path)
    docs = loader.load()
    
    # 初始化文本分割器（适配中文）
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,  # 文本块大小
        chunk_overlap=50,  # 块重叠部分大小
        separators=["\n", "。", "！", "？", "，", "、", ""]  # 中文分割符
    )
    
    # 分割文档为小文本块
    texts = text_splitter.split_documents(docs)
    
    # 初始化嵌入模型
    embeddings_model = OpenAIEmbeddings()
    
    # 创建向量数据库并存储文本向量
    db = FAISS.from_documents(texts, embeddings_model)
    
    # 创建检索器
    retriever = db.as_retriever()
    
    # 创建带记忆的检索增强对话链
    qa = ConversationalRetrievalChain.from_llm(
        llm=model,
        retriever=retriever,
        memory=memory
    )
    
    # 调用链获取回答
    response = qa.invoke({"chat_history": memory, "question": question})
    
    return response
```

### 4、关键逻辑说明

1. **临时文件处理**：解决 “内存中 PDF 无路径” 的问题，通过`temp.pdf`临时存储，加载后立即删除，避免残留。
2. **对话记忆连续性**：`memory`从外部传入（如`ConversationBufferMemory`实例），确保多次调用函数时历史对话不丢失。
3. **RAG 全流程整合**：函数内封装了 “加载→分割→嵌入→存储→检索→生成” 的完整 RAG 步骤，用户无需关注底层细节，直接调用即可得到基于 PDF 的回答。

## 三、创建网站页面

基于 Streamlit 构建的前端界面，可与后端`qa_agent`函数衔接，实现 “用户上传 PDF→提问→获取 AI 回答” 的完整交互流程。以下是前端实现的核心步骤与功能说明：

### 1、前期准备

#### 1. 新建文件与依赖

* 新建前端主文件（`main.py`），用于编写页面逻辑；
* 确保已安装`streamlit`（前端框架），若未安装可通过`pip install streamlit`添加。

```python
智能PDF问答工具/
├─ utils.py          # 核心逻辑：PDF处理与AI交互
├─ requirements.txt  # 依赖清单（需包含langchain、openai、pypdf等）
└─ main.py           # 前端界面
```

#### 2. 导入核心模块

```python
import streamlit as st
from langchain.memory import ConversationBufferMemory
from utils import qa_agent
```

### 2、页面初始化与基础配置

#### 1. 页面标题与运行

```python
# 设置页面标题
st.title("AI智能PDF文档工具")

# 终端运行命令（启动前端）：
# streamlit run app.py
```

* 运行后会自动打开浏览器页面，勾选 “Always rerun” 可实时预览代码修改效果。

#### 2. 侧边栏：API 密钥输入

```python
# 创建侧边栏，用于输入API密钥
with st.sidebar:
    openai_api_key = st.text_input("请输入OpenAI API密钥", type="password")
```

* `type="password"`确保输入的密钥以密码形式显示（隐藏明文）。

#### 3. 对话记忆初始化（保持连续性）

```python
# 初始化对话记忆（仅在首次加载或记忆丢失时创建）
if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(
        return_messages=True,  # 以消息列表形式存储（而非字符串）
        memory_key="chat_history",  # 与后端链的记忆键名一致
        output_key="answer"  # 与后端链的输出键名一致
    )
```

* 记忆存储在`st.session_state`中，避免页面刷新时重置历史对话。

### 3、核心交互组件

#### 1. PDF 文件上传器

```python
# 创建PDF上传组件（仅允许上传.pdf文件）
uploaded_file = st.file_uploader(
    "上传你的PDF文档",
    type="pdf"  # 限制文件类型为PDF
)
```

* 非 PDF 文件会被过滤（灰色不可选），确保后端能正确解析。

#### 2. 用户提问输入框

```python
# 提问输入框（未上传PDF时禁用）
user_question = st.text_input(
    "请输入你的问题（基于上传的PDF内容）",
    disabled=not uploaded_file  # 若未上传PDF，输入框灰显不可用
)
```

* 逻辑：只有上传 PDF 后，用户才能输入问题（避免无上下文的无效提问）。

### 4、问答逻辑与结果展示

#### 1. 触发问答的条件判断

```python
# 当用户已上传PDF、输入问题且提供API密钥时，触发回答
if uploaded_file and user_question and openai_api_key:
    # 显示加载状态（告知用户AI正在生成回答）
    with st.spinner("AI正在分析并生成回答..."):
        # 调用后端qa_agent函数，获取回答
        response = qa_agent(
            openai_api_key=openai_api_key,
            memory=st.session_state.memory,
            uploaded_file=uploaded_file,
            question=user_question
        )
    
    # 展示AI回答
    st.subheader("答案")
    st.write(response["answer"])
    
    # 保存历史对话到session_state（用于展示）
    st.session_state.chat_history = response["chat_history"]
```

#### 2. 历史对话展示（折叠面板）

```python
# 展示历史对话（若存在）
if "chat_history" in st.session_state and st.session_state.chat_history:
    with st.expander("历史消息"):  # 折叠面板，点击展开
        # 遍历历史对话（每轮包含用户提问和AI回答）
        for i in range(0, len(st.session_state.chat_history), 2):
            # 用户消息（偶数索引）
            st.write(f"**你**：{st.session_state.chat_history[i].content}")
            # AI消息（奇数索引，若存在）
            if i + 1 < len(st.session_state.chat_history):
                st.write(f"**AI**：{st.session_state.chat_history[i+1].content}")
            # 非最后一轮时，添加分隔线
            if i < len(st.session_state.chat_history) - 2:
                st.divider()
```

* 每轮对话按 “用户提问→AI 回答” 成对展示，用分隔线区分不同轮次。

### 5、完整代码总结（main.py）

```python
# 导入Streamlit库（用于构建网页界面）
import streamlit as st

# 导入对话记忆类（用于保存历史对话）
from langchain.memory import ConversationBufferMemory
# 导入后端问答函数（处理PDF问答逻辑）
from utils import qa_agent


# 设置页面标题
st.title("📑 AI智能PDF问答工具")

# 侧边栏：用于输入OpenAI API密钥
with st.sidebar:
    # 密码框输入API密钥（隐藏明文）
    openai_api_key = st.text_input("请输入OpenAI API密钥：", type="password")
    # 显示获取API密钥的链接
    st.markdown("[获取OpenAI API key](https://platform.openai.com/account/api-keys)")

# 初始化对话记忆（存到session_state，避免页面刷新丢失）
if "memory" not in st.session_state:
    st.session_state["memory"] = ConversationBufferMemory(
        return_messages=True,  # 以消息对象形式存储
        memory_key="chat_history",  # 记忆在链中的键名
        output_key="answer"  # 输出结果中回答的键名
    )

# PDF文件上传组件（仅允许上传.pdf格式）
uploaded_file = st.file_uploader("上传你的PDF文件：", type="pdf")

# 问题输入框（未上传PDF时禁用）
question = st.text_input("对PDF的内容进行提问", disabled=not uploaded_file)

# 提示用户输入API密钥（若未输入但已上传文件和问题）
if uploaded_file and question and not openai_api_key:
    st.info("请输入你的OpenAI API密钥")

# 当文件、问题、API密钥都齐全时，调用后端问答函数
if uploaded_file and question and openai_api_key:
    # 显示加载状态（提示用户等待）
    with st.spinner("AI正在思考中，请稍等..."):
        # 调用qa_agent获取回答
        response = qa_agent(openai_api_key, st.session_state["memory"],
                            uploaded_file, question)
    # 展示回答标题
    st.write("### 答案")
    # 显示AI的回答内容
    st.write(response["answer"])
    # 保存历史对话到session_state
    st.session_state["chat_history"] = response["chat_history"]

# 展示历史对话（若存在）
if "chat_history" in st.session_state:
    # 折叠面板展示历史消息
    with st.expander("历史消息"):
        # 遍历历史对话（每2条为一轮：用户+AI）
        for i in range(0, len(st.session_state["chat_history"]), 2):
            human_message = st.session_state["chat_history"][i]  # 用户消息
            ai_message = st.session_state["chat_history"][i+1]   # AI消息
            st.write(human_message.content)  # 显示用户消息内容
            st.write(ai_message.content)     # 显示AI消息内容
            # 非最后一轮时，添加分隔线
            if i < len(st.session_state["chat_history"]) - 2:
                st.divider()
```

---

---
url: /常用框架/SpringBoot/SpringBoot程序开发/6_使用内置对象.md
---

# 使用内置对象

SpringBoot中控制器的形式和SpringMVC中是一样的，因此在程序中使用JSP的内置对象也可以按照与SpringMVC同样的方式进行。

## 一、方法获取内置对象

```java
@GetMapping("/object")
public Object object(HttpServletRequest request, HttpServletResponse response) {
    Map<String, String> map = new HashMap<>();
    map.put("客户端IP地址", request.getRemoteAddr());
    map.put("客户端响应编码", request.getCharacterEncoding());
    map.put("SessionID", request.getSession().getId());
    map.put("项目真实路径", request.getServletContext().getRealPath("/"));
    return map;
}
```

此时采用了与SpringMVC同样的方式来获取内置对象，并且将所有的信息保存在Map集合中，最后以Restful形式返回获取的信息（将Map集合自动变为JSON数据）。

访问http://localhost:8080/object，返回如下结果

```json
{
    "客户端IP地址": "127.0.0.1",
    "SessionID": "node01d2647f0gxpak1t9m2tw8bva8p0",
    "项目真实路径": "C:\\Users\\xxl\\AppData\\Local\\Temp\\jetty-docbase.80.13183635351441868963",
    "客户端响应编码": "UTF-8"
}
```

## 二、ServletRequestAttributes获取内置对象

除了在控制器的方法上使用参数来接收内置对象外，也可以利用ServletRequestAttributes形式来获取内置对象。

```java
@GetMapping("/object2")
public Object object2() {
    // 获取 HttpServletRequest 内置对象
    HttpServletRequest request = ((ServletRequestAttributes) RequestContextHolder.getRequestAttributes()).getRequest();
    // 获取 HttpServletResponse 内置对象
    HttpServletResponse response = ((ServletRequestAttributes) RequestContextHolder.getRequestAttributes()).getResponse();
    Map<String, String> map = new HashMap<>();
    map.put("客户端IP地址", request.getRemoteAddr());
    map.put("客户端响应编码", request.getCharacterEncoding());
    map.put("SessionID", request.getSession().getId());
    map.put("项目真实路径", request.getServletContext().getRealPath("/"));
    return map;
}
```

本程序实现了与上一程序完全相同的处理效果，唯一的区别是，控制器的方法不再需要明确地接收内置对象的参数。

---

---
url: /常用框架/EasyCaptcha/0_使用EasyCaptcha生成验证码.md
---

# 使用EasyCaptcha生成验证码

## EasyCaptcha效果

**算数类型：**

![验证码](0_使用EasyCaptcha生成验证码.assets/mskKPg.png) ![验证码](0_使用EasyCaptcha生成验证码.assets/msknIS.png) ![验证码](0_使用EasyCaptcha生成验证码.assets/mskma8.png)

**英文字母：**

![验证码](0_使用EasyCaptcha生成验证码.assets/msFrE8.png) ![验证码](/assets/msF0DP.BLUUwrnG.png) ![验证码](0_使用EasyCaptcha生成验证码.assets/msFwut.png)

**中文类型：**

![验证码](0_使用EasyCaptcha生成验证码.assets/mskcdK.png) ![验证码](0_使用EasyCaptcha生成验证码.assets/msk6Z6.png) ![验证码](0_使用EasyCaptcha生成验证码.assets/msksqx.png)

**GIF效果：**

![验证码](/assets/msFzVK.DU-nGNoe.gif) ![验证码](/assets/msFvb6.BbX3zj0b.gif) ![验证码](/assets/msFXK1.Br0VIji3.gif)

**内置字体：**

![验证码](0_使用EasyCaptcha生成验证码.assets/msAVSJ.png) ![验证码](0_使用EasyCaptcha生成验证码.assets/msAAW4.png) ![验证码](0_使用EasyCaptcha生成验证码.assets/msAkYF.png)

## 基本使用

### 依赖引入

```xml
<dependency>
    <groupId>com.pig4cloud.plugin</groupId>
    <artifactId>captcha-spring-boot-starter</artifactId>
    <version>2.2.2</version>
</dependency>
```

### Controller层代码

在SpringBoot项目中一般生成图片后，转换成base64格式的字符串，前端直接在img标签的src设置返回的base64字符串即可展示图片，下面展示算数类型的验证码是如何生成的。

```java
@GetMapping("/getCode")
public String getCode() {
    SpecCaptcha specCaptcha = new SpecCaptcha(130, 48, 5);
    String verCode = specCaptcha.text().toLowerCase();
    log.info("生成的验证码结果为：{}，在项目中应该将结果保存到Redis中", verCode);

    // 将base64返回给前端
    return specCaptcha.toBase64();
}
```

base64在线转换工具：https://tool.chinaz.com/tools/imgtobase/

### 自定义验证码：类型、字体

CharType：验证码字符类型

| 类型               | 描述           |
| ------------------ | -------------- |
| TYPE\_DEFAULT       | 数字和字母混合 |
| TYPE\_ONLY\_NUMBER   | 纯数字         |
| TYPE\_ONLY\_CHAR     | 纯字母         |
| TYPE\_ONLY\_UPPER    | 纯大写字母     |
| TYPE\_ONLY\_LOWER    | 纯小写字母     |
| TYPE\_NUM\_AND\_UPPER | 数字和大写字母 |

Font：字体

| 字体            | 效果                                                  |
| --------------- | ----------------------------------------------------- |
| Captcha.FONT\_1  | ![img](/assets/msMe6U.C6EsLRp5.png) |
| Captcha.FONT\_2  | ![img](0_使用EasyCaptcha生成验证码.assets/msMAf0.png) |
| Captcha.FONT\_3  | ![img](0_使用EasyCaptcha生成验证码.assets/msMCwj.png) |
| Captcha.FONT\_4  | ![img](0_使用EasyCaptcha生成验证码.assets/msM9mQ.png) |
| Captcha.FONT\_5  | ![img](/assets/msKz6S.CxfYE-JB.png) |
| Captcha.FONT\_6  | ![img](0_使用EasyCaptcha生成验证码.assets/msKxl8.png) |
| Captcha.FONT\_7  | ![img](0_使用EasyCaptcha生成验证码.assets/msMPTs.png) |
| Captcha.FONT\_8  | ![img](0_使用EasyCaptcha生成验证码.assets/msMmXF.png) |
| Captcha.FONT\_9  | ![img](0_使用EasyCaptcha生成验证码.assets/msMVpV.png) |
| Captcha.FONT\_10 | ![img](0_使用EasyCaptcha生成验证码.assets/msMZlT.png) |

代码

```java
@GetMapping
public String getCode() throws IOException, FontFormatException {
    SpecCaptcha specCaptcha = new SpecCaptcha(130, 48, 5);
    String verCode = specCaptcha.text().toLowerCase();
    log.info("生成的验证码结果为：{}，在项目中应该将结果保存到Redis中", verCode);
    // 设置字体
    specCaptcha.setFont(Captcha.FONT_1);  // 有默认字体，可以不用设置
    // 设置类型，纯数字、纯字母、字母数字混合
    specCaptcha.setCharType(Captcha.TYPE_ONLY_NUMBER);
    // 将base64返回给前端
    return specCaptcha.toBase64();
}
```

### 自定义验证码：数字计算验证码

```java
@GetMapping
public String getCode() {
    ArithmeticCaptcha captcha = new ArithmeticCaptcha(130, 48);
    captcha.setLen(3);  // 几位数运算，默认是两位
    captcha.getArithmeticString();  // 获取运算的公式：3+2=?
    String code = captcha.text();// 获取运算的结果：5
    log.info("生成的验证码结果为：{}，在项目中应该将结果保存到Redis中", code);
    // 将base64返回给前端
    return captcha.toBase64();
}
```

---

---
url: /常用框架/SpringBoot/SpringBoot程序开发/7_使用Jetty容器.md
---

# 使用Jetty容器

## 一、前言

在我们使用 SpringBoot 开发Web应用时，会引入spring-boot-starter-web这个starter组件，其内嵌 Jetty , Tomcat , Undertow三种servlet 容器供大家选择，默认是Tomcat容器，所以我们平时新建项目启动起来，会看见Tomcat相关的一些信息。

```sh
Tomcat initialized with port(s): 8080 (http)
```

可以看出 Spring Boot 默认使用的是 Tomcat 容器启动的。

## 二、Tomcat 和Jetty对比

​		在日常工作中，Spring Boot 默认推荐的配置并一定适用于所有情况，根据项目配置以及环境，选择合适的容器，才能更好的搭建项目。

​		Tomcat 和Jetty都是一种Servlet引擎，可以将它们比作为中国与美国的关系，虽然 Jetty 正常成长为一个优秀的 Servlet 引擎，但是目前的 Tomcat 的地位仍然难以撼动。相比较来看，它们都有各自的优点与缺点。Tomcat 经过长时间的发展，它已经广泛的被市场接受和认可，相对 Jetty 来说 Tomcat 还是比较稳定和成熟，尤其在企业级应用方面，Tomcat 仍然是第一选择。但是随着 Jetty 的发展，Jetty 的市场份额也在不断提高。

  Jetty的架构比Tomcat的更为简单，Jetty的架构是基于Handler来实现的，主要的扩展功能都可以用Handler来实现，扩展简单。而Tomcat的架构是基于容器设计的，进行扩展是需要了解Tomcat的整体设计结构，不易扩展。

  Jetty和Tomcat性能方面差异不大。 Jetty可以同时处理大量连接而且可以长时间保持连接，适合于web聊天应用等等。Jetty的架构简单，因此作为服务器，Jetty可以按需加载组件，减少不需要的组件，减少了服务器内存开销，从而提高服务器性能。Jetty默认采用NIO（非阻塞IO）结束在处理I/O请求上更占优势，在处理静态资源时，性能较高。Tomcat适合处理少数非常繁忙的链接，也就是说链接生命周期短的话，Tomcat的总体性能更高。 另外，Tomcat默认采用BIO（阻塞IO）处理I/O请求，在处理静态资源时，性能较差。

  作为一个标准的 Servlet 引擎，它们都支持标准的 Servlet 和Java EE 规范。Jetty的应用更加快速，修改简单，对新的Servlet规范的支持较好。 Tomcat目前应用比较广泛，对JavaEE和Servlet的支持更加全面，很多特性会直接集成进来。

## 三、修改依赖，使用Jetty容器

默认情况下，因为 spring-boot-starter-web 自带了 Tomcat，所以我们要使用其它的容器的话，需要将其依赖包排除掉并重新引入新容器。在pom.xml文件，添加spring-boot-starter-jetty 依赖，同时我们需要排除 spring-boot-starter-web 默认的 spring-boot-starter-tomcat 依赖。

```xml
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-web</artifactId>
    <exclusions>
        <exclusion>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-tomcat</artifactId>
        </exclusion>
    </exclusions>
</dependency>
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-jetty</artifactId>
</dependency>
```

## 四、Jetty 属性配置

| **属性**                                    | **默认值** | **说明**          |
| ------------------------------------------- | ---------- | ----------------- |
| **server.jetty.accesslog.enabled**          | false      | 是否打开Jetty日志 |
| **server.jetty.accesslog.dir**              |            | 访问日志所在目录  |
| **server.jetty.threads.max**                |            | 最大线程数        |
| **server.jetty.threads.min**                |            | 最小线程数        |
| **server.jetty.threads.max-queue-capacity** |            | 最大队列容量      |
| **server.jetty.threads.idle-timeout**       |            | 线程最大空闲时间  |

## 五、测试

修改完pom.xml文件后，需要重新启动SpringBoot项目，此时就可以在日志中看到如下信息。

```sh
Jetty started on port(s) 8080 (http/1.1) with context path '/'
```

程序可以使用小巧的Jetty容器来运行SpringBoot项目，但是这种做法也仅仅是在开发过程中使用，在实际的生产环境下依然推荐使用Tomcat作为Web容器。

---

---
url: /daily/软件设计师/03_数据结构.md
---

# 数据结构

## 一、完整数据结构

### 🔎1.线性结构

* 线性表
* 栈和队列
* 串

### 🔎2.数组、矩阵和广义表

### 🔎3.树

* 树和二叉树的定义
* 二叉树的性质与存储结构
* 二叉树的遍历
* 线索二叉树
* 最优二叉树(哈夫曼树)
* 树和森林

### 🔎4.图

* 图的定义和存储
* 图的遍历
* 深度优先搜索
* 广度优先搜索
* 生成树和最小生成树
* 拓扑结构和关键路径

### 🔎5.查找

* 查找基本概念
* 静态查找表的查找方法
* 顺序查找
* 折半查找
* 分块查找
* 动态查找表
* 二叉排序树
* 平衡二叉树
* 哈希表

### 🔎6.排序

* 排序基本概念

* 简单排序

* 希尔排序

  改进的插入排序

* 快速排序

* 堆排序

* 归并排序

* 基数排序

* 外部排序

## 二、线性结构

```
:evergreen_tree:
```

### :evergreen\_tree:线性表

#### :blossom:概念

线性结构是指每个元素最多只有一个出度和一个入度，表现为一条线状。线性表是一种特殊的线性结构分为顺序表和链表。

* 顺序表：顺序表是使用一段连续的存储空间来存储线性表的元素，可以通过下标直接访问元素。顺序表的特点是插入和删除操作较慢，但是随机访问元素的效率较高。
* 链表：链表是通过一系列的节点来存储线性表的元素，每个节点包含数据域和指向下一个节点的指针。链表的特点是插入和删除操作较快，但是随机访问元素的效率较低。

在线性结构中，除了顺序表和链表，还有一些其他的线性结构，如栈和队列。栈是一种特殊的线性表，只能在表的一端进行插入和删除操作，遵循先进后出（LIFO）的原则。队列也是一种特殊的线性表，只能在表的一端进行插入操作（队尾），在表的另一端进行删除操作（队头），遵循先进先出（FIFO）的原则。

线性结构中元素在计算机内存中的存储方式，主要有顺序存储和链式存储两种方式。

* 顺序存储：顺序存储是将线性表中的元素依次存储在一组地址连续的存储单元中，使得逻辑上相邻的元素在物理上也相邻。顺序存储的特点是通过元素的下标可以直接访问元素，因此查找效率高。插入和删除元素时需要移动其他元素，效率较低。
* 链式存储：链式存储是通过存储各数据元素的结点的地址来实现元素的存储，每个结点包含数据域和指向下一个结点的指针。链式存储的特点是插入和删除元素时只需要修改指针，不需要移动其他元素，因此效率较高。但是链式存储无法直接访问中间的元素，需要从头节点开始顺序遍历。

![img](/assets/310f5bfe69f7905885b47fda9d78039f.7xkhFd3D.png)

### :evergreen\_tree:顺序存储和链式存储区别

在空间方面，链表需要额外存储指针，导致空间浪费。

在时间方面：

* 链表在插入和删除元素时效率更高，因为只需要修改指针指向，而顺序表因为地址连续，插入或删除元素后需要移动其他节点。
* 在读取和查找元素时，顺序表效率更高，因为物理地址连续，可以通过索引快速定位，而链表需要从头节点开始逐个查找。

![img](/assets/08dcc56facfa2c22e7ed037c4a3d6477.Dh7agzlX.png)

#### :blossom:单链表的插入和删除

单链表的插入

![img](/assets/8afa793faf5be94e3848b3a5f45cf34d.CfETfXE8.png)

在上图中p所指向的节点后插入s所指向的节点，操作为：

```clike
s->next=p->next;
p->next=s;
```

单链表的删除

![img](/assets/2acfb782597741e53c2e65eb7c387597.BiNHmbqq.png)

在单链表中删除p所指向节点的后继节点q时，操作为：

```clike
p->next=p->next->next;
free(q);
```

### :evergreen\_tree:栈和队列

栈、队列和循环队列是常见的数据结构，用于存储和操作数据。

1. 栈（Stack）是一种后进先出（Last In First Out，LIFO）的数据结构。它只允许在栈的一端进行插入和删除操作，这一端称为栈顶。栈的常用操作有入栈（push）、出栈（pop）和获取栈顶元素（top）。栈可以用数组或链表实现。
2. 队列（Queue）是一种先进先出（First In First Out，FIFO）的数据结构。它允许在队列的一端（队尾）插入新元素，而在另一端（队头）删除元素。队列的常用操作有入队（enqueue）、出队（dequeue）和获取队头元素（front）。队列可以使用数组或链表实现。
3. 循环队列（Circular Queue）是一种具有固定大小的队列，它可以像队列一样先进先出，但是它的队尾和队头是相连的。当队尾到达数组的末尾时，它可以循环回到数组的开头。循环队列的常用操作有入队（enqueue）、出队（dequeue）和获取队头元素（front）。循环队列可以使用数组实现，通过维护两个指针（队头和队尾的索引）来实现循环。

![img](/assets/b9110e5d409dac88000ae0a5aa05c808.DjLUhuFi.png)

在循环队列中，头指针指向第一个元素，尾指针指向最后一个元素的下一个位置。当队列为空时，头尾指针相等；当队列满时，头尾指针也相等，无法区分。因此，一般会将队列空出一个元素位置，这样队列满的条件就是尾指针的下一个位置等于头指针。考虑到循环队列特性，需要使用最大元素数取余运算来实现循环，即`(tail + 1) % size = head`。循环队列的长度可以通过`(Q.tail - Q.head) % size`公式得到。另外，优先队列是一种特殊的队列，其中的元素被赋予了优先级。在访问元素时，具有最高优先级的元素最先被删除。优先队列常使用堆来存储元素，因为堆的顺序不是按照元素在队列中的顺序来决定的。

### :evergreen\_tree:串

#### :blossom:串的定义

| 术语   | 定义                                                         |
| :----- | :----------------------------------------------------------- |
| 空串   | 长度为0的字符串，没有任何字符。                              |
| 空格串 | 由一个或多个空格组成的串，空格是空白字符，占一个字符长度。   |
| 子串   | 串中任意长度的连续字符构成的序列称为子串。含有子串的串称为主串，空串是任意串的子串。 |

#### :blossom:串的匹配

| 算法               | 定义                                                         |
| :----------------- | :----------------------------------------------------------- |
| 模式匹配算法       | 子串的定位操作，用于查找子串在主串中第一次出现的位置的算法。 |
| 基本的模式匹配算法 | 也称为布鲁特一福斯算法，其基本思想是从主串的第1个字符起与模式串的第1个字符比较，若相等，则继续逐个字符进行后续的比较；否则从主串中的第2个字符起与模式串的第1个字符重新比较，直至模式串中每个字符依次和主串中的一个连续的字符序列相等时为止，此时称为匹配成功，否则称为匹配失败。 |
| KMP算法            | 对基本模式匹配算法的改进，其改进之处在于：每当匹配过程中出现相比较的字符不相等时，不需要回溯主串的字符位置指针，而是利用已经得到的“部分匹配”结果将模式串向右“滑动”尽可能远的距离，再继续进行比较。 |

KMP算法相比于基本的模式匹配算法差别：

* 基本的模式匹配算法：匹配失败从第二位开始继续
* KMP算法：匹配失败从失败位置开始继续

## 三、数组、矩阵和广义表

### :evergreen\_tree:数组结构

#### 🦋数组的表示

数组的特点使得它非常适合用于存储和操作大量数据。由于数组在内存中是连续存储的，所以可以通过下标直接访问数组中的元素，而不需要像链表那样遍历整个结构。这样可以提高访问元素的效率。

另外，由于数组的元素类型相同且结构一致，可以利用数组的特性进行高效的数据处理和计算。例如，可以通过循环遍历数组中的元素进行逐个计算或操作。

数组的下标关系具有上下界的约束，可以有效地控制数组的访问和操作。通过下标，可以直接定位数组中的元素，而不需要进行复杂的查找操作。

虽然数组的长度是固定的，不支持插入和删除运算，但是可以通过重新分配内存空间来实现对数组的扩展或缩小。这样可以灵活地管理数组的大小。

假设有一个3行2列的数组：

```clike
[[1, 2],  
 [3, 4],  
 [5, 6]]
```

复制

行向量形式表示时，将每一行都排列在一行中：

```clike
[1, 2, 3, 4, 5, 6]
```

复制

列向量形式表示时，将每一列都排列在一列中：

```clike
[1, 3, 5, 2, 4, 6]
```

复制

行向量形式将数组按照行的方式展开成一行，而列向量形式将数组按照列的方式展开成一列。这样的表示方式有时可以方便进行矩阵运算和数据处理。

#### 🦋数组存储地址

数组在内存中是连续存储的，因此数组名本身就可以看作是存储数组首元素地址的指针。当我们定义一个数组时，编译器会分配一段连续的内存空间来存储数组元素，并将数组名指向该内存空间的首地址。

例如，假设我们定义了一个int类型的数组arr：

```bash
int arr[5] = {1, 2, 3, 4, 5};
```

复制

在内存中，该数组的元素将被连续存储，如下所示：

```bash
地址        内容
1000        1
1004        2
1008        3
1012        4
1016        5
```

复制

数组名arr在这种情况下可以看作是存储地址1000的指针。我们可以通过使用指针来访问数组元素，例如，访问arr的第一个元素可以使用*arr或者arr0，访问第二个元素可以使用*(arr+1)或者arr1，以此类推。

![img](/assets/1f7e096d8cda19f1c579a2a54cdc0b2b.BBZ8EBuP.png)

![img](/assets/db9eef43845cd4e38dff999eb2aa8564.zvWfHt7D.png)

### :evergreen\_tree:矩阵结构

矩阵是一种常见的数据结构，它由行和列组成的二维数组。矩阵可以用于表示和处理多种类型的数据，如数值、图像、文本等。

在计算机科学中，矩阵通常用于表示图形图像和图像处理算法。例如，图像可以表示为一个矩阵，其中每个元素表示一个像素的颜色值。通过对矩阵进行操作，可以实现图像的旋转、缩放、滤波等处理。

矩阵结构在数值计算和科学计算中也非常重要。矩阵可以用于表示线性方程组、矩阵乘法、求特征值和特征向量等数学运算。通过矩阵运算，可以解决线性方程组、最小二乘拟合、最优化等问题。

在编程中，矩阵通常用二维数组来表示。可以使用索引访问矩阵中的元素，并且可以使用循环遍历矩阵中的所有元素。还可以定义各种操作来处理矩阵，如矩阵相加、相乘等。

![img](/assets/7b702e994f39abfc683af04a7c9006e0.Ck97p9X5.png)

以下是一些常见的矩阵结构分类：

1. 方阵和非方阵：方阵是行数和列数相等的矩阵，即n x n的矩阵。非方阵则是行数和列数不相等的矩阵。
2. 稀疏矩阵和稠密矩阵：稀疏矩阵是指其中绝大多数元素为0的矩阵。而稠密矩阵则是指其中绝大多数元素不为0的矩阵。
3. 对称矩阵和非对称矩阵：对称矩阵是指以主对角线为对称轴对称的矩阵，即Ai = Aj。非对称矩阵则是指不满足对称性质的矩阵。
4. 上三角矩阵和下三角矩阵：上三角矩阵是指主对角线以下的元素全为0的矩阵，即Ai = 0，当i > j。下三角矩阵则是指主对角线以上的元素全为0的矩阵，即Ai = 0，当i < j。
5. 对角矩阵和非对角矩阵：对角矩阵是指主对角线以外的元素全为0的矩阵。非对角矩阵则是指至少有一个主对角线以外的元素不为0的矩阵。

三元组结构是一种常用的存储矩阵的方式，它将矩阵中的每个非零元素存储为一个三元组，包括该元素的行索引、列索引和值。

通常情况下，三元组结构中的元素按矩阵的行优先的方式进行存储，即先按行遍历矩阵，再按列遍历。因此，三元组结构的存储方式会将矩阵中的非零元素按照行的顺序排列，并保持它们在矩阵中的相对位置不变。

以一个4x5的矩阵为例：

```clike
1 0 0 2 0
0 0 3 0 4
0 5 0 0 0
6 0 0 7 8
```

用三元组结构进行存储的结果为：

```clike
(0, 0, 1)
(0, 3, 2)
(1, 2, 3)
(1, 4, 4)
(2, 1, 5)
(3, 0, 6)
(3, 3, 7)
(3, 4, 8)
```

其中，每个三元组表示一个非零元素的行索引、列索引和值。

![img](/assets/b178236604bb96c82ba8ffa97c407668.DTN7X-vM.png)

### :evergreen\_tree:广义表

广义表是一种扩展的线性表，它可以存储不同数据类型的元素，包括原子元素和子表元素。

在广义表中，原子元素指的是不可再分的基本元素，例如整数、字符、布尔值等。子表元素则是指广义表中的另一个广义表，也就是说广义表可以嵌套存储。

广义表的存储结构通常可以使用链表或数组实现。如果使用链表实现，每个节点的数据域可以存储原子元素或指向子表的指针；如果使用数组实现，通常需要预先确定广义表的最大深度，并为每个元素分配固定大小的空间。

广义表的操作包括创建、插入、删除、修改、遍历等。递归是广义表操作的常用方法，可以通过递归遍历广义表的每个元素，从而实现各种操作。

广义表在实际应用中有广泛的用途，例如在编程语言解析中，可以使用广义表来表示语法树；在图形学中，可以使用广义表来表示复杂的图形结构；在人工智能中，可以使用广义表来表示知识库等。

广义表一般记为：
$$
LS=（α\_1,α\_2,···,α\_n）
$$
LS代表广义表的表名，αi代表广义表的元素，可以是表（子表）或者数据元素（原子）。n代表广义表的长度，即最外层包含的元素个数，当n=0时，广义表为空表。递归定义的重数是广义表的深度，即定义中所包含括号的个数（单边括号的个数），原子的深度为0，空表的深度为1。

head()和tail()是广义表的两个基本操作。head()用于取得广义表的第一个元素，无论是子表还是原子；tail()用于取得广义表中除了第一个元素之外的所有元素构成的表。需要注意的是，如果广义表是空表或只包含一个元素，则tail()操作返回一个空表。

## 四、树

### :evergreen\_tree:树的定义

在数据结构中，树是一种非线性的数据结构，它由一组节点和一组连接节点的边组成。树的定义如下：

1. 树由节点组成，每个节点包含一个值和指向零个或多个子节点的指针。
2. 有一个节点被指定为根节点，它没有父节点。根节点是树的起始点。
3. 除了根节点外，每个节点都有一个父节点。
4. 除了叶节点外，每个节点都可以有一个或多个子节点。
5. 每个节点之间的连接称为边。

树的形状类似于现实生活中的树，根节点对应树的顶部，叶节点对应树的底部。每个节点可以有任意数量的子节点，但每个节点只能有一个父节点。

![img](/assets/cc39a9f5e40ddb8d92061b4bc6622054.DDrfj_7y.png)

树可以有不同的类型，如二叉树、二叉搜索树、红黑树等。这些类型的树有各自的特点和应用场景。树结构在计算机科学中有广泛的应用，例如文件系统的目录结构、数据库索引、编译器语法分析等。

| 概念                                                         | 定义                                                         |
| :----------------------------------------------------------- | :----------------------------------------------------------- |
| 双亲、孩子和兄弟                                             | 结点的子树的根称为该结点的孩子；相应地，该结点称为其子结点的双亲。具有相同双亲的结点互为兄弟。 |
| [结点的度](https://blog.csdn.net/liuyiming2019/article/details/113463039) | 一个结点的子树的个数，记为该结点的度。![在这里插入图片描述](/assets/087a5dff3f8bfdd22f1322153a8aec47.B-vgsWKl.png) |
| 叶子结点                                                     | 叶子结点也称为终端结点。指度为0的结点。                      |
| 内部结点                                                     | 度不为0的结点，也称为分支结点或非终端结点。除根结点以外，分支结点也称为内部结点。 |
| 结点的层次                                                   | 根为第一层，根的孩子为第二层，以此类推，若某结点在第 i 层，则其孩子结点在第 i+1 层。 |
| 树的高度（深度）                                             | 一棵树的最大层数，记为树的高度（或深度）。                   |
| 有序（无序）树                                               | 若将树中结点的各子树看成是从左到右具有次序的，即不能交换，则称该树为有序树，否则称为无序树。 |

### :evergreen\_tree:二叉树的定义和性质

二叉树是一种常见的数据结构，它由节点组成，每个节点最多有两个子节点：左子节点和右子节点。每个节点都包含一个值或者数据，值可以是任何类型的数据。

二叉树的特点是每个节点最多有两个子节点，而且子节点的位置是有序的，即左子节点在父节点的左边，右子节点在父节点的右边。

对于二叉树，每一个节点都可以看作是一个子二叉树的根节点。如果一个节点没有子节点，我们称其为叶子节点。另外，如果某个节点不是叶子节点，则它至少有一个子节点。

二叉树可以有不同的特殊类型，比如满二叉树、完全二叉树等。在满二叉树中，除了叶子节点外的每个节点都有两个子节点，并且所有的叶子节点都在同一层上。在完全二叉树中，除了最后一层，其他层都是满的，并且最后一层的叶子节点都尽可能地靠左排列。

二叉树可以用来表示各种各样的数据，比如二叉查找树（Binary Search Tree，简称BST），用来实现快速的查找和插入操作。二叉树还可以用来表示表达式，构建语法树，以及图的遍历等。

二叉树的重要特性如下：

![img](/assets/4c6248cf8495d3cec2c7a96978f0f307.D1G22Ai3.png)

设一棵二叉树上叶结点数为n0，单分支结点数为n1，双分支结点数为n2，则`总结点数=n0+n1+n2`。

在一棵二叉树中，所有结点的分支数（即度数）应等于单分支结点数加上双分支结点数的2倍，即`总的分支数=n1+2n2`。

由于二叉树中除根结点以外，每个结点都有唯一的一个分支指向它，因此二叉树中：`总的分支数=总结点数-1`。

### :evergreen\_tree:二叉树的存储结构

二叉树的存储结构有三种常见的形式：

1、顺序存储：就是用一组连续的存储单元存储二叉树中的节点，按照从上到下，从左到右的顺序依次存储每个节点。对于深度为k的完全二叉树，除第k层外，其余每层中节点数都是上一层的两倍，由此，从一个节点的编号可推知其双亲、左孩子、右孩子结点的编号。假设有编号为i的节点，则有：

```bash
（1）若i=1，该结点为根结点，无双亲
（2）若i＞1，该阶段的双亲为（i+1）/2（取整数）。
（3）若2i≤n，则该结点的左孩子编号为2i，否则无左孩子
（4）若2i+1≤n，则该结点的右孩子编号为2i+1，否则无右孩子。
（5）若i奇数且不为1，则该结点左兄弟的编号为i-1，否则无左兄弟。
（6）若i为偶数且小于n，则该结点右兄弟的编号为i+1，否则无右兄弟。
```

2、链式存储：一般用二叉链表来存储二叉树节点，二叉链表中除了该节点本身的数据外，还存储有左孩子结点的指针、右孩子结点的指针，即一个数据+两个指针。每个二叉链表节点存储一个二叉树节点，头指针则指向根节点。

### :evergreen\_tree:二叉树的遍历

二叉树的遍历是指按照某种顺序访问二叉树的所有节点。常用的二叉树遍历方式有三种：前序遍历、中序遍历和后序遍历。

1. 前序遍历（preorder traversal）：**先访问根节点，再遍历左子树，最后遍历右子树**。具体步骤是：先访问当前节点，然后递归地前序遍历左子树，最后递归地前序遍历右子树。
2. 中序遍历（inorder traversal）：**先遍历左子树，再访问根节点，最后遍历右子树**。具体步骤是：先递归地中序遍历左子树，然后访问当前节点，最后递归地中序遍历右子树。
3. 后序遍历（postorder traversal）：**先遍历左子树，再遍历右子树，最后访问根节点**。具体步骤是：先递归地后序遍历左子树，然后递归地后序遍历右子树，最后访问当前节点。

此外，还有两种特殊的遍历方式：层序遍历和逆序遍历。

1. 层序遍历（level order traversal）：按层级从上到下、从左到右的顺序遍历二叉树。具体步骤是：使用队列，首先将根节点入队，然后循环执行以下操作：出队当前节点，访问当前节点，将当前节点的左子节点和右子节点分别入队。直到队列为空。
2. 逆序遍历：将前序、中序和后序遍历的访问顺序反转。例如，逆序前序遍历即为后序遍历。

![img](/assets/53fb558253bc1b62a67c1e8c570c787b.Bmb_3jfr.png)

示例：

前序：12457836

中序：42785136

后序：48752631

![img](/assets/8058175bbf363fd75b2a6082308e2b3a.DVL6Y2QH.png)

![img](/assets/2ffb745b8f16089bda1614eca35e689f.BMiGyMaO.png)

### :evergreen\_tree:二叉树的分类

二叉树可以根据特定的属性进行分类，以下是常见的二叉树分类：

1. 满二叉树（Full Binary Tree）：除了叶子节点，每个节点都有两个子节点。
2. 完全二叉树（Complete Binary Tree）：除了最后一层外，其它层的节点都是满的，最后一层的节点都靠左对齐。
3. 二叉搜索树（Binary Search Tree，BST）：对于每个节点，左子树上的所有节点的值都小于等于该节点的值，右子树上的所有节点的值都大于等于该节点的值。
4. 平衡二叉树（Balanced Binary Tree）：对于任意节点，它的左子树和右子树的高度差不大于1。
5. 线索二叉树（Threaded Binary Tree）：在二叉树节点中设置了指向前驱节点和后继节点的线索，可以方便地进行遍历。
6. 哈夫曼树（Huffman Tree）：用于数据压缩，根据数据出现的频率构建的二叉树，频率越高的节点离根节点越近。
7. Trie树（前缀树）：用于字符串的存储和搜索，每个节点代表一个字符串的字符，从根节点到叶子节点的路径表示一个完整的字符串。
8. B树（B-Tree）：一种平衡多路查找树，用于大规模数据的存储和检索，每个节点可以有多个子节点。

#### :seedling:完全二叉树和满二叉树

完全二叉树是一种特殊的二叉树，除了最后一层外，每一层的节点都是从左到右连续排列的，最后一层的节点从左到右填充。

满二叉树是一种特殊的完全二叉树，除了叶子节点外，每个节点都有两个子节点。满二叉树通常是一棵深度为h，拥有**2^h-1**个节点的二叉树。

![img](/assets/1bfa98a68700ec11d92111e3cc048308.BnZhPZZm.png)

#### :seedling:线索二叉树

线索二叉树是对二叉树进行加工，使其能够快速遍历所有节点。

在线索二叉树中，除了左右孩子指针，还添加了两个额外的指针：**前驱指针和后继指针**。这两个指针分别指向当前节点的前驱节点和后继节点。

对于一个二叉树来说，存在多种线索化方式。以下是两种常见的线索化方式：

1. 前序线索二叉树：在前序遍历过程中进行线索化。对于每个节点，先处理其前驱指针，然后处理左子树，再处理右子树，最后处理后继指针。对于树中的第一个节点，其前驱指针为空，对于树中的最后一个节点，其后继指针为空。
2. 中序线索二叉树：在中序遍历过程中进行线索化。对于每个节点，先处理其左子树，然后处理前驱指针，然后处理右子树，最后处理后继指针。对于树中的第一个节点，其前驱指针为空，对于树中的最后一个节点，其后继指针为空。

在线索二叉树中，通过前驱和后继指针，可以快速地找到节点的前驱节点和后继节点，从而实现快速遍历。

![img](/assets/8724e324d57e1593e231c5caac481f94.uWeZl-zi.png)

案例：假设有以下二叉树：

```bash
      1
    /   \
   2     3
  / \   / \
 4   5 6   7
```

对这棵二叉树进行中序线索化，得到的线索二叉树如下所示：

```bash
   4 - 2 - 5 - 1 - 6 - 3 - 7
```

在线索二叉树中，每个节点的左右孩子指针被替换为前驱和后继指针。

在这个例子中，节点1的左前驱指针指向节点5，右后继指针指向节点6；节点2的左前驱指针指向节点4，右后继指针指向节点1；节点3的左前驱指针指向节点6，右后继指针指向节点7；节点4的左前驱指针指向节点2，右后继指针指向节点5；节点5的左前驱指针指向节点4，右后继指针指向节点1；节点6的左前驱指针指向节点1，右后继指针指向节点3；节点7的左前驱指针指向节点3，右后继指针为空。

通过线索化后的二叉树，可以快速地找到每个节点的前驱和后继节点，从而实现快速的中序遍历。

#### :seedling:最优二叉树（哈夫曼树）

最优二叉树，也称为哈夫曼树，是一种特殊的二叉树结构，常用于编码和解码的应用中。

最优二叉树的特点是，频率高的节点离根节点较近，频率低的节点离根节点较远。通过这种方式，可以实现最小化编码的平均长度，从而达到最优的压缩效果。

构建最优二叉树的基本思路是，首先根据每个节点的权重（即出现频率）进行排序，然后选择权重最小的两个节点作为左右子节点，生成一个新的父节点，并将父节点的权重设置为左右子节点的权重之和。重复这个过程，直到所有节点构建成一棵树。

最优二叉树可以应用在哈夫曼编码中，通过树的路径来表示字符的编码，使得频率高的字符编码较短，频率低的字符编码较长，从而实现压缩数据的效果。

相关概念如下：

* 路径：树中一个结点到另一个结点之间的通路。
* 结点的路径长度：路径上的分支数目。
* 树的路径长度：根节点到达每一个叶子节点之间的路径长度之和。
* 权：节点代表的值。
* 结点的带权路径长度：该结点到根结点之间的路径长度乘以该节点的权值。
* 树的带权路径长度（树的代价）：树的所有叶子节点的带权路径长度之和。

![img](/assets/04d873218c5b2e076018fe75197ac518.DvCop24Z.png)

哈夫曼树的求法：给出一组权值，将其中两个最小的权值作为叶子节点，其和作为父节点，组成二叉树，而后删除这两个叶子节点权值，并将父节点的值添加到该组权值中。重复进行上述步骤，直至所有权值都被使用完。

构造出的哈夫曼树中，所有初始给出的权值都作为了叶子节点，此时，求出每个叶子节点的带权路径长度，而后相加，就是树的带权路径长度，这个长度是最小的。

![img](/assets/2b55ac6539e75f97425a31a06ad94205.DICMO0eC.png)

案例

假设有以下节点集合：

节点A，出现的频率为5

节点B，出现的频率为3

节点C，出现的频率为2

节点D，出现的频率为1

为了构建最优二叉树，我们可以按照以下步骤进行：

1. 将节点集合按照频率从小到大进行排序，得到排序后的节点集合：D，C，B，A。
2. 从集合中选择频率最小的两个节点作为叶子节点，并创建一个新的节点作为它们的父节点，父节点的频率为子节点的频率之和。在我们的例子中，D和C被选择为叶子节点，它们的频率之和为3。
3. 将新的父节点加入到集合中，并将集合重新排序。新的节点集合为：B，A，新的父节点。
4. 重复步骤2和步骤3，直到集合中只剩下一个节点。
5. 最后剩下的节点就是构建的最优二叉树的根节点。

根据上述步骤，我们可以得到如下的最优二叉树：

```bash
    B
   / \
  C   A
 /
D
```

![img](/assets/70f119086c71166e30146a7e9a0d570d.DGw86tI1.png)

#### :seedling:查找二叉树

查找二叉树，也称为二叉搜索树（Binary Search Tree，BST），是一种特殊的二叉树结构，它具有以下性质：

1. 左子树上所有节点的值都小于根节点的值。
2. 右子树上所有节点的值都大于根节点的值。
3. 每个子树也必须满足上述两个性质。

由于这种特殊的性质，查找二叉树的结构非常便于查找和插入操作。当我们需要查找一个特定的元素时，可以通过比较当前节点的值与目标值的大小关系，根据二叉树的有序性质，可以在左子树或右子树中继续查找，直到找到目标元素或者遍历到叶子节点。

插入操作也非常简单，只需要在合适的位置创建一个新的节点，并调整树的结构以保持其有序性。

对于查找二叉树的时间复杂度，最好的情况下是O(logn)，最坏的情况下是O(n)，其中n是树中节点的个数。这取决于树的形状，如果树是高度平衡的，则查找的时间复杂度会相对较低，否则会退化为链表，导致查找时间复杂度上升。

需要注意的是，查找二叉树并不保证是平衡的，因此在某些情况下可能会导致树的不平衡，从而影响查找的效率。为了解决这个问题，可以使用平衡二叉树的变种，如红黑树或AVL树，来保持树的平衡性。

![img](/assets/7738ed5ad6b0bd8797637ae6b3d1c361.BAMPs0jU.png)

逐点插入法

<https://blog.csdn.net/learner_Van/article/details/137401735>

#### :seedling:平衡二叉树

平衡二叉树（Balanced Binary Tree），也称为AVL树，是一种特殊的二叉搜索树（Binary Search Tree），它的左子树和右子树的高度差不超过1，并且左子树和右子树都是平衡二叉树。

平衡二叉树的一个重要性质是它的高度是O(log n)，其中n是二叉树中节点的数量。

平衡二叉树的结构使得在插入、删除、查找等操作时，可以保持树的平衡性，从而保证了操作的时间复杂度为O(log n)。

平衡二叉树的结构可以通过多种方式实现，比如AVL树、红黑树等。

在平衡二叉树中，**每个节点都保存了左子树和右子树的高度差，当插入或删除节点导致不平衡时，需要进行相应的旋转操作来恢复平衡**。

平衡二叉树的常见操作包括插入节点、删除节点、查找节点等。

平衡二叉树的应用非常广泛，常用于需要高效的插入、删除和查找操作的场景，比如字典、数据库索引等。

![img](/assets/da2d16a4d7c8fdf8682c6a376736da3b.DvijWdzq.png)

### :evergreen\_tree:树和森林

#### :seedling:树的结构

树的存储结构主要有三种：双亲表示法、孩子表示法和孩子兄弟表示法。

* 在双亲表示法中，树的节点连续存储在一组地址单元中，并在每个节点中附带一个指示器，指示其双亲节点所在数组元素的下标。
* 孩子表示法中，节点的每个孩子使用指针表示，并为每个节点的孩子建立一个链表。
* 孩子兄弟表示法又称为二叉链表表示法，为每个存储节点设置两个指针域，分别指向该节点的第一个孩子和下一个兄弟节点。

树和森林的遍历方法有两种：先根遍历和后根遍历。

* 在树的先根遍历中，先访问根节点，然后依次遍历根的各颗子树。
* 在后根遍历中，先遍历根的各颗子树，然后访问根节点。同样，在森林的遍历中，对于森林中的每棵树，都可以进行先根遍历或后根遍历的操作。

#### :seedling:树和二叉树的转换

树和二叉树是两种不同的数据结构，它们之间可以进行相互转换。

将树转换为二叉树的过程可以通过以下步骤进行：

1. 选择一个树节点作为根节点，并创建一个新的二叉树，将该节点作为根节点。
2. 将树的子节点按照从左到右的顺序，依次添加为二叉树该节点的左子树的右孩子（如果左子树的右孩子为空）或右子树的左孩子（如果右子树的左孩子为空）。
3. 对于每个添加的节点，再按照同样的方式处理它的子节点。

将二叉树转换为树的过程可以通过以下步骤进行：

1. 选择二叉树的根节点作为树的根节点。
2. 对于二叉树的每个节点，如果它有左子树，则将左子树作为该节点的一个子节点。
3. 对于二叉树的每个节点，如果它有右子树，则将右子树作为该节点的一个子节点。

需要注意的是，二叉树转换为树时，可能会有多个子节点指向同一个节点，而树转换为二叉树时，每个节点只有一个左孩子和一个右孩子。

示例如下图：采用连线法，将最左边节点和其兄弟节点都连接起来，而原来的父节点和兄弟节点的连线则断开，这种方法最简单，要求掌握。

![img](/assets/f97958b0b789b3270099591865d9bb52.B5nNQwKG.png)

## 五、图

### :evergreen\_tree:图的概念

图是一种非线性数据结构，它由节点（也称为顶点）和连接这些节点的边组成。图可以用来表示各种关系和连接，比如网络拓扑、社交网络、地图等等。图的节点可以包含任意类型的数据，而边则表示节点之间的关系。

| 概念                 | 定义                                                         |
| :------------------- | :----------------------------------------------------------- |
| 无向图               | 图的节点之间连接线是没有箭头的，不分方向。                   |
| 有向图               | 图的节点之间连接线是箭头，区分A到B，和B到A是两条线。         |
| 完全图               | 无向完全图中，节点两两之间都有连线，n个结点的连线数为(n-1)+(n-2)+...+1=n\*(n-1)/2；有向完全图中，节点两两之间都有互通的两个箭头，n个节点的连线数为n\*(n-1)。 |
| 度、出度和入度       | 顶点的度是关联与该顶点的边的数目。在有向图中，顶点的度为出度和入度之和。出度是以该顶点为起点的有向边的数目。入度是以该顶点为终点的有向边的数目。 |
| 路径                 | 存在一条通路，可以从一个顶点到达另一个顶点，有向图的路径也是有方向的。 |
| 连通图和连通分量     | 针对无向图。若从顶点v到顶点u之间是有路径的，则说明v和u之间是连通的，若无向图中任意两个顶点之间都是连通的，则称为连通图。 |
| 强连通图的强连通分量 | 针对有向图。若有向图任意两个顶点间都相互存在路径，则称为强连通图。有向图中的极大联通子图称为其强联通分量。 |
| 网                   | 边带权值的图称为网。                                         |

### :evergreen\_tree:图的存储

#### :seedling:邻接矩阵

图的存储邻接矩阵是一种常见的图表示方式，适用于稠密图（边数接近于顶点数的平方）的存储。

邻接矩阵是一个二维数组，其中行和列表示图中的顶点，数组元素表示顶点之间的边或者权重。

具体的做法如下：

1. 创建一个大小为VxV的二维数组，其中V是图中顶点的个数。
2. 初始化数组的所有元素为0，表示顶点之间没有边。
3. 对于有边连接的两个顶点u和v，设定数组中的元素au和av为1，表示顶点u和v之间有边。
4. 如果图是带权重的，可以将数组中的元素au和av设为边的权重值。

邻接矩阵的存储优点是可以快速判断两个顶点之间是否有边，时间复杂度为O(1)。但是对于稀疏图（边数远小于顶点数的平方）来说，邻接矩阵会浪费大量的空间。

在使用邻接矩阵存储图时，需要考虑到数组的大小限制和边的存储方式。通常可以使用二维数组、动态数组或稀疏矩阵等数据结构来实现邻接矩阵的存储。

![img](/assets/2c55fa15f35781ef6ab39b2a1c2dfa99.DjHYj36j.png)

![img](/assets/f015459da8c155dcd071d823b4a047a3.BiDN020c.png)

#### :seedling:邻接表

图的邻接表是一种常用的图的存储方式，它使用一个数组来存储图中的每个顶点，数组中的每个元素是一个链表，链表中存储了与该顶点相邻的顶点。

例如，考虑以下无向图：

```bash
A -- B -- C
|         |
D -- E -- F
```

复制

可以使用邻接表来表示这个图：

```bash
顶点 A: B, D
顶点 B: A, C
顶点 C: B, F
顶点 D: A, E
顶点 E: D, F
顶点 F: C, E
```

复制

在邻接表中，每个顶点对应一个链表，链表中的每个节点表示与该顶点相邻的另一个顶点。例如，顶点 A 对应的链表中有节点 B 和 D，表示 A 与 B 和 D 相邻。同样地，顶点 B 对应的链表中有节点 A 和 C，表示 B 与 A 和 C 相邻。

邻接表的优点是可以有效地表示稀疏图，节省了存储空间。同时，邻接表也可以方便地找到一个顶点的所有邻接顶点，因为它们都存储在同一个链表中。但是，对于密集图，邻接表的查询效率可能较低，因为需要遍历链表来寻找相邻顶点。

![img](/assets/00295f0a790e059da775956b89c72c58.9y5rDbnr.png)

![img](/assets/025049ff4ccd28f8c84b22d93bc18bae.DtBoeocj.png)

### :evergreen\_tree:图的遍历

图的遍历是指按照某种规则访问图中的所有节点。图的遍历分为深度优先搜索（DFS）和广度优先搜索（BFS）两种常见的方法。

**1、深度优先搜索（DFS）：**

DFS是一种递归的搜索方法。它从图中的某个节点开始，然后递归地访问该节点的所有邻接节点，直到所有可达的节点都被访问一次。然后，返回到上一个节点，尝试访问它的其他邻接节点，直到遍历完整个图。

DFS的过程可以使用栈来实现，首先将起始节点入栈，然后弹出栈顶节点，并将其标记为已访问，接着将该节点的所有未访问的邻接节点入栈。重复这个过程，直到栈为空。

**2、广度优先搜索（BFS）：**

BFS使用队列来实现。它从图的某个节点开始，首先将该节点入队列，然后访问该节点的所有邻接节点，并将其入队列。接下来，从队列中取出一个节点并访问它的所有邻接节点，将它们入队列。重复这个过程，直到队列为空。

DFS和BFS都可以用来遍历无向图和有向图。它们之间的主要区别在于访问节点的顺序不同，DFS优先访问深度较大的节点，而BFS优先访问离起始节点近的节点。

![img](/assets/e2e561ed353d1c89a46020d44e693992.Tk4UVejo.png)

### :evergreen\_tree:图的最小生成树

最小生成树是一个连通无向图的生成树中，边的权值和最小的生成树。图的最小生成树算法有普里姆(Prim)算法和克鲁斯卡尔(Kruskal)算法。

普里姆算法：

1. 选择一个起始顶点，将起始顶点标记为已访问；
2. 在已访问的顶点集合中，选择一条与未访问顶点相连的最小权值边，并将该边的另外一个顶点标记为已访问；
3. 重复步骤2，直到所有顶点都标记为已访问，最小生成树构建完成。

克鲁斯卡尔算法：

1. 将图中的所有边按照权值从小到大排序；
2. 依次选择权值最小的边，并判断该边的两个顶点是否属于不同的连通分量。如果属于不同的连通分量，则将该边加入最小生成树，否则舍弃该边；
3. 重复步骤2，直到最小生成树的边数等于图的顶点数减一。

这两种算法都是局部最优原则，所以都是贪心法算法，并且没有谁的效率高谁的效率差，因为克鲁斯卡尔算法是数边的，所以边越多，它算起来越麻烦。

![img](/assets/8116c86f21a43fef0f6b371ee119190e.Bnoz_k7t.png)

![img](/assets/718499bd1928df03e7092d562ec3bebd.BhoVLvuV.png)

![img](/assets/41edf887fbd802c57439a29cb46b1b36.C_Jzdj9c.png)

### :evergreen\_tree:图的拓扑序列

图的拓扑序列是指一个有向无环图（DAG）的顶点的一种线性排序，使得对于任意的有向边(u, v)，u在拓扑序列中都出现在v之前。

拓扑排序可以用来解决一些实际问题，比如任务调度、编译顺序等。在一个任务调度的问题中，每个顶点表示一个任务，有向边(u, v)表示任务u必须在任务v之前执行。拓扑序列可以用来确定任务的执行顺序，保证所有的依赖关系都得到满足。

拓扑序列可能不是唯一的，一个图可以有多个拓扑序列。可以使用深度优先搜索（DFS）或广度优先搜索（BFS）等算法来生成拓扑序列。

拓扑序列的生成过程如下：

1. 选择一个没有前驱（即入度为0）的顶点，将其加入拓扑序列中。
2. 移除该顶点及其相邻的边。
3. 重复步骤1和2，直到所有的顶点都加入了拓扑序列。

如果图中存在环路，则无法生成拓扑序列，因为环路表示存在循环依赖关系，无法确定任务的执行顺序。

将有向图的有向边作为活动开始的顺序，若图中一个节点入度为0，则应该最先执行此活动，而后删除掉此节点和其关联的有向边，再去找图中其他没有入度的结点，执行活动，依次进行，示例如下：

![img](/assets/146b7d92d51e560571896b80a9328875.DfndL1SU.png)

![img](/assets/84d2971ee8ce0f38b7c8962a83605ea3.ReD14F0R.png)

## 六、查找算法

* 查找基本概念
* 静态查找表的查找方法
* 顺序查找
* 折半查找
* 分块查找
* 动态查找表
* 二叉排序树
* 平衡二叉树
* 哈希表

### :evergreen\_tree:算法基础

#### :seedling:算法概念

算法是一组有序的操作指令，用于解决特定问题或完成特定任务。算法描述了问题的输入和输出，以及在给定输入时如何通过一系列步骤来产生所需的输出。算法可以用来解决各种问题，包括数学问题、计算问题、优化问题等。在计算机科学中，算法是计算机程序的基础，它指导计算机执行特定的计算和操作。一个好的算法应该具有正确性（能够产生正确的输出）、效率（能够在合理的时间内完成计算）和易读性（易于理解和实现）。简单的说就是某个问题的解题思路。

#### :seedling:算法的复杂度

算法的复杂度是衡量算法执行效率的一个指标，通常用时间复杂度和空间复杂度来描述。

1. 时间复杂度：描述随着问题规模的增大，算法执行时间的增长趋势。常见的时间复杂度包括：
   * 常数时间复杂度 O(1)：无论问题规模多大，算法的执行时间都不会随之增长。
   * 线性时间复杂度 O(n)：算法的执行时间与问题规模呈线性关系。
   * 对数时间复杂度 O(log n)：算法的执行时间与问题规模的对数呈线性关系。
   * 平方时间复杂度 O(n^2)：算法的执行时间与问题规模的平方呈线性关系。
   * 指数时间复杂度 O(2^n)：算法的执行时间与问题规模的指数呈线性关系。
2. 空间复杂度：描述算法执行中所需的额外空间随问题规模增大的趋势。常见的空间复杂度包括：
   * 常数空间复杂度 O(1)：算法的额外空间不随问题规模的增大而变化。
   * 线性空间复杂度 O(n)：算法的额外空间与问题规模呈线性关系。
   * 对数空间复杂度 O(log n)：算法的额外空间与问题规模的对数呈线性关系。
   * 平方空间复杂度 O(n^2)：算法的额外空间与问题规模的平方呈线性关系。
   * 指数空间复杂度 O(2^n)：算法的额外空间与问题规模的指数呈线性关系。

算法的复杂度分析可以帮助我们评估算法的执行效率，并选择合适的算法来解决问题。通常情况下，我们希望选择时间复杂度低且空间复杂度较小的算法。

常见的对算法执行所需时间的度量：
$$
O(1)\<O(log\_2n)\<O(n)\<O(nlog\_2n)\<O(n²)\<O(n³)\<O(2ⁿ)\<O(n!)
$$
上述的时间复杂度，经常考到，需要注意的是，时间复杂度是一个大概的规模表示，一般以循环次数表示，O(n)说明执行时间是n的正比，另外，log对数的时间复杂度一般在查找二叉树的算法中出现。渐进符号O表示一个渐进变化程度，实际变化必须小于等于O括号内的渐进变化程度。

### :evergreen\_tree:查找算法

#### :seedling:线性查找

线性查找是一种简单直接的查找算法，也称为顺序查找。它通过遍历待查找的数据集，逐个比较数据元素与目标值，直到找到目标值或遍历完整个数据集为止。

线性查找的基本思路如下：

1. 从第一个数据元素开始，逐个遍历数据集中的元素。
2. 每次比较当前元素与目标值是否相等，如果相等则返回当前位置，表示找到目标值。
3. 如果遍历完整个数据集仍未找到目标值，则返回-1，表示未找到目标值。

```java
public class LinearSearch {
    public static int linearSearch(int[] arr, int target) {
        for (int i = 0; i < arr.length; i++) {
            if (arr[i] == target) {
                return i;
            }
        }
        return -1;
    }

    public static void main(String[] args) {
        int[] arr = {3, 5, 2, 8, 9, 4, 1};
        int target = 9;
        int result = linearSearch(arr, target);
        if (result != -1) {
            System.out.println("目标值在位置：" + result);
        } else {
            System.out.println("未找到目标值");
        }
    }
}
```

在最坏情况下，线性查找的时间复杂度为O(n)，其中n为数据集的大小。因为需要逐个遍历数据元素，所以当数据集较大时，线性查找的效率相对较低。因此在实际应用中，当数据集较大时，可以考虑使用更高效的查找算法，如二分查找、哈希查找等。

#### :seedling:折半（二分）查找

折半查找（也称为二分查找）是一种高效的查找算法，常用于有序数组中查找某个特定元素的位置。

折半查找的基本思想是首先确定待查找区间的中间位置，然后将待查找元素与中间位置的元素进行比较。如果待查找元素等于中间位置的元素，则查找成功，返回中间位置；如果待查找元素小于中间位置的元素，则在中间位置的左侧区间继续查找；如果待查找元素大于中间位置的元素，则在中间位置的右侧区间继续查找。重复以上步骤直至找到目标元素或待查找区间为空。

折半(二分)查找是一种基于有序数组的查找算法，其时间复杂度为O(logn)。其基本思路如下：

1. 初始化左边界和右边界，将左边界设为0，将右边界设为数组长度减1。
2. 取中间位置的元素，与目标元素进行比较。
3. 如果中间元素等于目标元素，则返回中间元素的索引。
4. 如果中间元素大于目标元素，则在左半部分继续查找，将右边界更新为中间元素的前一个索引。
5. 如果中间元素小于目标元素，则在右半部分继续查找，将左边界更新为中间元素的后一个索引。
6. 重复步骤2至5，直到左边界大于右边界，表示查找失败。

```java
public int binarySearch(int[] arr, int target) {
    int left = 0;
    int right = arr.length - 1;

    while (left <= right) {
        int mid = (left + right) / 2;
        if (arr[mid] == target) {
            return mid;
        } else if (arr[mid] < target) {
            left = mid + 1;
        } else {
            right = mid - 1;
        }
    }
    return -1; // 目标元素不存在
}

// 测试案例
int[] arr = {1, 3, 5, 7, 9, 11, 13};
int target = 7;
int result = binarySearch(arr, target);
if (result != -1) {
    System.out.println("目标元素在数组中的索引为：" + result);
} else {
    System.out.println("目标元素不存在数组中");
}
```

我们首先定义了一个二分查找函数`binary_search`，它接受一个有序数组`arr`和目标元素`target`作为输入。然后我们在数组中查找目标元素并返回其索引，如果目标元素不存在，则返回-1。

时间复杂度分析：

折半查找每次将当前查找范围缩小一半，因此查找的次数取决于查找范围的大小，即查找次数为 logn (以2为底)。因此，折半查找的时间复杂度为O(logn)。

#### :seedling:散列（哈希）表

哈希查找（Hash Search）也被称为散列查找，是一种根据关键字直接进行访问的查找技术，具有快速查找的特点。哈希查找的基本思路是通过哈希函数将关键字映射到一个固定的位置，称为哈希地址。利用哈希地址来直接访问目标数据。

```java
class HashTable {
    private int size;
    private Entry[] table;

    public HashTable(int size) {
        this.size = size;
        table = new Entry[size];
        for (int i = 0; i < size; i++) {
            table[i] = new Entry();
        }
    }

    private int hash(int key) {
        return key % size;
    }

    public void insert(int key, String value) {
        int hashValue = hash(key);
        table[hashValue].add(key, value);
    }

    public String search(int key) {
        int hashValue = hash(key);
        for (Entry entry : table[hashValue]) {
            if (entry.key == key) {
                return entry.value;
            }
        }
        return null;
    }
    private class Entry {
        int key;
        String value;
        Entry next;

        public void add(int key, String value) {
            Entry newEntry = new Entry();
            newEntry.key = key;
            newEntry.value = value;
            newEntry.next = this.next;
            this.next = newEntry;
        }
    }
}

// 示例代码
HashTable hashTable = new HashTable(10);

hashTable.insert(1, "apple");
hashTable.insert(2, "banana");
hashTable.insert(11, "cherry");

System.out.println(hashTable.search(1)); // 输出：apple
System.out.println(hashTable.search(2)); // 输出：banana
System.out.println(hashTable.search(11)); // 输出：cherry
System.out.println(hashTable.search(3)); // 输出：None
```

上述代码中，我们创建了一个哈希表（HashTable）类，其中包含以下几个方法：

* `__init__(self, size)`：初始化哈希表，指定哈希表的大小。
* `hash_function(self, key)`：哈希函数，用于将关键字映射到哈希地址。
* `insert(self, key, value)`：插入方法，将关键字和值插入到哈希表中。
* `search(self, key)`：查找方法，根据关键字查找对应的值。

时间复杂度：

* 插入和查找的时间复杂度都为O(1)，即常数时间复杂度。这是因为哈希函数的设计使得每个关键字都能映射到唯一的哈希地址，因此可以直接在哈希地址对应的列表中进行操作。在没有冲突的情况下，插入和查找操作都只需要一次哈希映射和一次遍历操作即可完成。

##### **🌈**冲突解决

![img](/assets/7a1abe921f828f79d6d2d2c279ef325a.Bp2X8p33.png)

在上图中，很明显，哈希函数产生了冲突，使用的是线性探测法解决冲突，还有其他方法如下：

* 线性探测法：按物理地址顺序取下一个空闲的存储空间。
* 伪随机数法：将冲突的数据随机存入任意空闲的地址中。
* 再散列法：原有的散列函数冲突后，继续用此数据计算另外一个哈希函数，用以解决冲突。

##### **🌈**线性探测法

哈希查找中的线性探测法是一种解决哈希冲突的方法。当在哈希表中插入一个元素时，如果发生冲突，即要插入的位置已经被占用，线性探测法会顺序地往后查找，直到找到一个空槽或者遍历完整个哈希表。

具体的插入过程如下：

1. 使用哈希函数计算要插入元素的哈希值，得到在哈希表中的初始位置。
2. 如果初始位置为空槽，则直接将元素插入到该位置。
3. 如果初始位置已经被占用，即发生冲突，就顺序地往后查找，直到找到一个空槽或者遍历完整个哈希表。
4. 如果找到了空槽，则将元素插入到该空槽中。
5. 如果遍历完整个哈希表，仍然没有找到空槽，表示哈希表已满，插入失败。

在查找元素时，也使用相同的过程：

1. 使用哈希函数计算要查找元素的哈希值，得到在哈希表中的初始位置。
2. 如果初始位置为空槽，则表示要查找的元素不存在。
3. 如果初始位置不为空槽，需要顺序地往后查找，直到找到目标元素或者遍历完整个哈希表。
4. 如果找到了目标元素，则返回其位置。
5. 如果遍历完整个哈希表，仍然没有找到目标元素，则表示要查找的元素不存在。

线性探测法的优点是实现简单，插入和查找的平均时间复杂度都是O(1)。然而，它也有一些缺点。当哈希表中的装载因子（已占用槽数目与总槽数目的比值）较大时，会导致冲突的概率增加，从而使得线性探测法的性能下降。另外，线性探测法会产生聚集效应，即冲突的元素会集中在一起，导致哈希表中的空槽较少，进而影响插入和查找的效率。

**知识点额外补充：一致性哈希**

一致性哈希是一种解决分布式系统中数据分散和负载均衡的方法。在分布式系统中，数据通常按照某种规则被分散存储在不同的节点上，为了快速定位到存储数据的节点，需要使用哈希函数来将数据的键映射到一个节点的位置。然而，当系统中的节点发生变化（如节点的加入、删除或故障）时，传统的哈希方法需要重新计算所有的映射，导致大量数据的迁移工作，增加系统的开销和复杂性。

一致性哈希通过引入虚拟节点的概念，解决了传统哈希方法的这个问题。具体来说，一致性哈希将哈希空间（通常是一个固定的范围，如0-2^32）划分成一个圆环，并将节点和数据键使用哈希函数映射到圆环上的位置。每个节点在圆环上有多个虚拟节点，通过增加虚拟节点，可以使节点在哈希环上分布更加均匀。

当需要存储数据时，通过哈希函数将数据的键映射到圆环上的一个位置，然后沿着圆环顺时针查找，找到离该位置最近的节点，即为[数据存储](https://cloud.tencent.com/product/cdcs?from_column=20065\&from=20065)的节点。当节点发生变化时，只需要迁移受影响的数据部分，对于其他数据则不需要变动。

一致性哈希的优点包括：

* 节点的动态变化对数据的迁移影响较小，减少了系统的开销和复杂性。
* 数据在节点上的分布更加均匀，提高了系统的负载均衡性。
* 在节点变化时，只需要迁移少量数据，减少了数据迁移的开销。

一致性哈希在[分布式存储](https://cloud.tencent.com/product/cos?from_column=20065\&from=20065)、负载均衡、缓存系统等领域得到广泛应用。

##### **🌈**伪随机数法

伪随机数法是当哈希函数将多个键映射到同一个索引位置时，伪随机数法可以通过生成一系列伪随机数来确定下一个可用的位置。

伪随机数法的基本思想是，在冲突的位置上，通过计算一个伪随机数来确定下一个可用的位置。这个伪随机数可以是基于当前冲突位置和键的某种计算方式得出的结果。一旦找到了下一个可用的位置，就可以将键值对插入到该位置上。

伪随机数法的一个优点是，可以较好地解决哈希冲突问题，减少冲突的次数，提高查找效率。然而，伪随机数法也有一些限制和注意事项。首先，生成伪随机数的计算方式需要被设计得足够复杂，以保证生成的位置能够更加均匀地分布在哈希表中，避免过多的冲突。其次，伪随机数生成的效率可能较低，特别是在哈希表规模较大的情况下。因此，在实际应用中，需要根据具体的需求和场景选择适合的哈希冲突解决方法。

##### **🌈**再散列法

再散列法（Rehashing）它是在原有的哈希表中再次进行哈希运算，以找到一个新的位置存储冲突的元素。

具体来说，当发生冲突时，再散列法会使用不同的哈希函数或使用原有哈希函数的不同参数，将冲突元素重新计算哈希值，然后找到一个新的位置存储。

再散列法可以多次进行再散列，直到找到一个不冲突的位置为止。常见的再散列方法包括线性探测再散列、平方探测再散列、双散列等。

再散列法的优点是简单、易于实现，并且在处理小规模数据集时表现良好。然而，当数据量大或者哈希函数选择不当时，再散列法可能导致严重的哈希冲突问题，使查询效率下降。因此，在设计哈希表时，需要选择合适的哈希函数和再散列方法，以避免冲突。

![img](/assets/ec5796cf862a5b04ab26aa3aef0305ea.BEQbWTsZ.png)

![img](/assets/4214486aca6a111bde8e2e6c6df15bfd.IVevSmid.png)

## 七、排序算法

* 排序基本概念

* 简单排序

* 希尔排序

  改进的插入排序

* 快速排序

* 堆排序

* 归并排序

* 基数排序

* 外部排序

### :evergreen\_tree:排序算法分类

稳定与不稳定排序：依据是两个相同的值在一个待排序序列中的顺序和排序后的顺序应该是相对不变的，即开始时21在21前，那排序结束后，若该21还在21前，则为稳定排序，不改变相同值的相对顺序。
内排序和外排序：依据排序是在内存中进行的还是在外部进行的。

排序的算法有很多，大致可以分类如下：

* 插入类排序：直接插入排序、希尔排序。
* 交换类排序：冒泡排序、快速排序。
* 选择类排序：简单选择排序、堆排序。
* 归并排序。
* 基数排序。

常见的排序算法有以下几种：

1. 冒泡排序（Bubble Sort）：依次比较相邻的两个元素，将较大的元素交换到后面，每一轮比较都将最大的元素放到最后。时间复杂度为O(n^2)。
2. 选择排序（Selection Sort）：每次从待排序的元素中选取最小的元素，放置在已排序的末尾。时间复杂度为O(n^2)。
3. 插入排序（Insertion Sort）：将待排序的元素插入到已排序的序列中的适当位置，使得插入后仍然有序。时间复杂度平均为O(n^2)，最好情况下为O(n)，最坏情况下为O(n^2)。
4. 希尔排序（Shell Sort）：是插入排序的一种改进，通过将序列分组，每次对分组进行插入排序，然后逐步缩小分组的规模，最终完成排序。时间复杂度为O(nlogn)。
5. 归并排序（Merge Sort）：将序列不断地分割成两半，对每一半进行排序，然后合并两个已排序的子序列，最终完成排序。时间复杂度为O(nlogn)。
6. 快速排序（Quick Sort）：通过一趟排序将序列分成独立的两部分，其中一部分所有元素都比另一部分小，然后再对这两部分递归地进行快速排序。时间复杂度平均为O(nlogn)，最坏情况下为O(n^2)。
7. 堆排序（Heap Sort）：将待排序的序列构建成一个大顶堆，然后将堆顶元素与最后一个元素交换，再对剩余的n-1个元素进行调整，循环执行以上步骤，最终完成排序。时间复杂度为O(nlogn)。
8. 计数排序（Counting Sort）：统计待排序序列中每个元素的出现次数，然后根据元素的值从小到大依次输出。时间复杂度为O(n+k)，其中k表示序列中元素的范围。
9. 桶排序（Bucket Sort）：将待排序的元素映射到一个有限数量的桶中，每个桶再分别进行排序，最后将所有桶中的元素按次序合并成有序序列。时间复杂度为O(n+k)，其中k表示桶的数量。
10. 基数排序（Radix Sort）：将待排序的元素从低位到高位依次进行排序，每一位都使用一种稳定的排序算法（如计数排序或桶排序）。时间复杂度为O(d(n+k))，其中d表示元素的最大位数，k表示每一位的可能取值范围。

术语说明

| 术语       | 说明                                                         |
| :--------- | :----------------------------------------------------------- |
| 数据元素   | 要排序的数据的基本单位，可以是数字、字符、对象等             |
| 关键字     | 数据元素中用于排序的属性或值，可以是元素本身或元素的某个特定属性 |
| 升序       | 按照关键字的大小从小到大进行排序                             |
| 降序       | 按照关键字的大小从大到小进行排序                             |
| 稳定性     | 如果两个关键字相等的元素在排序后的序列中的相对位置保持不变，排序算法是稳定的；否则，排序算法是不稳定的 |
| 内部排序   | 排序过程中所有数据都能够全部加载到内存中进行排序             |
| 外部排序   | 排序过程中数据量太大，无法一次性加载到内存中，需要借助外部存储设备进行排序 |
| 比较排序   | 排序算法通过比较关键字的大小进行排序                         |
| 非比较排序 | 排序算法不直接通过比较关键字的大小进行排序，而是利用元素的其他特性进行排序，如计数排序、桶排序和基数排序 |
| 原地排序   | 排序过程中只使用了常数级别的额外空间                         |
| 时间复杂度 | 描述算法的耗时程度，即算法执行所需的时间                     |
| 空间复杂度 | 描述算法所需的额外空间                                       |

### :evergreen\_tree:直接插入排序（插入类）

直接插入排序是一种简单直观的排序算法，它的思想是将一个序列分为有序和无序两部分，每次从无序部分中取出一个元素，插入到有序部分的正确位置上，直到整个序列有序为止。

具体步骤如下：

1. 将序列分为有序和无序两部分，初始时有序部分只有一个元素（即序列的第一个元素），无序部分包括剩余的元素。
2. 从无序部分中取出一个元素，记为待插入元素。
3. 将待插入元素与有序部分的元素进行比较，找到待插入元素在有序部分的正确位置。
4. 将有序部分中大于待插入元素的元素后移一位，腾出位置给待插入元素。
5. 将待插入元素放入正确位置。
6. 重复步骤2到步骤5，直到无序部分为空。

时间复杂度：

* 最好情况下，当序列已经有序时，直接插入排序的时间复杂度为O(n)，因为只需遍历一次序列。
* 最坏情况下，当序列逆序时，直接插入排序的时间复杂度为O(n^2)，因为要进行n次插入操作，每次插入操作的时间复杂度为O(n)。
* 平均情况下，直接插入排序的时间复杂度也为O(n^2)。

| 直接插入排序—示例         | ![img](/assets/5c2a842607b2d15ce198fade54fb9cb1.Bu2dHDUZ.png) |
| --------------------------------------------------------- | :----------------------------------------------------------- |
| **直接插入排序—动态示例** | ![img](/assets/d9093ce0b73f9b991ada1e8567cf47eb.9rKS8xnT.gif) |

### :evergreen\_tree:希尔排序（插入类）

希尔排序是一种基于插入排序的排序算法，也称为缩小增量排序。它通过逐步减小增量的方式分组并对元素进行比较和交换，最终实现整体的有序。

希尔排序的算法步骤如下：

1. 选择一个增量序列，常用的是希尔增量序列，即初始增量gap为数组长度的一半，然后每次将gap缩小一半，直到gap为1。
2. 对每个增量间隔进行插入排序。从第gap个元素开始，将其与之前的元素进行比较，如果前面的元素更大，则将其向后移动gap个位置。重复这个过程直到无法向前移动为止。
3. 缩小增量，重新进行插入排序，直到最后一次增量为1，即进行最后一次插入排序，此时整个数组已经是有序的了。

时间复杂度：

* 希尔排序的时间复杂度取决于选取的增量序列，最好的情况下可以达到O(nlogn)，
* 最坏的情况下为O(n^2)。

| 希尔排序—示例         | ![img](/assets/0cf093b01c614aa451ab21a2187993f3.qf8Kq0Vi.png) |
| ----------------------------------------------------- | ------------------------------------------------------------ |
| **希尔排序—动态示例** | ![img](/assets/0c486a3ea191dd29538033d79fc27819.CZ-z1IVg.gif) |

### :evergreen\_tree:简单选择排序（选择类）

简单选择排序是一种简单直观的排序算法，它的基本思想是每次从待排序的数据中选择最小（或最大）的元素，然后放到已排序序列的末尾，直至所有元素排序完毕。

具体的排序过程如下：

1. 从待排序序列中，找到关键字最小的元素。
2. 如果最小元素不是待排序序列的第一个元素，将其和第一个元素互换位置。
3. 从剩余的待排序序列中，继续找到关键字最小的元素，重复步骤2。
4. 重复步骤2和步骤3，直到待排序序列中只剩下一个元素。

时间复杂度为：

简单选择排序的时间复杂度为O(n^2)，其中n为待排序序列的长度。虽然简单选择排序的时间复杂度较高，但对于小规模的数据排序还是比较高效的。

| 简单选择排序—示例         | ![img](/assets/c2ca4ddb4804ce5dfddea1ecf7bf98dc.PaAyB9FK.png) |
| --------------------------------------------------------- | ------------------------------------------------------------ |
| **简单选择排序—动态示例** | ![img](/assets/e5e0b93698d392bf30b29eacca44f68c.B8GwndZy.gif) |

### :evergreen\_tree:堆排序（选择类）

堆排序是一种基于二叉堆数据结构的排序算法。它的时间复杂度为O(nlogn)，空间复杂度为O(1)。

堆排序的具体步骤如下：

1. 将待排序序列构建成一个大顶堆（或小顶堆），从最后一个非叶子节点开始，自下而上地进行堆调整。
2. 交换堆顶元素（最大值或最小值）和堆中最后一个元素。
3. 从根节点开始，自上而下地进行堆调整，保持堆的性质。
4. 重复步骤2和步骤3，直到堆中只剩下一个元素。

堆排序适用于在多个元素中找出前几名的方案设计，因为堆排序是选择排序，而且选择出前几名的效率很高。

| 堆排序—示例         | ![img](/assets/f742b640e076e0882aa151f3fcda8aeb.EZJ29m7H.png) |
| --------------------------------------------------- | ------------------------------------------------------------ |
| **堆排序—动态示例** | ![img](/assets/052354c97a2567bf0bae666e54a757c0.D4myjC6C.gif) |

### :evergreen\_tree:冒泡排序（交换类）

冒泡排序是一种简单直观的排序算法。它重复地遍历要排序的列表，通过比较相邻元素并交换它们，将列表中的最大元素逐渐“冒泡”到列表的末尾。在每一次遍历中，比较相邻的两个元素，如果它们的顺序不正确，则交换它们的位置。重复这个过程，直到整个列表排序完成。

具体算法步骤如下：

1. 比较相邻的两个元素，如果它们的顺序不正确，则交换它们的位置。
2. 对每一对相邻的元素重复步骤1，直到最后一对元素。
3. 重复步骤1和步骤2，直到没有需要交换的元素，即列表已经有序。

冒泡排序的时间复杂度为O(n^2)，其中n是列表的长度。由于每次遍历都会将当前未排序部分的最大元素“冒泡”到末尾，因此需要遍历n次。每次遍历中需要比较相邻的元素并可能交换它们的位置，最坏情况下需要比较和交换(n-1)次，因此总的比较和交换次数为n\*(n-1)/2，即O(n^2)。

冒泡排序是一种稳定的排序算法，即相等元素的相对位置在排序后不会改变。

| 冒泡排序—示例         | ![img](/assets/ed46f7cfcbdbce88bdbaf3bdcaf121d9.DDVirasO.png) |
| ----------------------------------------------------- | ------------------------------------------------------------ |
| **冒泡排序—动态示例** | ![img](/assets/773cd143842a8b406a0b718dc75ab3fc.Csp5B4TH.gif) |

### :evergreen\_tree:快速排序（交换类）

快速排序是一种高效的排序算法，它基于分治的思想。

快速排序的基本思想是选择一个基准元素（通常选择数组的第一个元素），将数组分成两个子数组，使得左子数组的所有元素均小于基准元素，右子数组的所有元素均大于基准元素，然后对这两个子数组分别进行快速排序，最后将左子数组、基准元素和右子数组合并起来。

具体的实现步骤如下：

1. 选择一个基准元素，通常选择数组的第一个元素。
2. 定义两个指针，一个指向数组的第一个元素，一个指向数组的最后一个元素。
3. 将指针进行移动，直到找到左边大于等于基准元素的元素和右边小于等于基准元素的元素。
4. 如果左指针小于等于右指针，则交换这两个元素的位置。
5. 继续移动指针，直到左指针大于右指针。
6. 交换基准元素和左指针的元素的位置，使得左指针左边的元素都小于基准元素，右指针右边的元素都大于基准元素。
7. 对左子数组和右子数组分别进行快速排序，递归地进行上述步骤。
8. 当子数组的长度小于等于1时，停止递归。

快速排序的时间复杂度为O(nlogn)，其中n为数组的长度。

| 快速排序—示例         |                                                              |
| ----------------------------------------------------- | ------------------------------------------------------------ |
| **快速排序—动态示例** | ![img](/assets/819676-20210311160605077-1738860427.DD28bswc.gif) |

https://www.cnblogs.com/loong-hon/p/14518756.html

### :evergreen\_tree:归并排序

归并排序是一种分治算法，它将一个数组分成两个子数组，对每个子数组进行递归排序，然后将两个子数组合并为一个有序的数组。

具体步骤如下：

1. 将待排序数组分成两个子数组，分别递归地对两个子数组进行排序。
2. 合并两个有序的子数组，得到一个有序的数组。

合并两个有序的子数组的步骤如下：

1. 创建一个临时数组，用来存储合并后的有序数组。
2. 比较两个子数组的首元素，将较小的元素放入临时数组，并将对应子数组的指针向后移动一位。
3. 重复上述步骤，直到其中一个子数组的元素全部放入临时数组。
4. 将另一个子数组的剩余元素放入临时数组。
5. 将临时数组的元素复制回原数组的对应位置。

归并排序的时间复杂度为O(nlogn)，空间复杂度为O(n)。它是一种稳定的排序算法，适用于处理大规模数据和外部排序。

| 归并排序—示例         | ![img](/assets/0f9fa7f6f846ee812193cb1bdf26a16f.DDk4MyOf.png) |
| ----------------------------------------------------- | ------------------------------------------------------------ |
| **归并排序—动态示例** | ![img](/assets/6305841a02990e1040a2606554f23791.Bguw-KQu.gif) |

### :evergreen\_tree:基数排序

基数排序是一种非比较型的排序算法，它按照元素的各个位的值来进行排序。基数排序可以用于整数或者字符串的排序。

基数排序的基本思想是：将待排序的元素分配到有限数量的桶中，然后按照桶的顺序依次取出元素构成有序序列。桶的数量一般和基数的范围有关。

具体的算法步骤如下：

1. 找出待排序元素中的最大值，确定最大值的位数，这个位数决定了需要进行多少次排序操作；
2. 准备桶，桶的数量一般和基数的范围有关；
3. 对待排序的元素按照从低位到高位的顺序依次进行排序：
   * 将待排序的元素按照当前位的值分配到对应的桶中；
   * 按照桶的顺序依次取出元素构成有序序列；
   * 循环上述步骤，直到所有位都排序完成。

基数排序的时间复杂度取决于数据的位数和数据范围，一般情况下为O(d\*(n+r))，其中d是最大值的位数，n是元素个数，r是基数的范围。基数排序是一种稳定的排序算法。

| 基数排序—示例         | ![img](/assets/5ea1e0b740634c805f0040d62ce21ca0.CobM4iPI.png) |
| ----------------------------------------------------- | ------------------------------------------------------------ |
| **基数排序—动态示例** | ![img](/assets/0427802249e46bb0b7679e44802a77d8.Bwrylu8F.gif) |

### :evergreen\_tree:排序算法比较

![img](/assets/b62915f647968fcb51b7e77ee1815ce2.DyfQBSNe.png)

![img](/assets/87d39e6fb5df613c4480c4a25b73b3d6.B_UOLnyi.png)

![img](/assets/338f26f1979db78d38f1625e3f1c09cb._oPKSoFd.png)

![img](/assets/3ae61a2e4063519b352c98903f8c475c.yLiihzW2.png)

![img](/assets/c324154fcd99ab51f226541189bb6546.CUH6qizI.png)

---

---
url: /daily/软件设计师/09_数据库技术基础.md
---

# 数据库技术基础

## 数据库的基本概念

考点1：数据库体系结构

考点2：三级模式结构

考点3：数据仓库

### 考点1：数据库体系结构

#### :bouquet:数据库系统的体系结构

1. 集中式数据库系统
   * 数据是集中的
   * 数据管理是集中的
   * 数据库系统的素有功能 (从形式的用户接口到DBMS核心)都集中在DBMS所在的计算机。
2. C/S结构
   * 客户端负责数据表示
   * 服务服务器主要负责数据库服务
   * 数据库系统分为前端和后端
   * ODBC、JDBC
3. 分布式数据库
   * 物理上分布、逻辑上集中
   * 物理上分布、逻辑上分布
   * 特点
   * 透明性
4. 并行数据库
   * 共享内存式
   * 无共享式

#### :bouquet:分布式数据库特点

1. **数据独立性**。除了数据的逻辑独立性与物理独立性外，还有数据分布独立性（分布透明性）。
2. **集中与自治共享结合的控制结构**。各局部的DBMS可以独立地管理局部数据库，具有自治的功能。同时，系统又设有集中控制机制，协调各局部DBMS的工作，执行全局应用。
3. **适当增加数据冗余度**。在不同的场地存储同一数据的多个副本，可以提高**系统的可靠性和可用性**，同时也能提高系统性能。（提高系统的可用性，即当系统中某个节点发生故障时，因为数据有其他副本在非故障场地上，对其他所有场地来说，数据仍然是可用的，从而保证数据的完备性）。
4. **全局的一致性、可串行性和可恢复性**。

#### :bouquet:分布式数据库透明性

1. **分片透明**：是指用户不必关心数据是如何分片的，它们对数据的操作在全局关系上进行，即如何分片对用户是透明的。
2. **复制透明**：用户不用关心数据库在网络中各个节点的复制情况，被复制的数据的更新都由系统自动完成。
3. **位置透明**：是指用户不必知道所操作的数据放在何处，即数据分配到哪个或哪些站点存储对用户是透明的。
4. **局部映像透明性（逻辑透明）**：是最低层次的透明性，该透明性提供数据到局部数据库的映像，即用户不必关心局部DBMS支持哪种数据模型、使用哪种数据操纵语言，数据模型和操纵语言的转换是由系统完成的，因此，局部映像透明性对异构型和同构异质的分布式数据库系统是非常重要的。

#### :bouquet:例题

```
在分布式数据库中有分片透明、复制透明、位置透明和逻辑透明等基本概念，其中: (D)是指局部数据模型透明，即用户或应用程序无需知道局部使用的是哪种数据模型; (A) 是指用户或应用程序不需要知道逻辑上访问的表具体是如何分块存储的。
A分片透明 B复制透明 C位置透明 D逻辑透明
A分片透明 B复制透明 C位置透明 D逻辑透明

当某一场地故障时，系统可以使用其他场地上的副本而不至于使整个系统瘫痪。这称为分布式数据库的 (C)。
A共享性
B自治性
C可用性
D分布性
```

### 考点2：三级模式结构

#### :cherry\_blossom:三级模式和两级映像

![img](/assets/20190606180834733.D5tqJdTS.png)

三级模式

1. 用户级--> 外模式(反映了数据库系统的用户观)
   * 外模式又称子模式或用户模式，对应于用户级。它是某个或某几个用户所看到的数据库的数据视图，是与某一应用有关的数据的[逻辑表示](https://baike.baidu.com/item/逻辑表示)。外模式是从模式导出的一个子集，包含模式中允许特定用户使用的那部分数据。用户可以通过外模式描述语言来描述、定义对应于用户的[数据记录](https://baike.baidu.com/item/数据记录)(外模式)，也可以利用[数据操纵语言](https://baike.baidu.com/item/数据操纵语言)(Data Manipulation Language，DML)对这些数据记录进行操作。
2. 概念级--> 概念模式（反映了数据库系统的整体观）
   * 概念模式又称模式或逻辑模式，对应于概念级。它是由数据库设计者综合所有用户的数据，按照统一的观点构造的全局逻辑结构，是对数据库中全部数据的逻辑结构和特征的总体描述，是所有用户的公共数据视图(全局视图)。它是由数据库管理系统提供的数据模式描述语言(Data Description Language，DDL)来描述、定义的。
3. 物理级 --> 内模式（反映了数据库系统的存储观）
   * 内模式又称存储模式，对应于物理级。它是数据库中全体数据的内部表示或底层描述，是数据库最低一级的逻辑描述，它描述了数据在存储介质上的存储方式和物理结构，对应着实际存储在外存储介质上的数据库。内模式由内模式描述语言来描述、定义的。

两级映射

1. 外模式/模式的映像：实现外模式到概念模式之间的相互转换。
   * 逻辑独立性：数据的逻辑独立性是指用户的应用程序与数据库结构是相互独立的。数据的逻辑结构发生变化后，用户程序也可以不修改。但是，为了保证应用程序能够正确执行，需要修改外模式/概念模式之间的映像。
2. 模式/内模式的映像：实现概念模式到内模式之间的相互转换。
   * 物理独立性：数据的物理独立性是指当数据库的内模式发生改变时，数据的的逻辑结构不变。由于应用程序处理的只是数据的逻辑结构，这样物理独立性可以保证，当数据的物理结构改变了，应用程序不用改变。但是，为了保证应用程序能够正确执行，需要修改概念模式/内模式之间的映像。

[数据库模式（三级模式+两级映射）\_数据库的物理连接属于什么模式-CSDN博客](https://blog.csdn.net/mcb520wf/article/details/91047683)

#### :cherry\_blossom:例题

```
数据库系统通常采用三级模式结构:外模式、模式和内模式。这三级模式分别对应数据库的(B)
A基本表、存储文件和视图
B视图、基本表和存储文件
C基本表、视图和存储文件
D视图、存储文件和基本表

以下关于数据库两级映像的叙述中，正确的是(B)
A模式/内模式映像实现了外模式到内模式之间的相互转换
B模式/内模式映像实现了概念模式到内模式之间的相互转换
C外模式/模式的映像实现了概念模式到内模式之间的相互转换
D外模式/内模式的映像实现了外模式到内模式之间的相互转换

数据的物理独立性和逻辑独立性分别是通过修改 (D) 来完成的。
A外模式与内模式之间的映像、模式与内模式之间的映像
B外模式与内模式之间的映像、外模式与模式之间的映像
C外模式与模式之间的映像、模式与内模式之间的映像
D模式与内模式之间的映像、外模式与模式之间的映像
```

### 考点3：数据仓库

#### :tulip:数据仓库特点

1. 面向主题：数据按主题组织。
2. 集成的：消除了源数据中的不一致性，提供整个企业的一致性全局信息。
3. 相对稳定的（非易失的）：主要进行查询操作，只有少量的修改和删除操作（或是不删除）。
4. 反映历史变化（随着时间变化）：记录了企业从过去某一时刻到当前各个阶段的信息，可对发
   展历程和未来趋势做定量分析和预测。

![image-20240402010612109](/assets/image-20240402010612109.DkxR69QM.png)

[什么是数据仓库？它和数据库的区别是什么？看这一篇就够了 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/433495465)

[数据仓库--数据分层（ETL、ODS、DW、APP、DIM）\_数仓分层dim-CSDN博客](https://blog.csdn.net/hello_java_lcl/article/details/107025192)

#### :tulip:例题

```
某集团公司下属有多个超市，每个超市的所有销售数据最终要存入公司的数据仓库中。假设该公司高管需要从时间、地区和商品种类三个维度来分析某家电商品的销售数据，那么最适合采用(B) 来完成。
A DataExtraction 数据清理
B OLAP 连接分析
C OLTP 事务
D ETL 抽取
```

## 数据库设计过程

### 数据库设计过程

#### 数据库设计过程图

![软考-数据库设计过程](/assets/nUhEtf9xLKZX7Q2.C-pGjcxS.png)

数据流图简称（OFD）

数据字典简称（DD）

聚簇索引在物理设计里

#### 例题

```
关系规范化在数据库设计的 (C) 阶段进行。
A需求分析
B概念设计
C逻辑设计
D物理设计
```

### 概念设计阶段

#### 考点1：概念设计过程

![image-20240403001518713](/assets/image-20240403001518713.VdjtGtrM.png)

集成的方法：

1. 多个局部E-R图一次集成。
2. 逐步集成，用累加的方式一次集成两个局部E-R。

集成产生的冲突及解决办法：（针对同一对象）

1. 居性冲突：包括属性域冲突和属性取值冲突。
2. 命名冲突：包括同名异义和异名同义。
3. 结构冲突：包括同一对象在不同应用中具有不同的抽象，以及同一实体在不同局部E-R图中所包含的属性个数和属性排列次序不完全相同。

#### 考点2：E-R图

##### E-R模型

![image-20240403002146866](/assets/image-20240403002146866.BaeHsCzu.png)

\*\*实体型：\*\*用矩形表示，矩形框内写明实体名。

\*\*属性：\*\*用椭圆形表示，并用无向边将其与相应的实体型连接起来。

\*\*联系：\*\*用菱形表示，菱形框内写明联系名，并用无向边分别与有关实体型连接起来，同时在无向边旁标上联系的类型（1∶1，1∶n或m∶n）。（联系可以具有属性）。

**特殊化**：用线条加圈或者矩形加平行线。

实体：实体是现实世界中可以区别于其他对象的事件或事物。(实体集一实体的集合)

属性：居性是实体某方面的特性。

联系：突体的联系分为实体内部的联系和实体与实体间的联系。实体间联系类型: 1:1，1:\*，\*:\*

##### 属性

1. 简单属性和复合属性
   * **简单属性**是原子的，不可再分的；
   * **复合属性**可以细分为更小的部分 (即划分为别的属性)
2. 单值属性和多值属性
   * 定义的属性对于一个特定的实体都只有单独的一个值，称为单值属性
   * 在某些特定情况下，一个属性可能对应一组值，称为多值属性。
3. NULL属性：表示无意义或不知道。
4. 派生属性：可以从其他属性得来。

##### 联系类型判断

两个不同实体集之间联系

* 一对一（1:1）

* 一对多（1:n）

* 多对多（m:n）

两个以上不同实体集之间的联系（三元联系）多重度的确定可根据语义直接转换

* 以三元关系中的一个实体作为中心，假设另两个实体都只有一个实例；
* 若中心实体只有一个实例能与另两个实体的一个实例进行关联，则中心实体的连通数为“一”；
* 若中心实体有多于一个实例能与另两个实体实例进行关联，则中心实体的连通数为“多”。

同一个实体集内的二元联系

##### 扩充的E-R模型

弱实体：在现实世界中有一种特殊的依赖联系;该联系是指某实体是否存在对于另些实体具有很强的依赖关系，即一个实体的存在必须以另一个实体为前提，而将这类实体称为弱实体，如家属与职工的联系，附件与邮件。
特殊化：在现实世界中，某些实体一方面具有一些共性，另一方面还只有各自的特性，一个实体集可以按照某些特征区分为几个子实体。

聚集：一个联系作为另一个联系的一端。

### 逻辑结构设计

#### 考点1：关系模式相关概念

##### 数据模型

层次模型

网状模型

关系模型

面向对象模型

注：数据模型三要素:数据结构、数据操作、数据的约束条件

##### 关系模型相关概念

* 目或度：关系模式中属性的个数
* 侯选码（候选键）
* 主码（主键）
* 主属性与非主属性：组成候选码的属性就是主属性，其它的就是非主属性。
* 外码（外键）
* 全码（ALL-Key）：关系模式的所有属性组是这个关系的候选码。

##### 关系的3种类型

1. 基本关系
2. 查询表
3. 视图表

##### 完整性约束

1. 实体完整性约束（主键：唯一、非空）
2. 参照完整性约束（外键：其他关系主键或为空）
3. 用户自定义完整性约束（check条件）

注：触发器完成复杂完整性约束条件设定

#### 考点2：E-R图转关系模式

##### 逻辑结构设计-E-R模型转关系模式

* 一个实体型必须转换为一个关系模式
* 联系转关系模式

![image-20240405015332787](/assets/image-20240405015332787.CfRrV_fw.png)

（1）一对一联系的转换有两种方式。

* 独立的关系模式：并入两端主键及联系自身属性。（主键：任一键主键）
* 归并（可以并入任意一端）：并入另一端主键及联系自身属性。（主键：保持不变）

![image-20240405020042915](/assets/image-20240405020042915.bsGAaJUS.png)

（2）一对多联系的转换有两种方式。

* 独立的关系模式：并入两端主键及联系自身属性。（主键：多端主键）
* 归并（只能并入多端）：并入另一端主键及联系自身属性。（主键：保持不变）

![image-20240405020308355](/assets/image-20240405020308355.D0f6VSw2.png)

（3）多对多联系的转换只有一种方式

* 独立的关系模式：并入两端主键及联系自身属性。 （主键：两端主键的组合键）

![image-20240405020710688](/assets/image-20240405020710688.BnkLyGJK.png)

##### E-R图转关系模式

| 联系类型 | 实体（独立关系模式） | 联系（独立关系模式） | 联系（归并关系模式） | 备注       |
| -------- | -------------------- | -------------------- | -------------------- | ---------- |
| 1对1     | √                    | √                    | √                    | 并入任一端 |
| 1对多    | √                    | √                    | √                    | 并入多端   |
| 多对多   | √                    | √                    | ×                    |            |

## 关系代数

（2-4分）

### 关系代数–并、交、差

![image-20240405225245679](/assets/image-20240405225245679.6TGUhmbQ.png)

垂直：属性列，目，度

水平：元组行，记录，实例

并(结果为二者元组之和去除重复行)

交(结果为二者重复行)

差(前者去除二者重复行)

### 关系代数–笛卡尔积、投影、选择

![在这里插入图片描述](/assets/20201022161828318.DQVAvxO3.png)

笛卡尔积：属性列数为二者属性列数之和，元组行数为二者乘积。

投影：对属性列的选择列出，就是把需要的字段显示出来即可。

选择：选择是根据某些条件对关系做水平切割，对元组行的选择列出如上述中Sn0-Sn0003(S1)，是指从S1中选择Sno字段为Snooo3的数据。

选择操作不会操作表格结构，笛卡尔积和投影可能会修改

### 关系代数–自然连接

![在这里插入图片描述](/assets/20201022170824339.yqcJYwn2.png)

自然连接：属性列数是二者之和减去重复列数，元祖行与同名属性列取值相等

性能比较：

自然连接>笛卡尔

两侧数据应尽量先压缩、筛选

select 是投影的结果

from 多个表格是笛卡尔的结果

where 选择

### 例题

```
给定关系R (A，B，C，D)和关系S (A，C，E，F)，对其进行自然连接运R&S算后的属性列为 (C)个;
与oR.B>S.E(R&S)等价的关系代数表达式为(B)
A 4  B 5   C 6     D 8
A σ2>7(R×S)
B π1,2,3,4,7,8(σ1=5∧2>7∧3=6(R×S))
C σ2>'7'(R×S)
D π1,2,3,4,7,8(σ1=5∧2>'7'∧3=6(R×S))

下列查询B=“大数据”且F=“开发平台”，结果集属性列为A、B、C、F的关系代致表达式中，查询效率最高的是 (D) 。
A π1,2,3,8(σ2='大数据'∧1=5∧3=6∧8='开发平台'(R×S))
B π1,2,3,8(σ1=5∧3=6∧8='开发平台'(σ2='大数据'(R)×S)
C π1,2,3,8(σ2='大数据'∧1=5>3=6(R×σ4='开发平台'(S))
D π1,2,3,8(σ1=5∧3=6(σ2='大数据'(R)×σ4='开发平台'(S))
```

[数据库系统之：关系代数详解-超详细\_数据库关系代数-CSDN博客](https://blog.csdn.net/JavaEEKing/article/details/109223552)

## 规范化理论

考点1：规范化理论基本概念

考点2：范式判断

考点3：模式分解

### 考点1：规范化理论基本概念

考的很频繁（上午2分，下午可能有）

#### 函数依赖

**定义**：设R(U)是属性U上的一个关系模式，X和Y是U的子集，r为R的任一关系，如果对于r中的任意两个元组u，v，只要有u\[X]=v\[X]，就有u\[Y]=v\[Y]，则称X函数决定Y，或称Y函数依赖于X，记为X→Y。（X决定因素，Y被决定因素）

函数依赖可扩展以下两种规则：

1. **部分函数依赖**：A可确定C，（A，B）也可确定C，（A，B）中的一部分（即A）可以确定C，称为部分函数依赖。
2. **传递函数依赖**：当A和B不等价时，A可确定B，B可确定C，则A可确定C，是传递函数依赖；若A和B等价，则不存在传递，直接就可确定C。

![在这里插入图片描述](/assets/4f8570206a4b4d40bf7f03e7b488205a.DlP5ahtZ.png)

#### 规范化理论一Amstrong公理体系

关系模式R\<U，F>来说有以下的推理规则:

A1.**自反律** (Reflexivity) ：若Y∈X∈U，则X→Y成立。

A2.**增广律** (Augmentation) ：若Z∈U且X→Y，则XZ→YZ成立。

A3.**传递律** (Transitivity) ：若X→Y且Y→Z，则X→Z成立。

根据A1，A2，A3这三条推理规则可以得到下面三条推理规则：

**合并规则**：由X→Y，X→Z，有X→YZ。(A2，A3)

**伪传递规则**：由X→Y，WY→Z，有XW→Z。(A2，A3)

**分解规则**：由X→Y及Z∈Y，有X→Z。(A1，A3)

#### 候选键

超键：能唯一标识此表的属性的组合。

候选键：超键中去掉冗余的属性，剩余的属性就是候选键。

主键：任选一个候选键，即可作为主键。

外键：其他表中的主键。

图示法求候选

1. 将关系的函数依赖关系，用“有向图”的方式表示。
2. 找出**入度为0**的属性，并以该属性集合为起点，尝试遍历有向图，若能正常遍历图中所有结点，则该属性集即为关系模式的候选键。
3. 若入度为0的属性集不能遍历图中所有结点，则需要尝试性的将一些**中间结点** (既有入度，也有出度的结点)并入入度为0的属性集中，直至该集合能遍历所有结点，集合为候选键。

![image-20240407001349605](/assets/image-20240407001349605.DyUvij5e.png)

#### 主属性与非主属性

主属性：候选键内的属性为主属性，其他属性为非主属性。

```markdown
例1: 关系模式CSZ(CITV，ST，ZIP）, 其属性组上的函数依赖集为:F={（CITV，ST）→ZIP，ZIP→CITV},其中CITY表示城市，ST表示街道，ZIP表示邮政编码。
候选键有两个，都为组合键：（ST，CITV）（ST，ZIP）
主属性：CITV，ST，ZIP
非主属性：没有


例2: 若给定的关系模式为R，U={A,B,C}， F={AB→C,C→B}，则关系R(B)。
A有2个候选关键字AC和BC，并且有3个主属性
B有2个候选关键字AC和AB，并且有3个主属性
C只有一个候选关键字AC，并且有1个非主属性和2个主属性
D 只有一个候选关键字AB，并且有1个非主属性和2个主属性

例3: 给定关系模式R(U，F)，其中:U为关系模式R中的属性集，F是U上的一组函数依赖。假设U={A1，A2，A3，A4}，F={A1→A2，A1A2→A3，A1→A4，A2→A4}，那么关系R的主键应为 (A)。函数依赖集F中的(D)是冗余的。
A A1     B A1A2     C A1A3   D A1A2A3
A A1→A2  B A1A2→A3  C A1→A4  D A2→A4

例4: 给定关系模式RU，F>，其中U为属性集，F是U上的一组函数依赖，那么Armstrong公理系统的伪传递律是指 (B)。问题1选项
A 若X→Y，X→Z，则X→YZ为F所蕴涵   合并律
B 若X→Y，WY→Z，则XW→Z为F所蕴涵    伪传递律
C 若X→Y，Y→Z为F所蕴涵，则X→Z为F所蕴涵   传递率
D 若X→Y为F所蕴涵，且Z∈U，则XZ→YZ为F所蕴涵    增广律
```

实体完整性约束：即主键约束，主键值不能为空，也不能重复。

参照完整性约束：即外键约束，外键必须是其他表中已经存在的主键的值，或者为空。

用户自定义完整性约束：自定义表达式约束，如设定年龄属性的值必须在0到150之间。

### 考点2：范式判断

规范化问题：

数据冗余

修改异常

插入异常

删除异常

第一范式1NF：在关系模式R中，当且仅当所有域只包含原子值，即每个属性都是不可再分的数据项，则称关系模式R是第一范式。

第二范式2NF：当且仅当R是1NF，且每一个非主属性完全依赖主键（不存在部分依赖）时，R就是2NF。

比较典型的例子就是候选键是单属性，单属性是不可能存在部分函数依赖的。

```
思考题: 关系模式SC(学号，课程号，成绩，学分)，其中: (学号，课程号一成绩，课程号一学分，会存在哪些问题 (从数据兄余、更新异常、插入异常、删除异常这几个方面来考虑)，解决方案是什么?
1、找候选键（学号、课程号）
2、找非主属性（成绩、学分）
3、判断非主属性是否存在对候选键的部分函数依赖。组合候选键中学号可以决定非主属性学分，所以存在，只满足第一范式。（学号，课程号一成绩）（课程号一学分）
```

第三范式3NF：当且仅当R是2NF，且R中没有非主属性传递依赖于候选键时，R就是3NF（此时，也不会存在部分依赖）。

一般解决方法是拆分传递依赖的非主属性为一个新的关系模式。本质就是主键要直接决定所有非主属性，不能通过非主属性间接决定。

```
思考题:学生关系 (学号，姓名，系号，系名，系位置)各属性分别代表学号，姓名，所在系号，系名称，系地址。思考该关系模式会存在哪些问题 (从数据冗余、更新异常、插入异常、删除异常这几个方面来考虑)，解决方案是什么?
1、找候选键（学号）。单属性候选键，至少满足第二范式
2、找非主属性（姓名，系号，系名，系位置）
3、判断是否满足3NF。不满足。(学号，姓名，系号)(系号，系名，系位置)
```

BC范式 (BCNF)：设R是一个关系模式，F是它的依赖集，R属于BCNF当且仅当其F中**每个依赖的决定因素必定包含R的某个候选码**。

```
例: 关系模式ST (S，T，J ）中，S表示学生，T表示老师，J表示课程。每一老师只教一门课程。每门课程有若干老师，某一学生选定某门课，就对应一个固定老师。
（T——J）（SJ——T）
1、找候选键（SJ、ST）
2、找非主属性（无）
3、判断2NF。没有非主属性，也就没有非主属性对候选键的部分函数依赖
4、判断3NF。没有非主属性，也就没有非主属性对候选键的陈传递依赖。
5、判断BC。关系一中决定因素T不包含候选键
```

![数据库范式（1NF、2NF、3NF、BCNF）\_1nf,2nf,3nf,bcnf的理解-CSDN博客](/assets/bb0592deffb84b52a6578b19f03f9c8d.BnJq1gW7.jpeg)

| 范式 | 属性不可再分 | 非主属性部分函数依赖于候选键 | 非主属性传递函数依赖于候选键 | 函数依赖左侧决定因素包含候选键 |
| ---- | ------------ | ---------------------------- | ---------------------------- | ------------------------------ |
| 1NF  | √            | 存在                         |                              |                                |
| 2NF  | √            | 不存在                       | 存在                         |                                |
| 3NF  | √            | 不存在                       | 不存在                       | 不满足                         |
| BCNF | √            | 不存在                       | 不存在                       | 满足                           |

例题

```
某公司数据库中的元件关系模式为P(元件号，元件名称，供应商，供应商所在地，库存量)，函数依赖集F如下所示:F={元件号一元件名称， (元件号，供应商)一库存量，供应商一供应商所在地}
元件关系的主键为 (B)，该关系存在冗余以及插入异常和删除异常等问题。为了解决这一间题需要将元件关系分解(C);分解后的关系模式可以达到(C)。
A元件号，元件名称
B元件号，供应商
C元件号，供应商所在地
D供应商，供应商所在地
A元件1 (元件号，元件名称，库存量)、元件2(供应商，供应商所在地)
B元件1 (元件号，元件名称)、元件2 (供应商，供应商所在地，库存量
C元件1 (元件号，元件名称)、元件2(元件号，供应商，库存量)、元件3(供立商，供应商所在地
D元件1 (元件号，元件名称)、元件2(元件号，库存量)、元件3(供应商，供应商所在地)、元件4 (供应商所在地，库存量)
AINF
B2NF
C3NF
D4NF
```

### 考点3：模式分解

---

---
url: /Java/解决方案/数据同步/0_数据同步.md
---

# 数据同步

## 1. 前言

在实际项目开发中，我们经常将 MySQL 作为业务数据库，用于事务性处理和数据持久化。而ES 作为查询数据库，用来实现读写分离，缓解 MySQL 数据库的查询压力，应对海量数据的复杂查询。

这其中有一个很重要的问题，就是如何实现 MySQL 数据库和 ES 的数据同步。本文将探讨几种常见的实现方案和技术选型。

## 2.数据同步方案

### 2.1 同步双写

这是一种最为简单的方式，在将数据写到 MySQL 时，同时将数据写到 ES。

![5a87604b-ed89-4dfc-80a3-69f897b0cee6](/assets/5a87604b-ed89-4dfc-80a3-69f897b0cee6.Dxzprc0k.png)

伪代码如下

```java
/**
 * 新增商品
*/
@Transactional(rollbackFor = Exception.class)
public void addGoods(GoodsDto goodsDto) {
    //1、保存Mysql
    Goods goods = new Goods();
    BeanUtils.copyProperties(goodsDto,goods);
    GoodsMapper.insert();
     
     //2、保存ES
     IndexRequest indexRequest = new IndexRequest("goods_index","_doc");
     indexRequest.source(JSON.toJSONString(goods), XContentType.JSON);
     indexRequest.setRefreshPolicy(WriteRequest.RefreshPolicy.IMMEDIATE);
     highLevelClient.index(indexRequest);
}
```

优点：

* 业务逻辑简单；
* 实时性高。

缺点：

* 硬编码，有需要写入 MySQL 的地方都需要添加写入 ES 的代码；
* 业务强耦合；
* 存在双写失败丢数据风险；
* 性能较差，本来 MySQL 的性能不是很高，再加一个 ES，系统的性能必然会下降。

### 2.2 异步双写

针对多数据源写入的场景，可以借助 MQ 实现异步的多源写入。

![c098f954-4f05-4e27-9342-d16ee474edac](/assets/c098f954-4f05-4e27-9342-d16ee474edac.CY1u6SjJ.png)

优点：

* 性能高；
* 不易出现数据丢失问题，主要基于 MQ 消息的消费保障机制，比如 ES 宕机或者写入失败，还能重新消费 MQ 消息；
* 多源写入之间相互隔离，便于扩展更多的数据源写入。

缺点：

* 硬编码问题，接入新的数据源需要实现新的消费者代码；
* 系统复杂度增加，引入了消息中间件；
* MQ是异步消费模型，用户写入的数据不一定可以马上看到，造成延时。

### 2.3 基于 SQL 抽取

上面两种方案中都存在硬编码问题，代码的侵入性太强，如果对实时性要求不高的情况下，可以考虑用定时器来处理：

1. 数据库的相关表中增加一个字段为 timestamp 的字段，任何 CURD 操作都会导致该字段的时间发生变化；
2. 原来程序中的 CURD 操作不做任何变化；
3. 增加一个定时器程序，让该程序按一定的时间周期扫描指定的表，把该时间段内发生变化的数据提取出来；
4. 逐条写入到 ES 中。

![c0043f97-4770-4c85-92d7-62387e545e62](/assets/c0043f97-4770-4c85-92d7-62387e545e62.D6zyek6l.png)

优点：

* 不改变原来代码，没有侵入性、没有硬编码；
* 没有业务强耦合，不改变原来程序的性能；
* Worker 代码编写简单不需要考虑增删改查。

缺点：

* 时效性较差，由于是采用定时器根据固定频率查询表来同步数据，尽管将同步周期设置到秒级，也还是会存在一定时间的延迟；
* 对数据库有一定的轮询压力，一种改进方法是将轮询放到压力不大的从库上。

> 经典方案：借助 Logstash 实现数据同步，其底层实现原理就是根据配置定期使用 SQL 查询新增的数据写入 ES 中，实现数据的增量同步。

### 2.4 基于 Binlog 实时同步

上面三种方案要么有代码侵入，要么有硬编码，要么有延迟，那么有没有一种方案既能保证数据同步的实时性又没有代入侵入呢？

当然有，可以利用 MySQL 的 Binlog 来进行同步。

![0333083a-6fec-4321-829f-5fb9b247bbbd](/assets/0333083a-6fec-4321-829f-5fb9b247bbbd.B0ZYG0by.png)

具体步骤如下：

* 读取 MySQL 的 Binlog 日志，获取指定表的日志信息；
* 将读取的信息转为 MQ；
* 编写一个 MQ 消费程序；
* 不断消费 MQ，每消费完一条消息，将消息写入到 ES 中。

优点：

* 没有代码侵入、没有硬编码；
* 原有系统不需要任何变化，没有感知；
* 性能高；
* 业务解耦，不需要关注原来系统的业务逻辑。

缺点：

* 构建 Binlog 系统复杂；
* 如果采用 MQ 消费解析的 Binlog 信息，也会像方案二一样存在 MQ 延时的风险。

## 3. 数据迁移工具选型

对于上面 4 种数据同步方案，“基于 Binlog 实时同步”方案是目前最常用的，也诞生了很多优秀的数据迁移工具，这里主要对这些迁移工具进行介绍。

这些数据迁移工具，很多都是基于 Binlog 订阅的方式实现，**模拟一个 MySQL Slave 订阅 Binlog 日志，从而实现 CDC**（Change Data Capture），将已提交的更改发送到下游，包括 INSERT、DELETE、UPDATE。

至于如何伪装？大家需要先了解 MySQL 的主从复制原理，需要学习这块知识的同学，可以看我之前写的高并发教程，里面有详细讲解。

### 3.1 Cannel

基于数据库增量日志解析，提供增量数据订阅&消费，目前主要支持 MySQL。

Canal 原理就是伪装成 MySQL 的从节点，从而订阅 master 节点的 Binlog 日志，主要流程为：

1. Canal 服务端向 MySQL 的 master 节点传输 dump 协议；
2. MySQL 的 master 节点接收到 dump 请求后推送 Binlog 日志给 Canal 服务端，解析 Binlog 对象（原始为 byte 流）转成 Json 格式；
3. Canal 客户端通过 TCP 协议或 MQ 形式监听 Canal 服务端，同步数据到 ES。

![0e91763e-50e8-4870-8583-d4abba1a6959](/assets/0e91763e-50e8-4870-8583-d4abba1a6959.CtzPx8_g.png)

下面是 Cannel 执行的核心流程，其中 Binlog Parser 主要负责 Binlog 的提取、解析和推送，EventSink 负责数据的过滤 、路由和加工，仅作了解即可。

![5fb504e9-256d-41a0-8663-4b97790c29d9](/assets/5fb504e9-256d-41a0-8663-4b97790c29d9.PA_dxnu0.png)

### 3.2 阿里云 DTS

数据传输服务 DTS（Data Transmission Service）支持 RDBMS、NoSQL、OLAP 等多种数据源之间的数据传输。

它提供了数据迁移、实时数据订阅及数据实时同步等多种数据传输方式。相对于第三方数据流工具，DTS 提供丰富多样、高性能、高安全可靠的传输链路，同时它提供了诸多便利功能，极大方便了传输链路的创建及管理。

特点：

* 多数据源：支持 RDBMS、NoSQL、OLAP 等多种数据源间的数据传输；
* 多传输方式：支持多种传输方式，包括数据迁移、实时数据订阅及数据实时同步；
* 高性能：底层采用了多种性能优化措施，全量数据迁移高峰期时性能可以达到70MB/s，20万的TPS，使用高规格服务器来保证每条迁移或同步链路都能拥有良好的传输性能；
* 高可用：底层为服务集群，如果集群内任何一个节点宕机或发生故障，控制中心都能够将这个节点上的所有任务快速切换到其他节点上，链路稳定性高；
* 简单易用：提供可视化管理界面，提供向导式的链路创建流程，用户可以在其控制台简单轻松地创建传输链路；
* 需要付费。

再看看 DTS 的系统架构。

![56f3faeb-e1bb-49f5-9076-3f5b696be66e](/assets/56f3faeb-e1bb-49f5-9076-3f5b696be66e.BjUMdU1M.png)

* 高可用：数据传输服务内部每个模块都有主备架构，保证系统高可用。容灾系统实时检测每个节点的健康状况，一旦发现某个节点异常，会将链路快速切换到其他节点。
* 数据源地址动态适配：对于数据订阅及同步链路，容灾系统还会监测数据源的连接地址切换等变更操作，一旦发现数据源发生连接地址变更，它会动态适配数据源新的连接方式，在数据源变更的情况下，保证链路的稳定性。

更多内容，请查看阿里官方文档：https://help.aliyun.com/product/26590.html

### 3.3 Databus

Databus 是一个低延迟、可靠的、支持事务的、保持一致性的数据变更抓取系统。由 LinkedIn 于 2013 年开源。

Databus 通过挖掘数据库日志的方式，将数据库变更实时、可靠的从数据库拉取出来，业务可以通过定制化 client 实时获取变更并进行其他业务逻辑。

特点：

* 多数据源：Databus 支持多种数据来源的变更抓取，包括 Oracle 和 MySQL。
* 可扩展、高度可用：Databus 能扩展到支持数千消费者和事务数据来源，同时保持高度可用性。
* 事务按序提交：Databus 能保持来源数据库中的事务完整性，并按照事务分组和来源的提交顺寻交付变更事件。
* 低延迟、支持多种订阅机制：数据源变更完成后，Databus 能在毫秒级内将事务提交给消费者。同时，消费者使用D atabus 中的服务器端过滤功能，可以只获取自己需要的特定数据。
* 无限回溯：对消费者支持无限回溯能力，例如当消费者需要产生数据的完整拷贝时，它不会对数据库产生任何额外负担。当消费者的数据大大落后于来源数据库时，也可以使用该功能。

再看看 Databus 的系统架构。

Databus 由 Relays、bootstrap 服务和 Client lib 等组成，Bootstrap 服务中包括 Bootstrap Producer 和 Bootstrap Server。

![0ff175b6-de60-476c-9de0-5cac6f88dee2](/assets/0ff175b6-de60-476c-9de0-5cac6f88dee2.UGONFCbT.png)

* 快速变化的消费者直接从 Relay 中取事件；
* 如果一个消费者的数据更新大幅落后，它要的数据就不在 Relay 的日志中，而是需要**请求 Bootstrap 服务，返回的将会是自消费者上次处理变更之后的所有数据变更快照。**

开源地址：https://github.com/linkedin/databus

### 3.4 其它

**Flink**

* 有界数据流和无界数据流上进行有状态计算分布式处理引擎和框架。
* 官网地址：https://flink.apache.org

**CloudCanal**

* 数据同步迁移系统，商业产品。
* 官网地址：https://www.clougence.com/?utm\_source=wwek

**Maxwell**

* 使用简单，直接将数据变更输出为json字符串，不需要再编写客户端。
* 官网地址：http://maxwells-daemon.io

**DRD**

* 阿里巴巴集团自主研发的分布式数据库中间件产品，专注于解决单机关系型数据库扩展性问题，具备轻量(无状态)、灵活、稳定、高效等特性。
* 官方地址：https://www.aliyun.com/product/drds

**yugong**

* 帮助用户完成从 Oracle 数据迁移到 MySQL。
* 访问地址：https://github.com/alibaba/yugong

### Kettle

Kettle是一款国外开源的ETL工具，用java编写，可以在视窗、Linux和Unix上运行，数据抽取高效稳定，中文称水壶。这个项目的主要程序员MATT想把各种数据放进一个水壶，然后以指定的格式流出。它是一个ETL工具集，允许你管理来自不同数据库的数据，并通过提供一个图形用户环境来描述你想做什么，而不是你想怎么做。Kettle中有两个脚本文件，转换和作业。转换完成了数据的基本转换，而作业完成了对整个工作流的控制。

优点：功能强大，支持几乎所有数据库；

缺点: 需要用户自己一步步配置，学习成本高；通过查询语句select同步的；

项目地址：https://github.com/pentaho/pentaho-kettle

https://www.jb51.net/article/267074.htm

https://www.cnblogs.com/easyjie/p/15740627.html

### DBMotion

https://www.cnblogs.com/libruce/p/18011741

```
version: '3.0'
services:
  #dts-mysql:
  #  image: mysql:latest
  #  container_name: dts-mysql
  #  environment:
  #    MYSQL_ROOT_PASSWORD: admin
  #  command: ['mysqld', '--character-set-server=utf8mb4', '--collation-server=utf8mb4_unicode_ci', --default-time-zone=UTC]
  #  ports:
  #    - "3307:3306"
  dts-api-server:
    image: squids/dbmotion-community:latest
    container_name: dts-api-server
    environment:
      - SERVER_MODE=DOCKER
      # - MYSQL_URI=root:admin@(dts-mysql)/dbmotion
      - MYSQL_URI=root:ek4601@192.168.100.106:3306/dbmotion
    #depends_on:
    #  - dts-mysql
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /var/dbmotion:/dbmotion/log
    privileged: true
  dts-ui:
    image: squids/dbmotion-ui-community:latest
    container_name: dts-ui
    ports:
      - "30000:80"
    depends_on:
      # - dts-mysql
      - dts-api-server
```

### Apache Seatunnel

[Apache Seatunnel部署（apache-seatunnel-2.3.3-bin.tar.gz + apache-seatunnel-web-1.0.0-bin.tar.gz）\_com.hazelcast.internal.diagnostics.healthmonitor-CSDN博客](https://blog.csdn.net/qq_36434219/article/details/134857560)

### StreamSets ETL

双写：https://blog.csdn.net/pbrlovejava/article/details/125926775

---

---
url: /常用框架/SpringBoot/SpringBoot与Web应用/3_数据验证.md
---

# 数据验证

---

---
url: /daily/高等数学/02_数列极限.md
---

# 数列极限

---

---
url: /01.指南/10.使用/45.私密文章.md
---

# 私密文章

私密文章需要一个登录页进行登录，如果你想先体验登录页的效果，在导航栏 功能页 -> 登录页 点击查看。

您也可以通过 `teek-login-page` 插槽自定义登录页。

```vue
<script setup lang="ts" name="TeekLayoutProvider">
import Teek from "vitepress-theme-teek";
import YourLoginPageComponent from "./YourLoginPageComponent";
</script>

<template>
  <Teek.Layout>
    <template #teek-login-page>
      <YourLoginPageComponent />
    </template>
  </Teek.Layout>
</template>
```

## 前言

私密文章功能默认没有和后端集成，因此只能 **防君子不防小人**。

当然 Teek 也提供了一些钩子函数，支持您自定义登录逻辑和加密解密，此时您可以全部重写 Teek 的登录逻辑，比如集成后端。

当然如果全部重写登录逻辑，那么就要思考一个问题：为什么不完全自己写一个呢？毕竟去完全熟悉 Teek 的登录逻辑也许比自己实现耗费的精力更多 :dog:。

## 安全检测代码

因为 VitePress 是静态页面，所以我们无法往后端获取登录信息，那么也就有一个问题，如果用户禁用 JavaScript，那么私密文章将不会进行验证，也就可以直接浏览私密文章内容，那么如何处理这个问题呢？

打开 `.vitepress/config.mts` 文件，给 head 模块添加如下信息：

```js
["noscript", {}, '<meta http-equiv="refresh" content="0; url={your link}">'];
```

`{your link}` 不要填写自己博客的任意地址，而是填写博客以外的地址，因为博客的页面总会触发这段代码，导致反复跳转该页面。

## 开启私密文章认证功能

这一步是必须的，请阅读 [功能页配置](/reference/function-page-config#私密文章-登录页) 来开启私密文章认证功能。

## 文章开启私密功能

如果你想给某篇文章开启私密功能，请在 `frontmatter` 中添加如下内容：

```yml
---
private: true
---
```

这是 **最基本也是必须的步骤**，开启了私密文章后，还需要配置对应的用户名和密码，看下面。

## 认证级别

私密文章认证有 4 种级别：

1. 单文章级别，每个文章有自己的用户名和密码
2. 领域文章级别，可以给多个文章设置一个领域（组织），在该领域认证后，则该领域的其他所有文章都可以访问，它是一个组织的概念，理解就行
3. 全局文章级别，在任意全局文章登录认证后，其他全局文章都可以访问
4. 站点级别，在进入站点的时候需要进行认证，该级别和私密文章认证没有关系

如果同时设置多个文章级别的认证信息，那么有如下规则：

* 一个文章同时设置 `单文章级别认证信息`、`领域文章级别认证信息`，则这两个级别的认证信息都对该文章 **生效**
* 一个文章同时设置 `单文章级别认证信息`、`全局文章级别认证信息`，则 `全局文章级别认证信息` 对该文章 **失效**
* 一个文章同时设置 `领域文章级别认证信息`、`全局文章级别认证信息`，则 `全局文章级别认证信息` 对该文章 **失效**
* 一个文章同时设置 `单文章级别认证信息`、`领域文章级别认证信息`、`全局文章级别认证信息`，则 `全局文章级别认证信息` 对该文章 **失效**，另外两个认证信息都对该文章 **生效**

从上面可以看出，`全局文章级别认证信息` 优先级是最低的，是一个兜底的配置。

站点级别认证有一个角色的概念，默认为 `common`，即当进入站点并登录认证后，访问任何私密文章仍然需要重新进行认证，而如果是 `admin` 角色，则进入站点后，所有文章都可以访问。

### 单文章级别

在某个私密文章的 `frontmatter` 中设置用户名和密码等配置：

```yml
---
private: true # 开启文章私密
username: teek # 用户名
password: teek # 密码
expire: 2d # 可选，登录失效时间，如果不填则以全局配置为准，全局设置默认为 1d
session: false # 可选，开启是否在网页关闭或刷新后，清除登录状态，这样再次访问网页，需要重新登录，默认为 false
strategy: once # 可选，登录策略，once 代表一次登录，always 代表每次访问都登录，默认为 once
loginInfo: [{ username: "teek1", password: "teek1" }, { username: "teek2", password: "teek2" }]
```

::: warning
`username` 或 `password` 不允许是纯数字，如果您只想配置纯数字，则用引号起来，如 `"1234"`、`'1234'`。
:::

可以看到 `frontmatter` 出现了 `username`、`password`，并且 `loginInfo` 里也出现多个 `username`、`password`。

这两种方式没有什么区别，无论是以 `username`、`password` 登录，还是 `loginInfo` 里的多个 `username`、`password` 都可以登录。

### 领域文章级别

有的时候，我们并不需要给每一个文章设置用户名和密码，而是按「组」的概念设置。

比如我们有 `指南` 和 `配置` 两个专题的文章，那么我可以对这两个专题进行认证，在 `.vitepress/config.mts` 文件配置如下：

```ts
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  private: {
    enabled: true,
    realm: {
      guide: [
        { username: "teek-guide-1", password: "teek" },
        { username: "teek-guide-2", password: "teek" },
      ],
      config: [
        { username: "teek-config-1", password: "teek" },
        { username: "teek-config-2", password: "teek" },
      ],
    },
  },
});
```

在 `指南` 的各个文章 `frontmatter` 配置绑定对于的 `realm`：

```yaml
private: true
privateRealm: guide
```

在 `配置` 的各个文章 `frontmatter` 配置绑定对于的 `realm`：

```yaml
private: true
privateRealm: config
```

### 全局文章级别

有时候我们想直接提供一些用户名和密码，它能访问页面的所有文章，那么可以使用全局文章级别配置：

```ts
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  private: {
    enabled: true,
    pages: [
      { username: "tee-pages-1", password: "teek" },
      { username: "tee-pages-1", password: "teek" },
    ],
  },
});
```

此时可以访问 **非单文章级别、非领域文章级别** 的其他任何私密文章。

### 站点级别

站点级别的认证主要是卡控站点的访问，而不是文章的访问。

在 `.vitepress/config.mts` 里开启站点级别认证并设置认证信息：

```ts
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  private: {
    enabled: true,
    siteLogin: true,
    site: [
      { username: "teek-site-1", password: "teek" },
      { username: "teek-site-2", password: "teek" },
    ],
  },
});
```

这些用户名和密码默认是 `common` 角色，即只适用于进入站点时登录，当访问任意私密文章时，仍需单独对私密文章认证。

如果想登录站点后可以访问所有私密文章，则给账号开启 `admin` 角色进行如下配置：

```ts {11}
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  private: {
    enabled: true,
    siteLogin: true,
    site: [
      { username: "teek-site-1", password: "teek" },
      { username: "teek-site-2", password: "teek" },
      { username: "teek-site-2", password: "teek", role: "admin" },
    ],
  },
});
```

## 其他配置

除了用户名和密码之外，Teek 也有其他的配置项来加强私密文章功能，更多配置请阅读 [功能页配置](/reference/function-page-config#私密文章-登录页)

---

---
url: /Python/AI大模型应用开发/3_提示词工程.md
---

# 提示词工程

## 一、什么构成了一个好的提示？

与大语言模型高效沟通的核心——提示（Prompt）

### 1、什么是提示工程？

* 提示的定义
  * 提示是我们输入给AI的问题或指示，是AI生成回应的基础。
* 提示的重要性
  * 高质量的提示能显著提升AI的理解能力、执行效率和输出准确性。
* 提示工程的本质
  * 研究如何优化与AI的沟通方式，核心在于**提示的设计与迭代优化**。

***

### 2、OpenAI官方推荐的七大提示工程原则

1. 使用最新模型
2. 指令前置 + 分隔符
3. 具体、详细、描述性强
4. 提供输出格式示例
5. 先零样本，后小样本
6. 要求具体明确，避免空洞
7. 正向引导，明确应做之事

#### 原则一 —— 使用最新模型

* 模型版本演进
  * 早期模型如 `text-ada`, `text-babbage`, `text-curie`, `text-davinci` 主要用于文本补全。
  * 当前主流为擅长对话的 `GPT-3.5 Turbo`、`GPT-4` 等。
* 选择建议
  * 优先选用最新模型（字母靠后或数字更大），理解与生成能力更强。
  * 成本是次要考虑因素，需权衡性能与预算。

#### 原则二 —— 指令前置并清晰分隔

* 错误做法

  * 指令与上下文混在一起，如：“总结以下内容……\[文本]”。

* 正确做法

  * 将指令放在提示开头，并用`"""`或`!!!`明确分隔指令与上下文：

    ```markdown
    请将以下文本的要点用列表形式总结：
    """
    [要总结的文本内容]
    """
    ```

  * 这有助于模型更好地区分“做什么”和“对什么做”。

#### 原则三 —— 具体、详细、描述性强

* 模糊提示示例
  * “写一首关于OpenAI的诗” → 输出不可控。
* 优化后的提示示例
  * “写一首12行的现代诗，主题为‘AI如何改变人类创造力’，风格要富有哲思和诗意，避免技术术语。”
  * 包含了**长度、题材、风格、主题、限制条件**等细节，输出更符合预期。

#### 原则四 —— 提供输出格式示例

* 无格式引导的问题

  * “从文本中提取公司名、人名、主题、子主题” → 输出格式随意。

* 带模板的提示

  * 提供结构化模板：

    ```
    公司名: [用逗号分隔]
    人名: [用||分隔]
    主题: [用||分隔]
    子主题: [用||分隔]
    ```

  * 模型会模仿该格式输出，便于后续程序解析。

#### 原则五 —— 先零样本，后小样本

* 零样本提示（Zero-shot）
  * 直接给出指令，不提供任何示例。
  * 适用于简单任务或模型已具备相关能力。
* 小样本提示（Few-shot）
  * 当零样本效果不佳时，提供1-3个输入-输出示例。
  * 示例能帮助模型理解任务模式和期望输出。
  * 后续章节将深入讲解小样本学习技巧。

#### 原则六 —— 避免空洞描述，要求具体明确

* 模糊描述示例
  * “描述要短，不要太多” → 不够具体。
* 精确描述示例
  * “用3到5句话组成一个段落来描述此产品。”
  * 明确了长度和结构，减少歧义。

#### 原则七 —— 告诉AI“应该做什么”，而非“不要做什么”

* 负面指令的局限
  * 如：“不要问用户名或密码，不要重复。”
  * 只限制了行为，未指明正确方向。
* 正向引导更有效
  * 如：“你的任务是引导用户查阅帮助文档解决问题。请勿询问任何个人隐私信息。”
  * 明确了目标行为，引导AI采取积极行动。

## 二、限定输出格式

### 1、为何需要指定输出格式

* 影响信息消费效率
  * 明确的输出格式有助于用户更快地理解和消化信息。
* 便于后续处理
  * 当使用API与AI交互时，明确的输出格式可以简化从响应中提取信息的过程。
  * 通过指定格式（如JSON、XML、YAML等），可以减少代码处理模糊信息结构的复杂性。

### 2、OpenAI对输出格式的支持

* 支持JSON模式的新模型
  * OpenAI在新模型上开始支持JSON模式，确保输出的有效性和一致性。
* 旧模型仍需提示控制格式
  * 对于不支持JSON模式的旧模型，仍然需要通过提示来要求特定的输出格式。

### 3、JSON的基本概念与语法

* JSON简介
  * JSON（JavaScript Object Notation）是一种轻量级的数据交换格式，易于人类阅读和编写，也易于机器解析和生成。
* 数据结构
  * **对象**：由键值对组成，用大括号 `{}` 表示。键必须是字符串，值可以是字符串、数字、布尔值、数组或另一个对象。
  * **数组**：有序的值集合，用方括号 `[]` 表示。数组中的元素可以是任意类型的数据。

### 4、JSON的具体语法细节

* 键值对规则
  * 键必须是字符串，并且需要用双引号 `" "` 包围。
  * 值可以是字符串、数字、布尔值、数组或对象。
  * 每个键值对之间用逗号 `,` 分隔。
* 数据类型的注意事项
  * **字符串**：必须用双引号包围，单引号无效。
  * **数字**：整数或浮点数均可。
  * **布尔值**：`true` 和 `false`，注意区分大小写，与Python不同。
  * **数组**：支持嵌套，允许复杂的层次结构。
  * **对象**：同样支持嵌套，增加灵活性。
  * **空值**：用 `null` 表示，类似于Python中的 `None`。

### 5、Python与JSON的转换

* 使用Python的json库

  * 将JSON字符串转换为Python字典或列表：

    ```python
    import json
    json_string = '{"name": "Alice", "age": 30}'
    python_data = json.loads(json_string)
    print(python_data)  # 输出: {'name': 'Alice', 'age': 30}
    ```

  * 将Python字典或列表转换为JSON字符串：

    ```python
    python_dict = {'name': 'Alice', 'age': 30}
    json_string = json.dumps(python_dict)
    print(json_string)  # 输出: {"name": "Alice", "age": 30}
    ```

### 6、将AI返回值处理成JSON格式

```python
# 导入库并创建客户端
from openai import OpenAI
import json
client = OpenAI()
# 构建任务提示词
prompt = f"""
生成一个由三个虚构的订单信息所组成的列表，以JSON格式进行返回。
JSON列表里的每个元素包含以下信息：
order_id、customer_name、order_item、phone。
所有信息都是字符串。
除了JSON之外
"""
# 封装通用请求函数
response = client.chat.completions.create(
  model="gpt-3.5-turbo",
  messages=[
    {
      "role": "user",
      "content": prompt
    }
  ]
)
# 获取AI响应
content = response.choices[0].message.content
# JSON格式化
json.loads(content)
```

```python
[{'order_id': '123',
  'customer_name': 'John Smith',
  'order_item': 'Shoes',
  'phone': '1234567890'},
 {'order_id': '456',
  'customer_name': 'Jane Doe',
  'order_item': 'T-shirt',
  'phone': '9876543210'},
 {'order_id': '789',
  'customer_name': 'Tom Thompson',
  'order_item': 'Jeans',
  'phone': '4567890123'}]
```

```python
json.loads(content)[0]["phone"]
```

```python
'1234567890'
```

## 三、零样本VS小样本

### 1、什么是小样本提示？

* 零样本提示（Zero-Shot Prompting）
  * 直接向AI提出问题或指令，不提供任何示例。
  * 示例：`"总结以下文本：..."`
  * 缺点：输出可能不符合预期格式或风格，效果不稳定。
* 小样本提示（Few-Shot Prompting）
  * 在提问前，先提供1到多个“输入-输出”示例作为示范。
  * AI会基于这些示例进行上下文学习（In-Context Learning）：
    * 记忆示例中的知识。
    * 模仿示例的格式、风格和逻辑进行回应。
  * 优势：无需训练模型，即可让AI快速适应新任务，成本低且灵活。

### 2、如何实现小样本提示

* 使用 `messages` 参数构建对话历史
  * 在调用 `client.chat.completions.create()` 时，`messages` 参数可以是一个包含多轮对话的列表。
  * 每条消息是一个字典，包含`role`和`content`
    * `role="user"`：表示用户输入。
    * `role="assistant"`：表示AI的示范回答。

### 3、零样本提示示例

```python
# 导入库并创建客户端
from openai import OpenAI
client = OpenAI()
# 封装通用请求函数
response = client.chat.completions.create(
  model="gpt-3.5-turbo",
  messages=[
    {
      "role": "user",
      "content": "格式化以下信息：\n姓名 -> 张三\n年龄 -> 27\n客户ID -> 001"
    }
  ]
)
print(response.choices[0].message.content)
```

```python
姓名: 张三
年龄: 27
客户ID: 001
```

### 4、小样本提示示例

```python
# 封装通用请求函数
response = client.chat.completions.create(
  model="gpt-3.5-turbo",
  messages=[
    {
      "role": "user",
      "content": "格式化以下信息：\n姓名 -> 张三\n年龄 -> 27\n客户ID -> 001"
    },
    {
      "role": "assistant",
      "content": "##客户信息\n- 客户姓名：张三\n- 客户年龄：27岁\n- 客户ID：001"
    },
    {
      "role": "user",
      "content": "格式化以下信息：\n姓名 -> 李四\n年龄 -> 42\n客户ID -> 002"
    },
    {
      "role": "assistant",
      "content": "##客户信息\n- 客户姓名：李四\n- 客户年龄：42岁\n- 客户ID：002"
    },
    {
      "role": "user",
      "content": "格式化以下信息：\n姓名 -> 王五\n年龄 -> 32\n客户ID -> 003"
    }
  ]
)
response.choices[0].message.content
```

```python
'## 客户信息\n- 客户姓名：王五\n- 客户年龄：32岁\n- 客户ID：003'
```

## 四、思维链与分步骤思考

### 1、小样本提示的瓶颈

* 数学与逻辑推理的挑战
  * 尽管小样本提示在多数任务中表现优异，但在**数学计算、逻辑推理**等复杂任务上效果有限。
  * 示例：即使提供正确答案的示范，AI仍可能出错（如将奇数相加结果误算为53，实际应为41）。
* 根本原因分析
  * AI生成每个Token的时间基本恒定，不会因“需要更多思考”而延长。
  * 因此，面对复杂问题时，AI倾向于“跳步”或“猜测”，导致错误累积。
  * 单纯的结果示范无法教会AI“如何思考”。

***

### 2、思维链的核心理念

* 定义与起源

  * 思维链（Chain-of-Thought）由谷歌在2022年提出，是一种引导AI进行**分步推理**的提示技术。
  * 核心思想：让AI像人类一样，通过中间推理步骤逐步解决问题。

* 工作原理

  * 在小样本提示中，不仅提供**输入和最终答案**，还展示**详细的推理过程**。

  * AI会模仿这种“思考路径”，在生成答案时也输出中间步骤。

  * 示例：

    ```python
    问题：小明有3个苹果，买了5个，吃了2个，还剩几个？
    推理步骤：
    1. 初始数量：3个
    2. 购买后：3 + 5 = 8个
    3. 吃掉后：8 - 2 = 6个
    答案：6个
    ```

### 3、为何思维链更有效？

* 降低认知负荷
  * 将复杂任务分解为多个简单步骤，每步只需关注局部信息。
  * 类比：学生被点名回答问题时，边说边想比瞬间给出答案更容易成功。
* 减少上下文干扰
  * 每个推理步骤聚焦当前任务，避免被无关信息干扰。
  * 提高逻辑连贯性和准确性。
* 适用范围广泛
  * 不仅限于数学计算，还可用于：
    * 常识推理（如时间、因果关系）
    * 符号推理（如逻辑谜题）
    * 复杂决策分析
    * 文本理解与推断

### 4、低成本思维链技巧：Zero-Shot CoT

简单高效的提示词，即使不使用小样本提示，只需在问题后加上：

```python
Let's think step by step.
```

或中文：

```python
让我们来分步骤思考。
```

示例代码

```python
# 导入库并创建客户端
from openai import OpenAI
client = OpenAI()
```

不使用思维链提示

```python
# 封装通用请求函数
response = client.chat.completions.create(
  model="gpt-3.5-turbo",
  messages=[
    {
      "role": "user",
      "content": "该组中的奇数加起来为偶数：4、8、9、15、12、2、1，对吗？"
    },
    {
      "role": "assistant",
      "content": "所有奇数相加等于25。答案为否。"
    },
    {
      "role": "user",
      "content": "该组中的奇数加起来为偶数：17、10、19、4、8、12、24，对吗？"
    },
    {
      "role": "assistant",
      "content": "所有奇数相加等于36。答案为是。"
    },
    {
      "role": "user",
      "content": "该组中的奇数加起来为偶数：15、12、5、3、72、17、1，对吗？"
    },
  ]
)
print(response.choices[0].message.content)
```

```python
所有奇数相加等于53。答案为否。
```

```python
# 封装通用请求函数
response = client.chat.completions.create(
  model="gpt-3.5-turbo",
  messages=[
    {
      "role": "user",
      "content": "该组中的奇数加起来为偶数：4、8、9、15、12、2、1，对吗？"
    },
    {
      "role": "assistant",
      "content": "所有奇数（9、15、1）相加，9 + 15 + 1 = 25。答案为否。"
    },
    {
      "role": "user",
      "content": "该组中的奇数加起来为偶数：17、10、19、4、8、12、24，对吗？"
    },
    {
      "role": "assistant",
      "content": "所有奇数（17、19）相加，17 + 19 = 36。答案为是。"
    },
    {
      "role": "user",
      "content": "该组中的奇数加起来为偶数：15、12、5、3、72、17、1，对吗？"
    },
  ]
)
print(response.choices[0].message.content)
```

```python
所有奇数（15、5、3、17、1）相加，15 + 5 + 3 + 17 + 1 = 41。答案为否。
```

使用思维链提示

```python
# 封装通用请求函数
response = client.chat.completions.create(
  model="gpt-3.5-turbo",
  messages=[
    {
      "role": "user",
      "content": "该组中的奇数加起来为偶数：15、12、5、3、72、17、1，对吗？让我们来分步骤思考。"
    },
  ]
)
print(response.choices[0].message.content)
```

```python
是的，让我们来分步骤思考这个问题。

首先，我们将奇数从该组中提取出来，这些数字是：15、5、3、17、1。

然后，我们将这些奇数相加：15 + 5 + 3 + 17 + 1 = 41。

最后，我们检查41这个结果是否为偶数。因为41是奇数，所以该组中的奇数加起来不为偶数。

所以，该组中的奇数加起来不为偶数。
```

---

---
url: /Java/Java开发技巧/03.高效编程/6_替换jar包中单个class文件.md
---

# 替换jar包中单个class文件

正式环境问题紧急修复，只修改了一个类中的方法，直接替换单个class。

## 操作步骤

1、将本地要进行替换的java类，编译成.class文件；

2、 将服务器中的xxx.jar包下载下来，放在单独文件夹下，使用cmd或者git终端工具进入命令行，查看要替

换的.class文件在xxx.jar包中所在的具体路径，

命令如下：

```bash
 jar -tvf archive-manage-service.jar | grep PigeonholeCaseInfoSynHandlerImpl.class
```

3、通过第2步中获取到的路径，将xxxdemo.jar中指定的.class解压出来，

命令如下：

```bash
jar -xvf archive-manage-service.jar  BOOT-INF/classes/cn/com/chnsys/handler/impl/PigeonholeCaseInfoSynHandlerImpl.class
```

4、将第一步中的新的class文件替换到解压出来的文件夹中。

5、将替换后的.class重新打进xxxdemo.jar中即可。

命令如下：

```bash
jar -uvf archive-manage-service.jar  BOOT-INF/classes/cn/com/chnsys/handler/impl/PigeonholeCaseInfoSynHandlerImpl.class
```

只替换一个文件：`jar -uvf xxx.jar com/test.class`

替换文件夹中多个文件：`jar -uvf xxx.jar com/demo`

## 补充

替换 BOOT-INF/lib 下的jar包里的class

https://www.cnblogs.com/Marlo/p/15674097.html

https://wenku.baidu.com/view/10fabd6f7cd5360cba1aa8114431b90d6c858928.html

## 反编译软件

1. IDEA 反编译插件（Jadx Class Decompiler）
2. XJad
3. jd-gui

---

---
url: /常用框架/SpringBoot/SpringBoot程序开发/1_统一父pom管理.md
---

# 统一父pom管理

---

---
url: /常用框架/SpringBoot/SpringBoot程序开发/13_统一日志处理.md
---

# 一、日志

## 1、配置日志级别

日志记录器（Logger）的行为是分等级的。如下表所示：

分为：OFF、FATAL、ERROR、WARN、INFO、DEBUG、ALL

默认情况下，spring boot从控制台打印出来的日志级别只有INFO及以上级别，可以配置日志级别

```properties
# 设置日志级别
logging.level.root=WARN
```

这种方式只能将日志打印在控制台上

# 二、Logback日志

spring boot内部使用Logback作为日志实现的框架。

Logback和log4j非常相似，如果你对log4j很熟悉，那对logback很快就会得心应手。

logback相对于log4j的一些优点：https://blog.csdn.net/caisini\_vc/article/details/48551287

## 1、配置logback日志

删除application.properties中的日志配置

**安装idea彩色日志插件：grep-console**

`resources`中创建`logback-spring.xml`

```xml
<?xml version="1.0" encoding="UTF-8"?>
<configuration  scan="true" scanPeriod="10 seconds">
    <!-- 日志级别从低到高分为TRACE < DEBUG < INFO < WARN < ERROR < FATAL，如果设置为WARN，则低于WARN的信息都不会输出 -->
    <!-- scan:当此属性设置为true时，配置文件如果发生改变，将会被重新加载，默认值为true -->
    <!-- scanPeriod:设置监测配置文件是否有修改的时间间隔，如果没有给出时间单位，默认单位是毫秒。当scan为true时，此属性生效。默认的时间间隔为1分钟。 -->
    <!-- debug:当此属性设置为true时，将打印出logback内部日志信息，实时查看logback运行状态。默认值为false。 -->
    <contextName>logback</contextName>
    <!-- name的值是变量的名称，value的值时变量定义的值。通过定义的值会被插入到logger上下文中。定义变量后，可以使“${}”来使用变量。 -->
    <property name="log.path" value="D:/guli_log/edu" />
    <!-- 彩色日志 -->
    <!-- 配置格式变量：CONSOLE_LOG_PATTERN 彩色日志格式 -->
    <!-- magenta:洋红 -->
    <!-- boldMagenta:粗红-->
    <!-- cyan:青色 -->
    <!-- white:白色 -->
    <!-- magenta:洋红 -->
    <property name="CONSOLE_LOG_PATTERN"
              value="%yellow(%date{yyyy-MM-dd HH:mm:ss}) |%highlight(%-5level) |%blue(%thread) |%blue(%file:%line) |%green(%logger) |%cyan(%msg%n)"/>
    <!--输出到控制台-->
    <appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">
        <!--此日志appender是为开发使用，只配置最底级别，控制台输出的日志级别是大于或等于此级别的日志信息-->
        <!-- 例如：如果此处配置了INFO级别，则后面其他位置即使配置了DEBUG级别的日志，也不会被输出 -->
        <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
            <level>INFO</level>
        </filter>
        <encoder>
            <Pattern>${CONSOLE_LOG_PATTERN}</Pattern>
            <!-- 设置字符集 -->
            <charset>UTF-8</charset>
        </encoder>
    </appender>
    <!--输出到文件-->
    <!-- 时间滚动输出 level为 INFO 日志 -->
    <appender name="INFO_FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <!-- 正在记录的日志文件的路径及文件名 -->
        <file>${log.path}/log_info.log</file>
        <!--日志文件输出格式-->
        <encoder>
            <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{50} - %msg%n</pattern>
            <charset>UTF-8</charset>
        </encoder>
        <!-- 日志记录器的滚动策略，按日期，按大小记录 -->
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <!-- 每天日志归档路径以及格式 -->
            <fileNamePattern>${log.path}/info/log-info-%d{yyyy-MM-dd}.%i.log</fileNamePattern>
            <timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
                <maxFileSize>100MB</maxFileSize>
            </timeBasedFileNamingAndTriggeringPolicy>
            <!--日志文件保留天数-->
            <maxHistory>15</maxHistory>
        </rollingPolicy>
        <!-- 此日志文件只记录info级别的 -->
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <level>INFO</level>
            <onMatch>ACCEPT</onMatch>
            <onMismatch>DENY</onMismatch>
        </filter>
    </appender>
    <!-- 时间滚动输出 level为 WARN 日志 -->
    <appender name="WARN_FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <!-- 正在记录的日志文件的路径及文件名 -->
        <file>${log.path}/log_warn.log</file>
        <!--日志文件输出格式-->
        <encoder>
            <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{50} - %msg%n</pattern>
            <charset>UTF-8</charset> <!-- 此处设置字符集 -->
        </encoder>
        <!-- 日志记录器的滚动策略，按日期，按大小记录 -->
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${log.path}/warn/log-warn-%d{yyyy-MM-dd}.%i.log</fileNamePattern>
            <timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
                <maxFileSize>100MB</maxFileSize>
            </timeBasedFileNamingAndTriggeringPolicy>
            <!--日志文件保留天数-->
            <maxHistory>15</maxHistory>
        </rollingPolicy>
        <!-- 此日志文件只记录warn级别的 -->
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <level>warn</level>
            <onMatch>ACCEPT</onMatch>
            <onMismatch>DENY</onMismatch>
        </filter>
    </appender>
    <!-- 时间滚动输出 level为 ERROR 日志 -->
    <appender name="ERROR_FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <!-- 正在记录的日志文件的路径及文件名 -->
        <file>${log.path}/log_error.log</file>
        <!--日志文件输出格式-->
        <encoder>
            <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{50} - %msg%n</pattern>
            <charset>UTF-8</charset> <!-- 此处设置字符集 -->
        </encoder>
        <!-- 日志记录器的滚动策略，按日期，按大小记录 -->
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${log.path}/error/log-error-%d{yyyy-MM-dd}.%i.log</fileNamePattern>
            <timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
                <maxFileSize>100MB</maxFileSize>
            </timeBasedFileNamingAndTriggeringPolicy>
            <!--日志文件保留天数-->
            <maxHistory>15</maxHistory>
        </rollingPolicy>
        <!-- 此日志文件只记录ERROR级别的 -->
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <level>ERROR</level>
            <onMatch>ACCEPT</onMatch>
            <onMismatch>DENY</onMismatch>
        </filter>
    </appender>
    <!--
        <logger>用来设置某一个包或者具体的某一个类的日志打印级别、以及指定<appender>。
        <logger>仅有一个name属性，
        一个可选的level和一个可选的addtivity属性。
        name:用来指定受此logger约束的某一个包或者具体的某一个类。
        level:用来设置打印级别，大小写无关：TRACE, DEBUG, INFO, WARN, ERROR, ALL 和 OFF，
              如果未设置此属性，那么当前logger将会继承上级的级别。
    -->
    <!--
        使用mybatis的时候，sql语句是debug下才会打印，而这里我们只配置了info，所以想要查看sql语句的话，有以下两种操作：
        第一种把<root level="INFO">改成<root level="DEBUG">这样就会打印sql，不过这样日志那边会出现很多其他消息
        第二种就是单独给mapper下目录配置DEBUG模式，代码如下，这样配置sql语句会打印，其他还是正常DEBUG级别：
     -->
    <!--开发环境:打印控制台-->
    <springProfile name="dev">
        <!--可以输出项目中的debug日志，包括mybatis的sql日志-->
        <logger name="com.guli" level="INFO" />
        <!--
            root节点是必选节点，用来指定最基础的日志输出级别，只有一个level属性
            level:用来设置打印级别，大小写无关：TRACE, DEBUG, INFO, WARN, ERROR, ALL 和 OFF，默认是DEBUG
            可以包含零个或多个appender元素。
        -->
        <root level="INFO">
            <appender-ref ref="CONSOLE" />
            <appender-ref ref="INFO_FILE" />
            <appender-ref ref="WARN_FILE" />
            <appender-ref ref="ERROR_FILE" />
        </root>
    </springProfile>
    <!--生产环境:输出到文件-->
    <springProfile name="pro">
        <!--可以输出项目中的debug日志，包括mybatis的sql日志-->
        <logger name="com.guli" level="WARN" />
        <root level="INFO">
            <appender-ref ref="ERROR_FILE" />
            <appender-ref ref="WARN_FILE" />
        </root>
    </springProfile>
</configuration>
```

## 2、将错误日志输出到文件

GlobalExceptionHandler.java 中

类上添加注解

```
@Slf4j
```

修改异常输出语句

```
//e.printStackTrace();
log.error(e.getMessage());
```

## 3、将日志堆栈信息输出到文件

**为了保证日志的堆栈信息能够被输出，我们需要定义工具类**

guli-framework-common下创建util包，创建ExceptionUtil.java工具类

```
package com.guli.common.util;
public class ExceptionUtil {
    public static String getMessage(Exception e) {
        StringWriter sw = null;
        PrintWriter pw = null;
        try {
            sw = new StringWriter();
            pw = new PrintWriter(sw);
            // 将出错的栈信息输出到printWriter中
            e.printStackTrace(pw);
            pw.flush();
            sw.flush();
        } finally {
            if (sw != null) {
                try {
                    sw.close();
                } catch (IOException e1) {
                    e1.printStackTrace();
                }
            }
            if (pw != null) {
                pw.close();
            }
        }
        return sw.toString();
    }
}
```

修改异常输出语句

```
//e.printStackTrace();
//log.error(e.getMessage());
log.error(ExceptionUtil.getMessage(e));
```

GuliException中创建toString方法

```
@Override
public String toString() {
    return "GuliException{" +
        "message=" + this.getMessage() +
        ", code=" + code +
        '}';
}
```

---

---
url: /常用框架/SpringBoot/SpringBoot程序开发/12_统一异常处理.md
---

# 统一异常处理

## 一、什么是统一异常处理

我们想让异常结果也显示为统一的返回结果对象，并且统一处理系统的异常信息，那么需要统一异常处理

## 二、统一异常处理

### 1、创建统一异常处理器

`com.xxl.common.handler`包中，创建统一异常处理类`GlobalExceptionHandler.java`

```java
package com.xxl.common.handler;

/**
 * 统一异常处理类
 */
@ControllerAdvice
public class GlobalExceptionHandler {

    @ExceptionHandler(Exception.class)
    @ResponseBody
    public R error(Exception e){
        e.printStackTrace();
        return R.error();
    }
}
```

### 2、扫描异常处理器

确认启动类添加了注解`@ComponentScan`

```java
/**
 * 启动类
 */
@SpringBootApplication
@ComponentScan(basePackages={"com.xxl.common"})
public class SpringbootInitApplication {
    
	public static void main(String[] args) {
        SpringApplication.run(SpringbootInitApplication.class, args);
    }
}
```

### 3、测试

增加异常方法

```java
@RequestMapping("/testException")
@ResponseBody
public R testException() {
    Integer num = 1 / 0;
    return R.ok().data("num", num);
}
```

返回统一错误结果

```json
{
    "success": false,
    "code": 20001,
    "message": "未知错误",
    "data":{}
}
```

## 三、处理特定异常

### 1、添加异常处理方法

GlobalExceptionHandler.java中添加

```
@ExceptionHandler(BadSqlGrammarException.class)
@ResponseBody
public R error(BadSqlGrammarException e){
    e.printStackTrace();
    return R.setResult(ResultCodeEnum.BAD_SQL_GRAMMAR);
}
```

### 3、恢复制造的异常

```
@TableField(value = "is_deleted")
private Boolean deleted;
```

## 四、另一个例子

### 1、制造异常

在swagger中测试新增讲师方法，输入非法的json参数，得到

```
HttpMessageNotReadableException
```

### 2、添加异常处理方法

GlobalExceptionHandler.java中添加

```
@ExceptionHandler(HttpMessageNotReadableException.class)
@ResponseBody
public R error(JsonParseException e){
    e.printStackTrace();
    return R.setResult(ResultCodeEnum.JSON_PARSE_ERROR);
}
```

## 五、自定义异常

### 1、创建自定义异常类

创建`com.xxl.common.exception`包，

创建`MyException.java`通用异常类 继承`RuntimeException`

`RuntimeException`对代码没有侵入性

```java
package com.xxl.common.exception;

/**
 * 自定义异常
 *
 * @author xxl
 * @date 2025/1/5 18:04
 */
@Data
@ApiModel(value = "全局异常")
public class MyException extends RuntimeException {
    
    /**
     * 状态码
     */
    private Integer code;

    /**
     * 接受状态码和消息
     *
     * @param code    状态码
     * @param message 消息
     */
    public MyException(Integer code, String message) {
        super(message);
        this.code = code;
    }

    /**
     * 接收枚举类型
     *
     * @param resultCodeEnum 枚举类型
     */
    public MyException(ResultCodeEnum resultCodeEnum) {
        super(resultCodeEnum.getMessage());
        this.code = resultCodeEnum.getCode();
    }
}
```

### 2、业务中需要的位置抛出GuliException

讲师controller中分页查询方法中判断参数是否合法

```
public R pageQuery(......){
    if(page <= 0 || limit <= 0){
        //throw new GuliException(21003, "参数不正确1");
        throw new GuliException(ResultCodeEnum.PARAM_ERROR);
    }
    ......
}
```

### 3、添加异常处理方法

GlobalExceptionHandler.java中添加

```
@ExceptionHandler(GuliException.class)
@ResponseBody
public R error(GuliException e){
    e.printStackTrace();
    return R.error().message(e.getMessage()).code(e.getCode());
}
```

---

---
url: /01.指南/10.使用/40.图标使用.md
---

# 图标使用

Teek 默认注册了全局组件 `TkIcon`，因此你可以通过该组件快捷引入图标。

`TkIcon` 默认支持如下类型的图标：

* svg
* unicode
* iconfont
* symbol
* img
* component
* iconifyOffline
* iconifyOnline

除此之外，您可以通过默认插槽传入自定义图标组件。

`TkIcon` 组件的基础使用以及 API 介绍请看 [Icon 图标](/ecosystem/components/icon)。

::: tip
Teek 所有的 Icon 图标相关配置项，都使用 `TkIcon` 组件，因此怎么使用 `TkIcon` 的 `icon` 属性，就怎么使用图标相关配置项。
:::

## SVG 图标

您可以下载一个 `svg` 图标到项目里，然后传入 `TkIcon` 组件中。

在 Markdown 文档有两种格式输入：

::: code-group

```vue [props 方式]
<script setup>
const icon = `<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1024 1024"><path fill="currentColor" d="M512 320 192 704h639.936z"></path></svg>`;
</script>

<TkIcon :icon="icon" />
```

```html [插槽方式]
<TkIcon>
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1024 1024">
    <path fill="currentColor" d="M512 320 192 704h639.936z"></path>
  </svg>
</TkIcon>
```

:::

输出：

SVG 图标在哪里获取？您可以访问 [阿里巴巴矢量图标库](https://www.iconfont.cn/)，然后搜索需要的图标，最后在下载时选择 **复制 SVG 图标**。

除此之外，您也可以可以在该网站上下载 `Unicode`、`Font Class`、`Symbol` 图标，然后传入 `TkIcon` 组件中。

如：

```html
<TkIcon :icon="icon-info" />

<!-- 如果是 Font Class，则等于如下 -->
<i class="iconfont icon-info"></i>
```

## Iconify 图标

### 在线图标

如果您的项目部署在互联网上，那么可以使用 `Iconify` 的在线图标，只需要往 `TkIcon` 组件传入在线图标名称即可。

在 Markdown 文档输入：

```html
<TkIcon icon="mdi:github" />
```

输出：

其中 `mdi:github` 为在线图标名，更多的在线图标请访问：[Iconify 在线图标](https://icon-sets.iconify.design/)。

> 如果项目部署在内网，或担心网络访问速度慢导致无法加载图标怎么办？往下看。

### 离线 JSON 图标

您可以直接将 `Iconify` 图标以 JSON 方式注册到本地，然后引入到 `TkIcon` 组件里，如：

```sh
pnpm add @iconify-json/ant-design -d
```

然后在 `.vitepress/theme/index.ts` 里注册到 Teek 里：

```ts
import { addIcons } from "vitepress-theme-teek";
import icons from "@iconify-json/ant-design/icons.json";

addIcons(icons);
```

最后和在线方式一样使用 `TkIcon` 组件：

```html
<TkIcon icon="ant-design:account-book-filled" />
```

`TkIcon` 优先从已注册的图标名里获取，当获取不到时就会从互联网上下载。

这里演示安装了 `ant design` 的图标，其他的图标集合根据需要安装。

`Iconify` JSON 图标的依赖名约定是 `@iconify-json/{name}`，引入 JSON 图标数据的路径约定是 `@iconify-json/{name}/icons.json`。

### 离线 Icon 图标

您可以直接将 `Iconify` 的图标集合安装到本地，然后引入到 `TkIcon` 组件里，如：

```sh
pnpm add @iconify-icons/ant-design -d
```

然后使用：

```vue
<script setup>
import Upload from "@iconify-icons/ant-design/upload";
</script>

<TkIcon :icon="Upload" />
```

这里演示安装了 `ant design` 的图标，其他的图标集合根据需要安装。

`Iconify` Icon 图标的依赖名约定是 `@iconify-icons/{name}` 或者 `@iconify/icons-{name}`。

### 两个离线图标方式对比

* JSON 图标方式需要在项目初始化时注册进去，后续直接通过字符串引用
* Icon 图标方式在每次使用时需要手动引入

::: info
`TkIcon` 并没有实现 `Iconify` 相关逻辑，而是通过代理 `Iconify` 的 API 实现。
:::

## 内置图标

### SVG 图标

下面展示 Teek 内置的主题图标集合，Teek 只保留了自身引用的图标，您可以在页面上随处可见，如果您需要额外的图标，请参考上面的方式添加。

当点击图标后将会复制引用图标代码（参考下面高亮部分）到您的剪切板中，然后就可以粘贴到代码中：

```vue {2}
<script setup>
import { yourClickIconName } from "vitepress-theme-teek/icons";
</script>

<TkIcon :icon="yourClickIconName" />
```

### 社交图标（iconfont）

如下是 Teek 内置的社交图标集合，可以在导航栏、侧边栏、社交配置 `social` 里快速应用。

如果 Teek 提供的社交图标不满足您的要求，您可以使用访问 [阿里巴巴矢量图标库](https://www.iconfont.cn/) 来下载您需要的任何图标。

---

---
url: /StableDiffusion/WebUI/2_图生图.md
---

# 图生图

---

---
url: /Java/容器/Docker/3_图形化工具Portainer.md
---

# 图形化工具Portainer

Portainer是一个轻量级的容器管理工具，可以通过Web界面对Docker容器进行管理和监控。它提供了可视化的界面，使得用户可以更加方便地管理Docker容器、镜像、网络和数据卷等资源。

通过Portainer的Web界面，可以直观地查看和管理容器、镜像、网络等资源，还可以进行容器的启动、停止、删除等操作。此外，Portainer还支持多用户管理和RBAC权限控制，可以更好地保障系统的安全性。

## 部署Portainer

为了方便演示，本例子使用docker部署一个Portainer，首先拉取Portainer镜像。

```
docker pull portainer/portainer
```

然后运行容器，其中: 9000端口是我们要访问的Portainer Web界面

```
docker run -p 9000:9000 -p 8000:8000 --name portainer --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v /mydata/portainer/data:/data -d portainer/portainer
```

启动后，我们输入docker ps命令，即可看到运行Portainer的容器,状态Up表示运行正常。

## 本地访问Portainer

容器成功运行后，我们在外部浏览器访问`http://LinuxIP地址:9000端口`，即可看到Portainer管理界面。

![image-20231230235343074](/assets/image-20231230235343074.z9t2x3jF.png)

上面首次登录，需要设置新登陆密码，设置完成后，即可登录管理界面，看到容器列表，本地部署访问就成功了。

## 安装cpolar实现远程访问

上面在本地Linux中成功部署了Portainer，并局域网访问成功。下面我们在Linux安装cpolar内网穿透工具，通过cpolar转发本地端口映射的http公网地址，可以很容易实现远程访问，而无需自己注册域名购买云服务器，可节省大量的资金。下面是安装cpolar步骤：

cpolar官网地址: https://www.cpolar.com

使用一键脚本安装命令

```
curl -L https://www.cpolar.com/static/downloads/install-release-cpolar.sh | sudo bash
```

向系统添加服务

```
sudo systemctl enable cpolar
```

启动cpolar服务

```
sudo systemctl start cpolar
```

cpolar安装成功后，在外部浏览器上访问Linux的9200端口即:【http://局域网ip:9200】，使用cpolar账号登录，登录后即可看到cpolar Web配置界面，接下来在Web管理界面配置即可。

## 配置Portainer公网访问地址

点击左侧仪表盘的隧道管理——创建隧道，创建一个Portainer的cpolar公网地址隧道。

* 隧道名称：可自定义命名，注意不要与已有的隧道名称重复

* 协议：选择http

* 本地地址：9000(本地访问时的端口)

* 域名类型：免费选择随机域名

* 地区：选择China

点击创建

![image-20231230235343074](/assets/image-20231231000706816.CAl5Arw2.png)

隧道创建成功后，点击左侧的状态——在线隧道列表，查看所生成的公网访问地址，有两种访问方式，一种是http和https。

![image-20231230235343074](/assets/image-20231231000909822.Bbg5AmXp.png)

使用上面的cpolar https公网地址，在任意设备的浏览器进行访问，即可成功看到我们Portainer管理界面，这样一个利用公网地址可以进行远程访问的隧道就创建好了，隧道使用了cpolar的公网域名，无需自己购买云服务器，可节省大量资金。

但是上面使用cpolar生成的隧道，其公网地址是随机生成的。这种随机地址的优势在于建立速度快，可以立即使用。然而，它的缺点是网址由随机字符生成，不容易记忆（例如：3ad5da5.r10.cpolar.top）。另外，这个地址会在24小时内发生随机变化，更适合于临时使用。

我们一般会使用固定的二级子域名，它是一个固定的、容易记忆的、更专业的公网地址（例如：wbo.cpolar.cn），这样与（3ad5da5.r10.cpolar.top）相比更显正式，便于交流协作。固定域名属于cpolar收费功能，可参考官方文档操作。

![image-20231231002507038](/assets/image-20231231002507038.DYOaxqiC.png)

## 补充资料

\[1]. 本地Portainer管理界面结合cpolar内网穿透工具实现远程任意浏览器访问：https://www.bilibili.com/video/BV1Nu4y1G725

---

---
url: /daily/日常笔记/玩客云.md
---

# 玩客云

## 一、准备工作（海纳思）

### 1、刷机

玩客云刷机海纳斯系统官方教程：

<https://doc.ecoo.top/docs/tutorial-basics/s805>

遇到问题

1. 1、烧录软件无法连接：可能先通电后短接就无效，要先连接双公头USB，再短接住不松手，最后通电，通电后玩客云指示灯不会亮。也可能是短接镊子或者电线氧化，无法通电，换个短接线尝试。
2. 2、点击开始之后，提示romcode/初始化ddr/设备识别/usb控制命令出错：可能是usb不是2.0，换个USB接口重新连接上再开始烧录。
3. 3、提示设备限流相关，是因为插入了U盘或者SD卡等，先拔出，再重新开始烧录。

进入终端页面提示

```ssh
欢迎使用 海纳思 32-bit 系统

原创作者: 神雕Teasiu
贡献成员: Xjm MinaDee
教程地址: http://192.168.0.106
官网地址: https://www.ecoo.top
社区地址: https://bbs.histb.com | 欢迎访问我们的社区, 你的肯定是对我们最大的支持!

版权声明:
        1. 作者唯一闲鱼账号: 神雕teasiu
        2. 本系统仅在官网发布, 任何人均可免费下载安装和使用
        3. 如果你是在不良奸商手上购买本系统, 请尝试协商退款或予以差评
        3. 本系统开通的服务仅仅用于学习研究, 不得用于任何非法用途
        4. 未经作者授权, 不得将系统用于盈利售卖, 不得用于衍生的任何软件变相收费, 否则视为侵权行为


提示: 本内容仅显示一次, 烦请仔细阅读, 后续执行 'copyright' 指令即可再次阅读本页面!

提示: 本内容仅显示一次, 请首次进入终端后更改root和ubuntu用户密码为强密码!
```

### 2、查看IP为远程登录做准备

登录路由器后台查看

![Snipaste\_2024-10-20\_19-19-56](/assets/Snipaste_2024-10-20_19-19-56.DXWvrp0a.png)

点击管理按钮，查看其IP地址

![image-20241020192545317](/assets/image-20241020192545317.T6IY3zd1.png)

确定本机在路由器中分配的IP地址。

### 3、远程登录

使用ssh远程工具登录到后台。

系统提供了两个账户，一个是具有全部权限的 root 用户，一个是普通权限的 ubuntu 用户，

默认账号 `root`，默认密码 `ecoo1234`

默认账号 `ubuntu`，默认密码 `ecoo1234`

### 4、修改root密码

`passwd root`

```sh
root@hinas:~# passwd root
New password:
Retype new password:
passwd: password updated successfully
```

### 5、设置固定IP

默认情况下，设备的IP是被路由系统DHCP自动分配的，每次关机或重启后，路由器会重新给设备分配地址，这会造成我们每次重启后的IP不一样。会给我们的管理工作带来不便。为此我们需要固定设备的IP地址。

首先需要查看路由器的DHCPIP分配段，我们设备的IP需要设置到这个分配段之外，防止这个IP被DHCP自动分配给给它设备。

打开路由器的管理界面，点击路由设置，选择DHCP服务。

![image-20241020201151164](/assets/image-20241020201151164.CRt7W7H7.png)

可以看到，路由器DHCP的地址池是在192.168.14.100-192.168.199之间进行分配。对设备设置的地址要在这个地址段之外，如我设置为192.168.14.99。

在ssh终端输入`ifconfig`，可以看到设备的MAC地址为：00:11:22:33:44:66。记住这个MAC地址，后面需要用上。

在ssh终端中，输入`vi /etc/network/interfaces.d/eth0`，对网卡的配置文件进行编辑。

```sh
auto eth0
iface eth0 inet dhcp
pre-up ifconfig eth0 hw ether 00:11:22:33:44:66
```

修改后

```sh
auto eth0
iface eth0 inet static #设置网卡地址为静态
address 192.168.0.99 #设置网卡的IP地址，需要在路由分配地址之外
network 192.168.0.0
netmask 255.255.255.0 #子网掩码
broadcast 192.168.0.255 #广播地址
gateway 192.168.0.1 #网关地址
dns-nameservers 192.168.0.1 #DNS地址
pre-up ifconfig eth0 hw ether 00:11:22:33:44:66 #MAC地址：前面在ifconfig中查询的结果
```

编辑完成后输入ESC和:wq保存退出。

```sh
sudo ifdown eth0 && sudo ifup eth0
```

[用海纳思做web服务器并进行内网透传的实例-CSDN博客](https://blog.csdn.net/hnkkfan/article/details/138360833)

## 二、CasaOS

armbian casaos

https://blog.csdn.net/xianyun\_0355/article/details/137931849

https://blog.csdn.net/qyj19920704/article/details/139326876

https://post.smzdm.com/p/a96odq3p

## 三、使用

### 1、青龙面板

#### 1、安装青龙面板

玩客云提供安装脚本

```sh
# 直接执行 install-qinglong.sh
root@hinas:~#  install-qinglong.sh
```

但是脚本中拉取的版本不是最新的，老版本系统设置中缺少依赖设置

如果需要最新版，修改 `/usr/share/bak/gitweb/docker-compose-ql.yaml`

```yaml
version: '3.0'
services:
   ql:
    # image: registry.cn-hangzhou.aliyuncs.com/histb/qinglong:latest
    image: whyour/qinglong:latest # 最新版本
    container_name: ql
    restart: always
    volumes:
       - /qinglong/ql/config:/ql/config
       - /qinglong/ql/scripts:/ql/scripts
       - /qinglong/ql/repo:/ql/repo
       - /qinglong/ql/log:/ql/log
       - /qinglong/ql/db:/ql/db
       - /qinglong/ql/jbot:/ql/jbot
       - /qinglong/ql/raw:/ql/raw
    ports:
       - 5700:5700
```

再次执行`install-qinglong.sh`

https://www.bilibili.com/video/BV1saskeEEF4

docker安装青龙面板

https://blog.csdn.net/u012374381/article/details/128955553

https://www.voidking.com/dev-docker-qinglong

拉库命令

https://blog.csdn.net/weixin\_42565036/article/details/138579435
https://blog.csdn.net/liu52365/article/details/121233533
https://blog.csdn.net/u011027547/article/details/130703685
docker下载镜像慢

https://www.cnblogs.com/xietingfeng321/p/18451170

https://www.cnblogs.com/shenhuanjie/p/18428209

#### 2、京东定时任务

浏览器获取京东cookie教程

https://www.xujiahua.com/8405.html

[青龙面板快手极速版教程\_青龙 快手-CSDN博客](https://blog.csdn.net/xuekaitt/article/details/123037703)

#### 3、DDNSTO内网穿透

[Step1: 登录官网 控制台拿到“令牌” | 易有云产品中心 (linkease.com)](https://doc.linkease.com/zh/guide/ddnsto/start.html)

[UNRAID一篇就够！DDNSTO最简单的内网穿透\_NAS存储\_什么值得买 (smzdm.com)](https://post.smzdm.com/p/a3dd9qwn/)

#### 4、青龙面板设置飞书机器人

https://blog.csdn.net/weixin\_45207619/article/details/138449360
https://blog.csdn.net/A1682234/article/details/135520783

#### 5、青龙面板怎么更新依赖

![image-20241023211531426](/assets/image-20241023211531426.CwsxAEYl.png)

https://blog.csdn.net/xiaojing\_yu/article/details/124141113

```
crypto-js
prettytable
dotenv
jsdom
date-fns
tough-cookie
tslib
ws@7.4.3
ts-md5
jsdom -g
jieba
fs
form-data
json5
global-agent
png-js
@types/node
require
typescript
js-base64
axios
moment
qs
```

依赖下载慢，更换镜像源

https://blog.csdn.net/Xue1633851708/article/details/139997567

Node 软件包镜像源：https://npm.aliyun.com、https://registry.npmmirror.com（推荐）

Python 软件包镜像源：https://pypi.tuna.tsinghua.edu.cn/simple、https://mirrors.aliyun.com/pypi/simple/

Linux 软件包镜像源：http://mirrors.cloud.aliyuncs.com

https://help.yunjiutian.com/project-1/doc-75/

更新
https://blog.csdn.net/cfm\_gavin/article/details/142140699

https://github.com/trytrytogo/jinyinshouzhi?tab=readme-ov-file
https://github.com/dandanainiyo/SignBox/blob/master/config.py
https://github.com/Sitoi/dailycheckin?tab=readme-ov-file

---

---
url: /daily/软件设计师/10_网络与信息安全基础知识.md
---

# 网络与信息安全基础知识

## 一、网络概述

### :evergreen\_tree:计算机网络的概念

计算机网络是计算机技术与通信技术相结合的产物，它实现了远程通信、远程信息处理和资源共享。

计算机网络的功能：数据通信、资源共享、负载均衡、高可靠性。

### :evergreen\_tree:计算机网络的分类

![image-20240414003928665](/assets/image-20240414003928665.avJsdONz.png)

### :evergreen\_tree:网络的拓扑结构

1. 总线型（利用率低、干扰大、价格低）
2. 星型（交换机形成的局域网、中央单元负荷大，类似于路由器）
3. 环型（流动方向固定、效率低扩充难）
4. 树型（总线型的扩充、分级结构）
5. 分布式（网状结构，任意节点连接、管理难成本高）

![image-20240414004535544](/assets/image-20240414004535544.CPUoOcKg.png)

## 二、OSI/OSI网络体系结构【重点】

### :evergreen\_tree:ISO/OSI参考模型

开放系统互连参考模型 (Open System Interconnect 简称OSI）

OSI/RM（ISO/OSI）

七层模型（从下到上）

当发送数据时，数据是自上而下传输；当接收数据时，数据是自下而上传输。

![image-20240414004747282](/assets/image-20240414004747282.DLhS1m7l.png)

物理层：二进制数据传输，物理链路和物理特性相关。

数据链路层：将数据封装成帧进行传送，准确传送至局域网内的物理主机上。

网络层：数据分组传输和路由选择，能准确的将数据传送至互联网的网络主机上。

传输层：端到端的连接，传送数据至主机端口上

会话层：管理主机之间的会话，提供会话管理服务（建立、维护和结束会话）。

表示层：提供解释所交换信息含义的服务，包括数据之间的格式转换、压缩、加密等操作，对数据进行处理。

应用层：实现具体的应用功能。直接进程间的通信。
![image-20240414004905128](/assets/image-20240414004905128.DxlvmyJ_.png)

## 三、网络互联的硬件

物理层：中继器（只能连接两条物理线路，用于扩大信号）、集线器Hub(多路中继器，所有端口组成冲突域）。

数据链路层：网桥（分析帧地址）、交换机（多口网桥，MAC地址表，各个端口形成广播域）。

网络层：路由器（连接多个逻辑上分开的网络，路由选择）。

应用层：网关（连接不同类型且协议差别较大的网络，协议转换）。

### :evergreen\_tree:网络的传输介质

有线介质：双绞线（最大长度100m，每端需要一个RJ45插件）、同轴电缆、光纤。

无线介质：微波、红外线和激光、卫星通信。

### :evergreen\_tree:组建网络

组建网络：服务器、客户端、网络设备、通信介质、网络软件。

## 四、网络的协议与标准

### :evergreen\_tree:局域网协议

* IEEE802.3：标准以太网（CSMA/CD），速度为10Mbps（bit速率，转为字节需要除以8），传输介质为同轴电缆；

* IEEE802.3u：快速以太网，速度为100Mbps，传输介质为双绞线；

* IEEE802.3z：千兆以太网，速度为1000Mbps，传输介质为光纤或双绞线。

* IEEE802.4：令牌总线网。

* IEEE802.5：令牌环网。

* 无线局域网CSMA/CA（载波侦听多路访问方法）。

### :evergreen\_tree:广域网协议

* 点对点协议PPP（拨号上网）

* 数字用户线xDSL（ADSL上传网速和下载网速不对等，下载网速一般很快）

* 数字专线DDN（市内或长途的数据电路）

* 帧中继（以帧为传输单位）。

### :evergreen\_tree:TCP/IP协议族【考点】

TCP/IP不是一个简单的协议，而是一组专业化协议。TCP/IP协议族可被大致分为**应用层、传输层、网际层和网络接口层**四层。如下图所示（注意：TCP/IP协议族的划分并不像七层模型那么严格，所以有些人将其划为四层，也有将其划为五层，在此主要掌握每个层次所对应的协议）。

![image-20240414010404502](/assets/image-20240414010404502.AU0FYBqr.png)

#### 特性

逻辑编址（网卡-物理地址，Internet-逻辑地址）

路由选择（定义路由器如何选择网络路径）

域名解析（域名解析为IP地址）

错误检测和流量控制（可靠性、防止拥塞）

#### TCP/IP分层模型

应用层：具体应用功能。

传输层：提供应用程序间端到端的通信。

网际层：又称IP层，处理机器间的通信，数据以分组为单位。

网络接口层：又称为数据链路层，负责接收IP数据报，并把数据报通过选定的网络发送出去。

#### TCP/IP分层模型各层的网络协议

网际层协议：

* IP：最重要核心的协议，无连接、不可靠。

* ICMP：因特网控制信息协议，用来检测网络通信是否顺畅。

* ARP和RARP：地址解析协议，ARP（Address Resolution Protocol）是将IP地址转换为物理地址。

  RARP（反向地址转换协议（RARP：Reverse Address Resolution Protocol））是将物理地址转换为IP地址。

传输层协议：

* UDP协议【用户数据报协议（UDP，User Datagram Protocol）】：不可靠连接，无连接，因为数据传输只管发送，不用对方确认，因此可能会有丢包现象。一般用于视频、音频数据传输。
* TCP协议【传输控制协议（TCP，Transmission Control Protocol）】：可靠连接，因为有验证机制，每发送一个数据包，都要求对方回复确认；初始建立连接，有三次握手机制，即A发送连接信息给B（SEQ=X），B收到后回复确认帧（SEQ=Y,ACK=X+1)，A收到确认帧后再发送确认（SEQ=X+1,ACK=Y+1)，才能建立连接（上述SEQ表示本机发送的数据包序号，A和B之间的SEQ分别计数，而ACK为确认帧，ACK=X+1表示已经收到了A机发送的第X个数据包，期望下一个收到第X+1个数据包）。

TCP传输协议分类：

* 停止等待协议：TCP保证可靠传输的协议，”停止等待”就是指发送完一个分组就停止发送，等待对方的确认，只有对方确认过，才发送下一个分组。

* 连续ARQ协议：TCP保证可靠传输的协议，它是指发送方维护着一个窗口，这个窗口中不止一个分组，有好几个分组，窗口的大小是由接收方返回的win值决定的，所以窗口的大小是动态变化的，只要在窗口中的分组都可以被发送，这就使得TCP一次不是只发送一个分组了，从而大大提高了信道的利用率。并且它采用累积确认的方式，对于按序到达的最后一个分组发送确认。

* 可变大小的滑动窗口协议：TCP流量控制协议，可变的窗口是不断向前走的，该协议允许发送方在停止并等待确认前发送多个数据分组。由于发送方不必每发一个分组就停下来等待确认，因此该协议可以加速数据的传输，还可以控制流量的问题。

应用层协议：

* 基于TCP的FTP、HTTP等都是可靠传输。基于UDP的DHCP、DNS等都是不可靠传输。

* FTP：可靠的文件传输协议。

* HTTP：超文本传输协议，用于上网。使用SSL加密后的安全网页协议为HTTPS。

* SMTP和POP3：邮件传输协议，邮件报文采用ASClI格式表示。

* IMAP：邮件访问协议，是用于替代POP3协议的新协议，工作在143号端口上。

* Telnet：远程连接协议。

* TFTP：不可靠的小文件传输协议。

* SNMP：不可靠，简单网络管理协议，必须以管理员的身份登录才能完成配置。

* DHCP：不可靠，动态分配IP地址协议，客户机/服务器模型，租约默认为8天，当租约过半时，客户机需要向DHCP服务器申请续租，当租约超过87.5%时，如果仍然没有和当初提供IP地址的DHCP服务器联系上，则开始联系其他DHCP服务器。

* VoIP：Voice over Internet Protocol是一种以IP电话为主，并推出相应的增值业务的技术，不可靠。

* DNS：不可靠，域名解析协议，将域名解析为IP地址。

* DNS服务器：维持域名和IP地址对应的表格，层次结构为：本地域名服务器、权威域名服务器、顶级域名服务器、根域名服务器。

* 输入网址（即域名）后，首先会查询本地DNS缓存，无果后再查询本地DNS服务器，又分为递归查询和迭代查询两种方式。
  * 递归查询：主机提出一个查询请求，本地服务器会自动一层一层的查询下去，直到找到满足查询请求的IP地址，再返回给主机。即问一次，就得最终结果。
  * 迭代查询：服务器收到一次查询请求，就回答一次，但是回答的不一定是最终地址，也可能是其他层次服务器的地址，然后等待客户端再去提交查询请求。即问一次答一次，而后再去问其他服务器，直至问到结果。

* 主机向本地域名服务器额查询采用递归查询；本地域名服务器向根域名服务器的查询通常采用迭代查询。（依据是域名服务器是否空闲）。

#### 路由选择策略

* **静态路由选择**（不能根据网络流量和拓扑结构的变化来调整自身的路由表，也就不能找出最佳路由）：
  * **固定式路由选择**：每个网络节点存储一张表格，表格中每一项记录着对应某个目的节点的下一点或链路，当一个分组到达某节点时，该节点只要根据分组上的地址信息，便可从固定的路由表查出对应的目的节点及所应选择的下一节点。
  * **洪泛式路由选择**：又叫扩散法，一个分组由源站发送到与其相邻的所有节点，最先到达目的节点的一个或若干个分组肯定经过了最短的路径，其主要应用在诸如军事网络等强壮性要求很高的场合；
  * **随机路由选择**：一个分组只在与其相邻的节点中随机的选择一条转发；
* **动态路由选择**（节点的路由选择要依靠网络当前的状态信息来决定。这种策略能较好地适应网络流量、拓扑结构的变化，有利于改善网络的性能。但由于算法复杂，会增加网络的负担）：
  * **分布式路由选择**。基本算法有距离向量算法（各节点周期性地向所有相邻节点发送路由刷新报文）和链路状态算法（各节点独立计算最短通路、能够快速适应网络变化、交换的路由信息少，复杂难以实现）；
  * **集中式路由选择**。由网络控制中心（Network Control Center，NCC)负责全网状态信息的收集、路由计算及最佳路由的实现。最简单的方法是将最新路由定期发送到网络中各节点上去。
  * **混合式动态路由选择**。将分布路由选择与集中路由选择、以及其它路由选择方法混合使用。

## 五、Internet及应用

### :evergreen\_tree:IP地址

分类地址格式：IP地址分四段，每段八位，共32位二进制数组成。通常使用点分4个十进制数表示，如127.0.0.1。

![9a85c1599140448eb434b86699a4f318](/assets/9a85c1599140448eb434b86699a4f318.CKIEAOgU.png)

在逻辑上，这32位IP地址分为网络号和主机号，依据网络号位数的不同，可以将IP地址分为以下几类：

![36301c925c3241ca91cb93ce9a9ccc23](/assets/36301c925c3241ca91cb93ce9a9ccc23.CNHL9Ku-.png)

其中，A类地址网络号占8位，主机号则为32-8=24位，能分配的主机个数为2^24-2个（注意：主机号为全0和全1的不能分配，是特殊地址）；

同理，B类地址网络号为16位，主机号则为32-16=16位，能分配的主机个数为2^16-2个

C类地址网络号为24位，主机号则为32-24=8位，能分配的主机个数为2^8-2个。

上图中红色的位数表示该位固定为该值，是每类IP地址的标识。

#### 子网划分

按上述划分的ABC三类，一般是最常用的，但是却并不实用，因为主机数之间相差的太大了，不利于分配，因此，我们一般采用子网划分的方法来划分网络，即自定义网络号位数，就能自定义主机号位数，就能根据主机个数来划分出最适合的方案，不会造成资源的浪费。

因此就有子网的概念，一般的IP地址按标准划分为ABC类后，可以进行再一步的划分，将主机号拿出几位作为子网号，就可以划分出多个子网，此时lP地址组成为：**网络号+子网号+主机号。**

网络号和子网号都为1，主机号都为0，这样的地址为子网掩码。

要注意的是：子网号可以为全0和全1，主机号不能为全0或全1(特殊地址)，因此，主机数需要-2，而子网数不用。

还可以聚合网络为超网，就是划分子网的逆过程，将网络号取出几位作为主机号，此时，这个网络内的主机数量就变多了，成为一个更大的网络。

#### 无分类编址

除了上述的分类编址外，还有无分类编址，即不按照ABC类规则，自动规定网络号，无分类编址格式为：lP地址/网络号，示例：128.168.0.11/20表示的IP地址为128.168.0.11，其网络号占20位，因此主机号占32-20=12位，也可以划分子网。

特殊含义的IP地址：

![软件设计师笔记\_yzzheng\_60125的博客-CSDN博客](/assets/b744aac9ebfb021aa9123aaa8849da62.DUPNZq3g.png)

### :evergreen\_tree:IPv6

主要是为了解决IPv4地址数不够用的情况而提出的设计方案，IPv6具有以下特性：

* IPv6地址长度为128位，地址空间增大了2^96倍；

* 灵活的IP报文头部格式，使用一系列固定格式的扩展头部取代了IPv4中可变长度的选项字段。IPv6中选项部分的出现方式也有所变化，使路由器可以简单路过选项而不做任何处理，加快了报文处理速度；

* IPv6简化了报文头部格式，加快报文转发，提高了吞吐量；

* 提高安全性，身份认证和隐私权是IPv6的关键特性；

* 支持更多的服务类型；

* 允许协议继续演变，增加新的功能，使之适应未来技术的发展。

```
例（2013年下半年）：●在IPv4向IPv6过渡期间，如果要使得两个IPv6结点可以通过现有的IPv4网络进行通信则应该使用（67）；如果要使得纯IPv6结点可以与纯IPv4结点进行通信，则需要使用（68）。
（67）A.堆栈技术 B.双协议栈技术 C.隧道技术 D.翻译技术
（68）A.堆栈技术 B.双协议栈技术 C.隧道技术 D.翻译技术
答案：C D
解析：IPv4和IPv6的过渡期间，主要采用三种基本技术。
双协议栈：主机同时运行IPv4和IPv6两套协议栈，同时支持两套协议；
隧道技术：这种机制用来在IPv4网络上连接IPV6的站点，站点可以是一台主机，也可以是多个主机。
翻译技术：利用转换网关来在IPv4和IPv6网络之间转换IP报头的地址，同时根据协议不同对分组做相应的语义翻译，从而使纯IPv4和纯IPv6站点之间能够透明通信。
第1小题由于两个IPv6的结点通信需要在IPv4的网络上通信，使用隧道技术；

第2小题由于两个结点一个是纯lPv6，另一个是IPv4，由于IPv4和IPv6是两种不同地址长度和报头格式的协议，因此它们之间的通信需要使用翻译技术进行地址转换。
```

### :evergreen\_tree:其他重要应用

层次化网络模型从下至上分为三层：

* 接入层：功能单一，向本地网段提供用户接入。

* 汇聚层：功能多样，可以有多层，包括网络访问策略、数据包的处理、过滤、寻址、路由选择等中间操作。

* 核心层：功能单一，只负责高速的数据交换。

**网络地址翻译NAT**：公司内有很多电脑，在公司局域网内可以互联通信，但是要访问外部因特网时，只提供固定的少量IP地址能够访问因特网，将公司所有电脑这个大的地址集合映射到能够访问因特网的少量IP地址集合的过程就称为NAT。
很明显，使用了NAT后，一个公司只有少量固定lP地址可以上网，大大减少了IP地址的使用量。

**默认网关**：一台主机可以有多个网关。默认网关的意思是一台主机如果找不到可用的网关，就把数据包发给默认指定的网关，由这个网关来处理数据包。现在主机使用的网关，一般指的是默认网关。
默认网关的IP地址必须与本机IP地址在同一个网段内，即同网络号。

冲突域和广播域：路由器可以阻断广播域和冲突域，交换机只能阻断冲突域，因此一个路由器下可以划分多个广播域和多个冲突域；一个交换机下整体是一个广播域，但可以划分多个冲突域；而物理层设备集线器下整体作为一个冲突域和一个广播域。

虚拟局域网VLAN：是一组逻辑上的设备和用户，这些设备和用户并不受物理位置的限制，可以根据功能、部门及应用等因素将它们组织起来，相互之间的通信就好像它们在同一个网段中一样。VLAN工作在OSl参考模型的第2层和第3层，一个VLAN就是一个广播域，VLAN之间的通信是通过第3层的路由器来完成的。与传统的局域网技术相比较，VLAN技术更加灵活，它具有以下优点：网络设备的移动、添加和修改的管理开销减少；可以控制广播活动；可提高网络的安全性。

虚拟专用网VPN：是在公用网络上建立专用网络的技术。其之所以称为虚拟网，主要是因为整个VPN网络的任意两个节点之间的连接并没有传统专网所需的端到端的物理链路，而是架构在公用网络服务商所提供的网络平台，如Internet、ATM(异步传输模式)、Frame Relay(帧中继)等之上的逻辑网络，用户数据在逻辑链路中传输。

## 六、网络安全

### :evergreen\_tree:网络安全协议

网络安全

物理层主要使用物理手段，隔离、屏蔽物理设备等

其它层都是靠协议来保证传输的安全，具体如下图所示：

![img](/assets/36d5eca1ae284ec19818e80cf3cb65e4.MIbBMbDj.png)

SSL协议用于网银交易。提供三方面的服务：用户和服务器的合法性验证、加密数据以隐藏被传输的数据、保护数据的完整性。

实现过程：接通阶段——密码交换阶段（客户端和服务器之间交换双方认可的密码）——会谈密码阶段（客户端和服务器之间产生彼此交谈的会谈密码）——检验阶段——客户认证阶段——结束阶段。

HTTPS即安全的超文本传输协议，处于应用层，应该采用应用层协议，SET为传输层协议。

SSL被设计为加强Web安全传输（HTTP/HTTPS/)的协议（还有SMTP/NNTP等）

SSH被设计为加强Telnet/FTP安全的传输协议。

### :evergreen\_tree:防火墙技术

防火墙是在内部网络和外部因特网之间增加的一道安全防护措施，它认为内部网络是安全的，外部网络是不安全的。分为网络级防火墙和应用级防火墙，两级之间的安全手段如下所示：

网络级防火墙（包过滤防火墙）层次低，但是效率高，因为其使用包过滤和状态监测手段，一般只检验网络包外在（起始地址、状态）属性是否异常，若异常，则过滤掉，不与内网通信，因此对应用和用户是透明的。但是这样的问题是，如果遇到伪装的危险数据包就没办法过滤。

应用级防火墙（代理服务防火墙），层次高，效率低，因为应用级防火墙会将网络包拆开，具体检查里面的数据是否有问题，会消耗大量时间，造成效率低下，但是安全强度高。由防火墙自动完成，对用户透明。包括双宿主主机、屏蔽主机网关、被屏蔽子网等方法。

被屏蔽子网方法，是在内网和外网之间增加了一个屏蔽子网，相当于多了一层网络，称为DMZ（非军事区），这样，内网和外网通信必须多经过一道防火墙，屏蔽子网中一般存放的是邮件服务器、WEB服务器、论坛等这些内外网数据交互的服务器，可以屏蔽掉一些来自内部的攻击，但是完全来自系统内部服务器的攻击还是无法屏蔽掉。

### :evergreen\_tree:计算机病毒

病毒：编制或者在计算机程序中插入的破坏计算机功能或者破坏数据，影响计算机使用并且能够自我复制的一组计算机指令或者程序代码。病毒具有：传染性、隐蔽性、潜伏性、破坏性、针对性、衍生性、寄生性、未知性。

木马：是一种后门程序，场被黑客用作控制远程计算机的工具，隐藏在被控制电脑上的一个小程序监控电脑一切操作并盗取信息。

病毒和木马种类：

* 系统引导型病毒；

* 文件外壳型病毒；

* 目录型病毒；

* **蠕虫病毒（感染EXE文件）**：熊猫烧香，罗密欧与朱丽叶，恶鹰，尼姆达，冲击波，红色代码，爱虫病毒。

* 宏病毒（感染word、excel等文件中的宏变量）：美丽沙，台湾1号。

* ClH病毒：史上唯一破坏硬件的病毒。

* 木马：QQ消息尾巴木马，特洛伊木马，冰河。木马的作用一般强调控制操作宏病毒破坏的是OFFICE文件相关

* 红色代码：蠕虫病毒+木马。

## 七、多媒体

媒体可分为下面五类：

1. 感觉媒体：直接作用于人的感觉器官，使人产生直接感觉的媒体。如：视觉、听觉、触觉等。

2. 表示媒体：指传输感觉媒体的中介媒体/载体，即用于数据交换的编码。如：图像编码、文字、图形、动画、音频和视频等。

3. 表现媒体：进行信息输入和信息输出的媒体。如：键盘、鼠标和麦克风；显示器、打印机和音响等。

4. 存储媒体：存储表示媒体的物理介质。如磁盘、光盘和内存等。

5. 传输媒体：传输表示媒体的物理介质。如电缆、光纤、双绞线等。

### :evergreen\_tree:声音

以声音的带宽来衡量声音的大小，单位是HZ。声音是一种模拟信号，要对其进行处理，就必须将其转化为数字信号。模数转换过程有三个步骤：采样、量化、编码。

人耳能听到的音频信号的频率范围是20Hz~20KHz。

声音的采样频率一般为**最高频率的两倍**，才能保证不失真。

未经压缩的数字音频数据的传输率计算公式如下：

数据传输率（bps）= 采样频率（Hz）× 量化位数（bit）× 声道数
声音波形经过数字化后所需占用的存储空间计算公式如下：
声音信号数据量（Byte）= 数据传输率（bps）× 持续时间（s）/ 8

#### 数字音乐合成方法

数字调频合成法FM：使高频振荡波的频率按调制信号规律变化的一种调制方式。采用不同调制波频率和调制指数，就可以方便的合成具有不同频谱分布的波形，再现某些乐器的音色。可以采用这种方法得到具有独特效果的“电子模拟声“，创造出丰富多彩的声音，是真实乐器所不具备的音色。

波表合成法Wavetable：将各种真实乐器所能发出的所有声音（包括各个音域、声调）录制下来，存贮为一个波表文件。播放时，根据MIDI文件纪录的乐曲信息向波表发出指令，从”表格”中逐一找出对应的声音信息，经过合成、加工后回放出来。合成的音质更好。

#### 声音特性

音量：即响度，表示声音的强弱程度，主要取决于声波振幅的大小。

音高：表示各种声音的高低，主要取决于声波的振动频率，振动频率越高则音越高。

音调：表示声音的调子的高低，由声音本身的频率决定。

音色：又称为音品，由声音波形的谐波频谱和包络决定。

声音文件格式：.wav、.snd、.au、.aif、.voc、.mp3、.ra、.mid等。

### :evergreen\_tree:图形和图像

#### 颜色三要素

亮度：彩色明暗深浅程度。

色调：颜色的类别（红、绿等）。

饱和度：某一颜色的深浅程度。

#### 彩色空间

即设备显示图片所使用的色彩空间，

普通的电脑显示器是RGB色彩空间，除了红、绿、蓝三原色外，其他颜色都是通过这三原色叠加形成的；

电视中使用YUV色彩空间，主要是为了兼容黑白电视，使用的是亮度原理，即调不同的亮度，显示不同的颜色；

CMY(CMYK)，印刷书籍时采用的色彩空间，这个采用的是和RGB相反的减法原理，浅蓝、粉红、黄三原色的印刷颜料实际上是吸收除了本身色彩之外的其他颜色的，因此，印刷出来才是这些颜色；

HSV(HSB)，艺术家彩色空间，是从艺术的角度划分的。

图像的属性：分辨率（每英寸像素点数dpi)、像素深度（存储每个像素所使用的二进制位数）。

图像文件格式：.bmp、gif、jpg、.png、tif、wmf等。

图像深度是图像文件中记录一个像素点所需要的位数。是示深度表示显示缓存中记录屏幕上一个点的位数（bit），也即显示器可以显示的颜色数，计算方法：2^n，n表示每个像素具有n位颜色深度。

水平分辨率：显示器在横向上具有的像素点数目。

垂直分辨率：显示器在纵向上具有的像素点数目。

矢量图的基本组成单位是图元，位图的基本组成单位是像素，视频和动画的基本组成单元是帧。

### :evergreen\_tree:多媒体相关计算问题

![img](/assets/20210320114149281.C_Z4-wBr.png)

上述计算中，图像中要理解色数的概念。要理解音频容量计算的原理，就是每个采样通道的采样次数×每次采样的位数×总的采用通道数。

视频就是一帧帧图像的组合，因此本质是求图像容量，当然要加上音频容量。注意单位B和b的区别和换算，注意K（大写，1024，存储时才用）和k(小写，1000)的区别，注意结果单位。

```
例（2016年上半年）：使用150DPl的扫描分辨率扫描一幅3×4英寸的彩色照片，得到原始的24位真彩色图像的数据量是（14）Byte。
A.1800 B.90000 C.270000 D.810000
答案：D
解析：DPl是每英寸像素点数，因此扫描后像素点数为3×150×4×150=270000个，24位彩色图像的含义是每个像素点占24bit=3Byte，因此数据量为270000×3=810000Byte。

例（2015年下半年）：颜色深度是表达图像中单个像素的颜色或灰度所占的位数(bit)。若每个像素具有8位的颜色深度，则可表示（13）种不同的颜色。
（13)A.8 B.64 C.256 D.512
答案：C
```

---

---
url: /@pages/nav.md
---

# 网站导航

## AI工具

::: navCard

```yaml
- name: 通义千问
  desc: 阿里巴巴推出的AI对话助手，使用阿里最新最全的Qwen大模型。
  link: http://www.qianwen.com/
  img: /svg/website/通义千问.svg
  badge: AI工具
  badgeType: tip
- name: 豆包
  desc: 字节跳动推出的智能AI助手，提供知识问答、文档解析、编程辅助等核心功能。
  link: https://www.doubao.com/chat/
  img: /svg/website/豆包.svg
  badge: AI工具
- name: 文心一言
  desc: 百度推出的人工智能聊天机器人，具备多种功能，如对话生成、内容创作、智能助手等，广泛应用于各个领域。
  link: https://yiyan.baidu.com/
  img: /svg/website/文心一言.svg
  badge: AI工具
- name: DeepSeek
  desc: DeepSeek是杭州深度求索人工智能基础技术研究有限公司推出的AI助手。
  link: https://www.deepseek.com/
  img: /svg/website/DeepSeek.svg
  badge: AI工具
- name: kimi
  desc: Kimi是由月之暗面公司打造的国产AI智能助手，以其支持200万字超长上下文和出色的文件解析能力脱颖而出。它能够深度处理PDF、Word等多种格式文档，并具备联网搜索功能，在中文场景下表现出色。
  link: https://www.kimi.com/
  img: /svg/website/kimi.svg
  badge: AI工具
- name: 智谱AI
  desc: 智谱AI是一家源自清华大学的高科技公司，专注于认知智能和大规模语言模型的研发，致力于推动人工智能技术的边界。
  link: https://bigmodel.cn/
  img: /svg/website/智谱AI.svg
  badge: AI工具
```

:::

***

## AI编程工具

### 国外

::: navCard

```yaml
- name: Cursor
  desc: Cursor 是Anysphere 公司开发的基于 Visual Studio Code 构建的 AI 代码编辑器，通过深度集成 AI 技术，使编码过程更加智能、高效。
  link: https://cursor.com/cn
  img: /svg/website/cursor-favicon-48x48.png
  badge: AI编程工具
  badgeType: tip
- name: Claude Code
  desc: Anthropic 推出的一款 AI 编程助手，基于其强大的 Claude Opus 4 模型，旨在通过自然语言命令帮助开发者更高效地编写和管理代码。它支持在终端运行，并与常见开发工具无缝集成，提供了全新的编程体验。
  link: https://code.claude.com/docs/zh-CN/overview
  img: /svg/website/ClaudeCode.png
  badge: AI编程工具
- name: Windsurf
  desc: Codeium公司推出的AI编程工具，采用 AI Flow 范式，支持多步骤、多工具协同，自动维护上下文状态，像 Copilot 一样与开发者协作，像 Agent 一样独立处理复杂任务。
  link: https://windsurf.com/
  img: /svg/website/windsurf.png
  badge: AI编程工具
- name: Firebase Studio
  desc: 谷歌推出的基于云端的全栈应用开发AI编程工具，能快速构建和交付集成 AI 功能的高质量应用。整合了 Project IDX 和专用的 Firebase AI 代理，支持从后端到前端、移动应用等多维度的开发。
  link: https://firebase.google.cn/docs/studio?hl=zh-cn
  img: /svg/website/FirebaseStudio.png
  badge: AI编程工具
```

:::

### 国产

::: navCard

```yaml
- name: TRAE
  desc: 字节跳动推出的免费 AI IDE。支持原生中文，集成了 Claude 4 和 GPT-5 等主流 AI 模型（国内版为豆包大模型和DeepSeek模型），完全免费使用。
  link: https://www.trae.cn/
  img: /svg/website/Trae.png
  badge: AI编程工具, 国产
  badgeType: tip
- name: Qoder
  desc: 阿里巴巴推出的 Agentic 编程平台，定位为“面向真实工程的 AI 开发伙伴”，解决复杂软件开发的效率与认知瓶颈。
  link: https://qoder.com/
  img: /svg/website/qoder.svg
  badge: AI编程工具, 国产
- name: CodeBuddy
  desc: 腾讯云推出的AI代码助手，定位为智能编程伙伴，支持自然语言编程、多文件代码生成、代码补全及单元测试等功能。
  link: https://copilot.tencent.com/ide/
  img: /svg/website/CodeBuddy.svg
  badge: AI编程工具, 国产
- name: CodeGeeX
  desc: 智谱AI推出的开源的免费AI编程助手，该工具基于130亿参数的预训练大模型，可以快速生成代码，帮助开发者提升开发效率。
  link: https://codegeex.cn/
  img: /svg/website/codegeex.png
  badge: AI编程工具, 国产
```

:::

---

---
url: /Java/微服务专栏/05.流控组件Sentinel/微服务保护.md
---

# 微服务保护

# 1.初识Sentinel

## 1.1.雪崩问题及解决方案

### 1.1.1.雪崩问题

微服务中，服务间调用关系错综复杂，一个微服务往往依赖于多个其它微服务。

![1533829099748](/assets/1533829099748.iLca7IX0.png)

如图，如果服务提供者I发生了故障，当前的应用的部分业务因为依赖于服务I，因此也会被阻塞。此时，其它不依赖于服务I的业务似乎不受影响。

![1533829198240](/assets/1533829198240.DS9XJvBD.png)

但是，依赖服务I的业务请求被阻塞，用户不会得到响应，则tomcat的这个线程不会释放，于是越来越多的用户请求到来，越来越多的线程会阻塞：

![1533829307389](/assets/1533829307389.zOP0PcL6.png)

服务器支持的线程和并发数有限，请求一直阻塞，会导致服务器资源耗尽，从而导致所有其它服务都不可用，那么当前服务也就不可用了。

那么，依赖于当前服务的其它服务随着时间的推移，最终也都会变的不可用，形成级联失败，雪崩就发生了：

![image-20210715172710340](/assets/image-20210715172710340.B78nivPM.png)

### 1.1.2.超时处理

解决雪崩问题的常见方式有四种：

•超时处理：设定超时时间，请求超过一定时间没有响应就返回错误信息，不会无休止等待

![image-20210715172820438](/assets/image-20210715172820438.DLZ78DuS.png)

### 1.1.3.仓壁模式

方案2：仓壁模式

仓壁模式来源于船舱的设计：

![image-20210715172946352](/assets/image-20210715172946352.Dz2_EH_0.png)

船舱都会被隔板分离为多个独立空间，当船体破损时，只会导致部分空间进入，将故障控制在一定范围内，避免整个船体都被淹没。

于此类似，我们可以限定每个业务能使用的线程数，避免耗尽整个tomcat的资源，因此也叫线程隔离。

![image-20210715173215243](/assets/image-20210715173215243.CYp1sONX.png)

### 1.1.4.断路器

断路器模式：由**断路器**统计业务执行的异常比例，如果超出阈值则会**熔断**该业务，拦截访问该业务的一切请求。

断路器会统计访问某个服务的请求数量，异常比例：

![image-20210715173327075](/assets/image-20210715173327075.C6yyesDw.png)

当发现访问服务D的请求异常比例过高时，认为服务D有导致雪崩的风险，会拦截访问服务D的一切请求，形成熔断：

![image-20210715173428073](/assets/image-20210715173428073.VrjA15ns.png)

### 1.1.5.限流

**流量控制**：限制业务访问的QPS，避免服务因流量的突增而故障。

![image-20210715173555158](/assets/image-20210715173555158.FT7II5i6.png)

### 1.1.6.总结

什么是雪崩问题？

* 微服务之间相互调用，因为调用链中的一个服务故障，引起整个链路都无法访问的情况。

可以认为：

**限流**是对服务的保护，避免因瞬间高并发流量而导致服务故障，进而避免雪崩。是一种**预防**措施。

**超时处理、线程隔离、降级熔断**是在部分服务故障时，将故障控制在一定范围，避免雪崩。是一种**补救**措施。

## 1.2.服务保护技术对比

在SpringCloud当中支持多种服务保护技术：

* [Netfix Hystrix](https://github.com/Netflix/Hystrix)
* [Sentinel](https://github.com/alibaba/Sentinel)
* [Resilience4J](https://github.com/resilience4j/resilience4j)

早期比较流行的是Hystrix框架，但目前国内实用最广泛的还是阿里巴巴的Sentinel框架，这里我们做下对比：

|                | **Sentinel**                                   | **Hystrix**                   |
| -------------- | ---------------------------------------------- | ----------------------------- |
| 隔离策略       | 信号量隔离                                     | 线程池隔离/信号量隔离         |
| 熔断降级策略   | 基于慢调用比例或异常比例                       | 基于失败比率                  |
| 实时指标实现   | 滑动窗口                                       | 滑动窗口（基于 RxJava）       |
| 规则配置       | 支持多种数据源                                 | 支持多种数据源                |
| 扩展性         | 多个扩展点                                     | 插件的形式                    |
| 基于注解的支持 | 支持                                           | 支持                          |
| 限流           | 基于 QPS，支持基于调用关系的限流               | 有限的支持                    |
| 流量整形       | 支持慢启动、匀速排队模式                       | 不支持                        |
| 系统自适应保护 | 支持                                           | 不支持                        |
| 控制台         | 开箱即用，可配置规则、查看秒级监控、机器发现等 | 不完善                        |
| 常见框架的适配 | Servlet、Spring Cloud、Dubbo、gRPC  等         | Servlet、Spring Cloud Netflix |

## 1.3.Sentinel介绍和安装

### 1.3.1.初识Sentinel

Sentinel是阿里巴巴开源的一款微服务流量控制组件。官网地址：https://sentinelguard.io/zh-cn/index.html

Sentinel 具有以下特征:

•**丰富的应用场景**：Sentinel 承接了阿里巴巴近 10 年的双十一大促流量的核心场景，例如秒杀（即突发流量控制在系统容量可以承受的范围）、消息削峰填谷、集群流量控制、实时熔断下游不可用应用等。

•**完备的实时监控**：Sentinel 同时提供实时的监控功能。您可以在控制台中看到接入应用的单台机器秒级数据，甚至 500 台以下规模的集群的汇总运行情况。

•**广泛的开源生态**：Sentinel 提供开箱即用的与其它开源框架/库的整合模块，例如与 Spring Cloud、Dubbo、gRPC 的整合。您只需要引入相应的依赖并进行简单的配置即可快速地接入 Sentinel。

•**完善的** **SPI** **扩展点**：Sentinel 提供简单易用、完善的 SPI 扩展接口。您可以通过实现扩展接口来快速地定制逻辑。例如定制规则管理、适配动态数据源等。

### 1.3.2.安装Sentinel

1）下载

sentinel官方提供了UI控制台，方便我们对系统做限流设置。大家可以在[GitHub](https://github.com/alibaba/Sentinel/releases)下载。

课前资料也提供了下载好的jar包：

![image-20210715174252531](/assets/image-20210715174252531.5Gq5JhZo.png)

2）运行

将jar包放到任意非中文目录，执行命令：

```sh
java -jar sentinel-dashboard-1.8.1.jar
```

如果要修改Sentinel的默认端口、账户、密码，可以通过下列配置：

| **配置项**                       | **默认值** | **说明**   |
| -------------------------------- | ---------- | ---------- |
| server.port                      | 8080       | 服务端口   |
| sentinel.dashboard.auth.username | sentinel   | 默认用户名 |
| sentinel.dashboard.auth.password | sentinel   | 默认密码   |

例如，修改端口：

```sh
java -Dserver.port=8090 -jar sentinel-dashboard-1.8.1.jar
```

3）访问

访问http://localhost:8080页面，就可以看到sentinel的控制台了：

![image-20210715190827846](/assets/image-20210715190827846.BW8ywjPL.png)

需要输入账号和密码，默认都是：sentinel

登录后，发现一片空白，什么都没有：

![image-20210715191134448](/assets/image-20210715191134448.Bs-UT_oF.png)

这是因为我们还没有与微服务整合。

## 1.4.微服务整合Sentinel

我们在order-service中整合sentinel，并连接sentinel的控制台，步骤如下：

1）引入sentinel依赖

```xml
<!--sentinel-->
<dependency>
    <groupId>com.alibaba.cloud</groupId> 
    <artifactId>spring-cloud-starter-alibaba-sentinel</artifactId>
</dependency>
```

2）配置控制台

修改application.yaml文件，添加下面内容：

```yaml
server:
  port: 8088
spring:
  cloud: 
    sentinel:
      transport:
        dashboard: localhost:8080
```

3）访问order-service的任意端点

打开浏览器，访问http://localhost:8088/order/101，这样才能触发sentinel的监控。

然后再访问sentinel的控制台，查看效果：

![image-20210715191241799](/assets/image-20210715191241799.BCkCJYIp.png)

# 2.流量控制

雪崩问题虽然有四种方案，但是限流是避免服务因突发的流量而发生故障，是对微服务雪崩问题的预防。我们先学习这种模式。

## 2.1.簇点链路

当请求进入微服务时，首先会访问DispatcherServlet，然后进入Controller、Service、Mapper，这样的一个调用链就叫做**簇点链路**。簇点链路中被监控的每一个接口就是一个**资源**。

默认情况下sentinel会监控SpringMVC的每一个端点（Endpoint，也就是controller中的方法），因此SpringMVC的每一个端点（Endpoint）就是调用链路中的一个资源。

例如，我们刚才访问的order-service中的OrderController中的端点：/order/{orderId}

![image-20210715191757319](/assets/image-20210715191757319.D7kECnJ4.png)

流控、熔断等都是针对簇点链路中的资源来设置的，因此我们可以点击对应资源后面的按钮来设置规则：

* 流控：流量控制
* 降级：降级熔断
* 热点：热点参数限流，是限流的一种
* 授权：请求的权限控制

## 2.1.快速入门

### 2.1.1.示例

点击资源/order/{orderId}后面的流控按钮，就可以弹出表单。

![image-20210715191757319](/assets/image-20210715191757319.D7kECnJ4.png)

表单中可以填写限流规则，如下：

![image-20210715192010657](/assets/image-20210715192010657.DB4PbqvT.png)

其含义是限制 /order/{orderId}这个资源的单机QPS为1，即每秒只允许1次请求，超出的请求会被拦截并报错。

### 2.1.2.练习：

需求：给 /order/{orderId}这个资源设置流控规则，QPS不能超过 5，然后测试。

1）首先在sentinel控制台添加限流规则

![image-20210715192455429](/assets/image-20210715192455429.CGfy_9HM.png)

2）利用jmeter测试

如果没有用过jmeter，可以参考课前资料提供的文档《Jmeter快速入门.md》

课前资料提供了编写好的Jmeter测试样例：

![image-20210715200431615](/assets/image-20210715200431615.2ReXwksx.png)

打开jmeter，导入课前资料提供的测试样例：

![image-20210715200537171](/assets/image-20210715200537171.K8PPusVA.png)

选择：

![image-20210715200635414](/assets/image-20210715200635414.DOv4WL9d.png)

20个用户，2秒内运行完，QPS是10，超过了5.

选中`流控入门，QPS<5`右键运行：

![image-20210715200804594](/assets/image-20210715200804594.R3NbF-Bl.png)

> 注意，不要点击菜单中的执行按钮来运行。

结果：

![image-20210715200853671](/assets/image-20210715200853671.BijK1Uoy.png)

可以看到，成功的请求每次只有5个

## 2.2.流控模式

在添加限流规则时，点击高级选项，可以选择三种**流控模式**：

* 直接：统计当前资源的请求，触发阈值时对当前资源直接限流，也是默认的模式
* 关联：统计与当前资源相关的另一个资源，触发阈值时，对当前资源限流
* 链路：统计从指定链路访问到本资源的请求，触发阈值时，对指定链路限流

![image-20210715201827886](/assets/image-20210715201827886.Dscg0iiO.png)

快速入门测试的就是直接模式。

### 2.2.1.关联模式

**关联模式**：统计与当前资源相关的另一个资源，触发阈值时，对当前资源限流

**配置规则**：

![image-20210715202540786](/assets/image-20210715202540786.BpZqQuZ9.png)

**语法说明**：当/write资源访问量触发阈值时，就会对/read资源限流，避免影响/write资源。

**使用场景**：比如用户支付时需要修改订单状态，同时用户要查询订单。查询和修改操作会争抢数据库锁，产生竞争。业务需求是优先支付和更新订单的业务，因此当修改订单业务触发阈值时，需要对查询订单业务限流。

**需求说明**：

* 在OrderController新建两个端点：/order/query和/order/update，无需实现业务

* 配置流控规则，当/order/ update资源被访问的QPS超过5时，对/order/query请求限流

1）定义/order/query端点，模拟订单查询

```java
@GetMapping("/query")
public String queryOrder() {
    return "查询订单成功";
}
```

2）定义/order/update端点，模拟订单更新

```java
@GetMapping("/update")
public String updateOrder() {
    return "更新订单成功";
}
```

重启服务，查看sentinel控制台的簇点链路：

![image-20210716101805951](/assets/image-20210716101805951.BGcxRRaN.png)

3）配置流控规则

对哪个端点限流，就点击哪个端点后面的按钮。我们是对订单查询/order/query限流，因此点击它后面的按钮：

![image-20210716101934499](/assets/image-20210716101934499.CPpdqEY_.png)

在表单中填写流控规则：

![image-20210716102103814](/assets/image-20210716102103814.CL4qJswp.png)

4）在Jmeter测试

选择《流控模式-关联》：

![image-20210716102416266](/assets/image-20210716102416266.DiSNMFDa.png)

可以看到1000个用户，100秒，因此QPS为10，超过了我们设定的阈值：5

查看http请求：

![image-20210716102532554](/assets/image-20210716102532554.DzPRYWjn.png)

请求的目标是/order/update，这样这个断点就会触发阈值。

但限流的目标是/order/query，我们在浏览器访问，可以发现：

![image-20210716102636030](/assets/image-20210716102636030.CopVopCx.png)

确实被限流了。

5）总结

![image-20210716103143002](/assets/image-20210716103143002.CHCgZ5LY.png)

### 2.2.2.链路模式

**链路模式**：只针对从指定链路访问到本资源的请求做统计，判断是否超过阈值。

**配置示例**：

例如有两条请求链路：

* /test1 --> /common

* /test2 --> /common

如果只希望统计从/test2进入到/common的请求，则可以这样配置：

![image-20210716103536346](/assets/image-20210716103536346.DDg-MfZK.png)

**实战案例**

需求：有查询订单和创建订单业务，两者都需要查询商品。针对从查询订单进入到查询商品的请求统计，并设置限流。

步骤：

1. 在OrderService中添加一个queryGoods方法，不用实现业务

2. 在OrderController中，改造/order/query端点，调用OrderService中的queryGoods方法

3. 在OrderController中添加一个/order/save的端点，调用OrderService的queryGoods方法

4. 给queryGoods设置限流规则，从/order/query进入queryGoods的方法限制QPS必须小于2

实现：

#### 1）添加查询商品方法

在order-service服务中，给OrderService类添加一个queryGoods方法：

```java
public void queryGoods(){
    System.err.println("查询商品");
}
```

#### 2）查询订单时，查询商品

在order-service的OrderController中，修改/order/query端点的业务逻辑：

```java
@GetMapping("/query")
public String queryOrder() {
    // 查询商品
    orderService.queryGoods();
    // 查询订单
    System.out.println("查询订单");
    return "查询订单成功";
}
```

#### 3）新增订单，查询商品

在order-service的OrderController中，修改/order/save端点，模拟新增订单：

```java
@GetMapping("/save")
public String saveOrder() {
    // 查询商品
    orderService.queryGoods();
    // 查询订单
    System.err.println("新增订单");
    return "新增订单成功";
}
```

#### 4）给查询商品添加资源标记

默认情况下，OrderService中的方法是不被Sentinel监控的，需要我们自己通过注解来标记要监控的方法。

给OrderService的queryGoods方法添加@SentinelResource注解：

```java
@SentinelResource("goods")
public void queryGoods(){
    System.err.println("查询商品");
}
```

链路模式中，是对不同来源的两个链路做监控。但是sentinel默认会给进入SpringMVC的所有请求设置同一个root资源，会导致链路模式失效。

我们需要关闭这种对SpringMVC的资源聚合，修改order-service服务的application.yml文件：

```yaml
spring:
  cloud:
    sentinel:
      web-context-unify: false # 关闭context整合
```

重启服务，访问/order/query和/order/save，可以查看到sentinel的簇点链路规则中，出现了新的资源：

![image-20210716105227163](/assets/image-20210716105227163.CuUa6ZnE.png)

#### 5）添加流控规则

点击goods资源后面的流控按钮，在弹出的表单中填写下面信息：

![image-20210716105408723](/assets/image-20210716105408723.DzFUY0nd.png)

只统计从/order/query进入/goods的资源，QPS阈值为2，超出则被限流。

#### 6）Jmeter测试

选择《流控模式-链路》：

![image-20210716105612312](/assets/image-20210716105612312.CVxtNSCY.png)

可以看到这里200个用户，50秒内发完，QPS为4，超过了我们设定的阈值2

一个http请求是访问/order/save：

![image-20210716105812789](/assets/image-20210716105812789.DG3VcqKD.png)

运行的结果：

![image-20210716110027064](/assets/image-20210716110027064.D8jQQhRs.png)

完全不受影响。

另一个是访问/order/query：

![image-20210716105855951](/assets/image-20210716105855951.Bei8tNYM.png)

运行结果：

![image-20210716105956401](/assets/image-20210716105956401.Bs_ABP3U.png)

每次只有2个通过。

### 2.2.3.总结

流控模式有哪些？

•直接：对当前资源限流

•关联：高优先级资源触发阈值，对低优先级资源限流。

•链路：阈值统计时，只统计从指定资源进入当前资源的请求，是对请求来源的限流

## 2.3.流控效果

在流控的高级选项中，还有一个流控效果选项：

![image-20210716110225104](/assets/image-20210716110225104.8jr4iGbu.png)

流控效果是指请求达到流控阈值时应该采取的措施，包括三种：

* 快速失败：达到阈值后，新的请求会被立即拒绝并抛出FlowException异常。是默认的处理方式。

* warm up：预热模式，对超出阈值的请求同样是拒绝并抛出异常。但这种模式阈值会动态变化，从一个较小值逐渐增加到最大阈值。

* 排队等待：让所有的请求按照先后次序排队执行，两个请求的间隔不能小于指定时长

### 2.3.1.warm up

阈值一般是一个微服务能承担的最大QPS，但是一个服务刚刚启动时，一切资源尚未初始化（**冷启动**），如果直接将QPS跑到最大值，可能导致服务瞬间宕机。

warm up也叫**预热模式**，是应对服务冷启动的一种方案。请求阈值初始值是 maxThreshold / coldFactor，持续指定时长后，逐渐提高到maxThreshold值。而coldFactor的默认值是3.

例如，我设置QPS的maxThreshold为10，预热时间为5秒，那么初始阈值就是 10 / 3 ，也就是3，然后在5秒后逐渐增长到10.

![image-20210716110629796](/assets/image-20210716110629796.ByIT8GNh.png)

**案例**

需求：给/order/{orderId}这个资源设置限流，最大QPS为10，利用warm up效果，预热时长为5秒

#### 1）配置流控规则：

![image-20210716111012387](/assets/image-20210716111012387.BDhVuNoM.png)

#### 2）Jmeter测试

选择《流控效果，warm up》：

![image-20210716111136699](/assets/image-20210716111136699.CzNWAxx6.png)

QPS为10.

刚刚启动时，大部分请求失败，成功的只有3个，说明QPS被限定在3：

![image-20210716111303701](/assets/image-20210716111303701.DRFzsktN.png)

随着时间推移，成功比例越来越高：

![image-20210716111404717](/assets/image-20210716111404717.DLJDhf6b.png)

到Sentinel控制台查看实时监控：

![image-20210716111526480](/assets/image-20210716111526480.DA5EiKV2.png)

一段时间后：

![image-20210716111658541](/assets/image-20210716111658541.CuDY5LFG.png)

### 2.3.2.排队等待

当请求超过QPS阈值时，快速失败和warm up 会拒绝新的请求并抛出异常。

而排队等待则是让所有请求进入一个队列中，然后按照阈值允许的时间间隔依次执行。后来的请求必须等待前面执行完成，如果请求预期的等待时间超出最大时长，则会被拒绝。

工作原理

例如：QPS = 5，意味着每200ms处理一个队列中的请求；timeout = 2000，意味着**预期等待时长**超过2000ms的请求会被拒绝并抛出异常。

那什么叫做预期等待时长呢？

比如现在一下子来了12 个请求，因为每200ms执行一个请求，那么：

* 第6个请求的**预期等待时长** =  200 \* （6 - 1） = 1000ms
* 第12个请求的预期等待时长 = 200 \* （12-1） = 2200ms

现在，第1秒同时接收到10个请求，但第2秒只有1个请求，此时QPS的曲线这样的：

![image-20210716113147176](/assets/image-20210716113147176.C7vt6XG2.png)

如果使用队列模式做流控，所有进入的请求都要排队，以固定的200ms的间隔执行，QPS会变的很平滑：

![image-20210716113426524](assets/image-20210716113426524.png)

平滑的QPS曲线，对于服务器来说是更友好的。

**案例**

需求：给/order/{orderId}这个资源设置限流，最大QPS为10，利用排队的流控效果，超时时长设置为5s

#### 1）添加流控规则

![image-20210716114048918](/assets/image-20210716114048918.BeFJt0xk.png)

#### 2）Jmeter测试

选择《流控效果，队列》：

![image-20210716114243558](/assets/image-20210716114243558.DfejGxSA.png)

QPS为15，已经超过了我们设定的10。

如果是之前的 快速失败、warmup模式，超出的请求应该会直接报错。

但是我们看看队列模式的运行结果：

![image-20210716114429361](/assets/image-20210716114429361.CaMKdS5X.png)

全部都通过了。

再去sentinel查看实时监控的QPS曲线：

![image-20210716114522935](/assets/image-20210716114522935.CYwxbo7H.png)

QPS非常平滑，一致保持在10，但是超出的请求没有被拒绝，而是放入队列。因此**响应时间**（等待时间）会越来越长。

当队列满了以后，才会有部分请求失败：

![image-20210716114651137](/assets/image-20210716114651137.Ba2mVEbQ.png)

### 2.3.3.总结

流控效果有哪些？

* 快速失败：QPS超过阈值时，拒绝新的请求

* warm up： QPS超过阈值时，拒绝新的请求；QPS阈值是逐渐提升的，可以避免冷启动时高并发导致服务宕机。

* 排队等待：请求会进入队列，按照阈值允许的时间间隔依次执行请求；如果请求预期等待时长大于超时时间，直接拒绝

## 2.4.热点参数限流

之前的限流是统计访问某个资源的所有请求，判断是否超过QPS阈值。而热点参数限流是**分别统计参数值相同的请求**，判断是否超过QPS阈值。

### 2.4.1.全局参数限流

例如，一个根据id查询商品的接口：

![image-20210716115014663](/assets/image-20210716115014663.CWHFKmoq.png)

访问/goods/{id}的请求中，id参数值会有变化，热点参数限流会根据参数值分别统计QPS，统计结果：

![image-20210716115131463](/assets/image-20210716115131463.B1lJu553.png)

当id=1的请求触发阈值被限流时，id值不为1的请求不受影响。

配置示例：

![image-20210716115232426](/assets/image-20210716115232426.CowUzrWE.png)

代表的含义是：对hot这个资源的0号参数（第一个参数）做统计，每1秒**相同参数值**的请求数不能超过5

### 2.4.2.热点参数限流

刚才的配置中，对查询商品这个接口的所有商品一视同仁，QPS都限定为5.

而在实际开发中，可能部分商品是热点商品，例如秒杀商品，我们希望这部分商品的QPS限制与其它商品不一样，高一些。那就需要配置热点参数限流的高级选项了：

![image-20210716115717523](/assets/image-20210716115717523.NL_Zoj90.png)

结合上一个配置，这里的含义是对0号的long类型参数限流，每1秒相同参数的QPS不能超过5，有两个例外：

•如果参数值是100，则每1秒允许的QPS为10

•如果参数值是101，则每1秒允许的QPS为15

### 2.4.4.案例

**案例需求**：给/order/{orderId}这个资源添加热点参数限流，规则如下：

•默认的热点参数规则是每1秒请求量不超过2

•给102这个参数设置例外：每1秒请求量不超过4

•给103这个参数设置例外：每1秒请求量不超过10

**注意事项**：热点参数限流对默认的SpringMVC资源无效，需要利用@SentinelResource注解标记资源

#### 1）标记资源

给order-service中的OrderController中的/order/{orderId}资源添加注解：

![image-20210716120033572](/assets/image-20210716120033572.HIQzy_rj.png)

#### 2）热点参数限流规则

访问该接口，可以看到我们标记的hot资源出现了：

![image-20210716120208509](/assets/image-20210716120208509.CmxtJ5_o.png)

这里不要点击hot后面的按钮，页面有BUG

点击左侧菜单中**热点规则**菜单：

![image-20210716120319009](/assets/image-20210716120319009.DDZ_dbXw.png)

点击新增，填写表单：

![image-20210716120536714](/assets/image-20210716120536714.C6Lkp3xU.png)

#### 3）Jmeter测试

选择《热点参数限流 QPS1》：

![image-20210716120754527](/assets/image-20210716120754527.B1-ppQr7.png)

这里发起请求的QPS为5.

包含3个http请求：

普通参数，QPS阈值为2

![image-20210716120840501](/assets/image-20210716120840501.uhkFPBi4.png)

运行结果：

![image-20210716121105567](/assets/image-20210716121105567.BaKEGcze.png)

例外项，QPS阈值为4

![image-20210716120900365](/assets/image-20210716120900365.rGfszXXb.png)

运行结果：

![image-20210716121201630](/assets/image-20210716121201630.D1k71nRN.png)

例外项，QPS阈值为10

![image-20210716120919131](/assets/image-20210716120919131.CmZ9PEWE.png)

运行结果：

![image-20210716121220305](/assets/image-20210716121220305.CYEiVhFY.png)

# 3.隔离和降级

限流是一种预防措施，虽然限流可以尽量避免因高并发而引起的服务故障，但服务还会因为其它原因而故障。

而要将这些故障控制在一定范围，避免雪崩，就要靠**线程隔离**（舱壁模式）和**熔断降级**手段了。

**线程隔离**之前讲到过：调用者在调用服务提供者时，给每个调用的请求分配独立线程池，出现故障时，最多消耗这个线程池内资源，避免把调用者的所有资源耗尽。

![image-20210715173215243](/assets/image-20210715173215243.CYp1sONX.png)

**熔断降级**：是在调用方这边加入断路器，统计对服务提供者的调用，如果调用的失败比例过高，则熔断该业务，不允许访问该服务的提供者了。

![image-20210715173428073](/assets/image-20210715173428073.VrjA15ns.png)

可以看到，不管是线程隔离还是熔断降级，都是对**客户端**（调用方）的保护。需要在**调用方** 发起远程调用时做线程隔离、或者服务熔断。

而我们的微服务远程调用都是基于Feign来完成的，因此我们需要将Feign与Sentinel整合，在Feign里面实现线程隔离和服务熔断。

## 3.1.FeignClient整合Sentinel

SpringCloud中，微服务调用都是通过Feign来实现的，因此做客户端保护必须整合Feign和Sentinel。

### 3.1.1.修改配置，开启sentinel功能

修改OrderService的application.yml文件，开启Feign的Sentinel功能：

```yaml
feign:
  sentinel:
    enabled: true # 开启feign对sentinel的支持
```

### 3.1.2.编写失败降级逻辑

业务失败后，不能直接报错，而应该返回用户一个友好提示或者默认结果，这个就是失败降级逻辑。

给FeignClient编写失败后的降级逻辑

①方式一：FallbackClass，无法对远程调用的异常做处理

②方式二：FallbackFactory，可以对远程调用的异常做处理，我们选择这种

这里我们演示方式二的失败降级处理。

**步骤一**：在feing-api项目中定义类，实现FallbackFactory：

![image-20210716122403502](/assets/image-20210716122403502.Vnnxmi7A.png)

代码：

```java
package cn.itcast.feign.clients.fallback;

import cn.itcast.feign.clients.UserClient;
import cn.itcast.feign.pojo.User;
import feign.hystrix.FallbackFactory;
import lombok.extern.slf4j.Slf4j;

@Slf4j
public class UserClientFallbackFactory implements FallbackFactory<UserClient> {
    @Override
    public UserClient create(Throwable throwable) {
        return new UserClient() {
            @Override
            public User findById(Long id) {
                log.error("查询用户异常", throwable);
                return new User();
            }
        };
    }
}

```

**步骤二**：在feing-api项目中的DefaultFeignConfiguration类中将UserClientFallbackFactory注册为一个Bean：

```java
@Bean
public UserClientFallbackFactory userClientFallbackFactory(){
    return new UserClientFallbackFactory();
}
```

**步骤三**：在feing-api项目中的UserClient接口中使用UserClientFallbackFactory：

```java
import cn.itcast.feign.clients.fallback.UserClientFallbackFactory;
import cn.itcast.feign.pojo.User;
import org.springframework.cloud.openfeign.FeignClient;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.PathVariable;

@FeignClient(value = "userservice", fallbackFactory = UserClientFallbackFactory.class)
public interface UserClient {

    @GetMapping("/user/{id}")
    User findById(@PathVariable("id") Long id);
}
```

重启后，访问一次订单查询业务，然后查看sentinel控制台，可以看到新的簇点链路：

![image-20210716123705780](/assets/image-20210716123705780.DIZGvUTE.png)

### 3.1.3.总结

Sentinel支持的雪崩解决方案：

* 线程隔离（仓壁模式）
* 降级熔断

Feign整合Sentinel的步骤：

* 在application.yml中配置：feign.sentienl.enable=true
* 给FeignClient编写FallbackFactory并注册为Bean
* 将FallbackFactory配置到FeignClient

## 3.2.线程隔离（舱壁模式）

### 3.2.1.线程隔离的实现方式

线程隔离有两种方式实现：

* 线程池隔离

* 信号量隔离（Sentinel默认采用）

如图：

![image-20210716123036937](/assets/image-20210716123036937.DE22Zw1Q.png)

**线程池隔离**：给每个服务调用业务分配一个线程池，利用线程池本身实现隔离效果

**信号量隔离**：不创建线程池，而是计数器模式，记录业务使用的线程数量，达到信号量上限时，禁止新的请求。

两者的优缺点：

![image-20210716123240518](/assets/image-20210716123240518.B0pUA-lv.png)

### 3.2.2.sentinel的线程隔离

**用法说明**：

在添加限流规则时，可以选择两种阈值类型：

![image-20210716123411217](/assets/image-20210716123411217.CiPOjStz.png)

* QPS：就是每秒的请求数，在快速入门中已经演示过

* 线程数：是该资源能使用用的tomcat线程数的最大值。也就是通过限制线程数量，实现**线程隔离**（舱壁模式）。

**案例需求**：给 order-service服务中的UserClient的查询用户接口设置流控规则，线程数不能超过 2。然后利用jemeter测试。

#### 1）配置隔离规则

选择feign接口后面的流控按钮：

![image-20210716123831992](/assets/image-20210716123831992.BrrwJX_7.png)

填写表单：

![image-20210716123936844](/assets/image-20210716123936844.wzvZRYBB.png)

#### 2）Jmeter测试

选择《阈值类型-线程数<2》：

![image-20210716124229894](/assets/image-20210716124229894.D0jkvWZg.png)

一次发生10个请求，有较大概率并发线程数超过2，而超出的请求会走之前定义的失败降级逻辑。

查看运行结果：

![image-20210716124147820](/assets/image-20210716124147820.DXWQ9JGW.png)

发现虽然结果都是通过了，不过部分请求得到的响应是降级返回的null信息。

### 3.2.3.总结

线程隔离的两种手段是？

* 信号量隔离

* 线程池隔离

信号量隔离的特点是？

* 基于计数器模式，简单，开销小

线程池隔离的特点是？

* 基于线程池模式，有额外开销，但隔离控制更强

## 3.3.熔断降级

熔断降级是解决雪崩问题的重要手段。其思路是由**断路器**统计服务调用的异常比例、慢请求比例，如果超出阈值则会**熔断**该服务。即拦截访问该服务的一切请求；而当服务恢复时，断路器会放行访问该服务的请求。

断路器控制熔断和放行是通过状态机来完成的：

![image-20210716130958518](/assets/image-20210716130958518.Cv1W9oKN.png)

状态机包括三个状态：

* closed：关闭状态，断路器放行所有请求，并开始统计异常比例、慢请求比例。超过阈值则切换到open状态
* open：打开状态，服务调用被**熔断**，访问被熔断服务的请求会被拒绝，快速失败，直接走降级逻辑。Open状态5秒后会进入half-open状态
* half-open：半开状态，放行一次请求，根据执行结果来判断接下来的操作。
  * 请求成功：则切换到closed状态
  * 请求失败：则切换到open状态

断路器熔断策略有三种：慢调用、异常比例、异常数

### 3.3.1.慢调用

**慢调用**：业务的响应时长（RT）大于指定时长的请求认定为慢调用请求。在指定时间内，如果请求数量超过设定的最小数量，慢调用比例大于设定的阈值，则触发熔断。

例如：

![image-20210716145934347](/assets/image-20210716145934347.BIJw44jw.png)

解读：RT超过500ms的调用是慢调用，统计最近10000ms内的请求，如果请求量超过10次，并且慢调用比例不低于0.5，则触发熔断，熔断时长为5秒。然后进入half-open状态，放行一次请求做测试。

**案例**

需求：给 UserClient的查询用户接口设置降级规则，慢调用的RT阈值为50ms，统计时间为1秒，最小请求数量为5，失败阈值比例为0.4，熔断时长为5

#### 1）设置慢调用

修改user-service中的/user/{id}这个接口的业务。通过休眠模拟一个延迟时间：

![image-20210716150234787](/assets/image-20210716150234787.DQaTs35o.png)

此时，orderId=101的订单，关联的是id为1的用户，调用时长为60ms：

![image-20210716150510956](/assets/image-20210716150510956.BlFlNsir.png)

orderId=102的订单，关联的是id为2的用户，调用时长为非常短；

![image-20210716150605208](/assets/image-20210716150605208.CgevDCsB.png)

#### 2）设置熔断规则

下面，给feign接口设置降级规则：

![image-20210716150654094](/assets/image-20210716150654094.C2xN_dQW.png)

规则：

![image-20210716150740434](/assets/image-20210716150740434.eK9ps-1_.png)

超过50ms的请求都会被认为是慢请求

#### 3）测试

在浏览器访问：http://localhost:8088/order/101，快速刷新5次，可以发现：

![image-20210716150911004](/assets/image-20210716150911004.Cn1xFn_j.png)

触发了熔断，请求时长缩短至5ms，快速失败了，并且走降级逻辑，返回的null

在浏览器访问：http://localhost:8088/order/102，竟然也被熔断了：

![image-20210716151107785](/assets/image-20210716151107785.CasUIuvy.png)

### 3.3.2.异常比例、异常数

**异常比例或异常数**：统计指定时间内的调用，如果调用次数超过指定请求数，并且出现异常的比例达到设定的比例阈值（或超过指定异常数），则触发熔断。

例如，一个异常比例设置：

![image-20210716131430682](/assets/image-20210716131430682.BFQNpctl.png)

解读：统计最近1000ms内的请求，如果请求量超过10次，并且异常比例不低于0.4，则触发熔断。

一个异常数设置：

![image-20210716131522912](/assets/image-20210716131522912.oOQr6SSE.png)

解读：统计最近1000ms内的请求，如果请求量超过10次，并且异常比例不低于2次，则触发熔断。

**案例**

需求：给 UserClient的查询用户接口设置降级规则，统计时间为1秒，最小请求数量为5，失败阈值比例为0.4，熔断时长为5s

#### 1）设置异常请求

首先，修改user-service中的/user/{id}这个接口的业务。手动抛出异常，以触发异常比例的熔断：

![image-20210716151348183](/assets/image-20210716151348183.B4Nx-1qc.png)

也就是说，id 为 2时，就会触发异常

#### 2）设置熔断规则

下面，给feign接口设置降级规则：

![image-20210716150654094](/assets/image-20210716150654094.C2xN_dQW.png)

规则：

![image-20210716151538785](/assets/image-20210716151538785.B4wQ4-dt.png)

在5次请求中，只要异常比例超过0.4，也就是有2次以上的异常，就会触发熔断。

#### 3）测试

在浏览器快速访问：http://localhost:8088/order/102，快速刷新5次，触发熔断：

![image-20210716151722916](/assets/image-20210716151722916.DkpvWLZM.png)

此时，我们去访问本来应该正常的103：

![image-20210716151844817](/assets/image-20210716151844817.pP68hIY8.png)

# 4.授权规则

授权规则可以对请求方来源做判断和控制。

## 4.1.授权规则

### 4.1.1.基本规则

授权规则可以对调用方的来源做控制，有白名单和黑名单两种方式。

* 白名单：来源（origin）在白名单内的调用者允许访问

* 黑名单：来源（origin）在黑名单内的调用者不允许访问

点击左侧菜单的授权，可以看到授权规则：

![image-20210716152010750](/assets/image-20210716152010750.nLPdj--_.png)

* 资源名：就是受保护的资源，例如/order/{orderId}

* 流控应用：是来源者的名单，
  * 如果是勾选白名单，则名单中的来源被许可访问。
  * 如果是勾选黑名单，则名单中的来源被禁止访问。

比如：

![image-20210716152349191](/assets/image-20210716152349191.BBF6mwQN.png)

我们允许请求从gateway到order-service，不允许浏览器访问order-service，那么白名单中就要填写**网关的来源名称（origin）**。

### 4.1.2.如何获取origin

Sentinel是通过RequestOriginParser这个接口的parseOrigin来获取请求的来源的。

```java
public interface RequestOriginParser {
    /**
     * 从请求request对象中获取origin，获取方式自定义
     */
    String parseOrigin(HttpServletRequest request);
}
```

这个方法的作用就是从request对象中，获取请求者的origin值并返回。

默认情况下，sentinel不管请求者从哪里来，返回值永远是default，也就是说一切请求的来源都被认为是一样的值default。

因此，我们需要自定义这个接口的实现，让**不同的请求，返回不同的origin**。

例如order-service服务中，我们定义一个RequestOriginParser的实现类：

```java
package cn.itcast.order.sentinel;

import com.alibaba.csp.sentinel.adapter.spring.webmvc.callback.RequestOriginParser;
import org.springframework.stereotype.Component;
import org.springframework.util.StringUtils;

import javax.servlet.http.HttpServletRequest;

@Component
public class HeaderOriginParser implements RequestOriginParser {
    @Override
    public String parseOrigin(HttpServletRequest request) {
        // 1.获取请求头
        String origin = request.getHeader("origin");
        // 2.非空判断
        if (StringUtils.isEmpty(origin)) {
            origin = "blank";
        }
        return origin;
    }
}
```

我们会尝试从request-header中获取origin值。

### 4.1.3.给网关添加请求头

既然获取请求origin的方式是从reques-header中获取origin值，我们必须让**所有从gateway路由到微服务的请求都带上origin头**。

这个需要利用之前学习的一个GatewayFilter来实现，AddRequestHeaderGatewayFilter。

修改gateway服务中的application.yml，添加一个defaultFilter：

```yaml
spring:
  cloud:
    gateway:
      default-filters:
        - AddRequestHeader=origin,gateway
      routes:
       # ...略
```

这样，从gateway路由的所有请求都会带上origin头，值为gateway。而从其它地方到达微服务的请求则没有这个头。

### 4.1.4.配置授权规则

接下来，我们添加一个授权规则，放行origin值为gateway的请求。

![image-20210716153250134](/assets/image-20210716153250134.DmVc6Q5h.png)

配置如下：

![image-20210716153301069](/assets/image-20210716153301069.CUo8qrhi.png)

现在，我们直接跳过网关，访问order-service服务：

![image-20210716153348396](/assets/image-20210716153348396.DzXctlKP.png)

通过网关访问：

![image-20210716153434095](/assets/image-20210716153434095.DJJ6Nhk_.png)

## 4.2.自定义异常结果

默认情况下，发生限流、降级、授权拦截时，都会抛出异常到调用方。异常结果都是flow limmiting（限流）。这样不够友好，无法得知是限流还是降级还是授权拦截。

### 4.2.1.异常类型

而如果要自定义异常时的返回结果，需要实现BlockExceptionHandler接口：

```java
public interface BlockExceptionHandler {
    /**
     * 处理请求被限流、降级、授权拦截时抛出的异常：BlockException
     */
    void handle(HttpServletRequest request, HttpServletResponse response, BlockException e) throws Exception;
}
```

这个方法有三个参数：

* HttpServletRequest request：request对象
* HttpServletResponse response：response对象
* BlockException e：被sentinel拦截时抛出的异常

这里的BlockException包含多个不同的子类：

| **异常**             | **说明**           |
| -------------------- | ------------------ |
| FlowException        | 限流异常           |
| ParamFlowException   | 热点参数限流的异常 |
| DegradeException     | 降级异常           |
| AuthorityException   | 授权规则异常       |
| SystemBlockException | 系统规则异常       |

### 4.2.2.自定义异常处理

下面，我们就在order-service定义一个自定义异常处理类：

```java
package cn.itcast.order.sentinel;

import com.alibaba.csp.sentinel.adapter.spring.webmvc.callback.BlockExceptionHandler;
import com.alibaba.csp.sentinel.slots.block.BlockException;
import com.alibaba.csp.sentinel.slots.block.authority.AuthorityException;
import com.alibaba.csp.sentinel.slots.block.degrade.DegradeException;
import com.alibaba.csp.sentinel.slots.block.flow.FlowException;
import com.alibaba.csp.sentinel.slots.block.flow.param.ParamFlowException;
import org.springframework.stereotype.Component;

import javax.servlet.http.HttpServletRequest;
import javax.servlet.http.HttpServletResponse;

@Component
public class SentinelExceptionHandler implements BlockExceptionHandler {
    @Override
    public void handle(HttpServletRequest request, HttpServletResponse response, BlockException e) throws Exception {
        String msg = "未知异常";
        int status = 429;

        if (e instanceof FlowException) {
            msg = "请求被限流了";
        } else if (e instanceof ParamFlowException) {
            msg = "请求被热点参数限流";
        } else if (e instanceof DegradeException) {
            msg = "请求被降级了";
        } else if (e instanceof AuthorityException) {
            msg = "没有权限访问";
            status = 401;
        }

        response.setContentType("application/json;charset=utf-8");
        response.setStatus(status);
        response.getWriter().println("{\"msg\": " + msg + ", \"status\": " + status + "}");
    }
}
```

重启测试，在不同场景下，会返回不同的异常消息.

限流：

![image-20210716153938887](/assets/image-20210716153938887.Di6O5BmW.png)

授权拦截时：

![image-20210716154012736](/assets/image-20210716154012736.8K-8jmJe.png)

# 5.规则持久化

现在，sentinel的所有规则都是内存存储，重启后所有规则都会丢失。在生产环境下，我们必须确保这些规则的持久化，避免丢失。

## 5.1.规则管理模式

规则是否能持久化，取决于规则管理模式，sentinel支持三种规则管理模式：

* 原始模式：Sentinel的默认模式，将规则保存在内存，重启服务会丢失。
* pull模式
* push模式

### 5.1.1.pull模式

pull模式：控制台将配置的规则推送到Sentinel客户端，而客户端会将配置规则保存在本地文件或数据库中。以后会定时去本地文件或数据库中查询，更新本地规则。

![image-20210716154155238](/assets/image-20210716154155238.WrY-TcjH.png)

### 5.1.2.push模式

push模式：控制台将配置规则推送到远程配置中心，例如Nacos。Sentinel客户端监听Nacos，获取配置变更的推送消息，完成本地配置更新。

![image-20210716154215456](/assets/image-20210716154215456.DIZzXOnk.png)

## 5.2.实现push模式

详细步骤可以参考课前资料的《sentinel规则持久化》：

![image-20210716154255466](/assets/image-20210716154255466.COefct0x.png)

---

---
url: /daily/面试专栏/微服务相关/1_微服务概述.md
---

# 微服务概述

## 1.什么是微服务？

微服务（Microservices）是一种软件架构风格，将一个大型应用程序划分为一组小型、自治且松耦合的服务。每个微服务负责执行特定的业务功能，并通过轻量级通信机制（如HTTP）相互协作。每个微服务可以独立开发、部署和扩展，使得应用程序更加灵活、可伸缩和可维护。

在微服务的架构演进中，一般可能会存在这样的演进方向：单体式-->服务化-->微服务。

单体服务一般是所有项目最开始的样子：

* 单体服务（Monolithic Service）是一种传统的软件架构方式，将整个应用程序作为一个单一的、紧耦合的单元进行开发和部署。单体服务通常由多个模块组成，这些模块共享同一个数据库和代码库。然而，随着应用程序规模的增长，单体服务可能变得庞大且难以维护，且部署和扩展困难。

后来，单体服务过大，维护困难，渐渐演变到了分布式的SOA：

* SOA（Service-Oriented Architecture，面向服务的架构）是一种软件架构设计原则，强调将应用程序拆分为相互独立的服务，通过标准化的接口进行通信。SOA关注于服务的重用性和组合性，但并没有具体规定服务的大小。
* 微服务是在SOA的基础上进一步发展而来，是一种特定规模下的服务拆分和部署方式。微服务架构强调将应用程序拆分为小型、自治且松耦合的服务，每个服务都专注于特定的业务功能。这种架构使得应用程序更加灵活、可伸缩和可维护。

需要注意的是，微服务是一种特定的架构风格，而SOA是一种设计原则。微服务可以看作是对SOA思想的一种具体实践方式，但并不等同于SOA。

![架构演进简图](/assets/7772fb7d80d51fb7a2c4ee9d9bc1e773.CE9YjneH.png)

微服务与单体服务的区别在于规模和部署方式。微服务将应用程序拆分为更小的、自治的服务单元，每个服务都有自己的数据库和代码库，可以独立开发、测试、部署和扩展，带来了更大的灵活性、可维护性、可扩展性和容错性。

## 2.微服务带来了哪些挑战？

微服务架构不是万金油，尽它有很多优点，但是对于是否采用微服务架构，是否将原来的单体服务进行拆分，还是要考虑到服务拆分后可能带来的一些挑战和问题：

![微服务带来的挑战](/assets/196672410bdb8aa936f70b2c9d3c77e4.DsGxgGcj.png)

微服务带来的挑战

1. 系统复杂性增加：一个服务拆成了多个服务，整体系统的复杂性增加，需要处理服务之间的通信、部署、监控和维护等方面的复杂性。
2. 服务间通信开销：微服务之间通过网络进行通信，传递数据需要额外的网络开销和序列化开销，可能导致性能瓶颈和增加系统延迟。
3. 数据一致性和事务管理：每个微服务都有自己的数据存储，数据一致性和跨服务的事务管理变得更加复杂，需要额外解决分布式事务和数据同步的问题。
4. 部署和运维复杂性：微服务架构涉及多个独立部署的服务，对于部署、监控和容错机制的要求更高，需要建立适当的部署管道和自动化工具，以简化部署和运维过程。
5. 团队沟通和协作成本：每个微服务都由专门的团队负责，可能增加团队之间的沟通和协作成本。需要有效的沟通渠道和协作机制，确保服务之间的协调和一致性。
6. 服务治理和版本管理：随着微服务数量的增加，服务的治理和版本管理变得更加复杂。需要考虑服务的注册发现、负载均衡、监控和故障处理等方面，以确保整个系统的可靠性和稳定性。
7. 分布式系统的复杂性：微服务架构涉及构建和管理分布式系统，而分布式系统本身具有一些固有的挑战，如网络延迟、分布式一致性和容错性。

简单说，采用微服务需要权衡这些问题和挑战，根据实际的需求来选择对应的技术方案，很多时候单体能搞定的也可以用单体，不能为了微服务而微服务。

## 3.现在有哪些流行的微服务解决方案？

目前最主流的微服务开源解决方案有三种：

### Dubbo

![微服务带来的挑战](/assets/a4780c4f414c38627f5588dd778edb85.Y4dyltHr.png)

* Dubbo 是一个高性能、轻量级的 Java 微服务框架，最初由阿里巴巴（Alibaba）开发并于2011年开源。它提供了服务注册与发现、负载均衡、容错、分布式调用等功能，后来一度停止维护，在近两年，又重新开始迭代，并推出了Dubbo3。
* Dubbo 使用基于 RPC（Remote Procedure Call）的通信模型，具有较高的性能和可扩展性。它支持多种传输协议（如TCP、HTTP、Redis）和序列化方式（如JSON、Hessian、Protobuf），可根据需求进行配置。
* Dubbo更多地被认为是一个高性能的RPC（远程过程调用）框架，一些服务治理功能依赖于第三方组件实现，比如使用ZooKeeper、Apollo等等。

### Spring Cloud Netflix

* Spring Cloud Netflix 是 Spring Cloud 的一个子项目，结合了 Netflix 开源的多个组件，但是Netflix自2018年停止维护和更新Netflix OSS项目，包括Eureka、Hystrix等组件，所以Spring Cloud Netflix也逐渐进入了维护模式。
* 该项目包含了许多流行的 Netflix 组件，如Eureka（服务注册与发现）、Ribbon（客户端负载均衡）、Hystrix（断路器）、Zuul（API 网关）等。它们都是高度可扩展的、经过大规模实践验证的微服务组件。

### Spring Cloud Alibaba

* Spring Cloud Alibaba 是 Spring Cloud 的另一个子项目，与阿里巴巴的分布式应用开发框架相关。它提供了一整套与 Alibaba 生态系统集成的解决方案。
* 该项目包括 Nacos（服务注册与发现、配置管理）、Sentinel（流量控制、熔断降级）、RocketMQ（消息队列）等组件，以及与 Alibaba Cloud（阿里云）的集成。它为构建基于 Spring Cloud 的微服务架构提供了丰富的选项。
* 据说SpringCloud Alibaba项目的发起人已经跑路去了腾讯，并发起了SpringCloud Tecent项目，社区发展存在隐忧。

### 这三种方案有什么区别吗？

三种方案的区别：

| 特点             | Dubbo                  | Spring Cloud Netflix         | Spring Cloud Alibaba             |
| ---------------- | ---------------------- | ---------------------------- | -------------------------------- |
| 开发语言         | Java                   | Java                         | Java                             |
| 服务治理         | 提供完整的服务治理功能 | 提供部分服务治理功能         | 提供完整的服务治理功能           |
| 服务注册与发现   | ZooKeeper/Nacos        | Eureka/Consul                | Nacos                            |
| 负载均衡         | 自带负载均衡策略       | Ribbon                       | Ribbon\Dubbo负载均衡策略         |
| 服务调用         | RPC方式                | RestTemplate/Feign           | Feign/RestTemplate/Dubbo         |
| 熔断器           | Sentinel               | Hystrix                      | Sentinel/Resilience4j            |
| 配置中心         | Apollo                 | Spring Cloud Config          | Nacos Config                     |
| API网关          | Higress/APISIX         | Zuul/Gateway                 | Spring Cloud Gateway             |
| 分布式事务       | Seata                  | 不支持分布式事务             | Seata                            |
| 限流和降级       | Sentinel               | Hystrix                      | Sentinel                         |
| 分布式追踪和监控 | Skywalking             | Spring Cloud Sleuth + Zipkin | SkyWalking或Sentinel Dashboard   |
| 微服务网格       | Dubbo Mesh             | 不支持微服务网格             | Service Mesh（Nacos+Dubbo Mesh） |
| 社区活跃度       | 相对较高               | 目前较低                     | 相对较高                         |
| 孵化和成熟度     | 孵化较早，成熟度较高   | 成熟度较高                   | 孵化较新，但迅速发展             |

> 在面试中，微服务一般主要讨论的是Spring Cloud Netflix，其次是Spring Cloud Alibaba，Dubbo更多的是作为一个RPC框架来问。

## 4.说下微服务有哪些组件？

微服务给系统开发带来了一些问题和挑战，如服务调用的复杂性、分布式事务的处理、服务的动态管理等。为了更好地解决这些问题和挑战，各种微服务治理的组件应运而生，充当微服务架构的基石和支撑。

![微服务组件示意图](/assets/c6403db05cd3cbfad862de51f0547567.CYC-oXtN.png)

微服务的各个组件和常见实现：

1. 注册中心：用于服务的注册与发现，管理微服务的地址信息。常见的实现包括：
   * Spring Cloud Netflix：Eureka、Consul
   * Spring Cloud Alibaba：Nacos
2. 配置中心：用于集中管理微服务的配置信息，可以动态修改配置而不需要重启服务。常见的实现包括：
   * Spring Cloud Netflix：Spring Cloud Config
   * Spring Cloud Alibaba：Nacos Config
3. 远程调用：用于在不同的微服务之间进行通信和协作。常见的实现保包括：
   * RESTful API：如RestTemplate、Feign
   * RPC（远程过程调用）：如Dubbo、gRPC
4. API网关：作为微服务架构的入口，统一暴露服务，并提供路由、负载均衡、安全认证等功能。常见的实现包括：
   * Spring Cloud Netflix：Zuul、Gateway
   * Spring Cloud Alibaba：Gateway、Apisix等
5. 分布式事务：保证跨多个微服务的一致性和原子性操作。常见的实现包括：
   * Spring Cloud Alibaba：Seata
6. 熔断器：用于防止微服务之间的故障扩散，提高系统的容错能力。常见的实现包括：
   * Spring Cloud Netflix：Hystrix
   * Spring Cloud Alibaba：Sentinel、Resilience4j
7. 限流和降级：用于防止微服务过载，对请求进行限制和降级处理。常见的实现包括：
   * Spring Cloud Netflix：Hystrix
   * Spring Cloud Alibaba：Sentinel
8. 分布式追踪和监控：用于跟踪和监控微服务的请求流程和性能指标。常见的实现包括：
   * Spring Cloud Netflix：Spring Cloud Sleuth + Zipkin
   * Spring Cloud Alibaba：SkyWalking、Sentinel Dashboard

---

---
url: /常用框架/SpringBoot/SpringBoot与Web应用/6_文件上传.md
---

# 文件上传

---

---
url: /daily/开源项目/1_文件上传X-Spring-File-Storage.md
---

# 文件上传X Spring File Storage

## 一、简介

文件操作是平时开发工作中最常接触的一个功能，虽然难度不大，但确实有点繁琐。数据流的开闭、读取很容易出错，尤其是在对接一些云对象存储平台，接一个云平台写一大堆SDK代码，看起来乱糟糟的。X Spring File Storage工具宣称一行代码将文件存储到本地。

> 官网：<https://x-file-storage.xuyanwu.cn/>

## 二、SpringBoot快速集成

### 引入依赖

`pom.xml` 引入本项目，这里默认是 `SpringBoot` 环境，其它环境参考 [脱离 SpringBoot 单独使用](https://x-file-storage.xuyanwu.cn/#/脱离SpringBoot单独使用)

```xml
<dependency>
    <groupId>org.dromara.x-file-storage</groupId>
    <artifactId>x-file-storage-spring</artifactId>
    <version>2.1.0</version>
</dependency>
```

再引入对应平台的依赖（以阿里云OSS为例）

```xml
<!-- 阿里云OSS -->
<dependency>
    <groupId>com.aliyun.oss</groupId>
    <artifactId>aliyun-sdk-oss</artifactId>
    <version>3.16.1</version>
</dependency>
```

### 增加配置

`application.yml` 配置文件中先添加以下基础配置

```yaml
dromara:
  x-file-storage: #文件存储配置
    default-platform: aliyun-oss-1 #默认使用的存储平台
    thumbnail-suffix: ".min.jpg" #缩略图后缀，例如【.min.jpg】【.png】
    #对应平台的配置写在这里，注意缩进要对齐
```

再添加对应平台的配置

```yaml
aliyun-oss:
  - platform: aliyun-oss-1 # 存储平台标识
    enable-storage: true  # 启用存储
    access-key: ??
    secret-key: ??
    end-point: ??
    bucket-name: ??
    domain: ?? # 访问域名，注意“/”结尾，例如：https://abc.oss-cn-shanghai.aliyuncs.com/
    base-path: test/ # 基础路径
local-plus:
  - platform: local-plus-1 # 存储平台标识
    enable-storage: true  #启用存储
    enable-access: true #启用访问（线上请使用 Nginx 配置，效率更高）
    domain: http://127.0.0.1:8080/file/ # 访问域名，例如：“http://127.0.0.1:8030/file/”，注意后面要和 path-patterns 保持一致，“/”结尾，本地存储建议使用相对路径，方便后期更换域名
    base-path: local-plus/ # 基础路径
    path-patterns: /file/** # 访问路径
    storage-path: D:/Temp/ # 存储路径
```

更多参数请参考 `org.dromara.x.file.storage.spring.SpringFileStorageProperties.SpringAliyunOssConfig`

注意配置每个平台前面都有个`-`号，单个平台可以配置多个`platform`

### 编码

在启动类上加上`@EnableFileStorage`注解

```java
@EnableFileStorage
@SpringBootApplication
public class SpringFileStorageTestApplication {

    public static void main(String[] args) {
        SpringApplication.run(SpringFileStorageTestApplication.class,args);
    }

}
```

## 三、开始使用

### 上传

#### 基本上传方式

支持 File、MultipartFile、byte\[]、InputStream、URL、URI、String、HttpServletRequest，大文件会自动分片上传。

```java
// 直接上传
fileStorageService.of(file).upload();

// 如果要用 byte[]、InputStream、URL、URI、String 等方式上传，暂时无法获取 originalFilename 属性，最好手动设置
fileStorageService.of(inputStream).setOriginalFilename("a.jpg").upload();

// 上传到指定路径下
fileStorageService.of(file)
        .setPath("upload/") // 保存到相对路径下，为了方便管理，不需要可以不写
        .upload();

// 关联文件参数并上传
fileStorageService.of(file)
        .setObjectId("0")   // 关联对象id，为了方便管理，不需要可以不写
        .setObjectType("0") // 关联对象类型，为了方便管理，不需要可以不写
        .putAttr("role","admin") //保存一些属性，可以在切面、保存上传记录、自定义存储平台等地方获取使用，不需要可以不写
        .putAttr("username","007")
        .upload();

// 上传到指定的存储平台
fileStorageService.of(file)
        .setPlatform("aliyun-oss-1")    // 使用指定的存储平台
        .upload();

// 对图片进行处理并上传，有多个重载方法。图片处理使用的是 https://github.com/coobird/thumbnailator
fileStorageService.of(file)
        .setThumbnailSuffix(".jpg") //指定缩略图后缀，必须是 thumbnailator 支持的图片格式，默认使用全局的
        .setSaveThFilename("thabc") //指定缩略图的保存文件名，注意此文件名不含后缀，默认自动生成
        .image(img -> img.size(1000,1000))  // 将图片大小调整到 1000*1000
        .thumbnail(th -> th.size(200,200))  // 再生成一张 200*200 的缩略图
        .upload();

// 其它更多方法以实际 API 为准
```

代码示例

```java
@RestController
public class FileDetailController {

    @Autowired
    private FileStorageService fileStorageService;//注入实列

    /**
     * 上传文件
     */
    @PostMapping("/upload")
    public FileInfo upload(MultipartFile file) {
        return fileStorageService.of(file).upload();
    }
    
    /**
     * 上传文件，成功返回文件 url
     */
    @PostMapping("/upload2")
    public String upload2(MultipartFile file) {
        FileInfo fileInfo = fileStorageService.of(file)
                .setPath("upload/") //保存到相对路径下，为了方便管理，不需要可以不写
                .setObjectId("0")   //关联对象id，为了方便管理，不需要可以不写
                .setObjectType("0") //关联对象类型，为了方便管理，不需要可以不写
                .putAttr("role","admin") //保存一些属性，可以在切面、保存上传记录、自定义存储平台等地方获取使用，不需要可以不写
                .upload();  //将文件上传到对应地方
        return fileInfo == null ? "上传失败！" : fileInfo.getUrl();
    }

    /**
     * 上传图片，成功返回文件信息
     * 图片处理使用的是 https://github.com/coobird/thumbnailator
     */
    @PostMapping("/upload-image")
    public FileInfo uploadImage(MultipartFile file) {
        return fileStorageService.of(file)
                .image(img -> img.size(1000,1000))  //将图片大小调整到 1000*1000
                .thumbnail(th -> th.size(200,200))  //再生成一张 200*200 的缩略图
                .upload();
    }

    /**
     * 上传文件到指定存储平台，成功返回文件信息
     */
    @PostMapping("/upload-platform")
    public FileInfo uploadPlatform(MultipartFile file) {
        return fileStorageService.of(file)
                .setPlatform("aliyun-oss-1")    //使用指定的存储平台
                .upload();
    }

    /**
     * 直接读取 HttpServletRequest 中的文件进行上传，成功返回文件信息
     * 使用这种方式有些注意事项，请查看文档 基础功能-上传 章节
     */
    @PostMapping("/upload-request")
    public FileInfo uploadPlatform(HttpServletRequest request) {
        return fileStorageService.of(request).upload();
    }
}
```

FileInfo 结果集

```json
{
    "id": null,
    "url": "http://127.0.0.1:8080/file/local-plus/6633532212876ccd660cd50f.xlsx",
    "size": 3750,
    "filename": "6633532212876ccd660cd50f.xlsx",
    "originalFilename": "yuanJiHuoDongDaoChu20240430052317.xlsx",
    "basePath": "local-plus/",
    "path": "",
    "ext": "xlsx",
    "contentType": "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
    "platform": "local-plus-1",
    "thUrl": null,
    "thFilename": null,
    "thSize": null,
    "thContentType": null,
    "objectId": null,
    "objectType": null,
    "metadata": {},
    "userMetadata": {},
    "thMetadata": {},
    "thUserMetadata": {},
    "attr": {},
    "fileAcl": null,
    "thFileAcl": null,
    "hashInfo": {},
    "uploadId": null,
    "uploadStatus": null,
    "createTime": "2024-05-02T08:47:30.439+0000"
}
```

#### 直接上传HttpServletRequest

通过直接读取输入流进行上传，可以实现文件不落盘，边读取边上传，速度更快

需要先在配置文件中开启 `multipart` 懒加载，不然在 `Controller` 中拿到输入流是已经被读取过的

```yaml
spring.servlet.multipart.resolve-lazily: true
```

编码示例

```java
@RestController
public class FileDetailController {
    /**
     * 直接读取 HttpServletRequest 中的文件进行上传，成功返回文件信息
     */
    @PostMapping("/upload-request")
    public FileInfo uploadRequest(HttpServletRequest request) {
        return fileStorageService.of(request).upload();
    }

    /**
     * 这里演示了其它参数的获取方式
     */
    @PostMapping("/upload-request2")
    public FileInfo uploadRequest2(HttpServletRequest request) {
        HttpServletRequestFileWrapper wrapper = (HttpServletRequestFileWrapper) fileStorageService.wrapper(request);

        //获取指定参数，注意无法获取文件类型的参数
        String aaa = wrapper.getParameter("aaa");
        log.info("aaa：{}",aaa);

        //获取全部参数，注意无法获取文件类型的参数
        MultipartFormDataReader.MultipartFormData formData = wrapper.getMultipartFormData();
        Map<String, String[]> parameterMap = formData.getParameterMap();
        log.info("parameterMap：{}",parameterMap);
        
        //请求头还是通过 request 获取
        String auth = request.getHeader("Authorization");

        return fileStorageService.of(wrapper).upload();
    }

    /**
     * 注意这里是错误的用法，在方法上定义参数来接收请求中的参数，这样会导致输入流被提前读取
     */
    @PostMapping("/upload-request3")
    public FileInfo uploadRequest3(HttpServletRequest request,String aaa) {
        //包括但不限于下面这几种通过 request 获取参数的方式也是不行的，同样会导致输入流被提前读取
        String bbb = request.getParameter("bbb");
        Map<String, String[]> parameterMap = request.getParameterMap();
        
        //总之就是任何会导致输入流被提前读取的行为都是不可以的
        return fileStorageService.of(request).upload();
    }
}
```

#### 分片上传

一般情况下，使用上面的上传方式就已经足够使用了，大文件会在内部自动进行分片上传。
但是还存在着不足，例如无法多线程并行上传、无法断点续传等，现在可以参考以下方式使用手动分片上传来实现这些功能。

##### 手动分片上传-是否支持

```java
//当前默认的存储平台支付支持手动分片上传
MultipartUploadSupportInfo supportInfo = fileStorageService.isSupportMultipartUpload();
supportInfo.getIsSupport();//是否支持手动分片上传，正常情况下判断此参数就行了
supportInfo.getIsSupportListParts();//是否支持列举已上传的分片
supportInfo.getIsSupportAbort();//是否支持取消上传点击复制错误复制成功
```

##### 手动分片上传-初始化

又拍云 USS 比较特殊，需要传入分片大小，虽然已有默认值（1M），但为了方便使用还是单独设置一下（5MB）

```java
//是否为又拍云 USS
boolean isUpyunUss = fileStorageService.getFileStorage() instanceof UpyunUssFileStorage;
//手动分片上传-初始化
FileInfo fileInfo = fileStorageService.initiateMultipartUpload()
        .setPath("test/")   // 保存到相对路径下，为了方便管理，不需要可以不写
        .putMetadata(isUpyunUss, "X-Upyun-Multi-Part-Size", String.valueOf(5 * 1024 * 1024))// 设置 Metadata，不需要可以不写
        .init();点击复制错误复制成功
```

##### 手动分片上传-上传分片

这里支持多个线程同时上传，充分利用带宽

```java
int partNumber = 1;//分片号。每一个上传的分片都有一个分片号，一般情况下取值范围是1~10000
byte[] bytes = FileUtil.readBytes("C:\\001.part");//分片数据，和基本的上传方式一样，也支持各种数据源
FilePartInfo filePartInfo = fileStorageService.uploadPart(fileInfo, partNumber, bytes, (long) bytes.length).upload();点击复制错误复制成功
```

##### 手动分片上传-完成

```java
fileStorageService.completeMultipartUpload(fileInfo).complete();点击复制错误复制成功
```

##### 手动分片上传-列举已上传的分片

```java
FilePartInfoList partList = fileStorageService.listParts(fileInfo).listParts();点击复制错误复制成功
```

##### 手动分片上传-取消

```java
fileStorageService.abortMultipartUpload(fileInfo).abort();
```

#### 监听上传进度

```java
// 方式一
fileStorageService.of(file).setProgressListener(progressSize ->
    System.out.println("已上传：" + progressSize)
).upload();

// 方式二
fileStorageService.of(file).setProgressListener((progressSize,allSize) ->
    System.out.println("已上传 " + progressSize + " 总大小" + (allSize == null ? "未知" : allSize))
).upload();

// 方式三
fileStorageService.of(file).setProgressListener(new ProgressListener() {
    @Override
    public void start() {
    System.out.println("上传开始");
    }

    @Override
    public void progress(long progressSize,Long allSize) {
        System.out.println("已上传 " + progressSize + " 总大小" + (allSize == null ? "未知" : allSize));
    }

    @Override
    public void finish() {
        System.out.println("上传结束");
    }
}).upload();
```

### 保存上传记录

### 下载

#### 下载方式

```java
// 获取文件信息
FileInfo fileInfo = fileStorageService.getFileInfoByUrl("https://file.abc.com/test/a.jpg");

// 下载为字节数组
byte[] bytes = fileStorageService.download(fileInfo).bytes();

// 下载到文件
fileStorageService.download(fileInfo).file("C:\\a.jpg");

// 下载到 OutputStream 中
ByteArrayOutputStream out = new ByteArrayOutputStream();
fileStorageService.download(fileInfo).outputStream(out);

// 获取 InputStream 手动处理
fileStorageService.download(fileInfo).inputStream(in -> {
    //TODO 读取 InputStream
});

// 直接通过文件信息中的 url 下载，省去手动查询文件信息记录的过程
fileStorageService.download("https://file.abc.com/test/a.jpg").file("C:\\a.jpg");

// 下载缩略图
fileStorageService.downloadTh(fileInfo).file("C:\\th.jpg");
```

#### 监听下载进度

```java
// 方式一
fileStorageService.download(fileInfo).setProgressListener(progressSize ->
        System.out.println("已下载：" + progressSize)
).file("C:\\a.jpg");
        
// 方式二
fileStorageService.download(fileInfo).setProgressListener((progressSize,allSize) ->
        System.out.println("已下载 " + progressSize + " 总大小" + allSize)
).file("C:\\a.jpg");

// 方式三
fileStorageService.download(fileInfo).setProgressListener(new ProgressListener() {
    @Override
    public void start() {
        System.out.println("下载开始");
    }

    @Override
    public void progress(long progressSize,Long allSize) {
        System.out.println("已下载 " + progressSize + " 总大小" + allSize);
    }

    @Override
    public void finish() {
        System.out.println("下载结束");
    }
}).file("C:\\a.jpg");
```

### 删除

```java
//获取文件信息
FileInfo fileInfo = fileStorageService.getFileInfoByUrl("https://file.abc.com/test/a.jpg");

//直接删除
fileStorageService.delete(fileInfo);

//条件删除
fileStorageService.delete(fileInfo,info -> {
    //TODO 检查是否满足删除条件
    return true;
});

//直接通过文件信息中的 url 删除，省去手动查询文件信息记录的过程
fileStorageService.delete("https://file.abc.com/test/a.jpg");
```

### 判断文件是否存在

```java
//获取文件信息
FileInfo fileInfo = fileStorageService.getFileInfoByUrl("https://file.abc.com/test/a.jpg");

//判断文件是否存在
boolean exists = fileStorageService.exists(fileInfo);

//直接通过文件信息中的 url 判断文件是否存在，省去手动查询文件信息记录的过程
boolean exists2 = fileStorageService.exists("https://file.abc.com/test/a.jpg");
```

### 复制

复制分为 `同存储平台复制` 和 `跨存储平台复制`，默认会自动选择

`同存储平台复制` 直接调用每个存储平台提供的复制方法，速度快，不额外占用网络及本地硬盘空间

`跨存储平台复制` 是通过先下载再上传的方式实现的，正常情况下上传下载是同时进行的，不会过多占用内存，不占用硬盘空间，但是会占用网络带宽，速度受网络影响

`FTP` 、 `SFTP` 和 `FastDFS` 不支持 `同存储平台复制` ，默认会自动使用 `跨存储平台复制`

```java
// 上传源文件
FileInfo fileInfo = fileStorageService.of(new File("D:\\Desktop\\a.png")).thumbnail().upload();

// 复制到 copy 这个路径下（同存储平台复制）
FileInfo destFileInfo = fileStorageService.copy(fileInfo)
        .setPath("copy/")
        .copy();

//复制到同路径下不同文件名（同存储平台复制）
FileInfo destFileInfo = fileStorageService.copy(fileInfo)
        .setFilename("aaaCopy." + FileNameUtil.extName(fileInfo.getFilename()))
        .setThFilename("aaaCopy.min." + FileNameUtil.extName(fileInfo.getThFilename()))
        .copy();

//复制到其它存储平台（跨存储平台复制）
FileInfo destFileInfo = fileStorageService.copy(fileInfo)
        .setPlatform("local-plus-1")
        .setProgressListener((progressSize, allSize) ->
            log.info("文件复制进度：{} {}%", progressSize, progressSize * 100 / allSize))
        .copy();

//强制使用跨存储平台复制
FileInfo destFileInfo = fileStorageService.copy(fileInfo)
        .setCopyMode(Constant.CopyMode.CROSS)
        .setPath("copy/")
        .copy();

//是否支持同存储平台复制
boolean supportSameCopy = fileStorageService.isSupportSameCopy("aliyun-oss-1");
```

### 移动（重命名）

移动分为 `同存储平台移动` 和 `跨存储平台移动`，默认会自动选择

`同存储平台移动` 直接调用每个存储平台提供的移动方法，速度快，不额外占用网络及本地硬盘空间

`跨存储平台移动` 是通过先复制再删除源文件的方式实现的，`跨存储平台复制` 时速度受网络影响，详情见 [复制](https://x-file-storage.xuyanwu.cn/#/基础功能?id=复制) 章节

仅 `本地` 、 `FTP` 、`SFTP` 、`WebDAV` 、`七牛云 Kodo` 、`又拍云 USS` 支持 `同存储平台移动` ，其它不支持的存储平台默认会自动使用 `跨存储平台移动`

```java
// 上传源文件
FileInfo fileInfo = fileStorageService.of(new File("D:\\Desktop\\a.png")).thumbnail().upload();

// 移动到 move 这个路径下（同存储平台移动）
FileInfo destFileInfo = fileStorageService.move(fileInfo)
        .setPath("move/")
        .move();

//移动到同路径下不同文件名（同存储平台移动）
FileInfo destFileInfo = fileStorageService.move(fileInfo)
        .setFilename("aaaMove." + FileNameUtil.extName(fileInfo.getFilename()))
        .setThFilename("aaaMove.min." + FileNameUtil.extName(fileInfo.getThFilename()))
        .move();

//移动到其它存储平台（跨存储平台移动）
FileInfo destFileInfo = fileStorageService.move(fileInfo)
        .setPlatform("local-plus-1")
        .setProgressListener((progressSize, allSize) ->
            log.info("文件移动进度：{} {}%", progressSize, progressSize * 100 / allSize))
        .move();

//强制使用跨存储平台移动
FileInfo destFileInfo = fileStorageService.move(fileInfo)
        .setMoveMode(Constant.MoveMode.CROSS)
        .setPath("move/")
        .move();

//是否支持同存储平台移动
boolean supportSameMove = fileStorageService.isSupportSameMove("aliyun-oss-1");
```

> 上面代码可访问博主的仓库：[springboot\_chowder/springboot\_x\_file\_storage at main · Daneliya/springboot\_chowder (github.com)](https://github.com/Daneliya/springboot_chowder/tree/main/springboot_x_file_storage)

## 四、其它功能

X FIle Storage还有访问策略、签名生成、Metadata 和 UserMetadata上传、存储平台动态配置及自定义Client、文件适配器、MIME类型识别、哈希计算、切面增强等功能，详细可参考官方文档。

---

---
url: /StableDiffusion/WebUI/1_文生图.md
---

# 文生图

---

---
url: /10.配置/01.主题配置/15.文章列表配置.md
---

# 文章列表配置

## post

文章列表配置。

::: code-group

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  post: {
    postStyle: "list", // 文章列表风格
    excerptPosition: "top", // 文章摘要位置
    showMore: true, // 是否显示更多按钮
    moreLabel: "阅读全文 >", // 更多按钮文字
    coverImgMode: "default", // 文章封面图模式
    showCapture: false, // 是否在摘要位置显示文章部分文字，当为 true 且不使用 frontmatter.describe 和 <!-- more --> 时，会自动截取前 300 个字符作为摘要
  },
});
```

```yaml [index.md]
---
tk:
  post:
    postStyle: list
    excerptPosition: top
    showMore: true
    moreLabel: "阅读全文 >"
    coverImgMode: default
    showCapture: false
---
```

```ts [更多配置项]
import type { TitleTagProps } from "vitepress-theme-teek";

interface Post {
  /**
   * 文章模板风格，list 为列表风格，card 为卡片风格
   *
   * @since v1.1.5
   * @default list
   */
  postStyle?: "list" | "card";
  /**
   * 文章摘要位置
   *
   * @default bottom
   */
  excerptPosition?: "top" | "bottom";
  /**
   * 是否显示更多按钮
   *
   * @default true
   */
  showMore?: boolean;
  /**
   * 更多按钮文字
   *
   * @default '阅读全文 >'
   */
  moreLabel?: string;
  /**
   * 文章列表为空时的标签
   *
   * @default '暂无文章'
   */
  emptyLabel?: string;
  /**
   * 文章封面图模式
   *
   * @default 'default'
   */
  coverImgMode?: "default" | "full";
  /**
   * 是否在摘要位置显示文章部分文字，当为 true 且不使用 frontmatter.describe 和 `<!-- more -->` 时，会自动截取前 300 个字符作为摘要
   *
   * @default false
   */
  showCapture?: boolean;
  /**
   * 文章信息（作者、创建时间、分类、标签等信息）是否添加 | 分隔符
   *
   * @default false
   */
  splitSeparator?: boolean;
  /**
   * 是否开启过渡动画
   *
   * @default true
   */
  transition?: boolean;
  /**
   * 自定义过渡动画名称
   *
   * @default 'tk-slide-fade'
   */
  transitionName?: string;
  /**
   * 列表模式下的标题标签位置（postStyle 为 list）
   *
   * @since v1.1.5
   * @default 'right'
   */
  listStyleTitleTagPosition?: TitleTagProps["position"];
  /**
   * 卡片模式下的标题标签位置（postStyle 为 list）
   *
   * @since v1.1.5
   * @default 'left'
   */
  cardStyleTitleTagPosition?: TitleTagProps["position"];
  /**
   * 默认封面图地址，如果不设置封面图则使用默认封面图地址
   *
   * @since v1.2.1
   * @default []
   */
  defaultCoverImg?: string[];
}
```

:::

&#x20;您可以通过 `postStyle` 配置项来设置文章列表的风格：

* 当 `postStyle` 为 `list` 时，文章列表为列表风格
* 当 `postStyle` 为 `card` 时，文章列表为卡片风格，且 `excerptPosition`、`showMore`、`moreLabel`、`coverImgMode` 配置项失效

## page

首页 Post 文章列表的分页配置，完全是 [ElPagination](https://element-plus.org/zh-CN/component/pagination.html#api) 的 props。

::: code-group

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  page: {
    pageSize: 20,
  },
});
```

```yaml [index.md]
---
tk:
  page:
    pageSize: 20
---
```

```ts [更多配置项]
import type { IconProps } from "vitepress-theme-teek";

interface TeekConfig {
  /**
   * 首页 Post 文章列表的分页配置
   */
  page?: {
    /**
     * 总条目数
     */
    total?: number;
    /**
     * 总页数，与 total 二选一
     */
    pageCount?: number;
    /**
     * 设置最大页码按钮数。 页码按钮的数量，当总页数超过该值时会折叠
     *
     * @default 7
     */
    pagerCount?: number;
    /**
     * 组件布局，子组件名用逗号分隔
     *
     * @default 'prev, pager, next, jumper, ->, total'
     */
    layout?: string;
    /**
     * 替代图标显示的上一页文字
     */
    prevText?: string;
    /**
     * 上一页的图标， 比 prev-text 优先级更高
     */
    prevIcon?: IconProps["icon"];
    /**
     * 替代图标显示的下一页文字
     */
    nextText?: string;
    /**
     * 下一页的图标， 比 next-text 优先级更高
     */
    nextIcon?: IconProps["icon"];
    /**
     * 分页大小
     *
     * @default 'default'
     */
    size?: Size;
    /**
     * 是否为分页按钮添加背景色
     *
     * @default false
     */
    background?: boolean;
    /**
     * 是否禁用
     *
     * @default false
     */
    disabled?: boolean;
    /**
     * 只有一页时是否隐藏
     *
     * @default false
     */
    hideOnSinglePage?: boolean;
  };
}
```

:::

---

---
url: /10.配置/01.主题配置/30.文章配置.md
---

# 文章配置

## articleAnalyze

文章信息分析配置，分别作用在首页和文章页。

::: tip
如果在 `config.mts` 中配置，则首页和文章页都生效。

文章页的图片点击可以预览，但是当图片元素的 class 里存在 `no-preview`，则不会触发预览，这对于兼容 Teek 的图片相关插件有所帮助。
:::

::: code-group

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  articleAnalyze: {
    showIcon: true, // 作者、日期、分类、标签、字数、阅读时长、浏览量等文章信息的图标是否显示
    dateFormat: "yyyy-MM-dd hh:mm:ss", // 文章日期格式，首页和文章页解析日期时使用
    showInfo: true, // 是否展示作者、日期、分类、标签、字数、阅读时长、浏览量等文章信息，分别作用于首页和文章页
    showAuthor: true, // 是否展示作者
    showCreateDate: true, // 是否展示创建日期
    showUpdateDate: false, // 是否展示更新日期，仅在文章页显示
    showCategory: false, // 是否展示分类
    showTag: false, // 是否展示标签
  },
});
```

```yaml [首页 index.md]
---
tk:
  articleAnalyze:
    showIcon: true
    dateFormat: yyyy-MM-dd hh:mm:ss
    showInfo: true
    showAuthor: true
    showCreateDate: true
    showUpdateDate: false
    showCategory: false
    showTag: false
---
```

```yaml [文章页 xxx.md]
---
articleAnalyze:
  showIcon: true
  dateFormat: yyyy-MM-dd hh:mm:ss
  showInfo: true
  showAuthor: true
  showCreateDate: true
  showUpdateDate: false
  showCategory: false
  showTag: false
---
```

```ts [更多配置项]
interface Article {
  /**
   * 作者、日期、分类、标签、字数、阅读时长、浏览量等文章信息的图标是否显示
   *
   * @default true
   */
  showIcon?: boolean;
  /**
   * 文章日期格式，首页和文章页解析日期时使用
   *
   * @default 'yyyy-MM-dd'
   */
  dateFormat?: "yyyy-MM-dd" | "yyyy-MM-dd hh:mm:ss" | ((date: string) => string);
  /**
   * 是否展示作者、日期、分类、标签、字数、阅读时长、浏览量等文章信息，分别作用于首页和文章页
   * 如果 showInfo 为数组，则控制在哪里显示，如 ["post"] 只在首页的 Post 列表显示基本信息；如果为 boolean 值，则控制基本信息是否展示，如 false 则在首页和文章页都不显示基本信息
   *
   * @default true
   */
  showInfo?: boolean | ArticleInfoPosition[];
  /**
   * 是否展示作者
   *
   * @default true
   */
  showAuthor?: boolean | ArticleInfoPosition[];
  /**
   * 是否展示创建日期
   *
   * @default true
   */
  showCreateDate?: boolean | ArticleInfoPosition[];
  /**
   * 是否展示更新日期，仅在文章页显示
   *
   * @default false
   */
  showUpdateDate?: boolean;
  /**
   * 是否展示分类
   *
   * @default false
   */
  showCategory?: boolean | ArticleInfoPosition[];
  /**
   * 是否展示标签
   *
   * @default false
   */
  showTag?: boolean | ArticleInfoPosition[];
  /**
   * 指定文章信息的传送位置，仅限在文章页生效，默认在文章页顶部
   */
  teleport?: {
    /**
     * 指定需要传送的元素选择器
     */
    selector?: string;
    /**
     * 指定传送到元素的位置，before 在元素前，after 在元素后
     *
     * @default 'after'
     */
    position?: "before" | "after";
    /**
     * 指定一个 class 名，如果传送的位置和其他元素太接近，可以利用 class 来修改 margin
     *
     * @default teleport
     */
    className?: string;
  };
  /**
   * 文章页图片查看器配置
   */
  imageViewer?: Partial<ImageViewerProps>;
}
```

:::

配置项中，`teleport` 可以将文章信息传送到指定位置，仅限在文章页生效，默认在文章页顶部。

如将文章信息传到一级标题下面：

::: code-group

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  articleAnalyze: {
    teleport: {
      selector: "h1",
      position: "after",
      className: "h1-bottom-info",
    },
  },
});
```

```yaml [文章页 xxx.md]
---
tk:
  articleAnalyze:
    teleport:
      selector: h1
      position: after
      className: h1-bottom-info
---
```

:::

## breadcrumb

面包屑配置。

::: code-group

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  breadcrumb: {
    enabled: true, // 是否启用面包屑
    showCurrentName: false, // 面包屑最后一列是否显示当前文章的文件名
    separator: "/", // 面包屑分隔符
  },
});
```

```yaml [文章页 xxx.md]
---
tk:
  breadcrumb:
    enabled: true
    showCurrentName: false
    separator: /
---
```

```ts [更多配置项]
interface Breadcrumb {
  /**
   * 是否启用面包屑
   *
   * @default true
   */
  enabled?: boolean;
  /**
   * 面包屑最后一列是否显示当前文章的文件名
   *
   * @default false
   */
  showCurrentName?: boolean;
  /**
   * 面包屑分隔符
   *
   * @default '/'
   */
  separator?: string;
  /**
   * 鼠标悬停首页图标的提示文案
   *
   * @default '首页'
   */
  homeLabel?: string;
}
```

:::

## pageStyle

* 类型：`"default" | "card" | "segment" | "card-nav" | "segment-nav"`
* 默认值：`default`

文章页的样式风格，`default` 为 VitePress 原生风格，`card` 为单卡片风格，`segment` 为片段卡片风格，`card-nav` 和 `segment-nav` 会额外修改导航栏样式。

::: tip
在文章页的 `frontmatter` 配置 `pageStyle`，可以针对不同的文章页开启不同的样式风格。
:::

如果使用了主题增强的布局尺寸切换，且布局尺寸不是 VitePress 默认尺寸，则该配置项失效

::: code-group

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  pageStyle: "segment-nav",
});
```

```yaml [文章页 xx.md]
---
pageStyle: segment-nav
---
```

:::

## appreciation

赞赏功能配置。

赞赏功能提供 3 个位置选择：

* `doc-after`：文章页底部，评论区上方
* `doc-after-popper`：文章页底部，评论区上方，以弹框形式出现&#x20;
* `aside-bottom`：文章页大纲栏下方

每个位置分别有不同的配置项。

::: code-group

```ts [文章页底部]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  appreciation: {
    position: "doc-after",
    options: {
      icon: "weChatPay", // 赞赏图标，内置 weChatPay 和 alipay
      expandTitle: "打赏支持", // 展开标题，支持 HTML
      collapseTitle: "下次一定", // 折叠标题，支持 HTML
      content: `<img src='/teek-logo-large.png'>`, // 赞赏内容，支持 HTML
      expand: false, // 是否默认展开，默认 false
    },
  },
});
```

```ts [文章页底部 Popper]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  appreciation: {
    position: "doc-after-popper",
    options: {
      trigger: "click", // 触发方式
      icon: "weChatPay", // 赞赏图标，内置 weChatPay 和 alipay
      title: "打赏支持", // 展开标题，支持 HTML
      content: `<img src='/teek-logo-large.png'> <img src='/teek-logo-large.png'>`, // 赞赏内容，支持 HTML
    },
  },
});
```

```ts [文章页大纲栏下方]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  appreciation: {
    position: "aside-bottom",
    options: {
      title: `<span style="color: var(--tk-theme-color)">欢迎打赏支持</span>`, // 赞赏标题，支持 HTML
      content: `<img src='/teek-logo-large.png'>`, // 赞赏内容，支持 HTML
    },
  },
});
```

```ts [更多配置项]
import type { IconProps } from "vitepress-theme-teek";

type Appreciation<T extends keyof AppreciationPosition = ""> = {
  /**
   * 赞赏位置
   */
  position?: T;
  /**
   * 赞赏配置
   */
  options?: AppreciationPosition[T];
};

type AppreciationPosition = {
  "": object;
  "aside-bottom": {
    /**
     * 赞赏标题，支持 HTML
     */
    title?: string;
    /**
     * 赞赏内容，支持 HTML
     */
    content?: string;
  };
  "doc-after": {
    /**
     * 自定义按钮 HTML
     */
    buttonHtml?: string;
    /**
     * 赞赏图标，内置 weChatPay 和 alipay
     */
    icon?: IconProps["icon"] | "weChatPay" | "alipay";
    /**
     * 展开标题，支持 HTML
     */
    expandTitle?: string;
    /**
     * 折叠标题，支持 HTML
     */
    collapseTitle?: string;
    /**
     * 赞赏内容，支持 HTML
     */
    content?: string;
    /**
     * 是否默认展开
     *
     * @default false
     */
    expand?: boolean;
  };
  "doc-after-popper": {
    /**
     * 触发方式
     *
     * @default "click"
     */
    trigger?: "click" | "hover";
    /**
     * 自定义按钮 HTML
     */
    buttonHtml?: string;
    /**
     * 赞赏图标，内置 weChatPay 和 alipay
     */
    icon?: IconProps["icon"] | "weChatPay" | "alipay";
    /**
     * 赞赏标题，支持 HTML
     */
    title?: string;
    /**
     * 赞赏内容，支持 HTML
     */
    content?: string;
  };
};
```

:::

Teek 内置两个 icon：

* `weChatPay`：微信支付图标
* `alipay`：支付宝图标

如果您需要自定义图标，则通过 `icon` 配置项传入。

赞赏功能同样支持在单个 Markdown 的 `frontmatter` 配置来覆盖全局配置。

```yaml
---
appreciation:
  position: doc-after
  options:
    icon: weChatPay
    expandTitle: 打赏支持
    collapseTitle: 下次一定
    content: "<img src='/teek-logo-large.png'>"
    expand: false
---
```

## articleShare

文章分享配置。

本功能主要是在文章右侧的大纲栏添加一个按钮，点击后自动复制文章链接到剪贴板。

::: code-group

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  articleShare: {
    enabled: true, // 是否开启文章链接分享功能
    text: "分享此页面", // 分享按钮文本
    copiedText: "链接已复制", // 复制成功文本
    query: false, // 是否包含查询参数
    hash: false, // 是否包含哈希值
  },
});
```

```yaml [文章页 xx.md]
---
articleShare:
  enabled: true
  text: 分享此页面
  copiedText: 链接已复制
  query: false
  hash: false
---
```

```ts [更多配置项]
import type { IconProps } from "vitepress-theme-teek";

interface ArticleShare {
  /**
   * 是否开启文章链接分享功能
   *
   * @default false
   */
  enabled?: boolean;
  /**
   * 分析按钮图标
   */
  icon?: IconProps["icon"];
  /**
   * 分享按钮文本
   *
   * @default '分享此页面'
   */
  text?: string;
  /**
   * 复制成功图标
   */
  copiedIcon?: IconProps["icon"];
  /**
   * 复制成功文本
   *
   * @default '链接已复制'
   */
  copiedText?: string;
  /**
   * 是否包含查询参数
   *
   * @default false
   */
  query?: boolean;
  /**
   * 是否包含哈希值
   *
   * @default false
   */
  hash?: boolean;
}
```

:::

## articleTopTip

在每个文章页顶部显示 VitePress 容器添加提示，使用场景如超过半年的文章自动提示文章内容可能已过时。

::: code-group

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  articleTopTip: (frontmatter, localeIndex, page) => {
    const tip: Record<string, string> = {
      type: "warning",
      text: "文章发布较早，内容可能过时，阅读注意甄别。",
    };

    // 大于半年，添加提示
    const longTime = 6 * 30 * 24 * 60 * 60 * 1000;
    if (frontmatter.date && Date.now() - new Date(frontmatter.date).getTime() > longTime) return tip;
  },
});
```

```ts [类型]
import type { PageData } from "vitepress";
import type { VpContainerProps } from "@teek/components/common/VpContainer/src/vpContainer";

interface TeekConfig {
  /**
   * 文章页顶部使用 VitePress 容器添加提示
   *
   * @param frontmatter 文档 frontmatter
   * @param localeIndex 当前国际化语言
   * @param page 文章信息，即 useData().page 的信息
   */
  articleTopTip?: (
    frontmatter: PageData["frontmatter"],
    localeIndex: string,
    page: PageData
  ) => VpContainerProps | undefined;
}
```

:::

如果全局开启了该功能，但希望在某个文章页隐藏该功能，有两种方式实现：

* `frontmatter.articleTopTip` 设置为 `false`
* 在 `config.ts` 中配置 `articleTopTip` 时，第一个参数为 `frontmatter`，因此可以在函数自定义判断，如 `if(frontmatter.topTip === false) return`，那么就可以在 Markdown 的 `frontmatter.topTip` 设置为 `false`

## articleBottomTip&#x20;

在每个文章页顶部显示 VitePress 容器添加提示，使用场景如添加文章版权声明。

::: code-group

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  articleBottomTip: () => {
    return {
      type: "tip",
      // title: "声明", // 可选
      text: `<p>作者：Teek</p>
             <p>版权：此文章版权归 Teek 所有，如有转载，请注明出处!</p>
             <p style="margin-bottom: 0">链接：可点击右上角分享此页面复制文章链接</p>
            `,
    };
  },
});
```

```ts [类型]
import type { PageData } from "vitepress";
import type { VpContainerProps } from "@teek/components/common/VpContainer/src/vpContainer";

interface TeekConfig {
  /**
   * 文章页底部使用 VitePress 容器添加提示
   *
   * @param frontmatter 文档 frontmatter
   * @param localeIndex 当前国际化语言
   * @param page 文章信息，即 useData().page 的信息
   */
  articleBottomTip?: (
    frontmatter: PageData["frontmatter"],
    localeIndex: string,
    page: PageData
  ) => VpContainerProps | undefined;
}
```

:::

如果全局开启了该功能，但希望在某个文章页隐藏该功能，有两种方式实现：

* `frontmatter.articleBottomTip` 设置为 `false`
* 在 `config.ts` 中配置 `articleBottomTip` 时，第一个参数为 `frontmatter`，因此可以在函数自定义判断，如 `if(frontmatter.bottomTip === false) return`，那么就可以在 Markdown 的 `frontmatter.bottomTip` 设置为 `false`

## articleUpdate&#x20;

文章页底部的最近更新栏配置。

::: code-group

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  articleUpdate: {
    enabled: true, // 是否启用文章最近更新栏
    limit: 3, // 文章最近更新栏显示数量
  },
});
```

```yaml [文章页1 xx.md]
---
articleShare: false # 禁用文章分享栏
---
```

```yaml [文章页2 xx.md]
---
articleShare:
  enabled: true
  limit: 3
---
```

```ts [更多配置项]
interface ArticleUpdate {
  /**
   * 是否启用文章最近更新栏
   *
   * @since v1.2.1
   * @default true
   */
  enabled?: boolean;
  /**
   * 文章最近更新栏显示数量
   *
   * @since v1.2.1
   * @default 3
   */
  limit?: number;
}
```

:::

---

---
url: /@pages/articleOverviewPage.md
---


---

---
url: /Java/架构设计/高可用/限流/系统优化之限流.md
---

# 系统优化之限流

我们需要控制用户使用系统的次数，以避免超支，比如给不同等级的用户分配不同的调用次数，防止用户过度使用系统造成破产(例如给普通用户提供 50 次调用次数，会员用户提供 100 次)。但限制用户调用次数仍存在一定风险，用户仍有可能通过疯狂调用来刷量，从而导致系统成本过度消耗。
假设系统就一台服务器，能同时处理的用户对话数量是有限的，比如系统最多只能支持 10 个用户同时对话，如果某个用户一秒内使用 10 个账号登录，那么其他用户就无法使用系统。就像去自助餐厅吃饭，如果有人一股脑地把所有美食都拿光了，其他人就无法享用了。比如双 11 这种大促期间，阿里巴巴就要去限制，不能说所有的用户想抢购都能成功，在前端随机放行一部分用户，而对于其他用户则进行限制，以确保系统不会被恶意用户占满。现在要做一个解决方案，就是限流，比如说限制单个用户在每秒只能使用一次，那这里我们怎么去思考这个限流的阈值是多少？多少合适呢？

\*\*问题：\*\*使用系统是需要消耗成本的，用户有可能疯狂刷量，让你破产。
**解决方案：**

1. 控制成本 => 限制用户调用总次数
2. 用户在短时间内疯狂使用，导致服务器资源被占满，其他用户无法使用 => 限流

**思考：** 限流阈值多大合适？参考正常用户的使用，比如限制单个用户在每秒只能使用 1 次。

## 限流的算法

建议 [阅读文章](https://juejin.cn/post/6967742960540581918)。

![](https://cdn.nlark.com/yuque/0/2023/png/25430380/1685601301044-b47fba22-69b5-45d0-89fd-7bff7096b665.png?x-oss-process=image%2Fresize%2Cw_668%2Climit_0#averageHue=%23e8f7e2\&from=url\&id=rDCvT\&originHeight=703\&originWidth=668\&originalType=binary\&ratio=1.25\&rotation=0\&showTitle=false\&status=done\&style=none\&title=)

**食堂排队举例：**

1. 规定窗口限流 你去食堂买汉堡，食堂每一小时只允许 10 个用户买汉堡，汉堡一小时只能做 10 个，第 59 分钟来了 10 个人，第 60 分钟又来 10 个人，汉堡就不够了。
2. 滑动窗口限流 每 10 分钟食堂做一个汉堡，这样的话，假如前一个小时汉堡已经被抢光了，然后 1 小时 10 分钟来的一个新用户，他又能抢到 1 小时 10 分钟得到的那个汉堡。
3. 漏桶限流 大家排好队，一个一个去拿汉堡，前面一个人拿完，后面一个人才能拿。
4. 令牌桶限流 食堂事先做好 10 个汉堡，假如现在开抢了，前 10 个人能够同时拿到汉堡，不用排队，但剩下的 10 个人就只能等下一批汉堡做好才能拿。

## 限流粒度

1. 针对某个方法限流，即单位时间内最多允许同时 XX 个操作使用这个方法
2. 针对某个用户限流，比如单个用户单位时间内最多执行 XX 次操作
3. 针对某个用户 x 方法限流，比如单个用户单位时间内最多执行 XX 次这个方法

## 限流实现构思

### 1）本地限流(单机限流)

每个服务器单独限流，一般适用于单体项目，就是你的\*\*项目只有一个服务器 \*\*。
🪔 举个例子，假设你的系统有三台服务器，每台服务器限制用户每秒只能请求一次。你可以为每台服务器单独设置限流策略，这样每个服务器都能够独立地控制用户的请求频率。但是这种限流方式并不是很可靠，因为你并不知道用户的请求会落在哪台服务器上，它的分布是有一定的偶然性的。即使你采用负载均衡技术，让用户请求轮流发送到每台服务器，仍然存在一定的风险。
在 Java 中，有很多第三方库可以用来实现单机限流：Guava RateLimiter：这是谷歌 Guava 库提供的限流工具，可以对单位时间内的请求数量进行限制。

### 2）分布式限流(多机限流)

如果项目有多个服务器，比如微服务，那么建议使用分布式限流。

1. 把用户的使用频率等数据放到一个集中的存储进行统计； 比如 Redis，这样无论用户的请求落到了哪台服务器，都以集中存储中的数据为准。 (Redisson -- 是一个操作 Redis 的工具库 , 伙伴匹配系统讲过)
2. 在网关集中进行限流和统计（比如 Sentinel、Spring Cloud Gateway）

## Redisson 限流实现

Redisson 内置了一个限流工具类，可以帮助你利用 Redis 来存储、来统计。
根据官方文档提示，先引入依赖。

![](https://cdn.nlark.com/yuque/0/2023/png/25430380/1685438575065-6486eda8-fed0-4f63-96cd-421b03d8a63a.png?x-oss-process=image%2Fresize%2Cw_448%2Climit_0#averageHue=%23ebce9b\&from=url\&id=tWoN1\&originHeight=528\&originWidth=448\&originalType=binary\&ratio=1.25\&rotation=0\&showTitle=false\&status=done\&style=none\&title=)
粘贴至pom.xml。

![](https://cdn.nlark.com/yuque/0/2023/png/25430380/1685601583818-c9e31d13-322e-4edb-ab77-fc5994219192.png?x-oss-process=image%2Fresize%2Cw_663%2Climit_0#averageHue=%23f8f5f4\&from=url\&id=p6uLL\&originHeight=456\&originWidth=663\&originalType=binary\&ratio=1.25\&rotation=0\&showTitle=false\&status=done\&style=none\&title=)

创建 RedissonConfig 配置类，用于初始化 RedissonClient 对象单例； 在config目录下新建RedissonConfig.java。

![](https://cdn.nlark.com/yuque/0/2023/png/25430380/1685442029087-d5db1c45-4d0e-4cdc-b28c-4f7264863278.png?x-oss-process=image%2Fresize%2Cw_655%2Climit_0#averageHue=%23f9f7f5\&from=url\&id=XNoGk\&originHeight=393\&originWidth=655\&originalType=binary\&ratio=1.25\&rotation=0\&showTitle=false\&status=done\&style=none\&title=)
编写配置。
去application.yml取消 redis 配置注释。

![](https://cdn.nlark.com/yuque/0/2023/png/25430380/1685601731550-9ab95268-afcd-48cd-a7cc-358ee0ca1b8a.png?x-oss-process=image%2Fresize%2Cw_642%2Climit_0#averageHue=%23f7f5f4\&from=url\&id=RjUzR\&originHeight=404\&originWidth=642\&originalType=binary\&ratio=1.25\&rotation=0\&showTitle=false\&status=done\&style=none\&title=)

---

---
url: /Java/系统优化/系统优化/2_线程池隔离.md
---

# 线程池隔离

上一期使用 RxJava 实现 AI 题目生成的时候，用到了 `Schedulers.io()` 方法，创建了一个 I/O 密集型线程池来处理智谱 AI 返回的流。

```javascript
modelDataFlowable
    .observeOn(Schedulers.io())
    .map(modelData -> modelData.getChoices().get(0).getDelta().getContent())
    .map(message -> message.replaceAll("\\s", ""))
    .filter(StrUtil::isNotBlank)
    .flatMap(message -> {
        ...
    })
    .doOnNext(c -> {
        ...
    })
    .doOnError((e) -> log.error("sse error", e))
    .doOnComplete(sseEmitter::complete)
    .subscribe();
```

业务量级不大的时候，这么写当然没有问题；但随着业务量级的增长，这里可能会有安全隐患！

不知道大家了不了解 Java8 Stream 的 ForkJoinPool ，Java8 并发流的线程池是 **全局共享** 的，也就是任何的业务操作如果用到并发流，默认都是一个线程池，这会产生什么问题呢？

### 需求分析

如果所有业务操作都使用一个线程池，最大的问题就是 **相互影响**。

比如组内的一个同学上线了一个功能，用到了共享的线程池，但是他写的代码有 bug ，导致的线程池里的所有线程都被阻塞了。

你本来在那边喝着咖啡笑看他在那里手忙脚乱，突然发现报警群里面发出了新的告警通知，一看是你负责的业务。

原因是你的业务跟他共用了一个线程池，他的任务占着线程不放，你的任务当然也会被阻塞住，所以也告警了，你就傻眼了。

因此，对于一些业务共享的资源，使用的时候需要非常谨慎，需要考虑关联性、考虑最坏情况下会怎样，牵一发而动全身。有时候不是你写的代码没问题就稳了，小心“背刺”。

所以，在一些业务敏感的场景，需要隔离线程池，它有以下几点好处：

1. 故障隔离，缩小事故范围。
2. 资源隔离，防止业务之间抢占资源。同时支持更精细化地管理资源，比如不重要的场景给小一点的线程池，核心场景配置大线程池。
3. 性能优化，一些业务场景的任务是 CPU 密集型，一些是 I/O 密集型，不同任务类型需要配置不同的线程池。

本项目目前使用的 `Schedulers.io()` 就是全局共享的，先看下源码。通过静态代码块初始化了一个 IO 线程池：

![img](/assets/1716801574459-e13c5fa5-eadb-4eee-9095-d08a6f2d3c15.5yuAiu4X.png)

调用 io 方法时，会将该线程池实例作为默认的返回：

![img](/assets/1716801610130-2d89efe5-74ec-4122-a19b-cd188b93df04.BcExrPCz.png)

实际的内部实现是 RxJava 自定义的 CachedWorkerPool 线程池，它使用 ConcurrentLinkedQueue 无界队列，并且没有做线程约束，来一个任务就会新建一个线程！然后起了一个定时任务 evictor ，每间隔 60s 清理没有运行的线程。

![img](/assets/1716805556966-02b119b7-a456-4bdc-a971-0664bfb3efe6.brx2SScV.png)

这会导致什么问题？如果并发任务数特别多的情况下，会导致线程数不断增多，最终导致项目 OOM。

简单进行单测，JVM 设置成 `-Xms64m -Xmx64m`，执行以下代码：

```java
@Test
void test() {
    Scheduler io = Schedulers.io();
    while (true) {
        io.scheduleDirect(() -> {
            System.out.println(Thread.currentThread().getName() + " print hello");
            try {
                Thread.sleep(50000l);
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
        });
    }
}
```

可以看到，产生了 OOM：

![img](/assets/1716812245758-d7cde5b9-331a-4311-b77c-89a375748e46.Co2WPC4T.png)

所以，这块可以做一定的优化。假设 AI 答题用户分为了两类，一类是普通用户，一类是 VIP 用户。可以给 VIP 用户设置独立的线程池来处理 AI 题目生成功能，普通用户使用 `Schedulers.single()` 单线程，防止普通用户太多占用资源。是不是很真实？

### 方案设计

1）给 VIP 用户定制一个专用的线程池。

2）普通用户使用 Schedulers.single()

3）改造 AI 生成题目接口，根据用户类型选择不同的线程池。

### 开发实现

VIP 专用线程池定义，注册为 SpringBean，且设置线程名，并于调试时测试观察。

**注意**：线程池必须是 newScheduledThreadPool 类型！跟原来方法的返回值类型保持一致。

![img](/assets/1716806618895-99c8af64-79b2-42e8-b8f4-a6c8b348e64b.Dzei1xZr.png)

因为流式处理需要周期性线程池来循环处理数据（可以理解为有记忆性）；如果用普通线程池来处理，那么替换线程后，每个线程都会从流的起始位置开始消费。这段话可以不理解，总之如果用普通线程池程序会有问题就对了。

代码如下：

```java
import io.reactivex.Scheduler;
import io.reactivex.schedulers.Schedulers;

@Configuration
@Data
public class VipSchedulerConfig {

    @Bean
    public Scheduler vipScheduler() {
        ThreadFactory threadFactory = new ThreadFactory() {
            private final AtomicInteger threadNumber = new AtomicInteger(1);

            @Override
            public Thread newThread(Runnable r) {
                Thread t = new Thread(r, "VIPThreadPool-" + threadNumber.getAndIncrement());
                t.setDaemon(false); // 设置为非守护线程
                return t;
            }
        };

        ExecutorService executorService = Executors.newScheduledThreadPool(10, threadFactory);
        return Schedulers.from(executorService);
    }
}
```

改造通过 SSE 方式让 AI 生成题目的接口，补充根据用户身份使用不同的线程池的逻辑。

代码如下：

```java
// 注入 VIP 线程池
@Resource
private Scheduler vipScheduler;

@GetMapping("/ai_generate/sse")
public SseEmitter aiGenerateQuestionSSE(
    AiGenerateQuestionRequest aiGenerateQuestionRequest,
    HttpServletRequest request) {
    ...
    // 默认全局线程池
    Scheduler scheduler = Schedulers.single();
    User loginUser = userService.getLoginUser(request);
    // 如果用户是 VIP，则使用定制线程池
    if ("vip".equals(loginUser.getUserRole())) {
        scheduler = vipScheduler;
    }
    // 订阅流
    modelDataFlowable
        .observeOn(scheduler)
        .subscribe();
    return sseEmitter;
}
```

### 验证测试

在每次拼接出一道完整题目的 JSON 后，输出当前线程名称便于测试。如果是普通用户，使用 `Thread.sleep(10000L)` 模拟线程池被堵住场景。

代码如下：

```java
if (c == '}') {
    counter.addAndGet(-1);
    if (counter.get() == 0) {
        // 输出当前线程名称
        System.out.println(Thread.currentThread().getName());
        // 模拟普通用户阻塞
        if (!isVip) {
            Thread.sleep(10000L);
        }
        // 可以拼接题目，并且通过 SSE 返回给前端
        sseEmitter.send(JSONUtil.toJsonStr(stringBuilder.toString()));
        // 重置，准备拼接下一道题
        stringBuilder.setLength(0);
    }
}
```

可以通过临时修改请求参数，比如增加一个 isVip 字段，临时模拟用户身份进行测试。

```java
public SseEmitter aiGenerateQuestionSSE(
    AiGenerateQuestionRequest aiGenerateQuestionRequest,
    boolean isVip) {
    ...
}
```

执行流程：

1. 模拟普通用户调用
2. 再次进行普通用户调用
3. 进行 VIP 用户调用

期望结果：VIP 用户调用返回的结果不受 sleep 影响，普通用户则阻塞，每 10s 输出一题。

根据流程编写单元测试代码：

```java
@Resource
private QuestionController questionController;

@Test
void aiGenerateQuestionSSEVIPTest() throws InterruptedException {
    AiGenerateQuestionRequest request = new AiGenerateQuestionRequest();
    request.setAppId(3L);
    request.setQuestionNumber(10);
    request.setOptionNumber(2);

    questionController.aiGenerateQuestionSSE(request, false);
    questionController.aiGenerateQuestionSSE(request, false);
    questionController.aiGenerateQuestionSSE(request, true);

    Thread.sleep(1000000L);
}
```

结果：从线程名字来看，VIP 用户 10 道题目执行输出结束后，普通用户才生成 3 道题：

![img](/assets/1716803694812-af93b82e-d2c0-4e12-8bdf-59320dbedc4c.DxCXEjly.png)

这就是线程池隔离的优势，普通用户完全不会影响到会员用户。这也是企业开发中相对高级的操作，没经历过事故的同学一般情况下想不到，希望能通过这个项目理解吧~

---

---
url: /Java/架构设计/分布式/03.分布式消息队列/1_消息队列介绍.md
---

# 消息队列介绍

### 🌖 消息队列

消息队列是用于传输和保存消息的容器，也是大型分布式系统中常用的技术，主要解决应用耦合、异步消息、流量削锋等问题。后台开发必学，也是面试重点。

#### 知识

* 消息队列的作用
* RabbitMQ 消息队列
  * 生产消费模型
  * 交换机模型
  * 死信队列
  * 延迟队列
  * 消息持久化
  * Java 操作
  * 集群搭建
* 相关技术：ActiveMQ、RocketMQ、RabbitMQ、Kafka、TubeMQ

#### 学习建议

和缓存一样，学会如何使用消息队列并不难，无非就是调用 API 去生产、转发和消费消息。

因此，建议先能够独立使用它，了解消息队列的应用场景；再学习如何在 Java 中操作消息队列中间件，并尝试和项目相结合，感受消息队列带来的好处。

这里我建议初学者先学习 RabbitMQ，比 Kafka 要好理解一些。跟着视频教程实操一遍即可，可以等到面试前再去深入了解原理和高级特性。

#### 经典面试题

1. 使用消息队列有哪些优缺点？
2. 如何保证消息消费的幂等性？
3. 消息队列有哪些路由模型？
4. 你是否用过消息队列，解决过什么问题？

---

---
url: /Java/微服务专栏/06.消息总线Bus/README.md
---

# 消息总线Bus

https://cloud.tencent.com/developer/article/1669299

https://springdoc.cn/spring-cloud-bus

https://cloud.tencent.com/developer/article/2265531
https://cloud.tencent.com/developer/article/2265532

https://www.zhihu.com/question/394899246

---

---
url: /01.指南/20.相关/10.写作排版.md
---

# 写作排版

::: tip 序言

统一中文文案、排版的相关用法，降低团队成员之间的沟通成本，增强网站气质

:::

## 空格

「有研究显示，打字的时候不喜欢在中文和英文之间加空格的人，感情路都走得很辛苦，有七成的比例会在 34 岁的时候跟自己不爱的人结婚，而其余三成的人最后只能把遗产留给自己的猫。毕竟爱情跟书写都需要适时地留白。

与大家共勉之。

::: right
—— [vinta/paranoid-auto-spacing](https://github.com/vinta/pangu.js)
:::

### 中英文之间需要增加空格

正确：

> 在 LeanCloud 上，数据存储是围绕 `AVObject` 进行的。

错误：

> 在LeanCloud上，数据存储是围绕`AVObject`进行的。

> 在 LeanCloud上，数据存储是围绕`AVObject` 进行的。

完整的正确用法：

> 在 LeanCloud 上，数据存储是围绕 `AVObject` 进行的。每个 `AVObject` 都包含了与 JSON 兼容的 key-value 对应的数据。数据是 schema-free 的，你不需要在每个 `AVObject` 上提前指定存在哪些键，只要直接设定对应的 key-value 即可。

例外：「豆瓣FM」等产品名词，按照官方所定义的格式书写。

### 中文与数字之间需要增加空格

正确：

> 今天出去买菜花了 5000 元。

错误：

> 今天出去买菜花了 5000元。

> 今天出去买菜花了5000元。

### 数字与单位之间无需增加空格

正确：

> 我家的光纤入户宽带有 10Gbps，SSD 一共有 10TB。

错误：

> 我家的光纤入户宽带有 10 Gbps，SSD 一共有 20 TB。

另外，度／百分比与数字之间不需要增加空格：

正确：

> 今天是 233° 的高温。

> 新 MacBook Pro 有 15% 的 CPU 性能提升。

错误：

> 今天是 233 ° 的高温。

> 新 MacBook Pro 有 15 % 的 CPU 性能提升。

### 全角标点与其他字符之间不加空格

正确：

> 刚刚买了一部 iPhone，好开心！

错误：

> 刚刚买了一部 iPhone ，好开心！

### `-ms-text-autospace` to the rescue

Microsoft 有个 [`-ms-text-autospace`](http://msdn.microsoft.com/en-us/library/ie/ms531164\(v=vs.85\).aspx) 的 CSS 属性可以实现自动为中英文之间增加空白。不过目前并未普及，另外在其他应用场景，例如 OS X、iOS 的用户界面目前并不存在这个特性，所以请继续保持随手加空格的习惯。

## 标点符号

### 不重复使用标点符号

正确：

> 德国队竟然战胜了巴西队！

> 她竟然对你说「喵」？！

错误：

> 德国队竟然战胜了巴西队！！

> 德国队竟然战胜了巴西队！！！！！！！！

> 她竟然对你说「喵」？？！！

> 她竟然对你说「喵」？！？！？？！！

## 全角和半角

不明白什么是全角（全形）与半角（半形）符号？请查看维基百科词条『[全角和半角](http://zh.wikipedia.org/wiki/全形和半形)』或者百度百科词条『[全角](https://baike.baidu.com/item/%E5%85%A8%E8%A7%92/9323113?fr=aladdin)』和『[半角](https://baike.baidu.com/item/半角)』。

简单介绍：

「**全角**」指一个字符占用两个标准字符位置的状态，如中文模式下的逗号、句号等：，。？「」

「**半角**」就是 ASCII 方式的字符，在没有中文输入法起作用的时候输入的字母数字和字符都是半角的，如英文模式下的逗号、句号等: , . ; ? ""

### 直角符号

英文单词使用 "" 或者 ''；

中文词语使用 「」或者『』，不使用弯角符号 “” 和 ‘’，弯角符号更适用于手写。

其中 "" 对应「」，'' 对应『』

### 使用全角中文标点

正确：

> 嗨！你知道嘛？今天前台的小妹跟我说「喵」了哎！

> 核磁共振成像（NMRI）是什么原理都不知道？JFGI！

错误：

> 嗨! 你知道嘛? 今天前台的小妹跟我说 "喵" 了哎!

> 嗨!你知道嘛?今天前台的小妹跟我说"喵"了哎!

> 核磁共振成像 (NMRI) 是什么原理都不知道? JFGI!

> 核磁共振成像(NMRI)是什么原理都不知道?JFGI!

### 数字使用半角字符

正确：

> 这件蛋糕只卖 1000 元。

错误：

> 这件蛋糕只卖 １０００ 元。

例外：在设计稿、宣传海报中如出现极少量数字的情形时，为方便文字对齐，是可以使用全角数字的。

### 遇到完整的英文整句、特殊名词，其內容使用半角标点

正确：

> 乔布斯那句话是怎么说的？「Stay hungry, stay foolish.」

> 推荐你阅读《Hackers & Painters: Big Ideas from the Computer Age》，非常的有趣。

错误：

> 乔布斯那句话是怎么说的？「Stay hungry，stay foolish。」

> 推荐你阅读《Hackers＆Painters：Big Ideas from the Computer Age》，非常的有趣。

## 名词

### 专有名词使用正确的大小写

大小写相关用法原属于英文书写范畴，不属于本文档讨论內容，在这里只对部分易错用法进行简述。

正确：

> 使用 GitHub 登录

> 我们的客户有 GitHub、Foursquare、Microsoft Corporation、Google、Facebook, Inc.。

错误：

> 使用 github 登录

> 使用 GITHUB 登录

> 使用 Github 登录

> 使用 gitHub 登录

> 使用 gｲんĤЦ8 登录

> 我们的客户有 github、foursquare、microsoft corporation、google、facebook, inc.。

> 我们的客户有 GITHUB、FOURSQUARE、MICROSOFT CORPORATION、GOOGLE、FACEBOOK, INC.。

> 我们的客户有 Github、FourSquare、MicroSoft Corporation、Google、FaceBook, Inc.。

> 我们的客户有 gitHub、fourSquare、microSoft Corporation、google、faceBook, Inc.。

> 我们的客户有 gｲんĤЦ8、ｷouЯƧquﾑгє、๓เςг๏ร๏Ŧt ς๏гק๏гคtเ๏ภn、900913、ƒ4ᄃëв๏๏к, IПᄃ.。

注意：当网页中需要配合整体视觉风格而出现全部大写／小写的情形，HTML 中请使用标准的大小写规范进行书写；并通过 `text-transform: uppercase;`／`text-transform: lowercase;` 对表现形式进行定义。

### 不要使用不地道的缩写

正确：

> 我们需要一位熟悉 JavaScript、HTML5，至少理解一种框架（如 Backbone.js、AngularJS、React 等）的前端开发者。

错误：

> 我们需要一位熟悉 Js、h5，至少理解一种框架（如 backbone、angular、RJS 等）的 FED。

### 链接之间增加空格

用法：

> 请 [提交一个 issue](https://github.com/mzlogin/chinese-copywriting-guidelines/blob/Simplified/README.md#) 并分配给相关同事。

> 访问我们网站的最新动态，请 [点击这里](https://github.com/mzlogin/chinese-copywriting-guidelines/blob/Simplified/README.md#) 进行订阅！

对比用法：

> 请[提交一个 issue](https://github.com/mzlogin/chinese-copywriting-guidelines/blob/Simplified/README.md#) 并分配给相关同事。

> 访问我们网站的最新动态，请[点击这里](https://github.com/mzlogin/chinese-copywriting-guidelines/blob/Simplified/README.md#)进行订阅！

### 简体中文使用直角引号

用法：

> 「老师，『有条不紊』的『紊』是什么意思？」

对比用法：

> “老师，‘有条不紊’的‘紊’是什么意思？”

### 加粗文字增加空格

正确：

> 一个好的 **排版** 彰显好的文档。

错误：

> 一个好的**排版**彰显好的文档。

### 加粗文字与标点符号

加粗的文字如果是最后一行，或者独处一行，那么加粗范围包括标点符号；

加粗的文字如果后面还有文字，则加粗范围不包括标点符号。

正确：

> 欢迎来到我的博客，**请慢慢食用。**

> **欢迎来到我的博客**，请慢慢食用。

错误：

> 欢迎来到我的博客，**请慢慢食用**。
>
> **欢迎来到我的博客，** 请慢慢食用。

可能看不太清楚，这里解释一下：

* 错误的例子中，句号在加粗范围外面，逗号在加粗范围里面

* 正确的例子中，句号在加粗范围里面，逗号在加粗范围外面

## 个人风格

以下用法略带有个人色彩，即：无论是否遵循下述规则，从语法的角度来讲都是 **正确** 的。

### 体系化文档命名规范

正确：

> 关于 - 技巧
>
> 笔记 - 技巧
>
> 排版 - 技巧

错误：

> 关于技巧
>
> 笔记 技巧
>
> 排版 ~ 技巧

### 体系化文档开头添加目录

生成可以跳转的目录，方便他人阅读和选择。

如 VitePress 可以解析 `[[TOC]]` 字符串从而生成目录。

### 有序/无序列表末尾不加标点符合

因为开头的符号已经代表句号/感叹号/问号了。

正确：

> * 欢迎来到 `Teek`
>
> * 希望能入你法眼
>
> 1. 酒菜不多，但都是精华。请慢慢食用
> 2. 文章内容不恰当，可以在评论区留言

错误：

> * 欢迎来到 `Teek`。
>
> * 希望能入你法眼。
>
> 1. 酒菜不多，但都是精华。请慢慢食用。
> 2. 文章内容不恰当，可以在评论区留言。

## 格式化工具

使用这些工具，可以一次性把需要的文章按照工具的规定进行格式化，类似于杂乱的代码被格式化有序。

| 仓库                                                                                                                            | 语言            |
| ------------------------------------------------------------------------------------------------------------------------------- | --------------- |
| [vinta/paranoid-auto-spacing](https://github.com/vinta/paranoid-auto-spacing)                                                   | JavaScript      |
| [huei90/pangu.node](https://github.com/huei90/pangu.node)                                                                       | Node.js         |
| [huacnlee/auto-correct](https://github.com/huacnlee/auto-correct)                                                               | Ruby            |
| [sparanoid/space-lover](https://github.com/sparanoid/space-lover)                                                               | PHP (WordPress) |
| [nauxliu/auto-correct](https://github.com/NauxLiu/auto-correct)                                                                 | PHP             |
| [ricoa/copywriting-correct](https://github.com/ricoa/copywriting-correct)                                                       | PHP             |
| [hotoo/pangu.vim](https://github.com/hotoo/pangu.vim)                                                                           | Vim             |
| [sparanoid/grunt-auto-spacing](https://github.com/sparanoid/grunt-auto-spacing)                                                 | Node.js (Grunt) |
| [hjiang/scripts/add-space-between-latin-and-cjk](https://github.com/hjiang/scripts/blob/master/add-space-between-latin-and-cjk) | Python          |

## 谁在这样做？

| 网站                                              | 文案 | UGC          |
| ------------------------------------------------- | ---- | ------------ |
| [Apple 中国](http://www.apple.com/cn/)            | Yes  | N/A          |
| [Apple 香港](http://www.apple.com/hk/)            | Yes  | N/A          |
| [Apple 台湾](http://www.apple.com/tw/)            | Yes  | N/A          |
| [Microsoft 中国](http://www.microsoft.com/zh-cn/) | Yes  | N/A          |
| [Microsoft 香港](http://www.microsoft.com/zh-hk/) | Yes  | N/A          |
| [Microsoft 台湾](http://www.microsoft.com/zh-tw/) | Yes  | N/A          |
| [LeanCloud](https://leancloud.cn/)                | Yes  | N/A          |
| [知乎](https://www.zhihu.com/)                    | Yes  | 部分用户达成 |
| [V2EX](https://www.v2ex.com/)                     | Yes  | Yes          |
| [SegmentFault](https://segmentfault.com/)         | Yes  | 部分用户达成 |
| [Apple4us](http://apple4us.com/)                  | Yes  | N/A          |
| [豌豆荚](https://www.wandoujia.com/)              | Yes  | N/A          |
| [Ruby China](https://ruby-china.org/)             | Yes  | 标题达成     |
| [PHPHub](https://phphub.org/)                     | Yes  | 标题达成     |
| [少数派](http://sspai.com/)                       | Yes  | N/A          |
| [力扣 LeetCode](https://leetcode-cn.com/)         | Yes  | Yes          |

## 本文转载

添加了一些自己的理解

[中文文案排版指北](https://github.com/mzlogin/chinese-copywriting-guidelines/blob/Simplified/README.md)

## 参考文献

* [Guidelines for Using Capital Letters](http://grammar.about.com/od/punctuationandmechanics/a/Guidelines-For-Using-Capital-Letters.htm)
* [Letter case - Wikipedia](http://en.wikipedia.org/wiki/Letter_case)
* [Punctuation - Oxford Dictionaries](http://www.oxforddictionaries.com/words/punctuation)
* [Punctuation - The Purdue OWL](https://owl.english.purdue.edu/owl/section/1/6/)
* [How to Use English Punctuation Corrently - wikiHow](http://www.wikihow.com/Use-English-Punctuation-Correctly)
* [格式 - openSUSE](https://zh.opensuse.org/index.php?title=Help:格式)
* [全角和半角 - 维基百科](http://zh.wikipedia.org/wiki/全形和半形)
* [引号 - 维基百科](http://zh.wikipedia.org/wiki/引號)
* [疑问惊叹号 - 维基百科](http://zh.wikipedia.org/wiki/疑問驚嘆號)
* [全角 - 百度百科](https://baike.baidu.com/item/%E5%85%A8%E8%A7%92/9323113?fr=aladdin)
* [半角 - 百度百科](https://baike.baidu.com/item/%E5%8D%8A%E8%A7%92)

---

---
url: /Java/JVM性能调优/02.JVM性能监控与调优/0_性能调优概述.md
---

# 性能调优概述

## 调优概述

为什么要调优

* 防止出现OOM
* 解决OOM
* 减少Full GC出现的频率

生产环境中的问题

* 生产环境发生了内存溢出该如何处理？
* 生产环境应该给服务器分配多少内存合适？
* 如何对垃圾回收器的性能进行调优？
* 生产环境CPU负载飙高该如何处理？
* 生产环境应该给应用分配多少线程合适？
* 不加log，如何确定请求是否执行了某一行代码？
* 不加log，如何实时查看某个方法的入参与返回值？

监控的依据

* 运行日志
* 异常堆栈
* GC日志
* 线程快照
* 堆转储快照

调优的大方向

* 合理地编写代码
* 充分并合理的使用硬件资源
* 合理地进行JVM调优

## 性能优化的步骤

第1步：性能监控

* GC频繁
* cpu load过高
* OOM
* 内存泄露
* 死锁
* 程序响应时间较长

第2步：性能分析

* 打印GC日志，通过GCviewer或者 [http://gceasy.io](http://gceasy.io/) 来分析异常信息
* 灵活运用命令行工具、jstack、jmap、jinfo等
* dump出堆文件，使用内存分析工具分析文件
* 使用阿里Arthas、jconsole、JVisualVM来实时查看JVM状态
* jstack查看堆栈信息

```
Sun JDK监控和故障常用处理命令：

jps：JVM Process Status Tool，显示指定系统内所有的HotSpot虚拟机进程。
jstat：JVM statistics Monitoring是用于监视虚拟机运行时状态信息的命令，它可以显示出虚拟机进程中的类装载、内存、垃圾收集、JIT编译等运行数据。
jmap：JVM Memory Map命令用于生成heap dump文件。
jhat：JVM Heap Analysis Tool命令是与jmap搭配使用，用来分析jmap生成的dump，jhat内置了一个微型的HTTP/HTML服务器，生成dump的分析结果后，可以在浏览器中查看。
jstack：用于生成java虚拟机当前时刻的线程快照。
jinfo：JVM Configuration info 这个命令作用是实时查看和调整虚拟机运行参数。
```

第3步：性能调优

* 适当增加内存，根据业务背景选择垃圾回收器

  ```
  设定堆内存大小
  -Xmx：堆内存最大限制

  设定新生代大小， 新生代不宜太小，否则会有大量对象涌入老年代。 -XX:NewSize：新生代大小 -XX:NewRatio 新生代和老生代占比 -XX:SurvivorRatio：伊甸园空间和幸存者空间的占比

  设定垃圾回收器
  年轻代用 -XX:+UseParNewGC
  年老代用-XX:+UseConcMarkSweepGC
  ```

* 优化代码，控制内存使用

* 增加机器，分散节点压力

* 合理设置线程池线程数量

* 使用中间件提高程序效率，比如缓存、消息队列等

* 其他……

## 性能评价/测试指标

停顿时间（或响应时间）

提交请求和返回该请求的响应之间使用的时间，一般比较关注平均响应时间。常用操作的响应时间列表：

| 操作                               | 响应时间 |
| ---------------------------------- | -------- |
| 打开一个站点                       | 几秒     |
| 数据库查询一条记录（有索引）       | 十几毫秒 |
| 机械磁盘一次寻址定位               | 4毫秒    |
| 从机械磁盘顺序读取1M数据           | 2毫秒    |
| 从SSD磁盘顺序读取1M数据            | 0.3毫秒  |
| 从远程分布式换成Redis 读取一个数据 | 0.5毫秒  |
| 从内存读取 1M数据                  | 十几微妙 |
| Java程序本地方法调用               | 几微妙   |
| 网络传输2Kb数据                    | 1 微妙   |

在垃圾回收环节中：

* 暂停时间：执行垃圾收集时，程序的工作线程被暂停的时间。
* -XX:MaxGCPauseMillis

吞吐量

* 对单位时间内完成的工作量（请求）的量度
* 在GC中：运行用户代码的事件占总运行时间的比例（总运行时间：程序的运行时间+内存回收的时间）
* 吞吐量为1-1/(1+n)，其中-XX::GCTimeRatio=n

并发数

* 同一时刻，对服务器有实际交互的请求数

内存占用

* Java堆区所占的内存大小

相互间的关系

以高速公路通行状况为例

* 吞吐量：每天通过高速公路收费站的车辆的数据
* 并发数：高速公路上正在行驶的车辆的数目
* 响应时间：车速

## 大厂面试题

支付宝：

支付宝三面：JVM性能调优都做了什么？

小米：

有做过JVM内存优化吗？

从SQL、JVM、架构、数据库四个方面讲讲优化思路

蚂蚁金服：

JVM的编译优化

jvm性能调优都做了什么

JVM诊断调优工具用过哪些？

二面：jvm怎样调优，堆内存、栈空间设置多少合适

三面：JVM相关的分析工具使用过的有哪些？具体的性能调优步骤如何

阿里：

如何进行JVM调优？有哪些方法？

如何理解内存泄漏问题？有哪些情况会导致内存泄漏？如何解决？

字节跳动：

三面：JVM如何调优、参数怎么调？

拼多多：

从SQL、JVM、架构、数据库四个方面讲讲优化思路

京东：

JVM诊断调优工具用过哪些？

每秒几十万并发的秒杀系统为什么会频繁发生GC？

日均百万级交易系统如何优化JVM？

线上生产系统OOM如何监控及定位与解决？

高并发系统如何基于G1垃圾回收器优化性能？

---

---
url: /Java/JVM性能调优/02.JVM性能监控与调优/性能分析利器VisualVM的使用.md
---

# 性能分析利器VisualVM

> 下载地址：http://visualvm.github.io/

安装使用

监控远程JVM

https://blog.csdn.net/localhost01/article/details/83422902

https://blog.csdn.net/TheLongir/article/details/124629717

https://blog.csdn.net/lijie1010/article/details/78805837

https://blog.csdn.net/qq\_32641659/article/details/88035588

提示：Cannot find Java 1.8 or higher

修改etc\visualvm.conf文件，72行左右的jdk路径，修改保存重新打开visualvm.exe

```conf
visualvm_jdkhome="/path/to/jdk"
```

---

---
url: /15.主题开发/40.样式布局.md
---

# 样式布局

## 样式与组件分离

Teek 并没有和普通的 Vue 项目一样，使用如下模板进行编写组件：

```vue
<script setup lang="ts">
// 组件逻辑
</script>

<template>
  <!-- 组件模板 -->
</template>

<style lang="scss" scoped>
/* 组件样式 */
</style>
```

而是使用：

```vue
<script setup lang="ts">
// 组件逻辑
</script>

<template>
  <!-- 组件模板 -->
</template>
```

那么组件样式去哪里了呢？

Teek 专门创建 `theme-chalk/src/components` 目录用于存放组件样式，然后将所有的组件样式汇总到一个 `index.scss`（入口样式文件），最后在 `index.ts`（入口运行文件）分别引入 `theme-chalk/index.scss`（入口样式文件）和 `layout/index.vue`（入口组件）。

```
*.vue ——> Layout/index.vue ——> index.ts <—— theme-chalk/index.scss <—— *.scss
多个功能组件 ——> 入口组件 ——> 入口运行文件 <—— 入口样式文件 <—— 多个功能组件样式
```

::: tip
这也就是为什么在 `.vitepress/theme/index.ts` 里单独引入 Teek 样式的原因。
:::

## 样式结构设计

Teek 的样式结构设计遵循单一原则：每个样式文件只负责渲染单独的模块（组件），如：

* `nav-var.scss` 只提供导航栏的 `css var` 变量
* `nav-search-button.scss` 只渲染导航栏的搜索按钮，内部引用了 `nav-var.scss`
* `nav-switch-button.scss` 只渲染导航栏的深色、浅色切换按钮，内部引用了 `nav-var.scss`
* `nav-translation.scss` 只渲染导航栏的国际化下拉框，内部引用了 `nav-var.scss`

这样的好处是，不同的样式之间是独立的，需要根据自己的需求按需引入对应的样式文件，不会存在引入一个内容超大的样式文件，导致不想要的元素也发生了样式改变。

当然，对于不喜欢折腾、研究的小伙伴来说，Teek 也提供了 `nav.scss` 样式文件，该文件内部没有编写任何样式代码，而是引入了 `nav-xxx.scss` 等样式文件，用于快速引入 `nav` 的所有样式文件。

## 目录结构

下面给出 Teek 的样式目录结构：

```sh
theme-chalk/src.
├─ base.scss      # 基础样式文件
├─ index.scss     # 入口样式文件
│
├─ common         # 通用样式目录
├─ components     # 组件样式目录，对应每个 Vue 组件
├─ md-plugin      # Markdown 插件样式目录
├─ mixins         # 样式混入目录
├─ module         # 模块样式目录
├─ var            # 样式变量目录
└─ vp-plus        # VitePress 样式加强目录
```

在 `components` 目录下，Teek 会给每个组件生成对应的样式文件，文件名与组件名相近（字母小写 + `-` 分割来命名）。

`index.scss` 文件是入口样式文件，它将导入 `base.scss`、`var` 目录下的样式文件，以及 `components` 目录下所有组件的样式文件。

如果需要按需加载组件，您必须要引入 `base.scss` 文件，这是 Teek 的核心主题样式文件，然后按需引入 Vue 组件和 `components` 目录下的对应组件的样式文件。

## 命名空间

Teek 并没有简单的直接给一个 `div` 元素添加 `class="button"`，然后在 CSS 里 `.button {}` 定义样式，Teek 使用 **命名空间** 的设计思想，给每个组件添加唯一标识，确保不同组件的相同 `class` 发生样式冲突。

::: tip
命名空间等价于 Vue 组件 `style` 的 `scoped` 属性。
:::

### SCSS 定义命名空间

命名空间其实是一个唯一的前缀，如 ElementPlus 的命名空间为 `el`，在某个 `class` 中添加 `el-` 前缀，如 `<div class="el-button"></div>`

命名空间应该是一个变量，这样只需要修改该变量的值，那么所有的 `class` 以及样式都不会失效，如在 ElementPlus 的命名空间文件里修改 `el` 为 `tk`，那么所有的 `class` 都变为 `tk` 开头，且样式不会失效。

::: tip
Teek 的命名空间为 `tk`。
:::

首先 Teek 在 `theme-chalk/mixins/config.scss` 文件中定义了命名空间变量 `$namespace`：

```scss
$namespace: "tk" !default;
```

此时其他的 SCSS 文件都需要引入该文件，然后使用 `$namespace` 变量：

```scss
@use "../mixins/config";

.#{$namespace}-button {
}
```

当然这只是简单的 Demo，实际的使用请看 [样式文件使用 BEM](#样式文件使用-bem)。

### JS/TS 使用命名空间

在 [定义命名空间](#定义命名空间) 中通过 `$namespace` 定义了命名空间，那么 Vue 组件里的 `template` 元素如何使用呢？总不能直接写 `<div class="tk-button"></div>`，一旦这样，修改 `$namespace` 的值，那么所有的 `class` 都会失效，因此需要想办法直接在 `template` 使用 `$namespace` 变量。

通过 SCSS Module API 可以将 `$namespace` 变量导出到 `js` 或 `ts` 文件里，在 `theme-chalk/module/namespace.module.scss` 文件导出 `$namespace`：

```scss
/* theme-chalk/module/namespace.module.scss */
@use "../mixins/config" as *;

:export {
  namespace: #{$namespace};
}
```

::: info
SCSS Module API 只对 `.module.scss` 结尾的文件提供变量暴露功能。
:::

如果是 Typescript 环境使用，则还需要在同级目录下定义一个 `namespace.module.scss.d.ts` 文件：

```ts
// theme-chalk/module/namespace.module.scss.d.ts
export interface ScssVariables {
  [x: string]: unknown;
  namespace: string;
}

export let variables: ScssVariables;

export default variables;
```

最后在 Vue 组件引入 `namespace.module.scss`：

```vue
<script setup lang="ts">
import namespaceModule from "../theme-chalk/module/namespace.module.scss";
</script>

<template>
  <div :class="`${namespaceModule}-button`"></div>
</template>
```

当然这只是简单的 Demo，实际的使用请看 [组件元素使用 BEM](#组件元素使用-bem)。

::: tip
将 `$namespace: tk` 改为 `$namespace: xx`（xx 为你的项目名/框架名），那么没人知道它是 teek，它已经完全属于你。
:::

## 什么是 BEM

Teek 使用 BEM 规范进行样式编写，并使用 SCSS 进行样式编写。

BEM 是一种前端开发方法论，全称是 Block Element Modifier（块、元素、修饰符）。它提供了一种命名约定，用于组织和管理 CSS 类名，从而提高代码的可维护性、可扩展性和复用性。

* Block（块）

  * 独立的功能模块，可以独立存在
  * 示例：`button`、`menu`、`input`

* Element（元素）

  * 属于某个 Block 的一部分，不能单独存在
  * 使用双下划线 `__` 连接 Block 和 Element
  * 示例：`menu__item`、`button__text`

* Modifier（修饰符）

  * 用于改变 Block 或 Element 的外观或行为
  * 使用双横线 `--` 表示
  * 示例：`button--large`、`menu__item--active`

BEM 方法的引入主要是为了解决传统 CSS 开发中常见的问题，尤其是在大型项目或团队协作中，这些问题会变得更加突出。以下是使用 BEM 的主要原因以及它解决的痛点：

* 样式冲突：在传统的 CSS 开发中，类名可能会重复或不够具体，导致样式冲突。例如，多个开发者可能都定义了一个名为 `button` 的样式，但它们的行为和外观完全不同
* 可维护性差：随着项目的增长，CSS 文件变得越来越复杂，难以找到特定样式的定义位置，或者修改一个样式时意外影响到其他部分
* 样式复用困难：在没有明确规范的情况下，开发者可能需要重复编写类似的样式代码，增加了冗余
* 团队协作困难：在多人协作的项目中，不同开发者可能采用不同的命名习惯，导致代码风格不一致，难以统一管理
* 样式与结构分离不清晰：在某些情况下，开发者可能直接通过 HTML 结构（如标签选择器、后代选择器）来定义样式，这会导致样式与结构紧密耦合，难以迁移或重构
* 缺乏扩展性：当需要对现有样式进行扩展或修改时，可能会因为复杂的嵌套关系或不清晰的命名规则而感到困难

### BEM 命名规则

* Block: `blockName`
* Block + Element: `blockName__elementName`
* Block + Modifier: `blockName--modifierName`
* Element + Modifier: `blockName__elementName--modifierName`

```html
<div class="button">
  <span class="button__text">文字按钮</span>
  <button class="button--large">large 按钮</button>
  <span class="button__text--bold">文字加粗按钮</span>
</div>
```

```css
/* Block */
.button {
  /* 样式 */
}

/* Element */
.button__text {
  /* 样式 */
}

/* Modifier */
.button--large {
  /* 样式 */
}

/* Element + Modifier */
.button__text--bold {
  /* 样式 */
}
```

## 组件元素使用 BEM

定义一个 Hooks 文件 [useNamespace.ts](https://github.com/Kele-Bingtang/vitepress-theme-teek/tree/master/packages/composables/useNamespace.ts) 来封装命名空间和 BEM 规范，[命名空间](#命名空间) 和 [BEM 规范](#什么是-bem) 在上文已经介绍过了。

在组件中引入 `useNamespace.ts` 文件，使用命名空间 + BEM 规范来编写 `class`，如：

```vue
<script setup lang="ts" name="BEMDemo">
import { useNamespace } from "../../../composables/useNamespace";

const ns = useNamespace("button");
</script>

<template>
  <div :class="ns.b()">
    <span :class="ns.e('text')">文字按钮</span>
    <button :class="ns.m('large')">large 按钮</button>
    <span :class="ns.em('text', 'bold')">文字加粗按钮</span>
    <button :class="['button', ns.is('primary')]">primary 按钮</button>
  </div>
</template>
```

等于：

```vue
<script setup lang="ts" name="BEMDemo"></script>

<template>
  <div class="tk-button">
    <span class="tk-button__text">文字按钮</span>
    <button class="tk-button--large">large 按钮</button>
    <span class="tk-button__text--bold">文字加粗按钮</span>
    <button class="button is-primary">primary 按钮</button>
  </div>
</template>
```

具体使用请看 Teek 的组件源码。

## 样式文件使用 BEM

定义 SCSS 文件 [bem.scss](https://github.com/Kele-Bingtang/vitepress-theme-teek/tree/master/packages/theme-chalk/src/mixins/bem.scss) 来封装命名空间和 BEM 规范，

在样式文件引入 `bem.scss` 文件，使用 `bem.scss` 文件提供的 `mixins` 来编写样式，如：

```scss
@use "../mixins/bem" as *;

@include b("button") {
  @include e("text") {
    @include m("bold") {
    }
  }

  @include m("large") {
  }

  .button {
    @include is("primary") {
    }
  }
}
```

等于：

```scss
.tk-button {
  .tk-button__text {
    &--bold {
    }
  }

  .tk-button--large {
  }

  .button {
    &.is-primary {
    }
  }
}
```

具体使用请看 Teek 的样式源码。

## CSS Var 变量使用命名空间

Teek 给所有的 CSS Var 变量都添加命名空间，如：

```css
--tk-text-color-secondary: #86909c;
```

为了共用 `$namespace` 变量，所以 Teek 提供了 `set-css-var` Mixin 和 `getCss-Var` 函数来进行封装：

```scss
/* theme-chalk/mixins/mixin.scss */
@use "./function" as *;

@mixin set-css-var($name, $value) {
  #{joinVarName($name)}: #{$value};
}
```

```scss
/* theme-chalk/mixins/function.scss */
$namespace: "tk" !default; // 假设这里定义了命名空间

/* 合并变量名：joinVarName(('button', 'text-color')) => '--tk-button-text-color' */
@function joinVarName($list) {
  $name: "--" + config.$namespace;
  @each $item in $list {
    @if $item != "" {
      $name: $name + "-" + $item;
    }
  }
  @return $name;
}

/* getCssVar('button', 'text-color') => var(--tk-button-text-color) */
@function getCssVar($args...) {
  @return var(#{joinVarName($args)});
}
```

使用 `set-css-var` 来定义 CSS Var 变量：

```scss
@use "../mixins/mixins" as *;

:root {
  @include set-css-var("button-width", 84px);
  @include set-css-var("button-height", 32px);
  @include set-css-var("button-color", #3451b2);
  @include set-css-var("button-font-size", 16px);
}
```

等于

```scss
:root {
  --tk-button-width: 84px;
  --tk-button-height: 32px;
  --tk-button-color: #3451b2;
  --tk-button-font-size: 16px;
}
```

使用 CSS Var 变量：

```scss
@use "../mixins/function" as *;

.demo {
  width: getCssVar("button-width");
  height: getCssVar("button-height");
  color: getCssVar("button-color");
  font-size: getCssVar("button-font-size");
}
```

等于

```scss
.demo {
  width: var(--tk-button-width);
  height: var(--tk-button-height);
  color: var(--tk-button-color);
  font-size: var(--tk-button-font-size);
}
```

具体使用请看 Teek 的 CSS Var 源码。

---

---
url: /daily/博客文档/VitPress/6_样式美化.md
---

# 样式美化

## 主题美化

## 其他美化

### 引用颜色

在Markdown中，我们常用的引用符号是 `>`，我们可以稍微改动一下

在 `theme/style` 新建 `blockquote.css` 文件

```markdown
.
├─ docs
│  ├─ .vitepress
│  │  └─ config.mts
│  │  └─ theme
│  │     └─ style
│  │        └─ index.css
│  │        └─ blockquote.css
│  └─ index.md
└─ node_modules
```

复制下面代码，粘贴到 `blockquote.css` 中

```css
/* .vitepress/theme/style/blockquote.css */
.vp-doc blockquote {
    border-radius: 10px;
    padding: 18px 20px 20px 15px;
    position: relative;
    background-color: var(--vp-c-gray-soft);
    border-left: 6px solid var(--vp-c-green-2);
}
```

然后在 `index.css` 中引入生效

```css
/* .vitepress/theme/style/index.css */
/* 引用颜色 */
@import './blockquote.css';
```

输入：

```markdown
> 更新时间：2025年
```

输出：

> 更新时间：2025年

### 容器颜色

随着版本更新迭代，现在这 `tip`、 `warning`、 `danger` 颜色真的想吐槽，好丑！

[Vuepress/hope主题的容器颜色](https://theme-hope.vuejs.press/zh/guide/markdown/stylize/hint.html#演示) 就不错，参考着弄一下

在 `theme/style` 新建 `custom-block.css` 文件

```markdown
.
├─ docs
│  ├─ .vitepress
│  │  └─ config.mts
│  │  └─ theme
│  │     └─ style
│  │        └─ index.css
│  │        └─ custom-block.css
│  └─ index.md
└─ node_modules
```

复制下面代码，粘贴到 `custom-block.css` 中

::: details 点我查看代码

```css
/* .vitepress/theme/style/custom-block.css */
/* 深浅色卡 */
:root {
    --custom-block-info-left: #cccccc;
    --custom-block-info-bg: #fafafa;

    --custom-block-tip-left: #009400;
    --custom-block-tip-bg: #e6f6e6;

    --custom-block-warning-left: #e6a700;
    --custom-block-warning-bg: #fff8e6;

    --custom-block-danger-left: #e13238;
    --custom-block-danger-bg: #ffebec;

    --custom-block-note-left: #4cb3d4;
    --custom-block-note-bg: #eef9fd;

    --custom-block-important-left: #a371f7;
    --custom-block-important-bg: #f4eefe;

    --custom-block-caution-left: #e0575b;
    --custom-block-caution-bg: #fde4e8;
}

.dark {
    --custom-block-info-left: #cccccc;
    --custom-block-info-bg: #474748;

    --custom-block-tip-left: #009400;
    --custom-block-tip-bg: #003100;

    --custom-block-warning-left: #e6a700;
    --custom-block-warning-bg: #4d3800;

    --custom-block-danger-left: #e13238;
    --custom-block-danger-bg: #4b1113;

    --custom-block-note-left: #4cb3d4;
    --custom-block-note-bg: #193c47;

    --custom-block-important-left: #a371f7;
    --custom-block-important-bg: #230555;

    --custom-block-caution-left: #e0575b;
    --custom-block-caution-bg: #391c22;
}


/* 标题字体大小 */
.custom-block-title {
    font-size: 16px;
}

/* info容器:背景色、左侧 */
.custom-block.info {
    border-left: 5px solid var(--custom-block-info-left);
    background-color: var(--custom-block-info-bg);
}

/* info容器:svg图 */
.custom-block.info [class*="custom-block-title"]::before {
    content: '';
    background-image: url("data:image/svg+xml;utf8,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24'%3E%3Cpath d='M12 22C6.477 22 2 17.523 2 12S6.477 2 12 2s10 4.477 10 10-4.477 10-10 10zm-1-11v6h2v-6h-2zm0-4v2h2V7h-2z' fill='%23ccc'/%3E%3C/svg%3E");
    width: 20px;
    height: 20px;
    display: inline-block;
    vertical-align: middle;
    position: relative;
    margin-right: 4px;
    left: -5px;
    top: -1px;
}

/* 提示容器:边框色、背景色、左侧 */
.custom-block.tip {
    /* border-color: var(--custom-block-tip); */ 
    border-left: 5px solid var(--custom-block-tip-left);
    background-color: var(--custom-block-tip-bg);
}

/* 提示容器:svg图 */
.custom-block.tip [class*="custom-block-title"]::before {
    content: '';
    background-image: url("data:image/svg+xml;utf8,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24'%3E%3Cpath fill='%23009400' d='M7.941 18c-.297-1.273-1.637-2.314-2.187-3a8 8 0 1 1 12.49.002c-.55.685-1.888 1.726-2.185 2.998H7.94zM16 20v1a2 2 0 0 1-2 2h-4a2 2 0 0 1-2-2v-1h8zm-3-9.995V6l-4.5 6.005H11v4l4.5-6H13z'/%3E%3C/svg%3E");
    width: 20px;
    height: 20px;
    display: inline-block;
    vertical-align: middle;
    position: relative;
    margin-right: 4px;
    left: -5px;
    top: -2px;
}

/* 警告容器:背景色、左侧 */
.custom-block.warning {
    border-left: 5px solid var(--custom-block-warning-left);
    background-color: var(--custom-block-warning-bg);
}

/* 警告容器:svg图 */
.custom-block.warning [class*="custom-block-title"]::before {
    content: '';
    background-image: url("data:image/svg+xml;utf8,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1024 1024'%3E%3Cpath d='M576.286 752.57v-95.425q0-7.031-4.771-11.802t-11.3-4.772h-96.43q-6.528 0-11.3 4.772t-4.77 11.802v95.424q0 7.031 4.77 11.803t11.3 4.77h96.43q6.528 0 11.3-4.77t4.77-11.803zm-1.005-187.836 9.04-230.524q0-6.027-5.022-9.543-6.529-5.524-12.053-5.524H456.754q-5.524 0-12.053 5.524-5.022 3.516-5.022 10.547l8.538 229.52q0 5.023 5.022 8.287t12.053 3.265h92.913q7.032 0 11.803-3.265t5.273-8.287zM568.25 95.65l385.714 707.142q17.578 31.641-1.004 63.282-8.538 14.564-23.354 23.102t-31.892 8.538H126.286q-17.076 0-31.892-8.538T71.04 866.074q-18.582-31.641-1.004-63.282L455.75 95.65q8.538-15.57 23.605-24.61T512 62t32.645 9.04 23.605 24.61z' fill='%23e6a700'/%3E%3C/svg%3E");
    width: 20px;
    height: 20px;
    display: inline-block;
    vertical-align: middle;
    position: relative;
    margin-right: 4px;
    left: -5px;
}

/* 危险容器:背景色、左侧 */
.custom-block.danger {
    border-left: 5px solid var(--custom-block-danger-left);
    background-color: var(--custom-block-danger-bg);
}

/* 危险容器:svg图 */
.custom-block.danger [class*="custom-block-title"]::before {
    content: '';
    background-image: url("data:image/svg+xml;utf8,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24'%3E%3Cpath d='M12 2c5.523 0 10 4.477 10 10v3.764a2 2 0 0 1-1.106 1.789L18 19v1a3 3 0 0 1-2.824 2.995L14.95 23a2.5 2.5 0 0 0 .044-.33L15 22.5V22a2 2 0 0 0-1.85-1.995L13 20h-2a2 2 0 0 0-1.995 1.85L9 22v.5c0 .171.017.339.05.5H9a3 3 0 0 1-3-3v-1l-2.894-1.447A2 2 0 0 1 2 15.763V12C2 6.477 6.477 2 12 2zm-4 9a2 2 0 1 0 0 4 2 2 0 0 0 0-4zm8 0a2 2 0 1 0 0 4 2 2 0 0 0 0-4z' fill='%23e13238'/%3E%3C/svg%3E");
    width: 20px;
    height: 20px;
    display: inline-block;
    vertical-align: middle;
    position: relative;
    margin-right: 4px;
    left: -5px;
    top: -1px;
}

/* 提醒容器:背景色、左侧 */
.custom-block.note {
    border-left: 5px solid var(--custom-block-note-left);
    background-color: var(--custom-block-note-bg);
}

/* 提醒容器:svg图 */
.custom-block.note [class*="custom-block-title"]::before {
    content: '';
    background-image: url("data:image/svg+xml;utf8,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24'%3E%3Cpath d='M12 22C6.477 22 2 17.523 2 12S6.477 2 12 2s10 4.477 10 10-4.477 10-10 10zm-1-11v6h2v-6h-2zm0-4v2h2V7h-2z' fill='%234cb3d4'/%3E%3C/svg%3E");
    width: 20px;
    height: 20px;
    display: inline-block;
    vertical-align: middle;
    position: relative;
    margin-right: 4px;
    left: -5px;
    top: -1px;
}

/* 重要容器:背景色、左侧 */
.custom-block.important {
    border-left: 5px solid var(--custom-block-important-left);
    background-color: var(--custom-block-important-bg);
}

/* 重要容器:svg图 */
.custom-block.important [class*="custom-block-title"]::before {
    content: '';
    background-image: url("data:image/svg+xml;utf8,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1024 1024'%3E%3Cpath d='M512 981.333a84.992 84.992 0 0 1-84.907-84.906h169.814A84.992 84.992 0 0 1 512 981.333zm384-128H128v-42.666l85.333-85.334v-256A298.325 298.325 0 0 1 448 177.92V128a64 64 0 0 1 128 0v49.92a298.325 298.325 0 0 1 234.667 291.413v256L896 810.667v42.666zm-426.667-256v85.334h85.334v-85.334h-85.334zm0-256V512h85.334V341.333h-85.334z' fill='%23a371f7'/%3E%3C/svg%3E");
    width: 20px;
    height: 20px;
    display: inline-block;
    vertical-align: middle;
    position: relative;
    margin-right: 4px;
    left: -5px;
    top: -1px;
}

/* 注意容器:背景色、左侧 */
.custom-block.caution {
    border-left: 5px solid var(--custom-block-caution-left);
    background-color: var(--custom-block-caution-bg);
}

/* 注意容器:svg图 */
.custom-block.caution [class*="custom-block-title"]::before {
    content: '';
    background-image: url("data:image/svg+xml;utf8,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24'%3E%3Cpath d='M12 2c5.523 0 10 4.477 10 10v3.764a2 2 0 0 1-1.106 1.789L18 19v1a3 3 0 0 1-2.824 2.995L14.95 23a2.5 2.5 0 0 0 .044-.33L15 22.5V22a2 2 0 0 0-1.85-1.995L13 20h-2a2 2 0 0 0-1.995 1.85L9 22v.5c0 .171.017.339.05.5H9a3 3 0 0 1-3-3v-1l-2.894-1.447A2 2 0 0 1 2 15.763V12C2 6.477 6.477 2 12 2zm-4 9a2 2 0 1 0 0 4 2 2 0 0 0 0-4zm8 0a2 2 0 1 0 0 4 2 2 0 0 0 0-4z' fill='%23e13238'/%3E%3C/svg%3E");
    width: 20px;
    height: 20px;
    display: inline-block;
    vertical-align: middle;
    position: relative;
    margin-right: 4px;
    left: -5px;
    top: -1px;
}
```

:::

然后在 `index.css` 中引入生效

```css
/* .vitepress/theme/style/index.css */
/* 容器颜色 */
@import './custom-block.css';
```

看看效果

输入：

```markdown
::: info 注释
注释是灰色
:::

::: tip 提示
提示是绿色
:::

::: warning 警告
警告是橘色
:::

::: danger 危险
危险是红色
:::
```

输出：

::: info 注释
注释是灰色
:::

::: tip 提示
提示是绿色
:::

::: warning 警告
警告是橘色
:::

::: danger 危险
危险是红色
:::

### 导航栏毛玻璃

在 `theme/style` 文件夹，然后新建 `blur.css` 并填入如下代码

```markdown
.
├─ docs
│  ├─ .vitepress
│  │  └─ config.mts
│  │  └─ theme
│  │     └─ style
│  │        └─ index.css
│  │        └─ blur.css
│  └─ index.md
└─ node_modules
```

在主题原始文件中， `VPNavBar.vue` 组件有其对应的属性

复制下面代码，粘贴到 `blur.css` 中，可以自行增减

::: details 点我查看代码

```css
/* .vitepress\theme\style\blur.css */
/* 导航栏毛玻璃 */
:root {

    /* 首页下滑后导航透明 */
    .VPNavBar:not(.has-sidebar):not(.home.top) {
        background-color: rgba(255, 255, 255, 0);
        backdrop-filter: blur(10px);
    }

    /* 搜索框透明 */
    .DocSearch-Button {
        background-color: rgba(255, 255, 255, 0);
        backdrop-filter: blur(10px);
    }

    /* Feature透明 */
    .VPFeature {
        border: 1px solid transparent;
        box-shadow: 0 10px 30px 0 rgb(0 0 0 / 15%);
        background-color: transparent;
    }

    /* 文档页侧边栏顶部透明 */
    .curtain {
        background-color: rgba(255, 255, 255, 0);
        backdrop-filter: blur(10px);
    }

    @media (min-width: 960px) {

        /* 文档页导航中间透明 */
        .VPNavBar:not(.home.top) .content-body {
            background-color: rgba(255, 255, 255, 0);
            backdrop-filter: blur(10px);
        }
    }

    /* 移动端大纲栏透明 */
    .VPLocalNav {
        background-color: rgba(255, 255, 255, 0);
        backdrop-filter: blur(10px);
    }

}
```

:::

最后引入 `index.css` 中 即可看到效果

```css
/* .vitepress/theme/style/index.css */
@import './blur.css';
```

### 链接图标

在 [Vuejs官网的快速上手](https://cn.vuejs.org/guide/quick-start.html) 中 链接前有个图标，怎么做到呢

在 `theme/style` 新建 `link.css` 文件

```markdown
.
├─ docs
│  ├─ .vitepress
│  │  └─ config.mts
│  │  └─ theme
│  │     └─ style
│  │        └─ index.css
│  │        └─ link.css
│  └─ index.md
└─ node_modules
```

将下面代码，复制粘贴到 `link.css` 中

分别添加了 [油管](https://www.youtube.com/) 和 [B站](https://www.bilibili.com/) 的链接图标

::: tip SVG图形

* 建议使用 `32*32` 的尺寸

[iconfont](https://www.iconfont.cn/)、[xicons](https://www.xicons.org/#/zh-CN)、[iconpark](https://iconpark.oceanengine.com/official)

下载图标放到 `public/svg/` 下

:::

```css
/* .vitepress/theme/style/link.css */

/* YouTube */
.vp-doc a[href^="https://www.youtube.com/"]:before {
    content: '';
    background-image: url(/svg/youtube.svg);
    width: 20px;
    height: 20px;
    display: inline-block;
    vertical-align: middle;
    position: relative;
    background-size: cover;
    margin-right: 4px;
}

/* 哔哩哔哩 */
.vp-doc a[href^="https://www.bilibili.com/"]:before {
    content: '';
    background-image: url(/svg/bilibili.svg);
    width: 20px;
    height: 20px;
    display: inline-block;
    vertical-align: middle;
    position: relative;
    background-size: cover;
    top: -2px;
    margin-right: 4px;
}
```

然后在 `index.css` 中引入生效

```css
/* .vitepress/theme/style/index.css */
/* 链接图标 */
@import './link.css';
```

输入：

```
油管链接图标：[Youtube](https://www.youtube.com/)

B站链接图标：[哔哩哔哩](https://www.bilibili.com/)
```

输出：

油管链接图标：[Youtube](https://www.youtube.com/)

B站链接图标：[哔哩哔哩](https://www.bilibili.com/)

### 记号笔

在某些整段的文字中，我们可以用记号笔，划出重点

在 `theme/style` 新建 `marker.css` 文件

```markdown
.
├─ docs
│  ├─ .vitepress
│  │  └─ config.mts
│  │  └─ theme
│  │     └─ style
│  │        └─ index.css
│  │        └─ marker.css
│  └─ index.md
└─ node_modules
```

将下面代码，复制粘贴到 `marker.css` 中

```css
/* .vitepress/theme/style/marker.css */

/* 记号笔 不喜欢可自行调整 */
.marker-text {
    text-decoration: underline;
    text-decoration-thickness: 9px;
    text-decoration-color: rgba(255, 228, 0, 0.4);
    text-underline-offset: -4px;
    text-decoration-skip-ink: none;
}
```

然后在 `index.css` 中引入生效

```css
/* .vitepress/theme/style/index.css */
/* 记号笔 */
@import './marker.css';
```

输入：

```markdown
<sapn class="marker-text">这里是重重点</sapn>
```

输出：

这里是重重点

还可以实现类似荧光笔的效果

```css
/* .vitepress/theme/style/marker.css */

/* 荧光笔 不喜欢可自行调整*/
.marker-text-highlight {
    border-radius: 5px 5px;
    background: transparent;
    color: var(--vp-c-text-soft);
    background: linear-gradient(104deg, rgba(130, 255, 173, 0) 0.9%, rgba(130, 255, 173, 1.25) 2.4%, rgba(130, 255, 173, 0.5) 5.8%, rgba(130, 255, 173, 0.1) 93%, rgba(130, 255, 173, 0.7) 96%, rgba(130, 255, 1732, 0) 98%), linear-gradient(183deg, rgba(130, 255, 173, 0) 0%, rgba(130, 255, 173, 0.3) 7.9%, rgba(130, 255, 173, 0) 15%);
}
```

输入：

```markdown
<sapn class="marker-text-highlight">这里是荧光笔</sapn>
```

输出：

这里是荧光笔

原作者还从 [尤大的个人主页](https://evanyou.me/) copy了个 `hover`

```css
/* .vitepress/theme/style/marker.css */

/* 尤雨溪 不喜欢可自行调整 */
.marker-evy {
    white-space: nowrap;
    position: relative;
}

.marker-evy:after {
    content: '';
    position: absolute;
    z-index: -1;
    top: 66%;
    left: 0em;
    right: 0em;
    bottom: 0;
    transition: top 200ms cubic-bezier(0, 0.8, 0.13, 1);
    background-color: rgba(79, 192, 141, 0.5);
}

.marker-evy:hover:after {
    top: 0%;
}
```

输入：

```markdown
<sapn class="marker-evy">这里是尤雨溪的主页样式，鼠标放在我上面看效果</sapn>
```

输出：

这里是尤雨溪的主页样式，鼠标放在我上面看效果

### 代码块

将代码块改成Mac风格，三个小圆点

在 `.vitepress/theme/style` 目录新建一个 `vp-code.css` 文件

```markdown
.
├─ docs
│  ├─ .vitepress
│  │  └─ config.mts
│  │  └─ theme
│  │     └─ style
│  │        └─ index.css
│  │        └─ vp-code.css
│  └─ index.md
└─ node_modules
```

复制下面代码，粘贴到 `vp-code.css` 保存

原作者基于 [@Aurorxa](https://github.com/Aurorxa) 的代码进行一些修改

```css
/* .vitepress/theme/style/vp-code.css */

/* 代码块：增加留空边距 增加阴影 */
.vp-doc div[class*=language-] {
  box-shadow: 0 10px 30px 0 rgb(0 0 0 / 40%);
  padding-top: 20px;
}

/* 代码块：添加macOS风格的小圆点 */
.vp-doc div[class*=language-]::before {
  content: "";
  display: block;
  position: absolute;
  top: 12px;
  left: 12px;
  width: 12px;
  height: 12px;
  background-color: #ff5f56;
  border-radius: 50%;
  box-shadow: 20px 0 0 #ffbd2e, 40px 0 0 #27c93f;
  z-index: 1;
}

/* 代码块：下移行号 隐藏右侧竖线 */
.vp-doc .line-numbers-wrapper {
  padding-top: 40px;
  border-right: none;
}

/* 代码块：重建行号右侧竖线 */
.vp-doc .line-numbers-wrapper::after {
  content: "";
  position: absolute;
  top: 40px;
  right: 0;
  border-right: 1px solid var(--vp-code-block-divider-color);
  height: calc(100% - 60px);
}

.vp-doc div[class*='language-'].line-numbers-mode {
  margin-bottom: 20px;
}
```

然后在 `index.css` 中引入生效

```css
/* .vitepress/theme/style/index.css */
@import './vp-code.css';
```

输入：

````markdown
```sh
#默认有行号
pnpm -v
```

```sh:no-line-numbers
#关闭行号
pnpm -v
```
````

输出：

```sh
#默认有行号
pnpm -v
```

```sh:no-line-numbers
#关闭行号
pnpm -v
```

### 代码组

在 `.vitepress/theme/style` 目录新建一个 `vp-code-group.css` 文件

```css
.
├─ docs
│  ├─ .vitepress
│  │  └─ config.mts
│  │  └─ theme
│  │     └─ style
│  │        └─ index.css
│  │        └─ vp-code-group.css
│  └─ index.md
└─ node_modules
```

复制下面代码，粘贴到 `vp-code-group.css` 保存

```css
/* .vitepress/theme/style/vp-code-group.css */

/* 代码组：tab间距 */
.vp-code-group .tabs {
  padding-top: 20px;
}

/* 代码组：添加样式及阴影 */
.vp-code-group {
  color: var(--vp-c-black-soft);
  border-radius: 8px;
  box-shadow: 0 10px 30px 0 rgb(0 0 0 / 40%);
}

/* 代码组：添加macOS风格的小圆点 */
.vp-code-group .tabs::before {
  content: ' ';
  position: absolute;
  top: 12px;
  left: 12px;
  height: 12px;
  width: 12px;
  background: #fc625d;
  border-radius: 50%;
  box-shadow: 20px 0 #fdbc40, 40px 0 #35cd4b;
}


/* 代码组：修正倒角、阴影、边距 */
.vp-code-group div[class*="language-"] {
  border-radius: 8px;
  box-shadow: none;
  padding-top: 0px;
}

/* 代码组：隐藏小圆点 */
.vp-code-group div[class*="language-"]::before {
  display: none;
}

/* 代码组：修正行号位置 */
.vp-code-group .line-numbers-mode .line-numbers-wrapper {
  padding-top: 20px;
}

/* 代码组：修正行号右侧竖线位置 */
.vp-code-group .line-numbers-mode .line-numbers-wrapper::after {
  top: 24px;
  height: calc(100% - 45px);
}



/* 代码组（无行号）：修正倒角、阴影、边距 */
.vp-code-group div[class*="language-"].vp-adaptive-theme {
  border-radius: 8px;
  box-shadow: none;
  padding-top: 0px;
}

/* 代码组（无行号）：隐藏小圆点 */
.vp-code-group div[class*="language-"].vp-adaptive-theme::before {
  display: none;
}
```

然后在 `index.css` 中引入生效

```css
/* .vitepress/theme/style/index.css */
/* 代码组 */
@import './vp-code-group.css';
```

输入：

````markdown
::: code-group

```sh [pnpm]
#查询pnpm版本
pnpm -v
```

```sh [yarn]
#查询yarn版本
yarn -v
```

:::
````

输出：

::: code-group

```sh [pnpm]
#查询pnpm版本
pnpm -v
```

```sh [yarn]
#查询yarn版本
yarn -v
```

:::

### 侧边栏样式美化

默认的侧边栏不太容易区分到底是目录还是文件，我们可以进行美化

在 `.vitepress/theme/style` 目录新建一个 `sidebarIcon.css` 文件

```markdown
.
├─ docs
│  ├─ .vitepress
│  │  └─ config.mts
│  │  └─ theme
│  │     └─ style
│  │        └─ index.css
│  │        └─ sidebarIcon.css
│  └─ index.md
└─ node_modules
```

复制下面代码，粘贴到 `sidebarIcon.css` 保存

```css
/* .vitepress/theme/style/sidebarIcon.css */

/* 侧边栏缩放 */
.group:has([role='button']) .VPSidebarItem.level-0 .items {
  padding-left: 15px !important;
  border-left: 1px solid var(--vp-c-divider);
  border-radius: 2px;
  transition: background-color 0.25s;
}

/* 侧边栏图标 */
/* 选中所有 .VPSidebarItem 元素，排除带有 .is-link 类的 */
#VPSidebarNav .VPSidebarItem:not(.is-link).collapsed >.item {
    display: inline-flex;
    align-items: center;  /* 垂直居中对齐图标和文本 */
}

/* 为所有不带 .is-link 的 .VPSidebarItem 折叠状态添加图标 */
#VPSidebarNav .VPSidebarItem:not(.is-link).collapsed >.item::before {
    content: '';
    background-image: url('/svg/document.svg'); /* 设置图标路径 */
    width: 16px;
    height: 16px;
    display: inline-block;
    vertical-align: middle;  /* 确保图标与文本垂直居中 */
    background-size: cover;
    margin-right: 4px;  /* 给图标和文本之间增加间距 */
}

#VPSidebarNav .VPSidebarItem:not(.is-link) >.item {
    display: inline-flex;
    align-items: center;  /* 垂直居中对齐图标和文本 */
}

/* 为所有不带 .is-link 的 .VPSidebarItem 非折叠状态添加图标 */
#VPSidebarNav .VPSidebarItem:not(.is-link) >.item::before {
    content: '';
    background-image: url('/svg/document-open.svg'); /* 设置图标路径 */
    width: 16px;
    height: 16px;
    display: inline-block;
    vertical-align: middle;  /* 确保图标与文本垂直居中 */
    background-size: cover;
    margin-right: 4px;  /* 给图标和文本之间增加间距 */
}

/* 选中带有 .is-link 的 .VPSidebarItem 的直接子元素 .item */
#VPSidebarNav .VPSidebarItem.is-link > .item {
    display: inline-flex;
    align-items: center;  /* 垂直居中图标和文字 */
}

/* 为选中的 .item 添加图标 */
#VPSidebarNav .VPSidebarItem.is-link > .item::before {
    content: '';
    background-image: url('/svg/file.svg'); /* 图标路径 */
    width: 16px;
    height: 16px;
    display: inline-block;
    vertical-align: middle;
    background-size: cover;
    margin-right: 4px;  /* 图标与文字间距 */
}
```

然后在 `index.css` 中引入生效

```css
/* .vitepress/theme/style/index.css */
/* 侧边栏样式美化 */
@import './sidebarIcon.css';
```

---

---
url: /01.指南/10.使用/20.样式增强.md
---

# 样式增强

Teek 提供了一些样式文件来增强 VitePress 和 Teek 的样式。

比如现在看到的一级标题渐变色，如果只是单纯安装 Teek 是不会有这个效果，需要引入 Teek 内置的样式增强文件来实现：

```ts
// .vitepress/theme/index.ts
import "vitepress-theme-teek/theme-chalk/tk-doc-h1-gradient.css";
```

> VitePress

* 首页图片背景添加彩色渐变动画
* 文章一级标题添加渐变色
* 侧边栏标题组字号加粗
* VitePress 内容容器样式增强
* 导航栏样式增强
* 侧边栏样式增强
* ...

> Teek

* 首页 Banner 描述添加渐变效果
* 首页 Banner 壁纸添加缩放动画
* 首页卡片悬停效果增强

这些样式增强文件并不会默认开启，而是需要您自行引入来开启。

## VitePress 样式增强

在 [vp-plus](https://github.com/Kele-Bingtang/vitepress-theme-teek/tree/main/packages/theme-chalk/src/vp-plus) 目录下查看所有的样式样式增强文件内容。

SCSS 文件如下（可能不全）：

```sh
vp-plus.
├─ aside.scss                   # 右侧目栏录文字悬停和激活样式
├─ blockquote.scss              # > 引用块样式
├─ brand-color-animation.scss   # 主题色定时切换
├─ code-block-mobile.scss       # 代码块移动端样式
├─ container-bg.scss            # 容器背景样式更改，内置 container-var
├─ container-flow.scss          # container-fluid + container-icon 组合
├─ container-fluid.scss         # 容器流体样式
├─ container-icon.scss          # 容器 ICON 样式
├─ container-left.scss          # 容器左侧框样式
├─ container-var.scss           # 容器 css var 变量
├─ container.scss               # container-bg + container-icon + container-var 组合
├─ doc-fade-in.scss             # 文章页淡入效果
├─ doc-h1-gradient.scss         # 文章一级标题渐变色
├─ index-rainbow.scss           # 首页图片彩虹动画
├─ mark.scss                    # 文章内容标记样式（mark 标签）
├─ nav-blur.scss                # 导航栏毛玻璃样式
├─ nav-search-button.scss       # 导航栏搜索按钮样式
├─ nav-switch-button.scss       # 导航栏深色、浅色模式切换按钮样式
├─ nav-translation.scss         # 导航栏国际化下拉样式
├─ nav.scss                     # nav-search-button  + nav-switch-button + nav-translation 组合
├─ scrollbar.scss               # 滚动条样式
├─ sidebar.scss                 # 侧边栏样式
├─ table.scss                   # 表格样式调整，去掉单元格之间的线条
```

在 `.vitepress/theme/index.ts` 按需引入（css 文件需要以 `tk-` 开头）：

```ts
// .vitepress/theme/index.ts
import "vitepress-theme-teek/theme-chalk/tk-code-block-mobile.css";
import "vitepress-theme-teek/theme-chalk/tk-sidebar.css";
import "vitepress-theme-teek/theme-chalk/tk-aside.css";
import "vitepress-theme-teek/theme-chalk/tk-nav.css";
import "vitepress-theme-teek/theme-chalk/tk-doc-h1-gradient.css";
import "vitepress-theme-teek/theme-chalk/tk-doc-fade-in.css";

// ...
```

如果您的项目有 `scss` 依赖，可以直接引入 `scss` 样式文件：

```ts
// .vitepress/theme/index.ts
import "vitepress-theme-teek/vp-plus/code-block-mobile.scss";
import "vitepress-theme-teek/vp-plus/sidebar.scss";
import "vitepress-theme-teek/vp-plus/aside.scss";
import "vitepress-theme-teek/vp-plus/nav.scss";
import "vitepress-theme-teek/vp-plus/doc-h1-gradient.scss";
import "vitepress-theme-teek/vp-plus/doc-doc-fade-in.scss";

// ...
```

## Teek 样式增强

在 [tk-plus](https://github.com/Kele-Bingtang/vitepress-theme-teek/tree/main/packages/theme-chalk/src/tk-plus) 目录下查看所有的样式样式增强文件内容。

样式文件如下（可能不全）：

```sh
tk-plus.
├─ banner-desc-gradient.scss        # 首页 Banner 描述添加渐变效果
├─ banner-full-img-scale.scss       # 首页 Banner 壁纸添加缩放动画
├─ home-card-hover.scss             # 首页卡片悬停效果
```

在 `.vitepress/theme/index.ts` 按需引入（css 文件需要以 `tk-` 开头）：

```ts
// .vitepress/theme/index.ts
import "vitepress-theme-teek/theme-chalk/tk-banner-desc-gradient.css";
import "vitepress-theme-teek/theme-chalk/tk-banner-full-img-scale.css";
import "vitepress-theme-teek/theme-chalk/tk-home-card-hover.css";

// ...
```

如果您的项目有 `scss` 依赖，可以直接引入 `scss` 样式文件：

```ts
// .vitepress/theme/index.ts
import "vitepress-theme-teek/tk-plus/banner-desc-gradient.scss";
import "vitepress-theme-teek/tk-plus/banner-full-img-scale.scss";
import "vitepress-theme-teek/tk-plus/home-card-hover.scss";

// ...
```

---

---
url: /10.配置/01.主题配置/25.页脚配置.md
---

# 页脚配置

## social

社交信息配置，通常为一个社交图标，点击后将会跳转到社交软件的个人主页。

::: tip
`social` 在卡片栏的博主信息区和页脚都会生效。
:::

::: code-group

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  social: [
    {
      icon: "mdi:github",
      name: "GitHub",
      link: "https://github.com/kele-bingtang",
    },
    {
      icon: "simple-icons:gitee",
      name: "Gitee",
      link: "https://gitee.com/kele-bingtang",
    },
  ],
});
```

```yaml [index.md]
---
tk:
  social:
    - icon: mdi:github
      name: GitHub
      link: https://github.com/kele-bingtang
    - icon: simple-icons:gitee
      name: Gitee
      link: https://gitee.com/kele-bingtang
---
```

```ts [更多配置项]
import type { IconProps } from "vitepress-theme-teek";

interface Social {
  /**
   * 名称，如果作用在 a 标签，则鼠标悬停显示名称，否则在页面文字显示
   */
  name?: string;
  /**
   * 图标地址
   *
   * @remark 与 iconType 配合使用
   */
  icon?: IconProps["icon"];
  /**
   * 图标类型
   *
   * @default 'svg'
   */
  iconType?: IconProps["iconType"];
  /**
   * 链接，点击后跳转到新窗口，如果不设置，则无法点击
   */
  link?: string;
  /**
   * img 标签的 alt，当 iconType 为 img 时生效
   */
  imgAlt?: string;
}
```

:::

`icon` 配置项为 [Icon 图标](/ecosystem/components/icon) 组件的配置项。

你也可以直接传入 Teek 内置的社交图标，如：

```ts {7}
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  social: [
    {
      icon: "icon-github",
      name: "GitHub",
      link: "https://github.com/kele-bingtang",
    },
  ],
});
```

更多社交图标请看 [社交图标](/guide/icon-use.html#社交图标-iconfont)。

## footerInfo

页脚配置，不会影响 VitePress 自带的页脚功能。

配置项中的 `Social` 类型为 [Social](#social) 配置项。

::: code-group

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  footerInfo: {
    // 页脚信息，支持 HTML 格式（位于主题版权上方）
    topMessage: ["下面的内容和图标都可以修改（本条内容也可以隐藏的）"],
    // 页脚信息，支持 HTML 格式（位于主题版权下方）
    bottomMessage: ["上面的内容和图标都可以修改（本条内容也可以隐藏的）"],
    // 主题版权配置
    theme: {
      show: true, // 是否显示主题版权，建议显示
      name: "", // 自定义名称
      link: "", // 自定义链接
    },
    // 博客版权配置
    copyright: {
      show: true, // 是否显示博客版权
      createYear: 2021, // 创建年份
      suffix: "天客 Blog", // 后缀
    },
    // ICP 备案信息配置
    icpRecord: {
      name: "桂ICP备2021009994号",
      link: "http://beian.miit.gov.cn/",
    },
    // 网络安全备案信息配置
    securityRecord: {
      name: "",
      link: "",
    },
  },
});
```

```yaml [index.md]
---
tk:
  footerInfo:
    message:
      - 下面的内容都可以修改（本条内容也可以隐藏的）
    copyright:
      createYear: 2021
      suffix: 天客 Blog
    icpRecord:
      name: 桂ICP备2021009994号
      link: http://beian.miit.gov.cn/
---
```

```ts [更多配置项]
interface FooterInfo {
  /**
   * 页脚信息，支持 HTML 格式（位于主题版权上方）
   */
  topMessage?: string | string[];
  /**
   * 页脚信息，支持 HTML 格式（位于主题版权下方）
   */
  bottomMessage?: string | string[];
  /**
   * 主题版权配置
   */
  theme?: Social & {
    /**
     * 是否显示
     */
    show?: boolean;
  };
  /**
   * 博客版权配置
   */
  copyright?: Social & {
    /**
     * 是否显示
     */
    show?: boolean;
    /**
     * 创建年份
     */
    createYear?: number | string;
    /**
     * 后缀
     */
    suffix?: string;
  };
  /**
   * ICP 备案信息配置
   */
  icpRecord?: Social;
  /**
   * 网络安全备案信息配置
   */
  securityRecord?: Social;
  /**
   * 自定义 HTML 片段
   */
  customHtml?: string;
}
```

:::

::: tip
虽然 Teek 允许通过 `theme.show = false` 不显示主题版权信息，但是希望大家还是展示 😄。
:::

## footerGroup

页脚信息组配置。

::: code-group

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  footerGroup: [
    {
      title: "外部链接",
      links: [
        { name: "示例 1", link: "https://vp.teek.top" },
        { name: "示例 2", link: "https://vp.teek.top" },
        { name: "示例 3", link: "https://vp.teek.top" },
      ],
    },
    {
      title: "内部链接",
      links: [
        { name: "快速开始", link: "/guide/quickstart" },
        { name: "配置简介", link: "/reference/config" },
      ],
    },
  ],
});
```

```yaml [index.md]
---
tk:
  footerGroup:
    - title: 外部链接
      links:
        - name: 示例 1
          link: https://vp.teek.top
        - name: 示例 2
          link: https://vp.teek.top
        - name: 示例 3
          link: https://vp.teek.top
    - title: 内部链接
      links:
        - name: 快速开始
          link: /guide/quickstart
        - name: 配置简介
          link: /reference/config
---
```

```ts [更多配置项]
import type { IconProps } from "vitepress-theme-teek";

interface FooterGroup {
  /**
   * 分组标题
   */
  title?: string;
  /**
   * 分组前图标
   */
  icon?: IconProps["icon"];
  /**
   * 分组下的链接数组
   */
  links: FooterGroupLink[];
}

interface FooterGroupLink {
  /**
   * 链接名称
   */
  name?: string;
  /**
   * 链接地址
   */
  link?: string;
  /**
   * 链接前图标
   */
  icon?: IconProps["icon"];
}
```

:::

---

---
url: /daily/博客文档/VitPress/2_页面.md
---

# 页面

---

---
url: /daily/高等数学/04_一元函数微分学的概念与计算.md
---

# 一元函数微分学的概念与计算

---

---
url: /daily/高等数学/05_一元函数微分学的几何应用.md
---

# 一元函数微分学的几何应用

---

---
url: /Java/JVM性能调优/02.JVM性能监控与调优/Arthas.md
---

# 应用诊断利器Arthas

官方文档：https://arthas.gitee.io/

## 使用

### 从 Maven 仓库下载

最新版本，点击下载：[![Arthas](Arthas.assets/v.svg)](https://arthas.aliyun.com/download/latest_version?mirror=aliyun)

### 从 Github Releases 页下载

https://github.com/alibaba/arthas/releases

### 用 as.sh 启动

解压后，在文件夹里有`as.sh`，直接用`./as.sh`的方式启动：

```bash
./as.sh
```

打印帮助信息：

```bash
./as.sh -h
```

或者直接下载启动脚本文件 `as.sh` 到当前目录，直接在 shell 下面执行`./as.sh`，就会进入交互界面

```bash
curl -L https://arthas.aliyun.com/install.sh | sh
```

### 用 arthas-boot 启动

或者在解压后，在文件夹里有`arthas-boot.jar`，直接用`java -jar`的方式启动：

```bash
java -jar arthas-boot.jar
```

打印帮助信息：

```bash
java -jar arthas-boot.jar -h
```

## 实战

https://www.cnblogs.com/itxiaoshen/p/15854197.html

https://www.cnblogs.com/yaoyu1983/p/17373522.html

https://blog.csdn.net/weixin\_40816738/article/details/123308455

https://wenku.baidu.com/view/9b750a2fff4ffe4733687e21af45b307e871f91e.html?fr=sogou&*wkts*=1694659165451

https://zhuanlan.zhihu.com/p/322094284

## 问题

1、The telnet port 3658 is used by process 28221 instead of target process 96492, you will connect to an unexpected process

```bash
[ERROR] The telnet port 3658 is used by process 28221 instead of target process 96492, you will connect to an unexpected process.
[ERROR] 1. Try to restart arthas-boot, select process 28221, shutdown it first with running the ‘stop’ command.
[ERROR] 2. Or try to stop the existing arthas instance: java -jar arthas-client.jar 127.0.0.1 3658 -c “stop”
[ERROR] 3. Or try to use different telnet port, for example: java -jar arthas-boot.jar --telnet-port 9998 --http-port -1
```

上次连接的进程，未正常退出。重新选择上次的连接，执行stop。

如果连接超时，查询端口进程，强制杀掉。

```shell
netstat -tln
netstat -tln | grep 端口
# 查看端口被哪个进程占用
lsof -i :端口
# 杀掉进程
kill -9 进程id
```

2、arthas报“Can not find java process”

```sh
[root@iZm5e1egdpr5bckrj8gty5Z arthas]# sudo java -jar arthas-boot.jar
[INFO] JAVA_HOME: /usr/lib/jvm/java-11-openjdk-11.0.21.0.9-2.0.3.al8.x86_64
[INFO] arthas-boot version: 3.7.2
[INFO] Can not find java process. Try to run `jps` command lists the instrumented Java HotSpot VMs on the target system.
Please select an available pid.
```

解决方式一：未使用root权限，sudo执行命令

解决方式二：找不到jps命令，默认安装的openjdk是不支持jps的，所以需要卸载Open JDK，
安装Oracle JDK：https://www.cnblogs.com/andy020/p/17511279.html
or
安装关联包：https://blog.51cto.com/u\_13890915/4653202

```sh
# 在提示的版本上增加 devel 
yum install -y java-11-openjdk-devel-11.0.21.0.9-2.0.3.al8.x86_64
```

## 参考：

https://www.cnblogs.com/lyn8100/p/17377845.html

https://zhuanlan.zhihu.com/p/519213445

https://www.bilibili.com/video/BV19k4y1k7o9/

https://zhuanlan.zhihu.com/p/584170583

https://www.cnblogs.com/jaigejiayou/p/15735690.html

https://zhuanlan.zhihu.com/p/429565721

---

---
url: /01.指南/01.简介/30.永久链接.md
---

# 永久链接

## 永久链接

VitePress 默认以 Markdown 文件路径作为链接访问，这会存在一个缺陷，当文件路径改变时，链接也会改变（再访问原来链接就会 404），在配置侧边栏、导航栏、各个文档链接引用、分享等场景时造成很大困扰。

因此需要给文件添加一个 **永久链接**，无论文件路径改变，访问链接不会改变。

Teek 使用 [vitepress-plugin-permalink](https://github.com/Kele-Bingtang/vitepress-theme-teek/blob/master/plugins/vitepress-plugin-permalink) 来实现永久链接功能。

插件已经内置到 Teek 中，你只需要在 Markdown 文件的 `frontmatter` 中添加如下内容：

```yaml
---
permalink: /guide/quickstart
---
```

这样就可以通过 `/guide/quickstart` 访问该页面了。

如果您不需要永久链接功能，请不配置 `permalink` 或者直接禁用该插件：

```ts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  vitePlugins: {
    permalink: false, // 禁用该插件
  },
});
```

## 永久链接方式

`vitepress-plugin-permalink` 插件提供两种方式实现永久链接：

1. `Proxy` 方式
2. `Rewrites` 方式

`Proxy` 方式不会影响文件路径，而是在访问文件路径时，通过代理（拦截）将其转换 `Permalink`，因此既可以通过文件路径访问，也可以通过 `Permalink` 访问。

`Rewrites` 方式在项目运行或者构建时，通过改变文件路径达到永久链接功能，你可以在构建的 `dist` 文件夹查看修改后的文件路径。

两者只能二选一，如果都配置，则以 `Rewrites` 方式为主。

Teek 默认为 `Proxy` 方式，如果替换为 `Rewrites` 方式，在 `config.mts` 里添加如下代码：

```ts
import { defineConfig } from "vitepress";
import { createRewrites } from "vitepress-theme-teek/config";

export default defineConfig({
  rewrites: createRewrites(/** options */),
});
```

`createRewrites` 函数支持除了传入 `vitepress-plugin-permalink` 的 [配置项](https://github.com/Kele-Bingtang/vitepress-theme-teek/blob/master/plugins/vitepress-plugin-permalink/src/types.ts)，也支持额外传入两个配置项：

* `srcDir`：VitePress 的 [srcDir](https://vitepress.dev/zh/reference/site-config#srcdir)，默认为 `.`，即当前项目的绝对目录
* `locales`：VitePress 的 [locales](https://vitepress.dev/zh/guide/i18n#internationalization)

如果没有传入配置项，则默认为从文档的根目录进行扫描。

## 侧边栏方式

Teek 使用 [vitepress-plugin-sidebar-resolve](https://github.com/Kele-Bingtang/vitepress-theme-teek/blob/master/plugins/vitepress-plugin-sidebar-resolve) 来实现自动生成侧边栏功能。

默认情况下，Teek 按照项目的目录结构生成侧边栏，如果你修改永久链接方式为 `rewrites`，则目录结构作为侧边栏将会失效，因此需要手动开启 `rewrites` 生成侧边栏。

通过 `resolveRule` 配置项来配置侧边栏生成规则：

* 当 `resolveRule` 为 `filePath`，则按照本地文件路径生成侧边栏
* 当 `resolveRule` 为 `rewrites`，则按照 `rewrites` 结果生成侧边栏

```ts
import { defineConfig } from "vitepress";
import { createRewrites } from "vitepress-theme-teek/config";

export default defineConfig({
  rewrites: createRewrites(),
  vite: {
    plugins: [
      Sidebar({
        resolveRule: "rewrites",
      }),
    ],
  },
});
```

如果 `resolveRule` 为 `rewrites`，但是没有 `rewrites` 配置，则按照 `filePath` 配置生成侧边栏。

## 什么是 rewrites

`rewrites` 是什么？，请看 VitePress 的 [路由重写](https://vitepress.dev/zh/guide/routing#route-rewrites) 描述。

这里简单说下个人理解，`filePath` 称之为 **本地文件路径**，`rewrites` 称之为 **运行文件路径**。

本地文件路径通俗易懂，那什么是运行文件路径？

我们访问的 VitePress 文档的链接地址就是 **运行文件路径**。VitePress 启动后，默认会将本地文件路径当作运行文件路径，但是我们可以通过 `rewrites` 将本地文件路径重写为新的文件路径，新的文件路径就会取代本地文件路径成为运行文件路径。

假设文件 `quick-start.md` 在本地路径为 `guide/quick-start.md`，在 `rewrites` 中添加如下配置：

```ts
import { defineConfig } from "vitepress";

export default defineConfig({
  rewrites: {
    "guide/quick-start.md": "/config/quick.md",
  },
});
```

此时该文件的运行文件路径为 `/config/quick` 而不是 `/guide/quick-start`。

---

---
url: /Java/Java开发技巧/03.高效编程/1_语法糖.md
---

# 语法糖

## 什么是语法糖？

语法糖是一种被设计成使代码更容易阅读和编写的语言特性。语法糖可以让我们在不影响程序的功能和性能的情况下，以更加简洁和自然的方式编写代码。这个概念最初由Peter J. Landin提出，后来又被其他编程语言采用和发展。

语法糖本身并不影响程序的运行效果，它只是一种语言的特性，可以帮助开发者更加方便地编写代码。例如，Java的for-each循环就是一种语法糖。在早期版本的Java中，我们必须使用传统的for循环来遍历数组或集合。但是，通过引入for-each循环，我们可以使用更加简洁的语法来实现同样的功能。这种语法糖使代码更加易于理解和维护，同时也可以减少代码量。

## Java中的语法糖

**for-each循环**

for-each循环是Java中最常见的语法糖之一。它可以让我们更加方便地遍历数组或集合。例如，我们可以使用以下代码来遍历一个字符串数组：

```
String[] arr = {"hello", "world", "java"};for (String s : arr) {    System.out.println(s);}
```

在这个例子中，我们使用for-each循环遍历了一个字符串数组，并且在控制台输出了每个元素。这比传统的for循环要简单得多，因为我们不需要手动维护循环计数器，也不需要使用数组下标来访问每个元素。

**自动拆箱和装箱**

自动拆箱和装箱是Java中的另一个常见语法糖。它可以让我们在基本类型和对象类型之间自由转换，而无需手动编写代码。例如，我们可以使用以下代码来声明一个整数变量：

```
int x = 10;
```

在这个例子中，我们声明了一个基本类型整数变量，因此x的类型为int。但是，我们也可以使用以下代码来声明一个整数对象：

```
Integer y = 10;
```

在这个例子中，我们声明了一个Integer对象，但是我们赋给它的值是一个整数字面量。这就是自动装箱的效果，它可以让我们将一个基本类型的值自动转换为一个对象类型。同样地，我们也可以使用以下代码来将一个整数对象转换为基本类型：

```
int z = y;
```

在这个例子中，我们将一个Integer对象赋给了一个int变量。这就是自动拆箱的效果，它可以让我们将一个对象类型自动转换为一个基本类型。

自动拆箱和装箱使代码更加简洁，因为我们不需要手动编写类型转换的代码。此外，它们也可以帮助我们避免一些常见的类型错误。

**try-with-resources语句**

try-with-resources语句是Java 7中引入的一个新特性，它可以帮助我们更加方便地管理资源。在之前的Java版本中，我们必须手动编写代码来关闭打开的资源，例如文件或网络连接。但是，使用try-with-resources语句，我们可以在try块中打开资源，并在try块结束时自动关闭它们。

以下是一个使用try-with-resources语句读取文件的例子：

```
try (BufferedReader br = new BufferedReader(new FileReader("file.txt"))) {
	String line;
	while ((line = br.readLine()) != null) {
		System.out.println(line);
	}
} catch (IOException e) {
	e.printStackTrace();
}
```

在这个例子中，我们使用try-with-resources语句打开了一个文件，并使用BufferedReader来读取文件内容。在try块结束时，文件会自动关闭，无需手动编写代码。此外，如果发生任何异常，catch块将处理它们。

try-with-resources语句使代码更加简洁，同时也可以帮助我们避免一些常见的资源管理错误。

**Lambda表达式**

Lambda表达式是Java 8中引入的一个新特性，它可以让我们更加方便地编写函数式代码。Lambda表达式可以作为一个函数的参数传递，同时也可以作为一个函数的返回值返回。以下是一个使用Lambda表达式的例子：

```java
List<Integer> list = Arrays.asList(1, 2, 3, 4, 5);
list.forEach(n -> System.out.println(n));
```

在这个例子中，我们使用Lambda表达式作为forEach方法的参数，它可以打印出列表中的每个元素。Lambda表达式可以让代码更加简洁，因为我们不需要手动编写匿名类来实现函数接口。

## 总结

语法糖是一种被设计成使代码更容易阅读和编写的语言特性。Java中有许多常见的语法糖，例如for-each循环、自动拆箱和装箱、try-with-resources语句和Lambda表达式。这些语法糖可以帮助我们编写更简洁、更易读、更可维护的代码，并可以帮助我们避免一些常见的错误。

除了Java之外，其他编程语言也有许多语法糖。例如，Python中的列表推导式、JavaScript中的箭头函数和Ruby中的代码块都是常见的语法糖。

**尽管语法糖可以使代码更加简洁和易读，但它们并不总是完美的。在某些情况下，语法糖可能会导致代码更难以理解和维护。此外，过度使用语法糖也可能导致代码变得过于复杂和难以阅读。**

因此，在编写代码时，我们应该根据具体情况权衡使用语法糖的利弊。对于简单的代码，使用语法糖可以使代码更加简洁和易读。对于复杂的代码，我们可能需要使用更明确的代码来保证代码的可读性和可维护性。

---

---
url: /daily/面试专栏/微服务相关/4_远程调用.md
---

# 远程调用

---

---
url: /01.指南/10.使用/10.摘要与封面.md
---

# 摘要与封面

首页的文章列表中，可以显示文章摘要和封面图。

## 文章摘要

文章摘要的设置有三种方式：

* 使用 `frontmatter.description` 属性
* 使用 `<!-- more -->` 注释
* 使用 `post.showCapture` 属性

如果三种方式都设置，只有一种生效，优先级为：使用 `frontmatter.description` 属性 > 使用 `<!-- more -->` 注释 > 使用 `post.showCapture` 属性

### frontmatter.description 属性

在文章页的 `frontmatter` 使用 `description` 来当作文章摘要。

```yaml {5}
---
title: Description 示例
date: 2024-10-27 23:14:44
permalink: /description/demo
description: Teek 译为科技者、探索者，是一个神秘而富有诗意的探索者形象，同时有自然、坚韧、品质感的意象，以及一个连接自然与未来的中性符号，中文为天客。
---
```

`description` 支持 HTML 文本，你可以添加一些 CSS 样式

```yaml {5}
---
title: Description 示例
date: 2024-10-27 23:14:44
permalink: /description/demo
description: 'Teek 译为 <span style="color: #395AE3;">科技者、探索者</span>，是一个神秘而富有诗意的探索者形象，同时有自然、坚韧、品质感的意象，以及一个连接自然与未来的中性符号，中文为 <span style="color: #395AE3;">天客</span>。'
---
```

::: warning
HTML 文本必须使用引号包起来，否则报错。
:::

### `<!-- more -->` 注释

可以在首页的文章列表中，显示文章摘要。

在 Markdown 文档的某个位置添加 `<!-- more -->` 注释，Teek 会自动将 `<!-- more -->` 前的文本作为摘要，并且隐藏 `h1 ~ h3` 标题。

```markdown
## 摘要示例

这是一段文章摘要，将会显示在首页的文章礼包，默认隐藏 `h1 ~ h3` 标题（摘要示例会被隐藏）。

<!-- more -->

## 其他内容

这是一段其他内容。
```

::: tip
文章摘要会按照文章页的样式渲染，所以可以使用容器、链接、图片等功能。

摘要的内容也是文章内容的一部分，会显示在文章页中。
:::

### post.showCapture 属性

Teek 支持截取 Markdown 文档里的文本作为文章摘要显示在文章列表上，默认截取前 300 个文本，但是实际显示的文本会根据文章列表的空间限制而改变。

在 Teek 的主题配置中，将 `post.showCapture` 设为 `true` 来启用该功能：

```ts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  post: {
    showCapture: true,
  },
});
```

::: tip
`post.showCapture` 开启后，文章列表的所有文章都会显示摘要内容。
:::

### 文章摘要位置

Teek 支持通过 `post.excerptPosition` 设为 `top` 或 `bottom` 来改变文章摘要的位置。

文章摘要位置默认在文章列表的基本信息下面（`bottom`），可以将 `post.excerptPosition` 设为 `top` 来将文章摘要放在基本信息上面：

```ts {4-6}
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  post: {
    excerptPosition: "top",
  },
});
```

## 文章封面图

Teek 支持在文章列表中显示封面图，需要在 `frontmatter` 中添加 `coverImg` 字段，值为图片链接。

```yaml {5}
---
title: Description 示例
date: 2024-10-27 23:14:44
permalink: /description/demo
coverImg: 图片地址
---
```

### 封面图模式

封面图支持 `default` 和 `full` 两个模式：

* `default` 模式下，封面图会显示在文章列表的右边
* `full` 模式下，封面图会变大，尽量铺满整个空间（图片尺寸要足够），且奇数的文章列表封面图会显示在右边，偶数的文章列表封面图显示在左边。

封面图模式默认为 `default`，如果使用 `full` 模式，需要在 Teek 的主题配置中将 `post.coverImgMode` 设为 `full`：

```ts {4-6}
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  post: {
    coverImgMode: "full",
  },
});
```

---

---
url: /Java/JVM性能调优/01.JVM概念/3_栈.md
---

# 栈

## 栈的作用

栈内存，主管程序的运行，生命周期和线程同步；
线程结束，栈内存也就释放了，对于栈来说，**不存在垃圾回收问题**。

## 栈存储的东西

8大基本类型、对象引用，实例的方法。

## 栈运行原理

### 简单结构图

![img](/assets/kuangstudy6dbadc75-e9f6-42a0-909e-e571bc37e230.jiTcNTdH.jpg)

### 详细结构图

![img](/assets/kuangstudya1f29d5c-99ea-46ff-954f-0de694823f69.DVBdUo_r.jpg)

## 栈+堆+方法区的交互关系

![img](/assets/kuangstudy16832064-9e2c-4de9-8778-19c3d3b9a687.BwdylMI9.jpg)

---

---
url: /01.指南/10.使用/35.站点统计.md
---

# 站点统计

Teek 集成了三种常见的站点统计工具：

* 百度分析 `Baidu Analytics`
* 谷歌分析 `Google Analytics`
* `Umami` 分析

让你可以轻松地在 VitePress 网站中集成并管理这些分析工具。无论是谷歌分析的强大功能，还是百度统计对中国市场的适配，或者是 `Umami` 的隐私友好型方案，都可以通过这个插件快速集成并使用。

## 百度统计

```ts
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  siteAnalytics: [
    {
      provider: "baidu",
      options: {
        id: "******",
      },
    },
  ],
});
```

### 获取 Baidu Analytics ID

1. 访问 [百度统计](https://tongji.baidu.com/) 网站
2. 使用百度账号登录或注册一个新账号
3. 登录后，点击页面上方的 `我的报告`-`使用设置`-`网站列表`
4. 输入你的网站 URL，选择适当的分类，然后点击 `保存`
5. 保存后，点击获取代码。会看到类似于 `https://hm.baidu.com/hm.js?******` 的内容
6. 复制链接中`******`部分

**参考链接：** [百度统计官方文档](https://tongji.baidu.com/web/help/article?id=175\&type=0)

## 谷歌分析

```ts
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  siteAnalytics: [
    {
      provider: "google",
      options: {
        id: "******",
      },
    },
  ],
});
```

### 获取 Google Analytics ID

1. 访问 [Google Analytics](https://analytics.google.com/) 网站
2. 登录到你的 Google Analytics 帐号
3. 创建一个新的 Google Analytics 账户，或者选择已有的账户
4. 在左下角点击 `Admin`（管理）
5. 在 `Account`（账户）列下，选择你的账户
6. 在 `Property`（属性）列下，选择你的站点，或者创建一个新的站点
7. 在 `Property Settings`（属性设置）中，找到 `Tracking Info`（跟踪信息）
8. 点击 `Tracking Code`（跟踪代码），你会看到类似 `G-XXXXXXX` 的 ID

**参考链接：**[Google Analytics 帮助文档](https://support.google.com/analytics/answer/9304153?hl=zh-Hans)

## Umami

```ts
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  siteAnalytics: [
    {
      provider: "umami",
      options: {
        id: "******",
        src: "https://******",
      },
    },
  ],
});
```

### 获取 Umami Analytics ID

#### 自建 Umami

1. 首先，你需要搭建 Umami 服务器。你可以参考 [Umami 文档](https://umami.is/docs/guides/hosting) 来进行安装
2. 在你搭建好的 Umami 实例中，登录到 Umami 仪表盘
3. 创建一个新的站点，并为其生成一个站点 ID
4. 获取该站点的 ID 后，就可以在你的网站代码中使用它进行跟踪

#### 使用公共 Umami 服务

1. 你也可以使用公共的 Umami 服务提供商。例如，Umami 提供了一些第三方的 Umami 实例，允许用户直接使用。
2. 获取到公共实例的 Umami ID 后，可以直接在代码中配置使用。

你的 Umami ID 应该类似于：`123abc456def`。

**参考链接：**

* [Umami 文档](https://umami.is/docs/guides/hosting)
* [Umami 公共服务](https://umami.is/)

## 多个站点统计

你可以配置多个站点统计，只要在 `siteAnalytics` 数组中添加多个对象即可。

```ts
// .vitepress/config.mts
const teekConfig = defineTeekConfig({
  siteAnalytics: [
    {
      provider: "baidu",
      options: {
        id: "******",
      },
    },
    {
      provider: "google",
      options: {
        id: "******",
      },
    },
  ],
});
```

---

---
url: /Java/JVM性能调优/02.JVM性能监控与调优/找出某个Java进程中最耗费CPU的Java线程.md
---

# 找出某个Java进程中最耗费CPU的Java线程

## 一、问题原因

现实企业级Java应用开发、维护中，有时候我们会碰到下面这些问题：

* OutOfMemoryError，内存不足
* 内存泄露
* 线程死锁
* 锁争用（Lock Contention）
* Java进程消耗CPU过高 ......

这些问题在日常开发、维护中可能被很多人忽视（比如有的人遇到上面的问题只是重启服务器或者调大内存，而不会深究问题根源），但能够理解并解决这些问题是Java程序员进阶的必备要求。

## 二、常用命令

找出某个Java进程中最耗费CPU的Java线程并定位堆栈信息，用到的命令有：ps、top、printf、jstack、grep。

### jps

jps主要用来输出JVM中运行的进程状态信息。语法格式如下：

```shell
jps [options] [hostid]
```

如果不指定hostid就默认为当前主机或服务器。

命令行参数选项说明如下：

```shell
-q 不输出类名,Jar名和传入main方法的参数

-m 输出传入main方法的参数

-l 输出main类或Jar的全限名

-v 输出传入JVM的参数
```

比如下面：

```shell
root@ubuntu:/# jps -m -l
2458 org.artifactory.standalone.main.Main /usr/local/artifactory-2.2.5/etc/jetty.xml
29920 com.sun.tools.hat.Main -port 9998 /tmp/dump.dat
3149 org.apache.catalina.startup.Bootstrap start
30972 sun.tools.jps.Jps -m -l
8247 org.apache.catalina.startup.Bootstrap start
25687 com.sun.tools.hat.Main -port 9999 dump.dat
21711 mrf-center.jar
```

### jstack

jstack主要用来查看某个Java进程内的线程堆栈信息。语法格式如下：

```shell
jstack [option] pid
jstack [option] executable core
jstack [option] [server-id@]remote-hostname-or-ip
```

命令行参数选项说明如下：

```shell
# 会打印出额外的锁信息
# 在发生死锁时可以用 jstack -l pid 来观察锁持有情况
# -m mixed mode，不仅会输出Java堆栈信息，还会输出C/C++堆栈信息（比如Native方法）
-l long listings
```

jstack可以定位到线程堆栈，根据堆栈信息我们可以定位到具体代码，所以它在JVM性能调优中使用得非常多。

## 三、排查步骤

Java开发中遇到线上服务器cpu占用过高问题如何解决？

1. top拿到cpu占用高的进程ID
2. 根据进程ID查看cpu占用高的线程ID
3. 将线程ID转换成16进制
4. jstack分析线程栈信息

下面我们来一个实例找出某个Java进程中最耗费CPU的Java线程并定位堆栈信息，用到的命令有ps、top、printf、jstack、grep。

***

**第一步：** 先找出Java进程ID，服务器上的Java应用名称为 wordcount.jar：

```shell
[root@storm-master home] ps -ef | grep wordcount
root    2860  2547 13 02:09 pts/0  00:02:03 java -jar wordcount.jar /home/input 3 
```

得到进程ID为 2860 。

***

**第二步：** 找出该进程内最耗费CPU的线程，可以使用如下3个命令，这里我们使用第3个命令得出如下结果：

1. ps -Lfp pid ： 即 ps -Lfp 2860
2. ps -mp pid -o THREAD, tid, time ：即 ps -mp 2860 -o THREAD,tid,time
3. top -Hp pid： 即 **top -Hp 2860**

这里用第三个命令：`top -Hp 2860`

```shell
top -Hp 2860
```

输出如下：

![img](/assets/ThreadTopCpuList.BdNoygab.png)

TIME列就是各个Java线程耗费的CPU时间，显然CPU时间最长的是ID为2968的线程，用

```shell
printf "%x\n" 2968
```

得到2968的十六进制值为b98，下面会用到。

***

**第三步：** 终于轮到jstack上场了，它用来输出进程2860的堆栈信息，然后根据线程ID的十六进制值grep，如下：

```shell
[root@storm-master home] jstack 2860 | grep b98 
"SessionTracker" prio=10 tid=0x00007f55a44e4800 nid=0xb53 in Object.wait() [0x00007f558e06c000 
```

可以看到CPU消耗在SessionTracker这个类的Object.wait()，于是就能很容易的定位到相关的代码了。

当然，可以 `jstack -l pid > /tmp/thread.txt` 在导出的文件中搜索，就可以定位到具体的线程类。

参数：-l long listings，打印出额外的锁信息，在发生死锁时可用jstack -l pid来观察锁持有情况

示例

```
jstack -l 7052 >> thread.txt
```

***

\*\*第四步：\*\*分析堆栈信息

将thread.txt下载到本地，使用IBM Thread and Monitor Dump Analyzer for Java打开分析

![img](/assets/20201218101730997.DMzPhkbp.png)

![img](/assets/20201218101816478.CMAuzhX7.png)

***

## 四、其他问题排查思路

**查看某进程及某线程占用 CPU 的例子**

* `jps`: 列出 java 进程,找到 pid.
* `pidstat -p pid -u 1 3 -u -t`: 查看 pid 的进程所有线程的 cpu 使用情况。
* `jstack -l pid > /tmp/thread.txt`: 导出指定 Java 应用的所有线程。

然后查看 `nid=xxx`(即第二步里线程号的线程),即可定位到某段代码。

**查看某进程及某线程占用 IO 的例子**

* `jps`: 列出 java 进程,找到 pid.
* `pidstat -p pid -u 1 3 -d -t`: 查看 pid 的进程所有线程的 IO 使用情况。
* `jstack -l pid > /tmp/thread.txt`: 导出指定 Java 应用的所有线程。

然后查看 `nid=xxx`(即第二步里线程号的线程),即可定位到某段代码。

https://www.cnblogs.com/dennyzhangdd/p/11585971.html

---

---
url: /常用框架/SpringBoot/SpringBoot程序开发/10_整合Spring配置.md
---

# 整合Spring配置

在进行Spring项目配置的时候，可以通过\*.xml文件配置，也可以通过Bean（@Configuration注解）配置。SpringBoot延续了Spring这一特点，在SpringBoot项目中依然可以使用配置文件定义。

## 一、xml文件配置方式

建立一个`MessageUtil`的工具类，在`MessageUtil`类中定义一个`getInfo()`方法，该方法的主要功能是返回一个提示信息。

```java
public class MessageUtil {

    public String getInfo() {
        return "www.xxl.cn";
    }

}
```

在`src/main/resources`目录中创建spring的子目录，并且建立`spring-util.xml`配置文件。

```xml
<?xml version="1.0" encoding="UTF-8"?>
<beans xmlns="http://www.springframework.org/schema/beans"
       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
       xsi:schemaLocation="http://www.springframework.org/schema/beans
       http://www.springframework.org/schema/beans/spring-beans.xsd">
    <!-- 定义bean配置 -->
    <bean id="messageUtil" class="com.xxl.springboot.init.util.MessageUtil"/>
</beans>

```

在Controller程序类中注入`MessageUtil`类对象，并且调用方法返回信息。

```java
@Autowired
private MessageUtil messageUtil; // XML配置注入

@GetMapping("/info")
public Object info() {
    // 调用方法
    return this.messageUtil.getInfo();
}
```

修改程序启动主类，定义要导入的Spring配置文件。

```java
/**
 * 启动类
 */
@SpringBootApplication
@ImportResource(locations = {"classpath:spring/spring-util.xml"})
public class SpringbootInitApplication {

    public static void main(String[] args) {
        SpringApplication.run(SpringbootInitApplication.class, args);
    }

}
```

本程序在定义启动主类时，利用`@ImportResource`注解导入了所需要的Spring配置文件，而后会自动将配置文件中定义bean对象注入到Controller类的属性中。

访问：http://127.0.0.1/xxl/info，返回：www.xxl.cn

## 二、Bean配置方式

SpringBoot强调的就是“零配置”，虽然其本身支持配置文件定义，但很明显xml的配置形式不是最好的。如果确定要引入其他配置，强烈建议使用Bean的配置形式来完成。

```java
/**
 * 配置bean
 *
 * @author xxl
 * @date 2024/12/31 22:28
 */
@Configuration
public class DefaultConfig {

    /**
     * 定义bean
     *
     * @return
     */
    @Bean(name = "messageUtil")
    public MessageUtil getMessageUtil() {
        // 配置bean对象
        return new MessageUtil();
    }
}
```

DefaultConfig定义在程序主类所在的子包之中，这样就可以在SpringBoot程序启动时自动扫描配置并进行加载。对于程序的主类，也就没有必要使用@ImportResource注解读取配置文件了。

> **提问：实际开发中使用配置文件还是使用Bean类配置？**
>
> 在编写SpringBoot项目的过程之中，是采用\*.xml配置更好，还是利用Bean类配置会更好？
>
> **回答：崇尚“零配置”的SpringBoot项目建议使用Bean配置。**
>
> 在SpringBoot项目中进行配置的时候，实际上有3种支持，按照优先选择顺序为：application.yml、Bean配置和\*.xml配置文件。大部分的配置都可以在application.yml（相当于传统项目中的profile配置作用）里面完成，但很多情况下会利用Bean类来进行扩展配置（本书主要使用此形式来作为扩展配置）。之所以提供\*.xml配置文件的支持，主要目的是帮助开发者用已有代码快速整合SpringBoot开发框架。

扩展—依赖注入：https://blog.csdn.net/qq\_37391214/article/details/106863892

---

---
url: /personal.md
---

# 支持这个项目

如果您正在使用这个项目并感觉这个项目给你带来帮助，或者是想支持我继续开发，您可以通过如下任意方式支持我：

* Star 并分享 [VitePress Theme Teek](https://github.com/Kele-Bingtang/vitepress-theme-teek) 🚀
* 通过以下二维码进行赞助，打赏作者一杯茶 🍵

谢谢！❤️

|                                   微信赞赏                                   |                               微信                               |                            支付宝                            |
| :--------------------------------------------------------------------------: | :--------------------------------------------------------------: | :----------------------------------------------------------: |
|  |  |  |

您的赞助将帮助 Teek：

* 维护项目的基础设施
* 投入更多时间进行开发
* 提供更好的技术支持
* 开发更多实用功能

## 致谢

❤️ 感谢支持这个项目的朋友，您的每一份帮助都让这个项目变得更好！

❤️ 感谢为这个项目贡献代码的朋友 → [Contributors](https://github.com/Kele-Bingtang/vitepress-theme-teek/graphs/contributors)

---

---
url: /Java/解决方案/支付解决方案/支付宝支付API对接指南/4_支付宝支付成功回调通知.md
---

# 支付宝支付成功回调通知

当用户支付成功后，支付宝会根据我们设置的服务器通知地址，通知我们用户的支付结果，我们需要对结果进行验签，防止用户伪造，然后再提取需要的参数，来完善我们的业务逻辑，例如更改订单状态、填充订单支付成功时间和支付宝生成的交易号等等。

### 提取返回参数实体类对象

支付宝支付成功[回调参数说明](https://opendocs.alipay.com/open/270/105902)，我这里截取了项目常用到的参数，如果你有特殊需要可以去[异步通知参数说明](https://opendocs.alipay.com/open/270/105902)查询。

```java
import cn.hutool.core.date.DateUtil;

@Data
@NoArgsConstructor
@AllArgsConstructor
public class AliPayCallbackDto {

    /**
     * 商户订单号
     */
    private String orderId;
    /**
     * 支付宝交易号
     */
    private String waterId;
    /**
     * 交易状态
     */
    private String tradeStatus;

    /**
     * 付款时间
     */
    private Date payTime;

    public AliPayCallbackDto(HttpServletRequest request) {
        this.orderId = new String(request.getParameter("out_trade_no").getBytes(StandardCharsets.ISO_8859_1), StandardCharsets.UTF_8);
        this.waterId = new String(request.getParameter("trade_no").getBytes(StandardCharsets.ISO_8859_1), StandardCharsets.UTF_8);
        this.tradeStatus = new String(request.getParameter("trade_status").getBytes(StandardCharsets.ISO_8859_1), StandardCharsets.UTF_8);
        String payTime = new String(request.getParameter("gmt_payment").getBytes(StandardCharsets.ISO_8859_1), StandardCharsets.UTF_8);
        // 调用Hutool工具包将字符串转换成日期类型
        this.payTime = DateUtil.parse(payTime);
    }
}
```

### 支付宝验签方法

```java
@Slf4j
@Component
public class AliPayUtils {
    @Autowired
    private AliPayConfig aliPayConfig;

    @Autowired
    private AlipayClient alipayClient;


    /**
     * 支付宝支付成功回调通知验签
     * @return true验签通过
     */
    public boolean rsaCheck(HttpServletRequest request) {
        // 获取支付宝POST过来反馈信息，对数据进行验签
        Map<String, String> params = new HashMap<>();
        Map requestParams = request.getParameterMap();
        for (Iterator iter = requestParams.keySet().iterator(); iter.hasNext(); ) {
            String name = (String) iter.next();
            String[] values = (String[]) requestParams.get(name);
            String valueStr = "";
            for (int i = 0; i < values.length; i++) {
                valueStr = (i == values.length - 1) ? valueStr + values[i] : valueStr + values[i] + ",";
            }
            params.put(name, valueStr);
        }

        try {
            return AlipaySignature.rsaCheckV1(params, aliPayConfig.getAlipayPublicKey(), "UTF-8", aliPayConfig.getSignType());
        } catch (AlipayApiException e) {
            throw new RuntimeException(e);
        }
    }
}
```

### 回调接口开发

```java
@Slf4j
@RestController
@RequestMapping("/api/v1/ali")
public class AlipayController {

    @Autowired
    private AliPayUtils aliPayUtils;

    @PostMapping("/ali/pay/callback")
    public String courseAliCallback(HttpServletRequest request, HttpServletResponse response) {
        // 获取支付宝POST过来反馈信息，对数据进行验签
        boolean flag = aliPayUtils.rsaCheck(request);

        if (flag) {
            AliPayCallbackDto dto = new AliPayCallbackDto(request);
            if ("TRADE_SUCCESS".equals(dto.getTradeStatus()) || "TRADE_FINISHED".equals(dto.getTradeStatus())) {
                // TODO 执行你的业务逻辑
            }
            return "success";
        } else {
            //验证失败
            return "fail";
        }
    }
}
```

---

---
url: /Java/解决方案/支付解决方案/支付宝支付API对接指南/2_支付宝支付环境搭建及配置.md
---

# 支付宝支付环境搭建及配置

获取到appid、公私钥后，我们就可以进行开发环境的配置了。

### 引入依赖

```xml
<dependency>
    <groupId>com.alipay.sdk</groupId>
    <artifactId>alipay-sdk-java</artifactId>
    <version>4.22.0.ALL</version>
</dependency>
```

### 配置文件application.yml

```yaml
ali:
  appid: 20210*****24906
  private-key: # 一般最长的是私钥
  alipay-public-key: # 支付宝的应用公钥
  notify-domain: https://luckilyxxl.mynatapp.cc # 你自己的回调地址，可以使用接口回调软件进行测试
  gateway-host: https://openapi.alipay.com #支付宝统一网关
  sign-type: RSA2 #加密方式
  return-domain: https://luckilyxxl.xyz #H5或其他方式支付成功支付宝自动跳转地址
```

### 读取配置文件信息

```java
import lombok.Data;
import org.springframework.boot.context.properties.ConfigurationProperties;
import org.springframework.context.annotation.Configuration;
import org.springframework.context.annotation.PropertySource;

@Data
@Configuration
@PropertySource("classpath:application.yml") 
@ConfigurationProperties(prefix = "ali")
public class AliPayConfig {

    private String appid;

    private String privateKey;

    private String alipayPublicKey;

    /**
     * 接收结果通知地址
     */
    private String notifyDomain;

    /**
     * 支付宝统一网关
     */
    private String gatewayHost;

    /**
     * 加密方式
     */
    private String signType;
    
    /**
     * H5或其他方式支付成功支付宝自动跳转地址
     */
    private String returnDomain;
}
```

### 将Client实例存入Bean中

```java
import com.alipay.api.AlipayClient;
import com.alipay.api.DefaultAlipayClient;
import com.xk857.alipay.config.AliPayConfig;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.context.annotation.Bean;
import org.springframework.stereotype.Component;

@Component
public class AliPayClientBean {

    @Autowired
    private AliPayConfig aliPayConfig;

    @Bean
    public AlipayClient getAlipayClient() {
        return new DefaultAlipayClient(aliPayConfig.getGatewayHost(), aliPayConfig.getAppid(), aliPayConfig.getPrivateKey(), "json", "UTF-8", aliPayConfig.getAlipayPublicKey(), aliPayConfig.getSignType());
    }
}
```

---

---
url: /Java/解决方案/支付解决方案/支付宝支付API对接指南/3_支付宝支付请求工具类封装.md
---

# 支付宝支付请求工具类封装

相关地址（服务器端回调地址、客户端支付成功/失败回调都在）的参数我们都使用枚举类进行管理，请求参数单独封装成一个对象进行参数传递，最后对支付功能再进行进一步的封装。

### 创建服务器回调地址枚举类

回调地址我们使用枚举类进行统一管理。

```java
public enum AlipayNotifyUrlEnum {

    TEST_URL("/api/v1/ali/callback"),

    COURSE_URL("/api/v1/ali/course/callback");

    private String url;
    
    AlipayNotifyUrlEnum(String url) {
        this.url = url;
    }
    public String getUrl() {
        return url;
    }
}
```

### 创建客户端支付成功回调地址枚举类

如果是网页支付，那么支付成功时，客户端会返回我们回调的地址页面，我们对这个地址也创建枚举类。

```java
public enum AlipayReturnUrlEnum {

    COURSE_URL("/course/pay/success?orderId=");

    private String url;

    AlipayReturnUrlEnum(String url) {
        this.url = url;
    }

    public String getUrl() {
        return url;
    }
}
```

### 创建客户端支付失败回调地址枚举类

用户付款中途退出返回商户网站的地址，也就是用户没有真的付款所跳转的地址。

```java
public enum AlipayReturnFailUrlEnum {

    COURSE_URL("/course/pay/order?orderId=")
    ;

    private String url;

    AlipayReturnFailUrlEnum(String url) {
        this.url = url;
    }

    public String getUrl() {
        return url;
    }
}
```

### 支付宝请求参数传递类

```java
@Data
public class AlipayParam {

    /**
     * 服务器端回调地址，支付成功后支付宝将支付结果按这个地址进行通知
     */
    private AlipayNotifyUrlEnum callbackEnum;

    /**
     * 客户端支付成功回调地址，用户支付成功后跳转回网页或APP
     */
    private String returnUrl;

    /**
     * 订单id
     */
    private String orderId;

    /**
     * 支付金额
     */
    private BigDecimal price;

    /**
     * 商品标题或其他标注售卖物品的信息
     */
    private String title;

    /**
     * 客户端用户支付中途取消支付后跳转回网页或APP
     */
    private String returnFailUrl;
}
```

### 发送H5支付请求工具类

H5支付请求返回的是HTML页面，我们把页面直接返回出去即可。

```java
@Slf4j
@Component
public class AliPayUtils {
    @Autowired
    private AliPayConfig aliPayConfig;

    @Autowired
    private AlipayClient alipayClient;

    public void aliPayH5(AlipayParam alipayParam, HttpServletResponse httpResponse) {
        // 1.创建阿里云支付请求对象
        AlipayTradeWapPayRequest request = new AlipayTradeWapPayRequest();
        request.setNotifyUrl(aliPayConfig.getNotifyDomain() + alipayParam.getCallbackEnum().getUrl());
        request.setReturnUrl(aliPayConfig.getReturnDomain() + alipayParam.getReturnUrl());

        // 2.获取支付统一JSON格式请求参数
        JSONObject bizContent = getBizContent(alipayParam, aliPayConfig);
        // 3.JSON中设置支付方式是H5支付
        bizContent.put("product_code", "QUICK_WAP_WAY");
        // 4.将json数据插入到请求体中
        request.setBizContent(bizContent.toString());
        AlipayTradeWapPayResponse response = null;
        try {
            // 5.注意请求方式
            response = alipayClient.pageExecute(request);
            if (response.isSuccess()) {
                //直接将完整的表单html输出到页面
                httpResponse.setContentType("text/html;charset=UTF-8");
                httpResponse.getWriter().write(response.getBody());
                httpResponse.getWriter().flush();
                httpResponse.getWriter().close();
            } else {
                throw new DefaultException(ResultEnum.ERROR);
            }
        } catch (AlipayApiException | IOException e) {
            e.printStackTrace();
            throw new DefaultException(ResultEnum.ERROR);
        }
    }

    /**
     * 封装支付统一请求参数，订单号、支付金额、商品标题等数据都是各个都需要的
     *
     * @return 请求信息JSON对象
     */
    private JSONObject getBizContent(AlipayParam alipayParam, AliPayConfig aliPayConfig) {
        JSONObject bizContent = new JSONObject();
        bizContent.put("out_trade_no", alipayParam.getOrderId());
        bizContent.put("total_amount", alipayParam.getPrice());
        // 商品标题如果长度超过100则进行截取
        bizContent.put("subject", alipayParam.getTitle().length() > 100 ? alipayParam.getTitle().substring(0, 100) : alipayParam.getTitle());
        bizContent.put("quit_url", aliPayConfig.getReturnDomain() + alipayParam.getReturnFailUrl());
        return bizContent;
    }
}
```

### H5支付接口开发

直接在浏览器输入请求地址即可，如果有内网穿透更好，直接手机浏览器访问接口地址，即可自动跳转到支付宝。

```java
import com.baomidou.mybatisplus.core.toolkit.IdWorker;
import com.xk857.alipay.domain.AlipayParam;
import com.xk857.alipay.enums.AlipayNotifyUrlEnum;
import com.xk857.alipay.enums.AlipayReturnFailUrlEnum;
import com.xk857.alipay.enums.AlipayReturnUrlEnum;
import com.xk857.alipay.util.AliPayUtils;

import javax.servlet.http.HttpServletRequest;
import javax.servlet.http.HttpServletResponse;


@Slf4j
@RestController
@RequestMapping("/api/v1/ali")
public class AlipayController {

    @Autowired
    private AliPayUtils aliPayUtils;

    /**
     * H5测试
     */
    @GetMapping("/h5")
    public void submitOrderAliPay(HttpServletRequest request, HttpServletResponse response) {
        AlipayParam alipayParam = new AlipayParam();
        alipayParam.setCallbackEnum(AlipayNotifyUrlEnum.TEST_URL);
        String orderId = IdWorker.getIdStr();
        log.info("H5生成的订单id是:{}",orderId);
        alipayParam.setReturnUrl(AlipayReturnUrlEnum.COURSE_URL.getUrl()+orderId);
        alipayParam.setOrderId(orderId);
        alipayParam.setPrice(new BigDecimal("0.05"));
        alipayParam.setTitle("5分钱测试订单");
        alipayParam.setReturnFailUrl(AlipayReturnFailUrlEnum.COURSE_URL.getUrl()+orderId);
        aliPayUtils.aliPayH5(alipayParam,response);
    }
}
```

补充：[网页/移动应用文档指引 - 支付宝文档中心](https://opendocs.alipay.com/open)

---

---
url: /01.指南/目录.md
---


---

---
url: /15.主题开发/10.主题配置.md
---

# 主题配置

在主题开发中，往往需要提供一些配置来丰富主题的功能，最简单的是和 VitePress 的 `themeConfig` 配置合在一起：

```ts {8}
// .vitepress/config.mts
import { defineConfig } from "vitepress";

export default defineConfig({
  // ...
  themeConfig: {
    // vitepress 配置
    // 自定义主题配置
  },
});
```

然后在组件里通过 `useData` 获取 `themeConfig` 的内容：

```vue
<script setup lang="ts">
import { useData } from "vitepress";

// 获取 themeConfig
const { theme } = useData;

// 获取自定义主题配置项
const xx = theme.value.xx;
</script>

<template></template>
```

这种方式仅适合简单的主题，当主题需要添加一个 `head` 或者 `vite` 插件，需要让用户修改 VitePress 的配置，这样极其不方便。

因此可以先将主题配置抽离出来，然后使用 `extends` 来合并主题配置。

## extends 合并配置

VitePress 提供了 `extends` 来合并外界传来的配置项，比如外界的配置提供了部分 `head` 内容，并且在 VitePress 也配置了 `head`，则最终合并为一个全新的 head，而不是覆盖。

:::tip
VitePress 的配置项只有为对象/数组时可以合并，如果配置项为一个固定的值或者函数，则以 VitePress 的配置项为主。
:::

`extends` 合并主题配置的使用方式如下：

```ts {4,8}
import { defineConfig } from "vitepress";

// 主题配置
const teekConfig = {};

// vitepress 配置
export default defineConfig({
  extends: teekConfig,
  // ...
  themeConfig: {
    // ...
  },
});
```

在 VitePress 配置中通过 `extends` 可以将主题配置合并到 VitePress 配置里，也就是说完全可以在主题配置里添加 VitePress 的配置项，但是不能反过来，如：

::: code-group

```ts [各自配置]
// .vitepress/config.mts
import { defineConfig } from "vitepress";

// 主题配置
const myThemeConfig = { themeConfig: { teekTheme: true } };

// VitePress 配置
export default defineConfig({
  extends: myThemeConfig,
  base: "/",
});
```

```ts [统一配置]
// .vitepress/config.mts
import { defineConfig } from "vitepress";

// 主题配置 + VitePress 配置
const myThemeConfig = { base: "/", themeConfig: { teekTheme: true } };

export default defineConfig({
  extends: myThemeConfig,
});
```

:::

## 函数式构建配置

在主题配置里，如果要使用 Vite 插件或者想要修改 VitePress 默认的配置，则可以提供一个函数来返回主题配置：

```ts
// myThemeConfig.ts
import type { DefaultTheme, UserConfig } from "vitepress";
import type { PluginOption } from "vite";

interface ThemeConfig {
  useTheme?: boolean; // 是否开启主题
  // ...
}

export default function getThemeConfig(config: ThemeConfig & UserConfig<DefaultTheme.Config> = {}): UserConfig {
  // 获取用户的配置，进行逻辑处理
  const { useTheme = true, ...themeConfig } = config;

  if (!useTheme) return {};

  return {
    // ignoreDeadLinks 默认值修改为 true，当用户在 VitePress 手动改为 false 才为 false
    ignoreDeadLinks: true,
    // 添加主题需要的 head 信息
    head: [],
    vite: {
      // 添加主题需要的 Vite 插件
      plugins: [],
    },
    themeConfig,
  };
}
```

在 `.vitepress/config.mts` 引入该函数：

```ts
import { defineConfig } from "vitepress";
import getThemeConfig from "myThemeConfig";

const myThemeConfig = getThemeConfig({ useTheme: false });

// VitePress 配置
export default defineConfig({
  extends: myThemeConfig,
  // ...
});
```

---

---
url: /01.指南/10.使用/15.主题增强.md
---

# 主题增强

Teek 内置了 4 种布局模式、8 种主题风格可供切换，请将鼠标移到右上角的主题增强面板进行体验。

## 布局模式

4 种布局模式分别为：

* `fullWidth`：全部展开，使侧边栏和内容区域占据整个屏幕的全部宽度
* `sidebarWidthAdjustableOnly`：全部展开，侧边栏宽度可调，但内容区域宽度不变，调整后的侧边栏将可以占据整个屏幕的最大宽度
* `bothWidthAdjustable`：全部展开，侧边栏和内容区域宽度均可调，调整后的侧边栏和内容区域将可以占据整个屏幕的最大宽度
* `original`：原始的 VitePress 默认布局宽度

可以通过主题配置的 `themeEnhance.layoutSwitch.defaultMode` 来覆盖默认值，默认为 `original`。

```ts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  themeEnhance: {
    layoutSwitch: {
      defaultMode: "bothWidthAdjustable",
    },
  },
});
```

当处于 `bothWidthAdjustable` 布局模式下，您可以控制默认的页面最大宽度和内容最大宽度。

```ts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  themeEnhance: {
    layoutSwitch: {
      defaultMode: "bothWidthAdjustable",
      defaultDocMaxWidth: 90,
      defaultPageMaxWidth: 90,
    },
  },
});
```

::: tip
`defaultDocMaxWidth` 和 `defaultPageMaxWidth` 的值仅限 0-100。
:::

如果希望隐藏布局模式切换功能（不允许用户手动切换），可以设置 `themeEnhance.layoutSwitch.disabled` 为 `true`：

```ts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  themeEnhance: {
    layoutSwitch: {
      disabled: true,
    },
  },
});
```

## 主题风格

8 种主题风格分别为 `vp-default`、`vp-green`、`vp-yellow`、`vp-red`、`ep-blue`、`ep-green`、`ep-yellow`、`ep-red`。

其中 `vp-` 开头的使用 VitePress 内置的颜色，`ep-` 开头的使用 ElementPlus 的颜色。

可以通过主题配置的 `themeEnhance.themeColor.defaultColorName` 来覆盖默认值，默认为 `vp-default`。

```ts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  themeEnhance: {
    themeColor: {
      defaultColorName: "ep-blue",
    },
  },
});
```

如果希望隐藏主题风格切换功能（不允许用户手动切换），可以设置 `themeEnhance.themeColor.disabled` 为 `true`：

```ts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  themeEnhance: {
    themeColor: {
      disabled: true,
    },
  },
});
```

## 聚光灯&#x20;

可以通过主题配置的 `themeEnhance.spotlight.defaultValue` 来覆盖默认值，默认为 `true`。

```ts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  themeEnhance: {
    spotlight: {
      defaultValue: true,
    },
  },
});
```

如果希望隐藏聚光灯功能（不允许用户手动切换），可以设置 `themeEnhance.spotlight.disabled` 为 `true`：

```ts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  themeEnhance: {
    spotlight: {
      disabled: true,
    },
  },
});
```

## 文档单独配置

Teek 支持在 Markdown 的 `frontmatter` 单独进行如下配置来覆盖全局的设置。

```yaml
---
layoutMode: bothWidthAdjustable
themeColorName: ep-blue
spotlight: false
---
```

## 功能参考

* [阅读增强](https://github.com/nolebase/integrations/blob/main/packages/vitepress-plugin-enhanced-readabilities/README.md)

---

---
url: /10.配置/40.主题增强拓展.md
---

# 主题增强拓展

在 [主题增强](/guide/theme-enhance) 中介绍了主题风格的使用，而不同的用户有不同的审美需求，因此 Teek 支持用户修改自带的主题模式，也可以拓展全新的主题模式。

## 主题风格

### 主题风格修改

Teek 使用 VitePress 的 `css var` 变量来实现主题风格。当切换尺寸时，Teek 会修改 `html` 标签的 `theme-color` 属性，进而改变 `css var` 变量，从而达到修改主题风格的效果。

如果觉得 Teek 提供的主题风格不符合自己的风格，可以修改不同 `theme-color` 下对应的 `css var` 变量来达到目的。

Teek 主题风格的 `css var` 变量如下：

```scss
@use "../mixins/function" as *;

html[theme-color="vp-green"] {
  --vp-c-brand-1: var(--vp-c-green-1);
  --vp-c-brand-2: var(--vp-c-green-2);
  --vp-c-brand-3: var(--vp-c-green-3);
  --vp-c-brand-soft: var(--vp-c-green-soft);
}

/* VitePress 黄色 */
html[theme-color="vp-yellow"] {
  --vp-c-brand-1: var(--vp-c-yellow-1);
  --vp-c-brand-2: var(--vp-c-yellow-2);
  --vp-c-brand-3: var(--vp-c-yellow-3);
  --vp-c-brand-soft: var(--vp-c-yellow-soft);
}

/* VitePress 红色 */
html[theme-color="vp-red"] {
  --vp-c-brand-1: var(--vp-c-red-1);
  --vp-c-brand-2: var(--vp-c-red-2);
  --vp-c-brand-3: var(--vp-c-red-3);
  --vp-c-brand-soft: var(--vp-c-red-soft);
}

/* element plus 蓝色 */
html[theme-color="ep-blue"] {
  --vp-c-brand-1: #{getCssVar("el-color-primary")};
  --vp-c-brand-2: #{getCssVar("el-color-primary-light-3")};
  --vp-c-brand-3: #{getCssVar("el-color-primary-light-5")};
  --vp-c-brand-soft: #{getCssVar("el-color-primary-light-9")};
}

/* element plus 绿色 */
html[theme-color="ep-green"] {
  --vp-c-brand-1: #{getCssVar("el-color-success")};
  --vp-c-brand-2: #{getCssVar("el-color-success-light-3")};
  --vp-c-brand-3: #{getCssVar("el-color-success-light-5")};
  --vp-c-brand-soft: #{getCssVar("el-color-success-light-9")};
}

/* element plus 黄色 */
html[theme-color="ep-yellow"] {
  --vp-c-brand-1: #{getCssVar("el-color-warning")};
  --vp-c-brand-2: #{getCssVar("el-color-warning-light-3")};
  --vp-c-brand-3: #{getCssVar("el-color-warning-light-5")};
  --vp-c-brand-soft: #{getCssVar("el-color-warning-light-9")};
}

/* element plus 红色 */
html[theme-color="ep-red"] {
  --vp-c-brand-1: #{getCssVar("el-color-danger")};
  --vp-c-brand-2: #{getCssVar("el-color-danger-light-3")};
  --vp-c-brand-3: #{getCssVar("el-color-danger-light-5")};
  --vp-c-brand-soft: #{getCssVar("el-color-danger-light-9")};
}
```

::: tip
`--vp-c-brand-1` 为 VitePress 的核心主题色，在修改或者拓展时，您应该考虑优先修改该 var 变量。
:::

您可以创建一个 `css` 文件来修改上面提供的变量，如在 `vp-green` 主题风格下修改 `--vp-c-brand-1` 变量：

```css [tk-theme-color.css]
html[theme-color="vp-green"] {
  --vp-c-brand-1: #395ae3;
}
```

在 `.vitepress/theme/index.ts` 文件引入该 `css` 文件：

```ts [index.ts] {3}
import Teek from "vitepress-theme-teek";
import "vitepress-theme-teek/index.css";
import "./style/tk-theme-color.css";

export default {
  extends: Teek,
};
```

这样在 `vp-green` 主题风格下，`--vp-c-brand-1` 变量被设置为 `#395AE3`。

### 主题风格拓展

在右上角的主题增强面板可以看到 Teek 内置的 8 个主题风格，除此之外，Teek 支持额外追加自定义的主题风格。

有两种方式自定义主题风格

#### 预设主题变量

首先在 `scss` 文件定义自定义主题风格的 `css var` 变量

如添加 `github` 主题风格：

```scss
html[theme-color="github-blue"] {
  --vp-c-brand-1: xx;
  --vp-c-brand-2: xx;
  --vp-c-brand-3: xx;
  --vp-c-brand-soft: xx;
  // ...... 修改其他 VitePress 提供的 css var 变量
}

html[theme-color="github-green"] {
  --vp-c-brand-1: xxx;
  --vp-c-brand-2: xxx;
  --vp-c-brand-3: xxx;
  --vp-c-brand-soft: xxx;
  // ...... 修改其他 VitePress 提供的 css var 变量
}
```

在 `.vitepress/theme/index.ts` 文件引入该 `css` 文件：

```ts [index.ts] {3}
import Teek from "vitepress-theme-teek";
import "vitepress-theme-teek/index.css";
import "./style/tk-theme-color.css";

export default {
  extends: Teek,
};
```

然后通过主题配置的 `themeEnhance.themeColor.append` 追加自定义主题风格，如：

```ts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  themeEnhance: {
    append: {
      themeStyleAppend: [
        {
          label: "Github 主题", // 主题组名称
          tip: "Github 主题", // 主题组提示信息，鼠标悬停时显示
          options: [
            { label: "风格 1", value: "github-blue" },
            { label: "风格 2", value: "github-green" },
          ],
        },
      ],
    },
  },
});
```

这样您就可以在主题增强面板里看到注册的 `Github` 主题。

#### 预设主色&#x20;

第一种方式需要提前在 `html` 的 `theme-color` 属性上提前预设好 4 个颜色，然后在主题增强面板点击主题风格更新 `html` 的 `theme-color` 值，从而修改 `css var` 变量达到目的。

第二种方式更加简单，只需要指定一个主色 `color`，Teek 通过内置的算法自动计算出其他颜色。

```ts {11-18}
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  themeEnhance: {
    themeColor: {
      append: [
        {
          label: "博客扩展主题", // 主题组名称
          tip: "博客扩展主题", // 主题组提示信息，鼠标悬停时显示
          options: [
            { label: "紫罗兰", value: "violet", color: "#7166f0" },
            { label: "珊瑚粉", value: "coral-pink", color: "#ff6b6b" },
            { label: "天蓝", value: "sky-blue", color: "#00bbf9" },
            { label: "蓝绿", value: "blue-green", color: "#00f5d4" },
            { label: "石板灰", value: "slate-gray", color: "#708090" },
            { label: "粉红", value: "pink", color: "#f15bb5" },
            { label: "黄绿", value: "yellow-green", color: "#8ac926" },
            { label: "橙红", value: "orange-red", color: "#ff9e6b" },
          ],
        },
      ],
    },
  },
});
```

### 主题风格扩散

您可以在主题增强面板看到 扩散 单选框，激活后 Teek 将根据 `--vp-c-brand-1` 自动计算出其他颜色，然后扩散到全局。

## 主题尺寸

Teek 使用 `css var` 变量来实现主题尺寸。当切换尺寸时，Teek 会修改 `html` 标签的 `theme-size` 属性，进而改变 `css var` 变量，从而达到修改主题尺寸的效果。

如果觉得 Teek 提供的主题尺寸不符合自己的风格，可以修改不同 `theme-size` 下对应的 `css var` 变量来达到目的。

::: tip
主题尺寸仅作用在 Teek 的首页已经自定义页，不会修改 VitePress 的默认主题。
:::

Teek 主题尺寸的 `css var` 变量如下：

```scss
@use "../mixins/mixins" as *;
@use "../mixins/function" as *;

html[theme-size="wide"] {
  @include set-css-var("home-max-width", 1400px);
  @include set-css-var("home-gap", getCssVar("gap3"));
  @include set-css-var("home-post-simple-img-width", 160px);
  @include set-css-var("home-post-full-img-width", 360px);
  @include set-css-var("home-post-full-img-height", 100%);
  @include set-css-var("home-post-line-clamp", 4);
  @include set-css-var("home-card-padding", 15px);
  @include set-css-var("home-card-width", 350px);
  @include set-css-var("home-card-svg-margin-left", 10px);
  @include set-css-var("home-font-size-large", 19px);
  @include set-css-var("home-font-size-base", 17px);
  @include set-css-var("home-font-size-middle", 15px);
  @include set-css-var("home-font-size-sm", 14px);
  @include set-css-var("home-font-size-small", 13px);
  @include set-css-var("home-page-width", 1100px);
  @include set-css-var("home-footer-group-width", 100%);
}

html[theme-size="large"] {
  @include set-css-var("home-max-width", 1330px);
  @include set-css-var("home-gap", getCssVar("gap3"));
  @include set-css-var("home-post-simple-img-width", 160px);
  @include set-css-var("home-post-full-img-width", 360px);
  @include set-css-var("home-post-full-img-height", 100%);
  @include set-css-var("home-post-line-clamp", 4);
  @include set-css-var("home-card-padding", 15px);
  @include set-css-var("home-card-width", 350px);
  @include set-css-var("home-card-svg-margin-left", 10px);
  @include set-css-var("home-font-size-large", 19px);
  @include set-css-var("home-font-size-base", 17px);
  @include set-css-var("home-font-size-middle", 15px);
  @include set-css-var("home-font-size-sm", 14px);
  @include set-css-var("home-font-size-small", 13px);
  @include set-css-var("home-page-width", 1100px);
  @include set-css-var("home-footer-group-width", 90%);
}

:root,
html[theme-size="default"] {
  @include set-css-var("home-max-width", 1140px);
  @include set-css-var("home-gap", getCssVar("gap2"));
  @include set-css-var("home-post-simple-img-width", 120px);
  @include set-css-var("home-post-simple-img-height", 80px);
  @include set-css-var("home-post-full-img-width", 240px);
  @include set-css-var("home-post-full-img-height", 100%);
  @include set-css-var("home-post-line-clamp", 3);
  @include set-css-var("home-card-padding", 10px);
  @include set-css-var("home-card-width", 280px);
  @include set-css-var("home-card-svg-margin-left", 5px);
  @include set-css-var("home-font-size-large", 18px);
  @include set-css-var("home-font-size-base", 16px);
  @include set-css-var("home-font-size-middle", 14px);
  @include set-css-var("home-font-size-sm", 13px);
  @include set-css-var("home-font-size-small", 12px);
  @include set-css-var("page-width", 900px);
  @include set-css-var("home-footer-group-width", 80%);
}

html[theme-size="small"] {
  @include set-css-var("home-max-width", 1000px);
  @include set-css-var("home-gap", getCssVar("gap2"));
  @include set-css-var("home-post-simple-img-width", 100px);
  @include set-css-var("home-post-simple-img-height", 80px);
  @include set-css-var("home-post-full-img-width", 130px);
  @include set-css-var("home-post-full-img-height", 100%);
  @include set-css-var("home-post-line-clamp", 2);
  @include set-css-var("home-card-padding", 8px);
  @include set-css-var("home-card-width", 260px);
  @include set-css-var("home-card-svg-margin-left", 4px);
  @include set-css-var("home-font-size-large", 17px);
  @include set-css-var("home-font-size-base", 15px);
  @include set-css-var("home-font-size-middle", 13px);
  @include set-css-var("home-font-size-sm", 13px);
  @include set-css-var("home-font-size-small", 12px);
  @include set-css-var("page-width", 800px);
  @include set-css-var("home-footer-group-width", 70%);
}

@media (min-width: 768px) {
  :root,
  html[theme-size="large"],
  html[theme-size="default"],
  html[theme-size="small"] {
    @include set-css-var("home-card-width", 280px);
  }
}

@media (max-width: 768px) {
  :root,
  html[theme-size="large"],
  html[theme-size="default"],
  html[theme-size="small"] {
    @include set-css-var("home-card-width", 100%);
  }
}
```

您可以创建一个 `css` 文件来修改上面提供的变量，如在 `default` 尺寸下，将 `--tk-home-max-width` 变量设置为 `1280px`：

```css [tk-theme-size.css]
:root,
html[theme-size="default"] {
  --tk-home-max-width: 1280px; /* 将 1140px 改为 1280px */
}
```

在 `.vitepress/theme/index.ts` 文件引入该 `css` 文件：

```ts {3}
import Teek from "vitepress-theme-teek";
import "vitepress-theme-teek/index.css";
import "./style/tk-theme-size.css";

export default {
  extends: Teek,
};
```

这样 `default` 尺寸下，`--tk-home-max-width` 变量被设置为 `1280px`。

---

---
url: /daily/面试专栏/微服务相关/2_注册中心.md
---

# 注册中心

## 1. 注册中心是用来干什么的？

注册中心是用来管理和维护分布式系统中各个服务的地址和元数据的组件。它主要用于实现`服务发现`和`服务注册`功能。

![注册中心示意图](/assets/0f33faafc4f4a984894137d94d884011.2wyzjAHe.png)

总结一下注册中心的作用：

1. **服务注册**：各个服务在启动时向注册中心注册自己的网络地址、服务实例信息和其他相关元数据。这样，其他服务就可以通过注册中心获取到当前可用的服务列表。
2. **服务发现**：客户端通过向注册中心查询特定服务的注册信息，获得可用的服务实例列表。这样客户端就可以根据需要选择合适的服务进行调用，实现了服务间的解耦。
3. **负载均衡**：注册中心可以对同一服务的多个实例进行负载均衡，将请求分发到不同的实例上，提高整体的系统性能和可用性。
4. **故障恢复**：注册中心能够监测和检测服务的状态，当服务实例发生故障或下线时，可以及时更新注册信息，从而保证服务能够正常工作。
5. **服务治理**：通过注册中心可以进行服务的配置管理、动态扩缩容、服务路由、灰度发布等操作，实现对服务的动态管理和控制。

## 2. SpringCloud可以选择哪些注册中心？

SpringCloud可以与多种注册中心进行集成，常见的注册中心包括：

1. Eureka：Eureka 是 Netflix 开源的服务发现框架，具有高可用、弹性、可扩展等特点，并与 Spring Cloud 集成良好。
2. Consul：Consul 是一种分布式服务发现和配置管理系统，由 HashiCorp 开发。它提供了服务注册、服务发现、健康检查、键值存储等功能，并支持多数据中心部署。
3. ZooKeeper：ZooKeeper 是 Apache 基金会开源的分布式协调服务，可以用作服务注册中心。它具有高可用、一致性、可靠性等特点。
4. Nacos：Nacos 是阿里巴巴开源的一个动态服务发现、配置管理和服务管理平台。它提供了服务注册和发现、配置管理、动态 DNS 服务等功能。
5. etcd：etcd 是 CoreOS 开源的一种分布式键值存储系统，可以被用作服务注册中心。它具有高可用、强一致性、分布式复制等特性。

## 3. 说下Eureka、ZooKeeper、Nacos的区别？

| 特性     | Eureka                           | ZooKeeper                          | Nacos                              |
| -------- | -------------------------------- | ---------------------------------- | ---------------------------------- |
| 开发公司 | Netflix                          | Apache 基金会                      | 阿里巴巴                           |
| CAP      | AP（可用性和分区容忍性）         | CP（一致性和分区容忍性）           | 既支持AP，也支持CP                 |
| 功能     | 服务注册与发现                   | 分布式协调、配置管理、分布式锁     | 服务注册与发现、配置管理、服务管理 |
| 定位     | 适用于构建基于 HTTP 的微服务架构 | 通用的分布式协调服务框架           | 适用于微服务和云原生应用           |
| 访问协议 | HTTP                             | TCP                                | HTTP/DNS                           |
| 自我保护 | 支持                             | -                                  | 支持                               |
| 数据存储 | 内嵌数据库、多个实例形成集群     | ACID 特性的分布式文件系统 ZAB 协议 | 内嵌数据库、MySQL 等               |
| 健康检查 | Client Beat                      | Keep Alive                         | TCP/HTTP/MYSQL/Client Beat         |
| 特点     | 简单易用、自我保护机制           | 高性能、强一致性                   | 动态配置管理、流量管理、灰度发布等 |

可以看到Eureka和ZooKeeper的最大区别是一个支持`AP`，一个支持`CP`，Nacos既支持既支持`AP`，也支持`CP`。

## 4. Eureka实现原理了解吗？

![Eureka原理示意图](/assets/f0844e3d8e9d52d66da03e1fa0354a7d.D-g9HytM.png)

Eureka的实现原理，大概可以从这几个方面来看：

1. 服务注册与发现: 当一个服务实例启动时，它会向Eureka Server发送注册请求，将自己的信息注册到注册中心。Eureka Server会将这些信息保存在内存中，并提供REST接口供其他服务查询。服务消费者可以通过查询服务实例列表来获取可用的服务提供者实例，从而实现服务的发现。
2. 服务健康检查: Eureka通过心跳机制来检测服务实例的健康状态。服务实例会定期向Eureka Server发送心跳，也就是续约，以表明自己的存活状态。如果Eureka Server在一定时间内没有收到某个服务实例的心跳，则会将其标记为不可用，并从服务列表中移除，下线实例。
3. 服务负载均衡: Eureka客户端在调用其他服务时，会从本地缓存中获取服务的注册信息。如果缓存中没有对应的信息，则会向Eureka Server发送查询请求。Eureka Server会返回一个可用的服务实例列表给客户端，客户端可以使用负载均衡算法选择其中一个进行调用。

其它的注册中心，如Nacos、Consul等等，在服务注册和发现上，实现原理都是大同小异。

---

---
url: /Java/Java开发技巧/03.高效编程/5_资源关闭优化方案.md
---

# 资源关闭优化方案

传统的资源关闭方法，需要我们使用`if`加上`try……cache`语句，那么有没有更加优雅的资源关闭方法呢，使我们的代码看上去更精简，而不是一坨的摆在哪里。

### 传统的资源关闭写法

先来体验一下传统的资源关闭方式，注意 `finally` 类中的代码，其繁琐程度大家体验一下。

```java
@Test
public void copyFile() {
    // 定义输入路径和输出路径
    String originalUrl = "lib/FileCopyTest.java";
    String targetUrl = "targetTest/target.txt";

    // 声明文件输入流，文件输出流
    FileInputStream originalFileInputStream = null;
    FileOutputStream targetFileOutputStream = null;

    try {
        // 实例化文件流对象
        originalFileInputStream = new FileInputStream(originalUrl);
        targetFileOutputStream = new FileOutputStream(targetUrl);

        // 读取的字节信息
        int content;
        // 迭代，读取/写入字节
        while ((content = originalFileInputStream.read()) != -1) {
            targetFileOutputStream.write(content);
        }
    }  catch (IOException e) {
        e.printStackTrace();
    } finally {
        // 关闭流资源
        if (targetFileOutputStream != null) {
            try {
                targetFileOutputStream.close();
            } catch (IOException e) {
                e.printStackTrace();
            }
        }
        if (originalFileInputStream != null) {
            try {
                originalFileInputStream.close();
            } catch (IOException e) {
                e.printStackTrace();
            }
        }
    }
}
```

### TWR方式优化资源关闭

`TWR` 是简写，全称是 `try-with-resource` ，直接上代码体会一下

```java
@Test
public void copyFile2() {
    // 先定义输入/输出路径
    String originalUrl = "lib/NewFileCopyTest.java";
    String targetUrl = "targetTest/new.txt";

    // 初始化输入/输出流对象
    try (
        FileInputStream originalFileInputStream = new FileInputStream(originalUrl);
        FileOutputStream targetFileOutputStream = new FileOutputStream(targetUrl);
    ) {
        int content;
        // 迭代，拷贝数据
        while ((content = originalFileInputStream.read()) != -1) {
            targetFileOutputStream.write(content);
        }
    } catch (FileNotFoundException e) {
        e.printStackTrace();
    } catch (IOException e) {
        e.printStackTrace();
    }
}
```

解析： 在 `try()` 中定义的资源不需要关闭，`jdk` 自动帮我们处理了

注意：这种写法 `jdk7` 才开始支持，不会有执行效率上的提升，是 `jdk` 的语法糖，简化开发代码的。

---

---
url: /常用框架/SpringBoot/SpringBoot源码分析/资源填充类ResourceDatabasePopulator.md
---

# 资源填充类ResourceDatabasePopulator

## 1、类ResourceDatabasePopulator介绍

使用外部资源中定义的 SQL 脚本填充、初始化或清理数据库。

* 调用[addScript(org.springframework.core.io.Resource)](https://docs.spring.io/spring-framework/docs/5.3.13/javadoc-api/org/springframework/jdbc/datasource/init/ResourceDatabasePopulator.html#addScript-org.springframework.core.io.Resource-)以添加单个 SQL 脚本位置。
* 调用[addScripts(org.springframework.core.io.Resource...)](https://docs.spring.io/spring-framework/docs/5.3.13/javadoc-api/org/springframework/jdbc/datasource/init/ResourceDatabasePopulator.html#addScripts-org.springframework.core.io.Resource...-)以添加多个 SQL 脚本位置。
* 请参阅此类中的 setter 方法以获取更多配置选项。
* 调用[populate(java.sql.Connection)](https://docs.spring.io/spring-framework/docs/5.3.13/javadoc-api/org/springframework/jdbc/datasource/init/ResourceDatabasePopulator.html#populate-java.sql.Connection-)或[execute(javax.sql.DataSource)](https://docs.spring.io/spring-framework/docs/5.3.13/javadoc-api/org/springframework/jdbc/datasource/init/ResourceDatabasePopulator.html#execute-javax.sql.DataSource-)使用配置的脚本初始化或清理数据库。

## 2、方法总结

| 修饰符和类型 | 方法及说明                                                   |
| :----------- | :----------------------------------------------------------- |
| `void`       | `**`\**[addScript](https://docs.spring.io/spring-framework/docs/5.3.13/javadoc-api/org/springframework/jdbc/datasource/init/ResourceDatabasePopulator.html#addScript-org.springframework.core.io.Resource-)\**`**(Resource script)`添加要执行的脚本以初始化或清理数据库。 |
| `void`       | `**`\**[addScripts](https://docs.spring.io/spring-framework/docs/5.3.13/javadoc-api/org/springframework/jdbc/datasource/init/ResourceDatabasePopulator.html#addScripts-org.springframework.core.io.Resource...-)\**`**(Resource... scripts)`添加多个脚本来执行以初始化或清理数据库。 |
| `void`       | `**`\**[execute](https://docs.spring.io/spring-framework/docs/5.3.13/javadoc-api/org/springframework/jdbc/datasource/init/ResourceDatabasePopulator.html#execute-javax.sql.DataSource-)\**`**(DataSource dataSource)`针对`ResourceDatabasePopulator`给定的 [DataSource](https://docs.oracle.com/javase/8/docs/api/javax/sql/DataSource.html?is-external=true). |
| `void`       | `**`\**[populate](https://docs.spring.io/spring-framework/docs/5.3.13/javadoc-api/org/springframework/jdbc/datasource/init/ResourceDatabasePopulator.html#populate-java.sql.Connection-)\**`**(Connection connection)`使用提供的 JDBC 连接填充、初始化或清理数据库。 |
| `void`       | `**`\**[setBlockCommentEndDelimiter](https://docs.spring.io/spring-framework/docs/5.3.13/javadoc-api/org/springframework/jdbc/datasource/init/ResourceDatabasePopulator.html#setBlockCommentEndDelimiter-java.lang.String-)\**`**(String blockCommentEndDelimiter)`设置标识 SQL 脚本中的块注释的结束分隔符。 |
| `void`       | `**`\**[setBlockCommentStartDelimiter](https://docs.spring.io/spring-framework/docs/5.3.13/javadoc-api/org/springframework/jdbc/datasource/init/ResourceDatabasePopulator.html#setBlockCommentStartDelimiter-java.lang.String-)\**`**(String blockCommentStartDelimiter)`设置标识 SQL 脚本中的块注释的起始分隔符。 |
| `void`       | `**`\**[setCommentPrefix](https://docs.spring.io/spring-framework/docs/5.3.13/javadoc-api/org/springframework/jdbc/datasource/init/ResourceDatabasePopulator.html#setCommentPrefix-java.lang.String-)\**`**(String commentPrefix)`设置标识 SQL 脚本中单行注释的前缀。 |
| `void`       | `**`\**[setCommentPrefixes](https://docs.spring.io/spring-framework/docs/5.3.13/javadoc-api/org/springframework/jdbc/datasource/init/ResourceDatabasePopulator.html#setCommentPrefixes-java.lang.String...-)\**`**(String... commentPrefixes)`设置标识 SQL 脚本中单行注释的前缀。 |
| `void`       | `**`\**[setContinueOnError](https://docs.spring.io/spring-framework/docs/5.3.13/javadoc-api/org/springframework/jdbc/datasource/init/ResourceDatabasePopulator.html#setContinueOnError-boolean-)\**`**(boolean continueOnError)`指示应记录 SQL 中的所有失败但不会导致失败的标志。 |
| `void`       | `**`\**[setIgnoreFailedDrops](https://docs.spring.io/spring-framework/docs/5.3.13/javadoc-api/org/springframework/jdbc/datasource/init/ResourceDatabasePopulator.html#setIgnoreFailedDrops-boolean-)\**`**(boolean ignoreFailedDrops)`指示`DROP`可以忽略失败的 SQL 语句的标志。 |
| `void`       | `**`\**[setScripts](https://docs.spring.io/spring-framework/docs/5.3.13/javadoc-api/org/springframework/jdbc/datasource/init/ResourceDatabasePopulator.html#setScripts-org.springframework.core.io.Resource...-)\**`**(Resource... scripts)`设置要执行的脚本以初始化或清理数据库，替换之前添加的任何脚本。 |
| `void`       | `**`\**[setSeparator](https://docs.spring.io/spring-framework/docs/5.3.13/javadoc-api/org/springframework/jdbc/datasource/init/ResourceDatabasePopulator.html#setSeparator-java.lang.String-)\**`**(String separator)`指定语句分隔符（如果是自定义分隔符）。 |
| `void`       | `**`\**[setSqlScriptEncoding](https://docs.spring.io/spring-framework/docs/5.3.13/javadoc-api/org/springframework/jdbc/datasource/init/ResourceDatabasePopulator.html#setSqlScriptEncoding-java.lang.String-)\**`**(String sqlScriptEncoding)`如果与平台编码不同，请为配置的 SQL 脚本指定编码。 |

## 3、通过注解@PostConstruct实现SpringBoot项目启动初始化数据（执行sql文件）

```java
import cn.hutool.core.date.DateUtil;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.context.annotation.Configuration;
import org.springframework.core.io.ClassPathResource;
import org.springframework.jdbc.datasource.init.ResourceDatabasePopulator;

import javax.annotation.PostConstruct;
import javax.sql.DataSource;
import java.util.Date;

/**
 * 项目启动初始化数据
 *
 * @Author: xxl
 * @Date: 2024/5/3 下午6:02
 */
@Configuration
@Slf4j
public class DataInitializationConfig {

    @Autowired
    DataSource dataSource;

    @PostConstruct
    public void init() {
        // 项目启动初始化基本数据
        log.info("数据初始化开始: " + DateUtil.format(new Date(), "yyyy-MM-dd HH:mm:ss"));
        // 通过直接读取sql文件执行
        ClassPathResource resources = new ClassPathResource("sql/client_api_init.sql");
        ResourceDatabasePopulator resourceDatabasePopulator = new ResourceDatabasePopulator();
        resourceDatabasePopulator.addScripts(resources);
        resourceDatabasePopulator.execute(dataSource);
        log.info("数据初始化结束: " + DateUtil.format(new Date(), "yyyy-MM-dd HH:mm:ss"));
    }
}
```

参考资料

https://www.cnblogs.com/tanqingfu1/p/16551756.html

---

---
url: /Java/Java开发技巧/03.高效编程/2_自定义注解.md
---

# 自定义注解

自定义注解类编写的一些规则:

1. Annotation 类型定义为 **@interface**, 所有的Annotation 会自动继承java.lang.Annotation这一接口,并且不能再去继承别的类或是接口。
2. 参数成员只能用public 或默认(default) 这两个访问权修饰。**语法：类型 属性名() \[default 默认值];   default表示默认值 ，也可以不编写默认值的.**
3. 参数成员只能用基本类型byte、short、char、int、long、float、double、boolean八种基本数据类型和String、Enum、Class、annotations等数据类型，以及这一些类型的数组.
4. 要获取类方法和字段的注解信息，必须通过Java的反射技术来获取 Annotation 对象，因为你除此之外没有别的获取注解对象的方法。
5. 注解也可以没有定义成员,，不过这样注解就没啥用了。

**注意:** 自定义注解需要使用到元注解。

* 注解方法不能有参数。
* 注解方法的返回类型局限于原始类型，字符串，枚举，注解，或以上类型构成的数组。
* 注解方法可以包含默认值。

```java
@Target({ElementType.METHOD, ElementType.TYPE})
@Retention(RetentionPolicy.RUNTIME)
public @interface MyAnnotation {
	
	String color() default "blue";// 为属性指定缺省值
	String product() default "001";
}
```

参考

\[1]. https://blog.csdn.net/weixin\_40482816/article/details/112961560

---

---
url: /Java/架构设计/分布式/05.分布式任务调度/4_自动创建定时任务.md
---

# 自动创建定时任务

博主遇到的一个需求：创建活动任务，活动任务带有开始结束时间，需要在活动开始前几分钟进行提醒。

但是之前活动开始前提醒是通过配置XXL-JOB定时任务来实现的，该任务每分钟扫描一次，检查当前时间是否在活动开始前的指定时间段内。为了优化这一业务流程，希望在创建活动时直接生成一个指定时间的定时任务，从而减少不必要的数据库查询并提高系统的效率。

具体来说，需要在创建活动时计算出提醒的具体时间，并通过调用XXL-JOB的API动态创建一个定时任务。这样可以在活动创建时就确定好提醒的时间点，避免频繁的扫描操作。

### 一、添加依赖

确保你的项目中包含了 XXL-JOB 的客户端依赖。如果使用 Maven，可以在 `pom.xml` 中添加如下依赖：

```xml
<dependency>
    <groupId>com.xuxueli</groupId>
    <artifactId>xxl-job-core</artifactId>
    <version>2.3.0</version>
</dependency>
```

### 二、配置 XXL-JOB 客户端

在你的应用配置文件（如 `application.yml` 或 `application.properties`）中配置 XXL-JOB 客户端参数。

```yaml
xxl:
  job:
    admin:
      addresses: http://127.0.0.1:8080/xxl-job-admin
    executor:
      appname: your-app-name
      ip: 
      port: 9999
      logpath: /data/applogs/xxl-job/jobhandler
      logretentiondays: 30
```

### 三、创建活动并生成定时任务

在创建活动的方法中，调用 XXL-JOB 的 API 来创建一个新的定时任务。

```java
import com.xxl.job.core.biz.model.ReturnT;
import com.xxl.job.core.biz.model.TriggerParam;
import com.xxl.job.core.util.XxlJobDynamicScheduler;

import java.time.LocalDateTime;
import java.time.ZoneId;
import java.time.format.DateTimeFormatter;
import java.util.Date;

public class ActivityService {

    public void createActivity(Activity activity) {
        // 保存活动基本信息到数据库
        saveActivity(activity);

        // 计算提醒时间
        LocalDateTime reminderTime = activity.getStartTime().minusHours(1); // 假设提前1小时提醒
        Date triggerTime = Date.from(reminderTime.atZone(ZoneId.systemDefault()).toInstant());

        // 创建定时任务
        TriggerParam triggerParam = new TriggerParam();
        triggerParam.setJobId(1); // 你需要为这个提醒任务分配一个固定的jobId
        triggerParam.setExecutorHandler("activityReminderHandler"); // 对应执行器中的任务处理器
        triggerParam.setTriggerTime(triggerTime.getTime());

        // 调用XXL-JOB API创建定时任务
        ReturnT<String> result = XxlJobDynamicScheduler.triggerJob(triggerParam);
        if (result.getCode() == 200) {
            System.out.println("定时任务创建成功");
        } else {
            System.err.println("定时任务创建失败: " + result.getMsg());
        }
    }

    private void saveActivity(Activity activity) {
        // 保存活动到数据库的逻辑
    }
}
```

### 四、配置任务处理器

在你的 XXL-JOB 执行器中，配置一个任务处理器来处理提醒逻辑。

```java
import com.xxl.job.core.biz.model.ReturnT;
import com.xxl.job.core.handler.annotation.XxlJob;
import org.springframework.stereotype.Component;

@Component
public class ActivityReminderJobHandler {

    @XxlJob("activityReminderHandler")
    public ReturnT<String> execute(String param) {
        // 提醒逻辑
        System.out.println("活动提醒: " + param);
        return ReturnT.SUCCESS;
    }
}
```

### 参考资料

XXL-JOB调度中心与执行器通讯过程源码理解：https://blog.csdn.net/it\_boy\_elite/article/details/138980389

---

---
url: /15.主题开发/30.组件布局.md
---

# 组件结构

一个项目的功能组件虽然有很多，但是入口组件只有一个，如果您不知道这些功能组件都在哪里执行，不妨从入口组件开始解读，一步一步往下延伸，最终把项目功能吃透。

Teek 在首页、文章页、空白页、全局都写了组件来实现功能，但是这些组件并不是分开引入，而是统一在 `Layout` 组件里引入，并派发到 VitePress 不同的插槽，如：

```vue
<!-- src/layout/index.vue -->
<script setup lang="ts" name="TeekLayout">
import DefaultTheme from "vitepress/theme";
import { useData } from "vitepress";
import {
  RightBottomButton,
  Notice,
  Home,
  Footer,
  ArticleAnalyze,
  Comment,
  ArchivesPage,
  CataloguePage,
} from "../components";

const { Layout } = DefaultTheme;
const { theme } = useData();
const { teekTheme = true } = theme.value;

// 维护已使用的插槽，防止外界传来的插槽覆盖已使用的插槽
const usedSlots = [
  "home-hero-before",
  "nav-bar-content-after",
  // 其他模板里已使用的插槽 ...
];
</script>

<template>
  <template v-if="teekTheme">
    <ClientOnly>
      <!-- 全局组件 -->
      <RightBottomButton />
      <Notice />
    </ClientOnly>

    <Layout :class="ns.b()">
      <template #home-hero-before>
        <slot name="home-hero-before" />

        <ClientOnly>
          <!-- 自定义首页 -->
          <Home />
        </ClientOnly>
      </template>

      <template #layout-bottom>
        <!-- 底部组件 -->
        <Footer v-if="isHomePage()" />
        <slot name="layout-bottom" />
      </template>

      <template #doc-before>
        <slot name="doc-before" />
        <!--文章页信息组件 -->
        <ArticleAnalyze />
      </template>

      <template #doc-after>
        <slot name="doc-after" />
        <!-- 评论区组件 -->
        <Comment />
      </template>

      <template #page-top>
        <slot name="page-top" />
        <ArchivesPage />
        <CataloguePage />
      </template>

      <!-- 未使用的其他 VP 插槽 -->
      <template
        v-for="(_, name) in Object.keys($slots).filter(name => !usedSlots.includes(name))"
        :key="name"
        #[name]="slotData"
      >
        <slot :name="name" v-bind="slotData"></slot>
      </template>
    </Layout>
  </template>
  <template v-else>
    <Layout>
      <template v-for="(_, name) in $slots" :key="name" #[name]="slotData">
        <slot :name="name" v-bind="slotData"></slot>
      </template>
    </Layout>
  </template>
</template>
```

## 目录结构

```sh
src
│
├─ components
│  ├─ base                        # 基础样式组件
│  │
│  ├─ common                     # 公共组件
│  │  ├─ ArticlePage                # 文章页组件
│  │  ├─ Avatar                     # 头像组件
│  │  ├─ Breadcrumb                 # 面包屑组件
│  │  ├─ FocusTrap                  # 聚焦组件
│  │  ├─ icon                       # 图标组件
│  │  ├─ ImageViewer                # 图片查看器组件
│  │  ├─ InputSlide                 # 滑块组件
│  │  ├─ Message                    # 消息提示组件
│  │  ├─ HomeCard                   # 分页卡片组件
│  │  ├─ Pagination                 # 分页组件
│  │  ├─ Popover                    # 弹窗组件
│  │  ├─ Segmented                  # 分段控制器组件
│  │  ├─ TitleTag                   # 标题标签组件
│  │  ├─ TransitionCollapse         # 折叠动画组件
│  │  ├─ VerifyCode                 # 随机验证码组件
│  │  ├─ VpContainer                # VitePress 容器组件
│  │
│  ├─ theme                      # 主题组件
│  │  ├─ ArchivesPage               # 归档页组件
│  │  ├─ ArticleAnalyze             # 文章页分析组件
│  │  ├─ ArticleAppreciation        # 文章页赞赏组件
│  │  ├─ ArticleBreadcrumb          # 文章页面包屑组件
│  │  ├─ ArticleHeadingHighlight    # 文章页标题高亮组件
│  │  ├─ ArticleImagePreview        # 图片预览组件
│  │  ├─ ArticleInfo                # 文章信息组件
│  │  ├─ ArticleOverviewPage        # 清单页组件
│  │  ├─ ArticlePageStyle           # 文章页样式组件
│  │  ├─ ArticleShare               # 文章页分页组件
│  │  ├─ ArticleTitle               # 文章标题组件
│  │  ├─ ArticleUpdate              # 文章最近更新栏组件
│  │  ├─ BodyBgImage                # Body 背景图片组件
│  │  ├─ CataloguePage              # 目录页组件
│  │  ├─ CodeBlockToggle            # 代码块加强组件
│  │  ├─ CommentArtalk              # Artalk 评论区组件
│  │  ├─ CommentGiscus              # Giscus 评论区组件
│  │  ├─ CommentTwikoo              # Twikoo 评论区组件
│  │  ├─ CommentWaline              # Waline 评论区组件
│  │  ├─ ConfigProvider             # Teek 入口文件
│  │  ├─ DemoCode                   # Demo 容器组件
│  │  ├─ FooterGroup                # 底部信息组组件
│  │  ├─ FooterInfo                 # 底部信息组件
│  │  ├─ Home                       # 首页组件
│  │  ├─ HomeBanner                 # 首页 Banner 组件
│  │  ├─ HomeCardList               # 首页卡片栏组件
│  │  ├─ HomeCategoryCard           # 首页分类卡片组件
│  │  ├─ HomeDocAnalysisCard        # 首页文章分析卡片组件
│  │  ├─ HomeFriendLinkCard         # 首页友情链接卡片组件
│  │  ├─ HomeFullscreenWallpaper    # 壁纸模式组件
│  │  ├─ HomeMyCard                 # 首页我的卡片组件
│  │  ├─ HomePostList               # 首页文章列表组件
│  │  ├─ HomeTagCard                # 首页标签卡片组件
│  │  ├─ HomeTopArticleCard         # 首页置顶文章卡片组件
│  │  ├─ Layout                     # 布局组件（入口组件）
│  │  ├─ LoginPage                  # 登录页
│  │  ├─ Notice                     # 公告组件
│  │  ├─ RightBottomButton          # 右下角按钮组组件
│  │  ├─ RiskLinkPage               # 风险链接提示页
│  │  ├─ ThemeEnhance               # 主题增强面板组件
```

VitePress 从 `src/index.ts`（入口文件）解析 `Layout` 函数，该函数返回 `src/layout.index.vue` 组件（入口组件），该入口组件将 Teek 的各个功能组件派发到 VitePress 不同的插槽，最终形成现在的 Teek 主题。

## 配置项获取

在开发功能组件的时候，Teek 往往不会在组件内部固定功能，而是由用户通过配置项来开关功能。

在 VitePress 中，配置项往往有 2 种方式配置：

1. `.vitepress/config.mts` 全局配置
2. Markdown 的 `frontmatter` 局部配置

如果 2 种方式都配置，那么 Markdown 的 `frontmatter` 配置优先级更改，比如 Teek 的评论区功能，用户可以给每一个 Markdown 配置不同的评论区。

配置项获取的例子如：

```vue
<script setup lang="ts">
import { computed } from "vue";
import { useData } from "vitepress";

const { theme, frontmatter } = useData();

const commentConfig = computed(() => ({
  ...theme.comment,
  ...frontmatter.value.comment,
  ...frontmatter.value.tk.comment, // 首页 index.md 配置项
}));
</script>

<template></template>
```

这样编写方式既支持 2 种方式配置，也支持给配置项添加默认值。2 种配置方式如下（`index.md ` 和 `文章页.md` 是 `frontmatter` 方式）：

::: code-group

```ts [config]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
 comment: {
    provider: "giscus",
    options: {
      repo: "your repo",
      repoId: "your repoId",
      category: "your category",
      categoryId: "your categoryId",
    }
 };
});
```

```yaml [index.md]
---
tk:
  comment:
    provider: "giscus"
    options:
      repo: "your repo"
      repoId: "your repoId"
      category: "your category"
      categoryId: "your categoryId"
---
```

```yaml [文章页.md]
---
comment:
  provider: "giscus"
  options:
    repo: "your repo"
    repoId: "your repoId"
    category: "your category"
    categoryId: "your categoryId"
---
```

:::

---

---
url: /常用框架/SpringAIAlibaba/Agent Framework/15_A2A Agent分布式智能体.md
---

# A2A Agent 分布式智能体

---

---
url: /数据库/01.MySQL/ACID.md
---

# ACID是什么

> ACID，是指在可靠数据库管理系统（DBMS）中，事务(transaction)所应该具有的四个特性：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability）.这是可靠数据库所应具备的几个特性.所以ACID就是这四大特性的缩写。

1. 原子性（Atomicity）
   * 原子性意味着数据库中的事务执行是作为原子。即不可再分，整个语句要么执行，要么不执行。
     在SQL SERVER中，每一个单独的语句都可以看作是默认包含在一个事务之中，每一个语句本身具有原子性，要么全部执行，这么全部不执行，不会有中间状态。
   * 例如：银行转账功能，从A账户减去100，在B账户增加100，如果这两个语句不能保证原子性的话，比如从A账户减去100后，服务器断电，而在B账户中却没有增加100.虽然这种情况会让银行很开心，但作为开发人员的你可不希望这种结果.而默认事务中，即使出错了也不会整个事务进行回滚。而是失败的语句抛出异常，而正确的语句成功执行。这样会破坏原子性。所以SQL SERVER给予了一些选项来保证事务的原子性。
2. 一致性(Consistency)
   * 一致性即在事务开始之前和事务结束以后，数据库的完整性约束没有被破坏。
   * 一致性体现在两个层面：
   * 1. 数据库机制层面
        数据库层面的一致性是，在一个事务执行之前和之后，数据会符合你设置的约束（唯一约束，外键约束,Check约束等)和触发器设置.这一点是由SQL SERVER进行保证的。
   * 2. 业务层面
        对于业务层面来说,一致性是保持业务的一致性.这个业务一致性需要由开发人员进行保证.很多业务方面的一致性可以通过转移到数据库机制层面进行保证.比如，产品只有两个型号，则可以转移到使用CHECK约束使某一列必须只能存这两个型号。
3. 隔离性（Isolation）
   * 隔离性。事务的执行是互不干扰的，一个事务不可能看到其他事务运行时，中间某一时刻的数据。
   * 在Windows中，如果多个进程对同一个文件进行修改是不允许的，Windows通过这种方式来保证不同进程的隔离性，而SQL Server中，通过SQL SERVER对数据库文件进行管理，从而可以让多个进程可以同时访问数据库:SQL Server利用加锁和阻塞来保证事务之间不同等级的隔离性。
   * 一般情况下，完全的隔离性是不现实的，完全的隔离性要求数据库同一时间只执行一条事务，这样的性能可想而知.想要理解SQL Server中对于隔离性的保障，首先要了解事务之间是如何干扰的。
   * 事务之间的互相影响的情况分为几种，分别为：脏读(Dirty Read)，不可重复读，幻读。
4. 持久性（Durability)
   * 持久性，意味着在事务完成以后，该事务所对数据库所作的更改便持久的保存在数据库之中，并不会被回滚。即使出现了任何事故比如断电等，事务一旦提交，则持久化保存在数据库中。

---

---
url: /常用框架/Activiti7/Activiti整合Spring.md
---

# Activiti整合Spring

## 一、Activiti与Spring整合开发

## 1.1 Activiti与Spring整合的配置

### 1)、在pom.xml文件引入坐标

如下

```xml
<properties>
        <slf4j.version>1.6.6</slf4j.version>
        <log4j.version>1.2.12</log4j.version>
</properties>
<dependencies>
    <dependency>
        <groupId>org.activiti</groupId>
        <artifactId>activiti-engine</artifactId>
        <version>7.0.0.Beta1</version>
    </dependency>
    <dependency>
        <groupId>org.activiti</groupId>
        <artifactId>activiti-spring</artifactId>
        <version>7.0.0.Beta1</version>
    </dependency>
    <dependency>
        <groupId>org.activiti</groupId>
        <artifactId>activiti-bpmn-model</artifactId>
        <version>7.0.0.Beta1</version>
    </dependency>
    <dependency>
        <groupId>org.activiti</groupId>
        <artifactId>activiti-bpmn-converter</artifactId>
        <version>7.0.0.Beta1</version>
    </dependency>
    <dependency>
        <groupId>org.activiti</groupId>
        <artifactId>activiti-json-converter</artifactId>
        <version>7.0.0.Beta1</version>
    </dependency>
    <dependency>
        <groupId>org.activiti</groupId>
        <artifactId>activiti-bpmn-layout</artifactId>
        <version>7.0.0.Beta1</version>
    </dependency>
    <dependency>
        <groupId>org.activiti.cloud</groupId>
        <artifactId>activiti-cloud-services-api</artifactId>
        <version>7.0.0.Beta1</version>
    </dependency>
    <dependency>
        <groupId>aspectj</groupId>
        <artifactId>aspectjweaver</artifactId>
        <version>1.5.4</version>
    </dependency>
    <dependency>
        <groupId>mysql</groupId>
        <artifactId>mysql-connector-java</artifactId>
        <version>5.1.40</version>
    </dependency>
    <dependency>
        <groupId>junit</groupId>
        <artifactId>junit</artifactId>
        <version>4.12</version>
    </dependency>
    <dependency>
        <groupId>org.springframework</groupId>
        <artifactId>spring-test</artifactId>
        <version>5.0.7.RELEASE</version>
    </dependency>
    <!-- log start -->
    <dependency>
        <groupId>log4j</groupId>
        <artifactId>log4j</artifactId>
        <version>${log4j.version}</version>
    </dependency>
    <dependency>
        <groupId>org.slf4j</groupId>
        <artifactId>slf4j-api</artifactId>
        <version>${slf4j.version}</version>
    </dependency>
    <dependency>
        <groupId>org.slf4j</groupId>
        <artifactId>slf4j-log4j12</artifactId>
        <version>${slf4j.version}</version>
    </dependency>
    <dependency>
        <groupId>org.slf4j</groupId>
        <artifactId>slf4j-nop</artifactId>
        <version>${slf4j.version}</version>
    </dependency>
    <!-- log end -->
    <dependency>
        <groupId>org.mybatis</groupId>
        <artifactId>mybatis</artifactId>
        <version>3.4.5</version>
    </dependency>
    <dependency>
        <groupId>commons-dbcp</groupId>
        <artifactId>commons-dbcp</artifactId>
        <version>1.4</version>
    </dependency>
</dependencies>
<repositories>
    <repository>
        <id>alfresco</id>
        <name>Activiti Releases</name>
        <url>https://artifacts.alfresco.com/nexus/content/repositories/activiti-releases/</url>
        <releases>
            <enabled>true</enabled>
        </releases>
    </repository>
</repositories>
```

在Activiti中核心类的是ProcessEngine流程引擎，与Spring整合就是让Spring来管理ProcessEngine

通过org.activiti.spring.SpringProcessEngineConfiguration 与Spring整合方式来创建ProcessEngine对象。

创建spring与activiti的整合配置文件：activiti-spring.xml（名称不固定）

### 2)、创建activiti-spring.xml

```xml
<beans xmlns="http://www.springframework.org/schema/beans"
       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
       xmlns:tx="http://www.springframework.org/schema/tx"
       xmlns:aop="http://www.springframework.org/schema/aop"
       xsi:schemaLocation="http://www.springframework.org/schema/beans
        http://www.springframework.org/schema/beans/spring-beans.xsd
        http://www.springframework.org/schema/tx
        http://www.springframework.org/schema/tx/spring-tx.xsd
        http://www.springframework.org/schema/aop
        http://www.springframework.org/schema/aop/spring-aop.xsd">
    <!-- 数据源 -->
    <bean id="dataSource" class="org.apache.commons.dbcp.BasicDataSource">
        <property name="driverClassName" value="com.mysql.jdbc.Driver"/>
        <property name="url" value="jdbc:mysql://localhost:3306/activiti"/>
        <property name="username" value="root"/>
        <property name="password" value="123456"/>
        <property name="maxActive" value="3"/>
        <property name="maxIdle" value="1"/>
    </bean>
    <!-- 工作流引擎配置bean -->
    <bean id="processEngineConfiguration" class="org.activiti.spring.SpringProcessEngineConfiguration">
        <!-- 数据源 -->
        <property name="dataSource" ref="dataSource"/>
        <!-- 使用spring事务管理器 -->
        <property name="transactionManager" ref="transactionManager"/>
        <!-- 数据库策略 -->
        <property name="databaseSchemaUpdate" value="drop-create"/>
    </bean>
    <!-- 流程引擎 -->
    <bean id="processEngine" class="org.activiti.spring.ProcessEngineFactoryBean">
        <property name="processEngineConfiguration" ref="processEngineConfiguration"/>
    </bean>
    <!-- 资源服务service -->
    <bean id="repositoryService" factory-bean="processEngine" factory-method="getRepositoryService"/>
    <!-- 流程运行service -->
    <bean id="runtimeService" factory-bean="processEngine"  factory-method="getRuntimeService"/>
    <!-- 任务管理service -->
    <bean id="taskService" factory-bean="processEngine" factory-method="getTaskService"/>
    <!-- 历史管理service -->
    <bean id="historyService" factory-bean="processEngine" factory-method="getHistoryService"/>
    <!-- 事务管理器 -->
    <bean id="transactionManager" class="org.springframework.jdbc.datasource.DataSourceTransactionManager">
        <property name="dataSource" ref="dataSource"/>
    </bean>
    <!-- 通知 -->
    <tx:advice id="txAdvice" transaction-manager="transactionManager">
        <tx:attributes>
            <!-- 传播行为 -->
            <tx:method name="save*" propagation="REQUIRED"/>
            <tx:method name="insert*" propagation="REQUIRED"/>
            <tx:method name="delete*" propagation="REQUIRED"/>
            <tx:method name="update*" propagation="REQUIRED"/>
            <tx:method name="find*" propagation="SUPPORTS" read-only="true"/>
            <tx:method name="get*" propagation="SUPPORTS" read-only="true"/>
        </tx:attributes>
    </tx:advice>
    <!-- 切面，根据具体项目修改切点配置
    <aop:config proxy-target-class="true">
        <aop:advisor advice-ref="txAdvice"
                     pointcut="execution(*com.xxl.service.impl..(..))"/>
    </aop:config>-->
</beans>
```

databaseSchemaUpdate的取值内容：

flase：       默认值。activiti在启动时，会对比数据库表中保存的版本，如果没有表或者版本不匹配，将抛出异常。（生产环境常用）
true：        activiti会对数据库中所有表进行更新操作。如果表不存在，则自动创建。（开发时常用）
create\_drop： 在activiti启动时创建表，在关闭时删除表（必须手动关闭引擎，才能删除表）。（单元测试常用）
drop-create： 在activiti启动时删除原来的旧表，然后在创建新表（不需要手动关闭引擎）。

## 1.2 测试Activiti与Spring整合

### 1）、测试代码

```java
/**
   测试activiti与spring整合是否成功
**/
@RunWith(SpringJUnit4ClassRunner.class)
@ContextConfiguration(locations = "classpath:activiti-spring.xml")
 public class ActivitiTest {
     @Autowired
     private RepositoryService repositoryService;
     
     @Test
     public void test01(){
         System.out.println("部署对象:"+repositoryService);
     }
 }
```

### 2）、执行流程分析

Activiti与Spring整合加载的过程：

1、加载activiti-spring.xml配置文件

2、加载SpringProcessEngineConfiguration对象，这个对象它需要依赖注入dataSource对象和transactionManager对象。

3、加载ProcessEngineFactoryBean工厂来创建ProcessEngine对象，而ProcessEngineFactoryBean工厂又需要依赖注入processEngineConfiguration对象。

4、processEngine对象来负责创建我们的Service对象，从而简化Activiti的开发过程。

## 二、Activiti7与SpringBoot整合开发

Activiti7发布正式版之后，它与SpringBoot2.x已经完全支持整合开发。

## 2.1     SpringBoot整合Activiti7的配置

为了能够实现SpringBoot与Activiti7整合开发，首先我们要引入相关的依赖支持。

在工程的pom.xml文件中引入相关的依赖，其中activiti的依赖是：activiti-spring-boot-starter。

具体依赖如下所示：

```xml

<parent>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-parent</artifactId>
    <version>2.1.0.RELEASE</version>
</parent>
<properties>
    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>
    <java.version>1.8</java.version>
</properties>
<dependencies>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-web</artifactId>
    </dependency>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-jdbc</artifactId>
    </dependency>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-test</artifactId>
    </dependency>
    <dependency>
        <groupId>org.activiti</groupId>
        <artifactId>activiti-spring-boot-starter</artifactId>
        <version>7.0.0.Beta2</version>
    </dependency>
    <dependency>
        <groupId>mysql</groupId>
        <artifactId>mysql-connector-java</artifactId>
        <version>5.1.29</version>
    </dependency>
    <dependency>
        <groupId>org.projectlombok</groupId>
        <artifactId>lombok</artifactId>
    </dependency>
</dependencies>
<build>
    <plugins>
        <plugin>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-maven-plugin</artifactId>
        </plugin>
    </plugins>
</build>
```

通过该pom.xml文件所导入的坐标，我们就可以实现activiti7与Springboot整合。

## 2.2    SpringBoot的application.yml文件配置

为了能够实现Activiti7生成的表放到Mysql数据库中，需要在配置文件application.yml中添加相关的配置

注意：activiti7默认没有开启数据库历史记录，需要手动配置开启

```yaml
spring:
  datasource:
    url: jdbc:mysql:///activiti?useUnicode=true&characterEncoding=utf8&serverTimezone=GMT
    username: root
    password: 123456
    driver-class-name: com.mysql.jdbc.Driver
  activiti:
    #1.flase：默认值。activiti在启动时，对比数据库表中保存的版本，如果没有表或者版本不匹配，将抛出异常
    #2.true： activiti会对数据库中所有表进行更新操作。如果表不存在，则自动创建
    #3.create_drop： 在activiti启动时创建表，在关闭时删除表（必须手动关闭引擎，才能删除表）
    #4.drop-create： 在activiti启动时删除原来的旧表，然后在创建新表（不需要手动关闭引擎）
    database-schema-update: true
    #检测历史表是否存在 activiti7默认没有开启数据库历史记录 启动数据库历史记录
    db-history-used: true
    #记录历史等级 可配置的历史级别有none, activity, audit, full
    #none：不保存任何的历史数据，因此，在流程执行过程中，这是最高效的。
    #activity：级别高于none，保存流程实例与流程行为，其他数据不保存。
    #audit：除activity级别会保存的数据外，还会保存全部的流程任务及其属性。audit为history的默认值。
    #full：保存历史数据的最高级别，除了会保存audit级别的数据外，还会保存其他全部流程相关的细节数据，包括一些流程参数等。
    history-level: full
    #校验流程文件，默认校验resources下的processes文件夹里的流程文件
    check-process-definitions: false
```

## 2.3 编写启动类

```java
package com.xxl;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

@SpringBootApplication
public class ActApplication {
    public static void main(String[] args) {
        SpringApplication.run(ActApplication.class,args);
    }
}

```

## 2.4    添加SpringSecurity安全框架整合配置

因为Activiti7与SpringBoot整合后，默认情况下，集成了SpringSecurity安全框架，这样我们就要去准备SpringSecurity整合进来的相关用户权限配置信息。

SpringBoot的依赖包已经将SpringSecurity的依赖包也添加进项目中。

### 2.4.1  添加SecurityUtil类

为了能够快速实现SpringSecurity安全框架的配置，所添加的一个组件。

```java
package com.xxl.utils;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.security.core.Authentication;
import org.springframework.security.core.GrantedAuthority;
import org.springframework.security.core.context.SecurityContextHolder;
import org.springframework.security.core.context.SecurityContextImpl;
import org.springframework.security.core.userdetails.UserDetails;
import org.springframework.security.core.userdetails.UserDetailsService;
import org.springframework.stereotype.Component;

import java.util.Collection;


@Component
public class SecurityUtil {
    private Logger logger = LoggerFactory.getLogger(SecurityUtil.class);

     @Autowired
     @Qualifier("myUserDetailsService")
     private UserDetailsService userDetailsService;
 
    public void logInAs(String username) {
     UserDetails user = userDetailsService.loadUserByUsername(username);

     if (user == null) {
         throw new IllegalStateException("User " + username + " doesn't exist, please provide a valid user");
     }
     logger.info("> Logged in as: " + username);

     SecurityContextHolder.setContext(
             new SecurityContextImpl(
                     new Authentication() {
                         @Override
                         public Collection<? extends GrantedAuthority> getAuthorities() {
                             return user.getAuthorities();
                         }
                         @Override
                         public Object getCredentials() {
                             return user.getPassword();
                         }
                         @Override
                         public Object getDetails() {
                             return user;
                         }
                         @Override
                         public Object getPrincipal() {
                             return user;
                         }
                         @Override
                         public boolean isAuthenticated() {
                             return true;
                         }
                         @Override
                         public void setAuthenticated(boolean isAuthenticated) throws IllegalArgumentException { }
                         @Override
                         public String getName() {
                             return user.getUsername();
                         }
     }));
     org.activiti.engine.impl.identity.Authentication.setAuthenticatedUserId(username);
 }
  }
```

这个类可以从我们下载的Activiti7官方提供的Example中找到。

### 2.4.2  添加DemoApplicationConfig类

在Activiti7官方下载的Example中找到DemoApplicationConfig类，它的作用是为了实现SpringSecurity框架的用户权限的配置，这样我们就可以在系统中使用用户权限信息。

本次项目中基本是在文件中定义出来的用户信息，当然也可以是数据库中查询的用户权限信息。

后面处理流程时用到的任务负责人，需要添加在这里

```java
package com.xxl.config;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.security.core.authority.SimpleGrantedAuthority;
import org.springframework.security.core.userdetails.User;
import org.springframework.security.core.userdetails.UserDetailsService;
import org.springframework.security.crypto.bcrypt.BCryptPasswordEncoder;
import org.springframework.security.crypto.password.PasswordEncoder;
import org.springframework.security.provisioning.InMemoryUserDetailsManager;

import java.util.Arrays;
import java.util.List;
import java.util.stream.Collectors;

@Configuration
public class DemoApplicationConfiguration {
    private Logger logger = LoggerFactory.getLogger(DemoApplicationConfiguration.class);
     @Bean
     public UserDetailsService myUserDetailsService() {
         InMemoryUserDetailsManager inMemoryUserDetailsManager = new InMemoryUserDetailsManager();
         //这里添加用户，后面处理流程时用到的任务负责人，需要添加在这里
         String[][] usersGroupsAndRoles = {
                 {"jack", "password", "ROLE_ACTIVITI_USER", "GROUP_activitiTeam"},
                 {"rose", "password", "ROLE_ACTIVITI_USER", "GROUP_activitiTeam"},
                 {"tom", "password", "ROLE_ACTIVITI_USER", "GROUP_activitiTeam"},
                 {"other", "password", "ROLE_ACTIVITI_USER", "GROUP_otherTeam"},
                 {"system", "password", "ROLE_ACTIVITI_USER"},
                 {"admin", "password", "ROLE_ACTIVITI_ADMIN"},
         };

         for (String[] user : usersGroupsAndRoles) {
             List<String> authoritiesStrings = Arrays.asList(Arrays.copyOfRange(user, 2, user.length));
             logger.info("> Registering new user: " + user[0] + " with the following Authorities[" + authoritiesStrings + "]");
             inMemoryUserDetailsManager.createUser(new User(user[0], passwordEncoder().encode(user[1]),
                     authoritiesStrings.stream().map(s -> new SimpleGrantedAuthority(s)).collect(Collectors.toList())));
         }

         return inMemoryUserDetailsManager;
     }
     @Bean
     public PasswordEncoder passwordEncoder() {
         return new BCryptPasswordEncoder();
     }
}
```

## 2.5 创建Bpmn文件

Activiti7可以自动部署流程，前提是在resources目录下，创建一个新的目录processes，用来放置bpmn文件。

创建一个简单的Bpmn流程文件，并设置任务的用户组Candidate Groups。

Candidate Groups中的内容与上面DemoApplicationConfiguration类中出现的用户组名称要保持一致，可以填写：activitiTeam 或者 otherTeam。

这样填写的好处：当不确定到底由谁来负责当前任务的时候，只要是Groups内的用户都可以拾取这个任务

![](/assets/1578369213.xvPar8uK.png)

## 2.6    使用Junit方式测试

```java
package com.xxl.test;

import com.xxl.utils.SecurityUtil;
import org.activiti.api.process.model.ProcessInstance;
import org.activiti.api.process.model.builders.ProcessPayloadBuilder;
import org.activiti.api.process.runtime.ProcessRuntime;
import org.activiti.api.runtime.shared.query.Page;
import org.activiti.api.runtime.shared.query.Pageable;
import org.activiti.api.task.model.Task;
import org.activiti.api.task.model.builders.TaskPayloadBuilder;
import org.activiti.api.task.runtime.TaskRuntime;
import org.activiti.engine.repository.ProcessDefinition;
import org.junit.Test;
import org.junit.runner.RunWith;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.test.context.SpringBootTest;
import org.springframework.test.context.junit4.SpringRunner;

@RunWith(SpringRunner.class)
@SpringBootTest
 public class Actviti7DemoApplicationTests {
     @Autowired
     private ProcessRuntime processRuntime;
     @Autowired
     private TaskRuntime taskRuntime;
     @Autowired
     private SecurityUtil securityUtil;

    @Test
    public void testActBoot(){
        System.out.println(taskRuntime);
    }

    /**
     * 查看流程定义
     */
    @Test
    public void contextLoads() {
        securityUtil.logInAs("system");
        Page<org.activiti.api.process.model.ProcessDefinition> processDefinitionPage =
                processRuntime.processDefinitions(Pageable.of(0, 10));
        System.out.println("可用的流程定义数量：" + processDefinitionPage.getTotalItems());
        for (org.activiti.api.process.model.ProcessDefinition pd : processDefinitionPage.getContent()) {
            System.out.println("流程定义：" + pd);
        }
    }


    /**
     * 启动流程实例
     */
    @Test
    public void testStartProcess() {
        securityUtil.logInAs("system");
        ProcessInstance pi = processRuntime.start(ProcessPayloadBuilder.
                start().
                withProcessDefinitionKey("myProcess").
                build());
        System.out.println("流程实例ID：" + pi.getId());
    }


    /**
     **查询任务，并完成自己的任务
     **/
    @Test
    public void testTask() {
        securityUtil.logInAs("jack");
        Page<Task> taskPage=taskRuntime.tasks(Pageable.of(0,10));
        if (taskPage.getTotalItems()>0){
            for (Task task:taskPage.getContent()){
                taskRuntime.claim(TaskPayloadBuilder.
                        claim().
                        withTaskId(task.getId()).build());
                System.out.println("任务："+task);
                taskRuntime.complete(TaskPayloadBuilder.
                        complete().
                        withTaskId(task.getId()).build());
            }
        }
        Page<Task> taskPage2=taskRuntime.tasks(Pageable.of*(0,10));
        if (taskPage2.getTotalItems()>0){
            System.out.println("任务："+taskPage2.getContent());
        }
    }
}
```

---

---
url: /常用框架/SpringBoot/SpringBoot服务整合/8_Actuator监控.md
---

# Actuator监控

## Actuator监控

Actuator是SpringBoot中一个用来实现系统健康检测的模块，它提供一个Resetful的API接口，可以将系统运行过程中的磁盘空间、线程数以及程序连接的数据库情况通过JSON返回，然后再结合预警、监控模块进行实时系统监控。

Actuator访问路径

![Image00257](/assets/Image00257.EmrOGLrX.jpg)

### 引入依赖

修改pom.xml配置文件，追加依赖库。

```xml
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-actuator</artifactId>
</dependency>
```

### 修改配置文件

修改application.yml配置文件，关闭系统的安全配置。

```yaml
management:
  security:
    enabled: false # 现在关闭系统的安全配置 
```

配置完成之后，就可以通过路径进行相应信息的查看。

* 查看环境信息：http://localhost/env。
* 查看配置Bean：http://localhost/beans。

### 自定义项目信息

虽然现在可以实现Actuator监控，但却需要关闭安全配置。很显然，这样的配置并不合理，最好的做法是由开发者自行定义相应项目信息。下面将为项目建立一个健康信息。

```java
@Component
public class MyHealthIndicator implements HealthIndicator {
    @Override
    public Health health() {
        int errorCode = 100;        // 这个错误的码是通过其它程序获得的
        if (errorCode != 0) {
            return Health.down().withDetail("Error Code", errorCode).build();
        }
        return Health.up().build();
    }
}
```

本程序模拟了一个健康状态的处理，开发者可以通过其他程序来生成一个errorCode错误状态码，用户可以通过http://localhost/health路径进行健康信息访问。

### 配置信息访问路径

SpringBoot构建的主要是微服务，由于微服务中需要为开发者或使用者提供大量的信息，为此在Actuator中提供了信息访问路径（/info），这些服务信息可以直接通过application.yml文件进行配置。

```yaml
info:
  app.name: xxl-microboot              # 应用名称
  company.name: https://luckilyxxl.xyz   # 开发公司
  pom.artifactId: $project.artifactId$   # 项目名称，通过pom获得
  pom.version: $project.version$        # 项目版本，通过pom获得
```

### 修改Maven插件支持

信息配置需要Maven插件支持，为了让所有子模块都支持这种配置，修改pom.xml配置文件。

添加maven-resources-plugin插件。

```xml
<plugin>
    <groupId>org.apache.maven.plugins</groupId>
    <artifactId>maven-resources-plugin</artifactId>
    <configuration>
        <delimiters>
            <delimiter>$</delimiter>			<!-- 定义描述分割符 -->
        </delimiters>
    </configuration>
</plugin>
```

修改资源操作，启用过滤。

```xml
<resource>
    <directory>src/main/resources</directory>
    <includes>
        <include>**/*.properties</include>
        <include>**/*.yml</include>
        <include>**/*.xml</include>
        <include>**/*.tld</include>
        <include>**/*.p12</include>
    </includes>
    <filtering>true</filtering>
</resource>
```

此时，程序启动之后，可以输入信息访问路径http://localhost/info。

### 创建信息配置类

在开发中这种提示信息会成为微服务的重要组成部分，如果重复进行配置文件的定义，那么会比较麻烦。最简单的做法是直接做一个配置程序类，进行信息的配置。

```java
@Component
public class MicroServiceInfoContributor implements InfoContributor {
    @Override
    public void contribute(Info.Builder builder) {
        builder.withDetail("company.name", "https://luckilyxxl.xyz") ;
        builder.withDetail("version", "V1.0") ;
        builder.withDetail("author", "xxl") ;
    }
}
```

此时可以直接通过配置类获取微服务信息。

---

---
url: /常用框架/SpringAIAlibaba/Agent Framework/12_Agent Tool智能体作为工具.md
---

# Agent Tool 智能体作为工具

---

---
url: /常用框架/SpringAIAlibaba/Agent Chat UI/1_AgentChatUI.md
---

# AgentChatUI

---

---
url: /常用框架/SpringAIAlibaba/Agent Framework/1_Agents.md
---

# Agents

## 一、简介

Agents 将大语言模型与工具结合，创建具备任务推理、工具使用决策、工具调用的自动化系统，系统具备持续推理、工具调用的循环迭代能力，直至问题解决。

Spring AI Alibaba 提供了基于 `ReactAgent` 的生产级 Agent 实现。

**一个 LLM Agent 在循环中通过运行工具来实现目标**。Agent 会一直运行直到满足停止条件 —— 即当模型输出最终答案或达到迭代限制时。

[万字拆解：Agent 到底是什么 + 有哪些使用场景](https://www.woshipm.com/ai/6293843.html)

[ 7000长文：一文读懂Agent，大模型的下一站 - 知乎](https://zhuanlan.zhihu.com/p/678046050)

## 二、ReactAgent 理论基础

### 2.1、什么是 ReAct

ReAct（Reasoning + Acting）是一种将推理和行动相结合的 Agent 范式。在这个范式中，Agent 会：

1. **思考（Reasoning）**：分析当前情况，决定下一步该做什么
2. **行动（Acting）**：执行工具调用或生成最终答案
3. **观察（Observation）**：接收工具执行的结果
4. **迭代**：基于观察结果继续思考和行动，直到完成任务

这个循环使 Agent 能够：

* 将复杂问题分解为多个步骤
* 动态调整策略基于中间结果
* 处理需要多次工具调用的任务
* 在不确定的环境中做出决策

### 2.2、ReactAgent 的工作原理

Spring AI Alibaba 中的`ReactAgent` 基于 **Graph 运行时**构建。Graph 由节点（steps）和边（connections）组成，定义了 Agent 如何处理信息。Agent 在这个 Graph 中移动，执行如下节点：

* **Model Node (模型节点)**：调用 LLM 进行推理和决策
* **Tool Node (工具节点)**：执行工具调用
* **Hook Nodes (钩子节点)**：在关键位置插入自定义逻辑

ReactAgent 的核心执行流程：

![reactagent](/assets/reactagent.j2irOl0U.png)

## 三、核心组件

### 3.1、Model（模型）

Model 是 Agent 的推理引擎。Spring AI Alibaba 支持多种配置方式。

#### 基础模型配置

最直接的方式是使用 `ChatModel` 实例：

```java
package com.xxl.ai.example;

import com.alibaba.cloud.ai.dashscope.api.DashScopeApi;
import com.alibaba.cloud.ai.dashscope.chat.DashScopeChatModel;
import com.alibaba.cloud.ai.graph.agent.ReactAgent;

/**
 * 基础模型配置
 */
public static void basicModelConfiguration() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();

    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .build();

    // 创建 Agent
    ReactAgent agent = ReactAgent.builder()
        .name("my_agent")
        .model(chatModel)
        .build();
}
```

#### 高级模型配置

通过 `ChatOptions` 可以精细控制模型行为：

```java
package com.xxl.ai.example;

import com.alibaba.cloud.ai.dashscope.api.DashScopeApi;
import com.alibaba.cloud.ai.dashscope.chat.DashScopeChatModel;
import com.alibaba.cloud.ai.dashscope.chat.DashScopeChatOptions;
import com.alibaba.cloud.ai.graph.agent.ReactAgent;

/**
 * 高级模型配置
 */
public static void advancedModelConfiguration() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();

    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(DashScopeChatOptions.builder()
                        .withTemperature(0.7) // 控制随机性
                        .withMaxToken(2000) // 最大输出长度
                        .withTopP(0.9) // 核采样参数
                        .build())
        .build();

    // 创建 Agent
    ReactAgent agent = ReactAgent.builder()
        .name("my_agent")
        .model(chatModel)
        .build();
}
```

**常用参数说明**：

* `temperature`：控制输出的随机性（0.0-1.0），值越高越有创造性
* `maxTokens`：限制单次响应的最大 token 数
* `topP`：核采样，控制输出的多样性

### 3.2、Tools（工具）

工具赋予 Agent 执行操作的能力，支持顺序执行、并行调用、动态选择和错误处理。

#### 定义和使用工具

```java
package com.xxl.ai.tool;

import org.springframework.ai.chat.model.ToolContext;

import java.util.function.BiFunction;

/**
 * 搜索工具
 *
 * @Classname SearchTool
 * @Description TODO
 * @Date 2025/11/29 23:55
 * @Created by xxl
 */
public class SearchTool implements BiFunction<String, ToolContext, String> {

    @Override
    public String apply(String query, ToolContext context) {
        // 实现搜索逻辑
        return "搜索结果: " + query;
    }

}
```

调用工具

```java
/**
 * 工具组件——搜索工具
 *
 * @throws GraphRunnerException 异常
 */
public static void toolSearchModelConfiguration() throws GraphRunnerException {
    // 创建模型实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .build();
    // 创建工具回调
    ToolCallback searchTool = FunctionToolCallback.builder("search", new SearchTool())
        .description("搜索工具")
        .inputType(String.class)
        .build();
    // 创建 Agent
    ReactAgent agent = ReactAgent.builder()
        .name("my_agent")
        .model(chatModel)
        .tools(searchTool)
        .build();
    // 运行 Agent
    AssistantMessage response = agent.call("查询杭州天气并推荐活动");
    System.out.println(response.getText());
}
```

输出

```markdown
根据当前信息，杭州近期天气较为舒适，适合户外活动。以下是一些推荐：

1. **西湖景区游览**：漫步苏堤、白堤，欣赏湖光山色，可乘船游湖。
2. **灵隐寺祈福**：参观千年古刹，感受佛教文化，周边还有飞来峰景区。
3. **龙井村品茶**：前往龙井村，体验采茶、品茗，了解龙井茶文化。
4. **河坊街逛街**：品尝杭州特色小吃，如葱包桧、定胜糕，购买地方特产。
5. **西溪湿地公园**：适合骑行或徒步，亲近自然，观赏湿地生态。

建议根据当天具体天气情况（如是否有雨、温度变化）准备合适的衣物和防晒措施。若天气晴好，非常适合拍照打卡！
```

#### 工具错误处理

ToolErrorInterceptor 工具错误处理

```java
package com.xxl.ai.interceptor;

import com.alibaba.cloud.ai.graph.agent.interceptor.ToolCallHandler;
import com.alibaba.cloud.ai.graph.agent.interceptor.ToolCallRequest;
import com.alibaba.cloud.ai.graph.agent.interceptor.ToolCallResponse;
import com.alibaba.cloud.ai.graph.agent.interceptor.ToolInterceptor;

/**
 * 工具错误处理
 *
 * @Classname ToolErrorInterceptor
 * @Description TODO
 * @Date 2025/11/29 23:42
 * @Created by xxl
 */
public class ToolErrorInterceptor extends ToolInterceptor {

    @Override
    public ToolCallResponse interceptToolCall(ToolCallRequest request, ToolCallHandler handler) {
        try {
            return handler.call(request);
        } catch (Exception e) {
            return ToolCallResponse.of(request.getToolCallId(), request.getToolName(),
                    "Tool failed: " + e.getMessage());
        }
    }

    @Override
    public String getName() {
        return "ToolErrorInterceptor";
    }
}

```

调用工具

```java
/**
 * 工具组件——工具错误处理
 * 
 * @throws GraphRunnerException 异常
 */
public static void toolErrorModelConfiguration() throws GraphRunnerException {
    // 创建模型实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .build();
    // 创建 Agent
    ReactAgent agent = ReactAgent.builder()
        .name("my_agent")
        .model(chatModel)
        .interceptors(new ToolErrorInterceptor())
        .build();
    // 运行 Agent
    AssistantMessage response = agent.call("Who are you?");
    System.out.println(response.getText());
}
```

结果

```markdown
Hello! I'm Qwen, a large-scale language model independently developed by the Tongyi Lab under Alibaba Group. I can assist you with answering questions, writing, logical reasoning, programming, and more. I support 100 languages, including but not limited to Chinese, English, German, French, Spanish, etc., meeting international usage needs. If you have any questions or need help, feel free to let me know anytime!
```

**ReAct 循环示例**：Agent 自动交替进行推理和工具调用，直到获得最终答案。

```markdown
用户: 查询杭州天气并推荐活动
→ [推理] 需要查天气 → [行动] get_weather("杭州") → [观察] 晴，25°C
→ [推理] 需要推荐活动 → [行动] search("户外活动") → [观察] 西湖游玩...
→ [推理] 信息充足 → [行动] 生成答案
```

### 3.3、System Prompt（系统提示）

System Prompt 塑造 Agent 处理任务的方式。

#### 基础用法

通过 `systemPrompt` 参数提供字符串：

```java
ReactAgent agent = ReactAgent.builder()
    .name("my_agent")
    .model(chatModel)
    .systemPrompt("你是一个专业的技术助手。请准确、简洁地回答问题。")
    .build();
```

#### 使用 instruction

对于更详细的指令，使用 `instruction` 参数：

```java
String instruction = """
    你是一个经验丰富的软件架构师。

    在回答问题时，请：
    1. 首先理解用户的核心需求
    2. 分析可能的技术方案
    3. 提供清晰的建议和理由
    4. 如果需要更多信息，主动询问

    保持专业、友好的语气。
    """;

ReactAgent agent = ReactAgent.builder()
    .name("architect_agent")
    .model(chatModel)
    .instruction(instruction)
    .build();
```

#### 动态 System Prompt

使用 `ModelInterceptor` 实现基于上下文的动态提示：

DynamicPromptInterceptor 动态提示拦截器

```java
package com.xxl.ai.interceptor;

import com.alibaba.cloud.ai.graph.agent.interceptor.ModelCallHandler;
import com.alibaba.cloud.ai.graph.agent.interceptor.ModelInterceptor;
import com.alibaba.cloud.ai.graph.agent.interceptor.ModelRequest;
import com.alibaba.cloud.ai.graph.agent.interceptor.ModelResponse;
import org.springframework.ai.chat.messages.SystemMessage;

/**
 * DynamicPromptInterceptor 动态提示拦截器
 *
 * @Classname DynamicPromptInterceptor
 * @Description TODO
 * @Date 2025/11/30 00:25
 * @Created by xxl
 */
public class DynamicPromptInterceptor extends ModelInterceptor {

    @Override
    public ModelResponse interceptModel(ModelRequest request, ModelCallHandler handler) {
        // 基于上下文构建动态 system prompt
        String userRole = (String) request.getContext().getOrDefault("user_role", "default");
        String dynamicPrompt = switch (userRole) {
            case "expert" -> "你正在与技术专家对话。\n- 使用专业术语\n- 深入技术细节 ";
            case "beginner" -> "你正在与初学者对话。\n- 使用简单语言\n- 解释基础概念 ";
            default -> "你是一个专业的助手，保持友好和专业。";
        };

        SystemMessage enhancedSystemMessage;
        if (request.getSystemMessage() == null) {
            enhancedSystemMessage = new SystemMessage(dynamicPrompt);
        } else {
            enhancedSystemMessage = new SystemMessage(request.getSystemMessage().getText() + "\n" + dynamicPrompt);
        }

        ModelRequest modified = ModelRequest.builder(request)
                .systemMessage(enhancedSystemMessage)
                .build();
        return handler.call(modified);
    }

    @Override
    public String getName() {
        return "DynamicPromptInterceptor";
    }
}
```

调用工具

```java
/**
 * 动态 System Prompt
 *
 * @throws GraphRunnerException 异常
 */
public static void systemPromptModelConfiguration() throws GraphRunnerException {
    // 创建模型实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .build();
    // 创建 Agent
    ReactAgent agent = ReactAgent.builder()
        .name("adaptive_agent")
        .model(chatModel)
        .interceptors(new DynamicPromptInterceptor())
        .build();
    // 运行 Agent
    AssistantMessage response = agent.call("Spring AI Alibaba是个什么框架?");
    System.out.println(response.getText());
}
```

## 四、调用 Agent

### 4.1、基础调用

使用 `call` 方法获取最终响应：

```java
import org.springframework.ai.chat.messages.AssistantMessage;

// 字符串输入
AssistantMessage response = agent.call("杭州的天气怎么样？");
System.out.println(response.getText());

// UserMessage 输入
UserMessage userMessage = new UserMessage("帮我分析这个问题");
AssistantMessage response = agent.call(userMessage);

// 多个消息
List<Message> messages = List.of(
    new UserMessage("我想了解 Java 多线程"),
    new UserMessage("特别是线程池的使用")
);
AssistantMessage response = agent.call(messages);
```

### 4.2、获取完整状态

使用 `invoke` 方法获取完整的执行状态：

```java
import com.alibaba.cloud.ai.graph.OverAllState;
import java.util.Optional;

Optional<OverAllState> result = agent.invoke("帮我写一首诗");

if (result.isPresent()) {
    OverAllState state = result.get();

    // 访问消息历史
    Optional<Object> messages = state.value("messages");
    List<Message> messageList = (List<Message>) messages.get();

    // 访问自定义状态
    Optional<Object> customData = state.value("custom_key");

    System.out.println("完整状态：" + state);
}
```

### 4.3、使用配置

通过 `RunnableConfig` 传递运行时配置：

```java
import com.alibaba.cloud.ai.graph.RunnableConfig;

String threadId = "thread_123";
RunnableConfig runnableConfig = RunnableConfig.builder()
    .threadId(threadId)
    .addMetadata("key", "value")
    .build();

AssistantMessage response = agent.call("你的问题", runnableConfig);
```

## 五、高级特性

### 5.1、结构化输出

在某些情况下，你可能希望 Agent 以特定格式返回输出。ReactAgent 提供了两种策略。

#### 使用 outputType

通过 Java 类定义输出结构，Agent 会自动生成对应的 JSON Schema：

PoemOutput 结构化输出示例

```java
package com.xxl.ai.output;

/**
 * outputType 输出格式
 *
 * @Classname PoemOutput
 * @Description TODO
 * @Date 2025/11/30 22:48
 * @Created by xxl
 */
public class PoemOutput {
    private String title;
    private String content;
    private String style;

    // Getters and Setters
    public String getTitle() {
        return title;
    }

    public void setTitle(String title) {
        this.title = title;
    }

    public String getContent() {
        return content;
    }

    public void setContent(String content) {
        this.content = content;
    }

    public String getStyle() {
        return style;
    }

    public void setStyle(String style) {
        this.style = style;
    }
}
```

使用 outputType 定义输出格式

```java
/**
 * 高级功能——使用 outputType 定义输出格式
 *
 * @throws GraphRunnerException
 */
public static void advancedFeatureOutputType() throws GraphRunnerException {
    // 初始化 ChatModel
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .build();
    ReactAgent agent = ReactAgent.builder()
        .name("poem_agent")
        .model(chatModel)
        .outputType(PoemOutput.class)
        .saver(new MemorySaver())
        .build();

    AssistantMessage message = agent.call("帮我写一首关于春天的诗歌。");
}
```

输出

```json
{
  "content": "春风轻拂绿意生，\n细雨如丝润无声。\n桃花笑映晨光里，\n柳絮飘飞暮色中。\n溪水潺潺歌新曲，\n燕子呢喃筑旧踪。\n万物复苏心亦暖，\n人间最美是春浓。",
  "style": "古典",
  "title": "春之韵"
}
```

#### 使用 outputSchema

直接提供 JSON Schema 字符串进行更灵活的控制：

```java
/**
 * 高级功能——使用 outputSchema 定义输出格式
 *
 * @throws GraphRunnerException
 */
public static void advancedFeatureOutputSchema() throws GraphRunnerException {
    String customSchema = """
        请严格按照以下JSON格式返回结果：
    {
        "summary": "内容摘要",
        "keywords": ["关键词1", "关键词2", "关键词3"],
        "sentiment": "情感倾向（正面/负面/中性）",
        "confidence": 0.95
    }
    """;
    // 初始化 ChatModel
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .build();
    ReactAgent agent = ReactAgent.builder()
        .name("analysis_agent")
        .model(chatModel)
        .outputSchema(customSchema)
        .saver(new MemorySaver())
        .build();

    AssistantMessage response = agent.call("分析这段文本：春天来了，万物复苏。");
    System.out.println(response.getText());
}
```

输出

````markdown
```json
{
    "summary": "春天到来，自然界万物开始复苏，呈现出生机勃勃的景象。",
    "keywords": ["春天", "万物复苏", "生机"],
    "sentiment": "正面",
    "confidence": 0.95
}
```
````

**选择建议**：

* `outputType`：类型安全，适合结构固定的场景
* `outputSchema`：灵活性高，适合动态或复杂的输出格式

### 5.2、Memory（记忆）

Agent 通过状态自动维护对话历史。使用 `MemorySaver` 配置持久化存储。

Memory 配置示例

```java
/**
 * 高级功能——Memory 记忆
 *
 * @throws GraphRunnerException
 */
public static void advancedFeatureMemory() throws GraphRunnerException {
    // 初始化 ChatModel
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .build();
    // 配置内存存储
    ReactAgent agent = ReactAgent.builder()
        .name("chat_agent")
        .model(chatModel)
        .saver(new MemorySaver())
        .build();

    // 使用 thread_id 维护对话上下文
    RunnableConfig config = RunnableConfig.builder()
        .threadId("user_123")
        .build();

    AssistantMessage message01 = agent.call("我叫张三", config);
    System.out.println(message01.getText());
    AssistantMessage message02 = agent.call("我叫什么名字？", config);
    System.out.println(message02.getText());  // 输出: "你叫张三"
}
```

输出

```markdown
你好，张三！有什么我可以帮你的吗？😊
你叫张三。😊
```

**生产环境**：使用 `RedisSaver`、`MongoSaver` 等持久化存储替代 `MemorySaver`。

### 5.3、Hooks（钩子）

Hooks 允许在 Agent 执行的关键点插入自定义逻辑。

#### Hook 类型与使用

Hook 使用示例

```
import com.alibaba.cloud.ai.graph.agent.hook.*;

// 1. AgentHook - 在 Agent 开始/结束时执行，每次Agent调用只会运行一次
@HookPositions({HookPosition.BEFORE_AGENT, HookPosition.AFTER_AGENT})
public class LoggingHook extends AgentHook {
@Override
public String getName() { return "logging"; }

@Override
public CompletableFuture<Map<String, Object>> beforeAgent(OverAllState state, RunnableConfig config) {
System.out.println("Agent 开始执行");
return CompletableFuture.completedFuture(Map.of());
}

@Override
public CompletableFuture<Map<String, Object>> afterAgent(OverAllState state, RunnableConfig config) {
System.out.println("Agent 执行完成");
return CompletableFuture.completedFuture(Map.of());
}
}

// 2. ModelHook - 在模型调用前后执行（例如：消息修剪），区别于AgentHook，ModelHook在一次agent调用中可能会调用多次，也就是每次 reasoning-acting 迭代都会执行
public class MessageTrimmingHook extends ModelHook {
private static final int MAX_MESSAGES = 10;

@Override
public String getName() {
return "message_trimming";
}

@Override
public HookPosition[] getHookPositions() {
return new HookPosition[]{HookPosition.BEFORE_MODEL};
}

@Override
public CompletableFuture<Map<String, Object>> beforeModel(OverAllState state, RunnableConfig config) {
Optional<Object> messagesOpt = state.value("messages");
if (messagesOpt.isPresent()) {
List<Message> messages = (List<Message>) messagesOpt.get();
if (messages.size() > MAX_MESSAGES) {
return CompletableFuture.completedFuture(Map.of("messages",
messages.subList(messages.size() - MAX_MESSAGES, messages.size())));
}
}
return CompletableFuture.completedFuture(Map.of());
}

@Override
public CompletableFuture<Map<String, Object>> afterModel(OverAllState state, RunnableConfig config) {
return CompletableFuture.completedFuture(Map.of());
}
}
```

**Hook 执行位置**：

* `BEFORE_AGENT` / `AFTER_AGENT`：Agent 整体执行前后
* `BEFORE_MODEL` / `AFTER_MODEL`：Agent Loop 循环过程中，每次模型调用前后

### 5.4、Interceptors（拦截器）

Interceptors 提供更细粒度的控制，可以拦截和修改模型调用和工具执行。

```
import com.alibaba.cloud.ai.graph.agent.interceptor.*;

// ModelInterceptor - 内容安全检查
public class GuardrailInterceptor extends ModelInterceptor {
@Override
public ModelResponse interceptModel(ModelRequest request, ModelCallHandler handler) {
// 前置：检查输入
if (containsSensitiveContent(request.getMessages())) {
return ModelResponse.blocked("检测到不适当的内容");
}

// 执行调用
ModelResponse response = handler.call(request);

// 后置：检查输出
return sanitizeIfNeeded(response);
}
}

// ToolInterceptor - 监控和错误处理
public class ToolMonitoringInterceptor extends ToolInterceptor {
@Override
public ToolCallResponse interceptToolCall(ToolCallRequest request, ToolCallHandler handler) {
long startTime = System.currentTimeMillis();
try {
ToolCallResponse response = handler.call(request);
logSuccess(request, System.currentTimeMillis() - startTime);
return response;
} catch (Exception e) {
logError(request, e, System.currentTimeMillis() - startTime);
return ToolCallResponse.error(request.getToolCall(),
"工具执行遇到问题，请稍后重试");
}
}
}

// 组合使用
ReactAgent agent = ReactAgent.builder()
.name("my_agent")
.model(chatModel)
.interceptors(new GuardrailInterceptor(), new LoggingInterceptor(), new ToolMonitoringInterceptor())
.saver(new MemorySaver())
.build();
```

**常见用途**：

* **ModelInterceptor**：内容安全、动态提示、日志记录、性能监控
* **ToolInterceptor**：错误重试、权限检查、结果缓存、审计日志

### 5.5、控制与流式输出

#### 迭代控制

通过 Hooks 控制 Agent 的执行迭代，防止无限循环或过度成本。

使用 ModelCallLimitHook 限制模型调用次数

```
import com.alibaba.cloud.ai.graph.agent.hook.modelcalllimit.ModelCallLimitHook;
import com.alibaba.cloud.ai.graph.checkpoint.savers.MemorySaver;

// 使用内置的 ModelCallLimitHook 限制模型调用次数
ReactAgent agent = ReactAgent.builder()
.name("my_agent")
.model(chatModel)
.hooks(ModelCallLimitHook.builder().runLimit(5).build()) // 限制最多调用 5 次
.saver(new MemorySaver())
.build();
```

自定义停止条件 Hook

```
import com.alibaba.cloud.ai.graph.agent.hook.ModelHook;
import com.alibaba.cloud.ai.graph.agent.hook.HookPosition;
import com.alibaba.cloud.ai.graph.agent.hook.HookPositions;
import com.alibaba.cloud.ai.graph.agent.hook.JumpTo;
import org.springframework.ai.chat.messages.AssistantMessage;

// 自定义停止条件：基于状态判断是否继续
@HookPositions({HookPosition.BEFORE_MODEL})
public class CustomStopConditionHook extends ModelHook {

@Override
public String getName() {
return "custom_stop_condition";
}

@Override
public CompletableFuture<Map<String, Object>> beforeModel(OverAllState state, RunnableConfig config) {
// 检查是否找到答案，展示使用 OverAllState
boolean answerFound = (Boolean) state.value("answer_found").orElse(false);
// 检查错误次数，展示使用 RunnableConfig
int errorCount = (Integer) config.context().get("error_count").orElse(0);

// 找到答案或错误过多时停止
if (answerFound || errorCount > 3) {
List<Message> messages = new ArrayList<>(
(List<Message>) state.value("messages").orElse(new ArrayList<>())
);
messages.add(new AssistantMessage(
answerFound ? "已找到答案，Agent 执行完成。"
: "错误次数过多 (" + errorCount + ")，Agent 执行终止。"
));
return CompletableFuture.completedFuture(Map.of("messages", messages));
}

return CompletableFuture.completedFuture(Map.of());
}

}

// 使用自定义停止条件
ReactAgent agent = ReactAgent.builder()
.name("my_agent")
.model(chatModel)
.hooks(new CustomStopConditionHook())
.saver(new MemorySaver())
.build();
```

#### 流式输出

流式输出示例

```
import reactor.core.publisher.Flux;

Flux<NodeOutput> stream = agent.stream("复杂任务");
stream.subscribe(
response -> System.out.println("进度: " + response),
error -> System.err.println("错误: " + error),
() -> System.out.println("完成")
);
```

---

---
url: /Python/AI大模型应用开发/1_AI大模型科普.md
---

# AI大模型科普

## 一、啥是“AIGC”及一系列AI技术词

### 1、什么是 AIGC？

**AIGC** 是 **AI-Generated Content** 的缩写，意为“**人工智能生成内容**”。

* 它指的是由 AI 自动生成的文字、图片、音频、视频、代码等内容。
* 例如：
  * 用 ChatGPT 写文章 ✍️
  * 用 Midjourney 生成图像 🖼️
  * 用 GitHub Copilot 写代码 💻
  * 用 Sora 生成视频 🎥
* 这些都属于 **AIGC 的范畴**。

> 🔍 小知识：虽然“AIGC”在中国更流行，但在国际上更常用的是 **Generative AI（生成式 AI）**。两者本质相同，但语境略有差异。

### 2、AIGC 与 生成式 AI 的关系

| 术语                          | 含义                         | 关系             |
| ----------------------------- | ---------------------------- | ---------------- |
| **生成式 AI (Generative AI)** | 能够“创造”新内容的 AI 技术   | 是“工具”或“能力” |
| **AIGC**                      | 由生成式 AI 创造出的内容本身 | 是“产物”或“结果” |

✅ 所以：

> **生成式 AI → 生成 → AIGC**

👉 比如：**ChatGPT、Midjourney、Stable Diffusion** 都是生成式 AI 模型，它们输出的内容就是 AIGC。

### 3、AI 的大框架：从人工智能到大模型

为了理清这些概念，我们需要从宏观角度理解 AI 的层级结构。

#### 📊 AI 的“家族树”结构

```
                    人工智能 (AI)
                         ↓
                   机器学习 (ML)
                  ↙       ↓        ↘
            监督学习    无监督学习    强化学习
                         ↓
                    深度学习 (DL)
                  ↙              ↘
          生成式 AI           大语言模型 (LLM)
                ↘               ↙
                 AIGC（AI生成内容）
```

下面我们逐层解析：

***

#### 1. 什么是人工智能（AI）？

* **定义**：让机器模拟人类智能行为的技术，如理解语言、识别图像、推理决策等。
* 自 1956 年达特茅斯会议确立为独立学科以来，历经多次“寒冬”与“爆发”。

***

#### 2. 什么是机器学习（Machine Learning, ML）？

* **核心思想**：不靠人工编写规则，而是让计算机通过数据“自己学习”规律。
* ❌ 传统方式（非机器学习）：
  * “如果图片有红色 → 是玫瑰；有橙色 → 是向日葵” → 明确编程逻辑。
* ✅ 机器学习方式：
  * 给大量带标签的花的照片（输入 + 正确答案），让模型自己找规律，预测新图是什么花。

机器学习三大范式：

| 类型           | 特点                            | 应用举例            |
| -------------- | ------------------------------- | ------------------- |
| **监督学习**   | 数据带“标签”（正确答案）        | 图像分类、房价预测  |
| **无监督学习** | 数据无标签，模型自主发现模式    | 新闻聚类、用户分群  |
| **强化学习**   | 通过“奖励/惩罚”反馈学习最优策略 | 游戏 AI、机器人控制 |

> 🐶 类比：就像训练小狗，做对了给零食，做错了不给，逐渐学会听话。

***

#### 3. 什么是深度学习（Deep Learning）？

* 是机器学习的一种方法，核心是使用**人工神经网络**（模仿人脑结构）。
* “深度”指网络有很多层（输入层 → 多个隐藏层 → 输出层）。
* 每一层提取更复杂的特征：
  * 第一层：边缘
  * 第二层：形状
  * 第三层：器官（如眼睛、耳朵）
  * 最终：判断是否是“猫”

> ✅ 深度学习可应用于监督、无监督、强化学习。

***

#### 4. 什么是生成式 AI？

* 是深度学习的一个重要应用方向。
* 目标：**学习已有数据的模式，生成全新的、类似的内容**。
* 常见形式：
  * 文本生成（如 GPT）
  * 图像生成（如扩散模型 Diffusion）
  * 音频合成（如语音克隆）
  * 视频生成（如 Sora）

> ⚠️ 注意：**不是所有生成式 AI 都是大模型**。例如图像生成的扩散模型就不是大语言模型。

5. 什么是大语言模型（LLM, Large Language Model）？

* 是深度学习在自然语言处理领域的巅峰应用。
* “大”体现在：
  * 参数量巨大（数十亿到万亿级）
  * 训练数据海量（整个互联网文本）
* 能力强大：
  * 理解上下文
  * 生成流畅文本
  * 回答问题、写诗、编程等

常见 LLM 示例：

| 模型                  | 国家/公司      | 特点               |
| --------------------- | -------------- | ------------------ |
| GPT 系列（如 GPT-4）  | OpenAI（美国） | 强大的文本生成能力 |
| ChatGLM（智谱AI）     | 中国           | 中文优化           |
| Qwen（通义千问）      | 阿里云         | 多模态、开源       |
| ERNIE Bot（文心一言） | 百度           | 结合搜索优势       |

> ❓ 争议点：**所有大语言模型都是生成式 AI 吗？**
>
> * 多数是（如 GPT），但也有例外：
> * 例如 Google 的 **BERT**：擅长理解语言（用于搜索排序、情感分析），但**不擅长生成连贯长文本**，因此有人认为它不算“生成式 AI”。

### 4、一句话总结

> **AIGC 是结果，生成式 AI 是能力，大模型是工具，深度学习是方法，机器学习是路径，人工智能是目标。**

## 二、啥是大语言模型（LLM）

### 1、大语言模型的“出圈”时刻

* **2022年11月30日**，OpenAI 发布 **ChatGPT**。
* 它成为互联网历史上**最快突破1亿用户**的产品，引爆全球对 AI 的关注。
* 一夜之间，各类 AI 聊天助手如雨后春笋般涌现。
* 而这一切的核心技术基础，就是——**大语言模型（Large Language Model, LLM）**。

### 2、什么是大语言模型（LLM）？

#### ✅ 定义：

**大语言模型**（LLM），全称 *Large Language Model*，是一种基于**深度学习**的自然语言处理模型，能够理解并生成人类语言。

#### 🔧 核心能力：

给它一段文本输入，它可以完成多种任务，例如：

* 文本生成（写文章、写诗、写代码）
* 内容总结
* 情感分析
* 语言翻译
* 分类与改写

> 🌐 它不是“一个工具”，而是一个“通才型 AI 大脑”。

***

### 3、“大”在哪里？——参数与数据的双重爆炸

很多人以为“大”只是数据多，其实不然。**“大”主要体现在两个方面**：

| 维度                | 说明                                                         |
| ------------------- | ------------------------------------------------------------ |
| **1. 海量训练数据** | 使用整个互联网规模的文本：• 书籍、新闻、论文• Wikipedia、社交媒体帖子等让模型“读万卷书”，理解语言规律 |
| **2. 巨量参数**     | 参数是模型在训练中“学到的知识”决定了模型如何响应输入参数越多，模型越灵活、越强大 |

#### 📈 参数增长趋势（以 GPT 系列为例）：

| 模型               | 参数数量      | 相当于什么？                   |
| ------------------ | ------------- | ------------------------------ |
| GPT-1（2018-06）   | 1.17 亿       | 初级语言模型                   |
| GPT-2（2019-02）   | 15 亿         | 能写简单文章                   |
| GPT-3（2020-05）   | **1750 亿**   | 超大规模，接近人类语言理解能力 |
| ChatGPT（2020-11） | **1750 亿**   |                                |
| GPT-4（2023-03）   | **1.76 万亿** |                                |
| GPT-5（2025-08）   | **17.5万亿**  |                                |

> 🍞 类比理解： 就像做蛋糕，小模型只能调“面粉、糖、蛋”；大模型还能调“奶油、牛奶、苏打粉、可可粉、温度、时间”……变量越多，越能做出复杂美味的蛋糕，甚至创造新口味！

***

### 4、大模型 vs 小模型：通才 vs 专才

| 类型       | 特点                                                         | 举例                                                         |
| ---------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **小模型** | 针对单一任务训练如：情感分类、命名实体识别               | 训练一个模型只做“判断评论是好评还是差评”                     |
| **大模型** | 一个模型搞定多种任务无需重新训练，通过“提示”即可切换功能 | 同一个模型：→ 写文章→ 改写句子→ 回答问题→ 写代码 |

> ✅ 大模型的优势：**泛化能力强、部署成本低、适应性广**

***

### 5、技术里程碑：Transformer 架构的诞生

虽然 ChatGPT 是2022年“出圈”的，但它的技术根源要追溯到 **2017年**。

#### 📄 2017年6月：谷歌发布划时代论文

> **[《Attention is All You Need》](https://arxiv.org/pdf/1706.03762)**

这篇论文提出了 **Transformer 架构**，彻底改变了自然语言处理的发展方向。

#### 🔁 此前主流：RNN（循环神经网络）

* 问题：
  * **逐字顺序处理**：必须等前一个词处理完才能处理下一个 → 速度慢
  * **长距离依赖难捕捉**：比如句子开头的“猫”和结尾的“它”之间的关系容易丢失
* 改进版：LSTM（长短期记忆网络），但依然无法根本解决效率问题

#### 🚀 Transformer 的突破性创新

##### 1. **自注意力机制（Self-Attention）**

* 核心思想：**每个词在处理时，都会“关注”句子中所有其他词**
* 模型会为每个词分配一个“注意力权重”，表示它与其他词的相关性
* 权重是在训练中自动学习得到的

> 🎯 举例： 句子：“The cat was hungry, so it ate the food.”
>
> * 当处理 “it” 时，模型会发现它和 “cat” 的关联更强，而不是离得更近的 “hungry”
> * 即使相隔很远，也能准确理解指代关系

✅ 优势：**精准捕捉长距离语义依赖**

##### 2. **位置编码（Positional Encoding）**

* 问题：语言中“顺序”很重要（“你打我” ≠ “我打你”）
* RNN 天然按顺序处理，但 Transformer 是**并行处理所有词**
* 解决方案：给每个词加上“位置信息”的数字编码（位置向量）

> 🧮 输入 = 词向量 + 位置向量
> → 模型既知道“词是什么”，也知道“词在哪儿”

✅ 优势：**支持并行计算，大幅提升训练速度**

***

### 6、Transformer 为何如此重要？

| 优势           | 说明                                                |
| -------------- | --------------------------------------------------- |
| ✅ 并行计算     | 所有词同时处理，不再串行等待 → **训练速度快几十倍** |
| ✅ 长距离依赖   | 自注意力机制完美解决“遗忘远距离信息”问题            |
| ✅ 可扩展性强   | 支持训练超大规模模型（百亿、千亿参数）              |
| ✅ 成为行业标准 | 几乎所有现代大模型都基于 Transformer 或其变体       |

> 🌟 正是因为 Transformer 的出现，才使得 GPT、BERT、ChatGLM、通义千问等大模型成为可能。

***

### 7、GPT 名字的秘密

**GPT = Generative Pre-trained Transformer**

| 缩写            | 含义   | 说明                       |
| --------------- | ------ | -------------------------- |
| **G**enerative  | 生成式 | 能生成新文本（而非仅分类） |
| **P**re-trained | 预训练 | 先在海量文本上自学语言规律 |
| **T**ransformer | 架构   | 基于 Transformer 网络结构  |

> 🔍 所以，“GPT” 三个字母就揭示了它的核心技术路线。

***

### 8、常见大语言模型应用

你日常使用的这些 AI 工具，背后都是大模型驱动：

| 应用产品 | 所用大模型  | 国家/公司         |
| -------- | ----------- | ----------------- |
| ChatGPT  | GPT 系列    | OpenAI（美国）    |
| 文心一言 | ERNIE Bot   | 百度（中国）      |
| 通义千问 | Qwen        | 阿里云（中国）    |
| ChatGLM  | GLM 系列    | 智谱AI（中国）    |
| Claude   | Claude 系列 | Anthropic（美国） |

## 三、AI聊天助手背后的黑科技

一个常见的说法是，像GPT这样的生成式大模型通过“预测下一个最可能出现的词”来生成文本，类似于搜索引擎的自动补全。但这个过程背后是如何实现的？关键在于 **Transformer 架构**。

***

### 1、Transformer：大模型的基石

自2017年论文《Attention is All You Need》提出 **Transformer** 架构以来，它几乎统一了自然语言处理领域。无论是OpenAI的GPT、清华的JLM，还是百度的ERNIE，其核心都离不开Transformer。

Transformer由两个主要部分组成：

* **编码器（Encoder）**
* **解码器（Decoder）**

***

### 2、输入处理流程

1. **Token化（Tokenization）**
   * 输入文本被拆分为基本单位——**Token**。
   * Token可以是一个单词、子词或汉字。
   * 每个Token被映射为一个整数ID（Token ID），因为计算机只能处理数字。
2. **词嵌入（Embedding）**
   * 每个Token ID通过嵌入层转换为一个**向量**（一串数字）。
   * 向量能表达更丰富的语义和语法信息，比如“男人”与“国王”、“女人”与“女王”之间的类比关系可以在向量空间中体现。
   * 相似含义的词在向量空间中距离更近。
3. **位置编码（Positional Encoding）**
   * 由于Transformer本身不感知顺序，需要加入位置信息。
   * 将表示词序的位置向量与词向量相加，使模型能理解词语的先后顺序。

***

### 3、编码器（Encoder）的作用

编码器的任务是将输入文本转化为一种**抽象的向量表示**，包含词汇、语法、语义和上下文信息。

核心机制是 **自注意力机制（Self-Attention）**：

* 模型在处理每个词时，会关注句子中所有其他词。
* 计算每对词之间的相关性，赋予不同“注意力权重”。
* 例如，“it”指代“animal”还是“street”，模型会根据上下文判断并加强相关词的权重。

**多头自注意力（Multi-Head Attention）**：

* 多个并行的自注意力模块，各自关注不同特征（如语法、情感、实体等）。
* 提升模型对复杂语言结构的理解能力。

后续还有\*\*前馈神经网络（Feed-Forward Network）\*\*进一步处理信息。

编码器通常**多层堆叠**，逐层提取更高层次的语言特征。

***

### 4、解码器（Decoder）的作用

解码器负责**逐个生成输出文本**，是生成式模型的核心。

1. **输入**：
   * 来自编码器的输入文本抽象表示。
   * 已生成的部分输出（保持连贯性）。
   * 初始时输入一个“开始”标记（Start Token）。
2. **嵌入 + 位置编码**：
   * 与编码器相同，先将输入Token转为向量并加入位置信息。
3. **带掩码的多头自注意力（Masked Multi-Head Attention）**：
   * 只关注当前词及其之前的词，屏蔽后续词。
   * 确保生成过程遵循时间顺序，不“偷看”未来内容。
4. **编码器-解码器注意力（Encoder-Decoder Attention）**：
   * 将编码器的输出与解码器的状态关联，确保生成内容与原始输入相关。
5. **前馈神经网络**：
   * 进一步增强表达能力。

解码器也**多层堆叠**，提升生成质量。

***

### 5、输出生成

解码器最终通过两个层生成结果：

* **线性层（Linear Layer）**：将向量映射到词汇表大小的维度。
* **Softmax层**：输出每个Token的概率分布。

模型选择**概率最高的Token**作为下一个输出，重复此过程，直到生成“结束标记”（End Token）。

> 注意：模型并不知道输出是否真实，只是基于统计规律“猜测”，因此可能出现“**幻觉**”（一本正经胡说八道）。

***

### 6、Transformer的三大变体

1. **仅编码器模型（Encoder-only）**
   * 如 **BERT**
   * 擅长理解任务：填空、情感分析、命名实体识别等。
2. **仅解码器模型（Decoder-only）**
   * 如 **GPT系列**
   * 擅长生成任务：文本生成、对话、写作等。
3. **编码器-解码器模型（Encoder-Decoder）**
   * 如 **T5、BART**
   * 擅长序列到序列任务：翻译、摘要、问答等。

## 四、如何3步训练出一个AI聊天助手

三步法概述

### 1、无监督预训练

* 通过大量文本进行无监督学习，构建基础模型。
  * 利用互联网上的各种文本资源（如书籍、新闻文章、科学论文等）作为训练数据。
  * 模型从中学习语言的语法和语义规则，了解表达结构和模式。
* 理解token的概念及其在模型中的作用。
  * Token是模型处理文本的基本单位，短词可能是一个token，长词或中文字符则可能被拆分为多个token。
* 预训练过程的技术细节与挑战。
  * 预训练耗时费力且成本高昂，但最终得到一个能够预测下一个token的基础模型。

### 2、监督微调

* 使用高质量的人类对话数据对基础模型进行微调。
  * 微调使模型更加适应特定任务，比如回答问题的能力。
* 微调过程中的数据规模与训练时长。
  * 相较于预训练，微调所需的训练数据规模更小，训练时长也更短。
* 监督微调（SFT）的结果及其改进点。
  * 经过SFT后，模型能更好地对问题作出回应，但仍需进一步优化以提升性能。

### 3、强化学习与奖励模型

* 利用人类评估员对回答质量进行评分。
  * 基于评分数据训练出一个奖励模型，该模型用于预测回答的质量评分。
* 强化学习的过程与原理。
  * 强化学习类似于训练小狗，模型根据反馈调整策略，最大化奖励或最小化损失。
* 强化学习的应用与效果。
  * 强化学习帮助模型不断优化其生成策略，提高回答的质量。

---

---
url: /StableDiffusion/AIGC/AI猫咪剧情号.md
---

# AI猫咪剧情号

使用ChatGPT生成

---

---
url: /Python/AI大模型应用开发/6_AI模型与输入输出.md
---

# AI模型与输入输出

## 一、Model—玩转Open AI聊天模型

### 1、模型（Model）：AI应用的核心

模型是AI应用的核心，负责提供语言的理解和生成能力。LangChain的设计允许开发者集成多种不同的大语言模型（LLM），提供了极大的灵活性。

LangChain主要将模型分为两大类：

1. **LLM (Language Model - 语言模型)**
   * **功能**：本质上是**文本补全**模型。它接收一段文本，预测并生成接下来最可能的文本。
   * **接口**：通常接收一个**字符串**作为输入，并返回一个**字符串**作为输出。
2. **ChatModel (聊天模型)**
   * **功能**：是在对话数据上进行过专门**调优**的模型，更擅长处理多轮对话和理解对话历史。
   * **接口**：接收一个**消息列表**（Message List）作为输入，并返回一个**消息**（Message）作为输出。
   * **优势**：当前，ChatModel 通常是比基础 LLM 更先进和更实用的选择，因为它在对话任务上表现更好，能显著提升用户体验。

> **注意**：我们熟悉的 GPT-3.5-turbo、GPT-4 等都属于 **ChatModel**。

### 2、使用LangChain调用OpenAI聊天模型

以OpenAI的聊天模型为例，通过LangChain获取模型回复。

#### 第一步：安装依赖

首先，需要安装LangChain与OpenAI集成的专用库。

```sh
pip install langchain-openai
```

#### 第二步：导入并创建模型实例

1. **导入模块**： 从 `langchain_openai` 库中导入 `ChatOpenAI` 类。

   ```python
   from langchain_openai import ChatOpenAI
   ```

2. **创建实例**： 创建一个 `ChatOpenAI` 的实例。

   ```python
   # 假设你的OpenAI API密钥已设置在环境变量中
   llm = ChatOpenAI(model="gpt-3.5-turbo")  # 指定模型，如 gpt-3.5-turbo, gpt-4
   ```

#### 第三步：配置模型参数

在创建实例时，可以配置多种参数来控制模型的行为：

* 常用参数（可直接作为构造函数参数）：

  * `temperature`：控制输出的随机性。值越高，输出越随机、有创意；值越低，输出越确定、保守。
  * `max_tokens`：限制模型生成的最大token数量。

* 其他参数（可通过model\_kwargs字典传入）：

  * 例如 `frequency_penalty`（频率惩罚）、`presence_penalty`（存在惩罚）等不那么常用的参数。

  ```python
  llm = ChatOpenAI(
      model="gpt-3.5-turbo",
      temperature=0.7,
      max_tokens=100,
      model_kwargs={"frequency_penalty": 0.5}  # 其他参数
  )
  ```

> **提示**：如果未将API密钥存入环境变量，可以在创建实例时通过 `api_key` 参数直接传入。

### 3、构建消息列表（Message List）

由于 `ChatModel` 接收消息列表作为输入，我们需要了解消息的结构。

#### 消息类型

消息列表中的消息主要有三种类型：

1. **`SystemMessage`**：系统消息。用于向AI提供指令、设定角色或行为准则。例如：“你是一个乐于助人的助手。”
2. **`HumanMessage`**：人类消息。代表用户输入的对话内容。
3. **`AIMessage`**：AI消息。代表AI之前的回复，用于构建对话历史。

#### 创建和组合消息

1. **导入消息类**：

   ```python
   from langchain_core.messages import SystemMessage, HumanMessage
   ```

2. **创建消息实例**：

   ```python
   system_message = SystemMessage(content="你是一个翻译专家，擅长将英文翻译成中文。")
   human_message = HumanMessage(content="Hello, how are you?")
   ```

3. **组合成消息列表**：

   ```python
   messages = [system_message, human_message]
   ```

### 4、调用模型并获取回复

最后，调用模型的 `invoke` 方法（或 `__call__` 方法）来获取回复。

```python
# 调用模型
response = llm.invoke(messages)  # 或者 llm(messages)

# 打印回复
# response 是一个 AIMessage 对象
print(response.content)  # 输出: 你好，你怎么样？
```

### 5、LangChain支持的其他模型

LangChain的 `langchain-community` 库支持集成来自不同服务商的多种聊天模型。官方文档中列出了丰富的选择，例如：

* **百度**：千帆平台提供的 `ERNIE-Bot` 系列模型。
* **腾讯**：`混元` (HunYuan) 模型。
* **阿里**：`通义千问` (Qwen) 系列模型。

开发者可以申请相应服务的API密钥，并通过类似的方式集成这些国产大模型，实现模型的灵活切换。

### 6、Model使用示例

```python
from langchain_openai import ChatOpenAI
```

```python
model = ChatOpenAI(model="gpt-3.5-turbo")
# model = ChatOpenAI(model="gpt-3.5-turbo", temperature=1.2, max_tokens=500, model_kwargs={"frequency_penalty":1.1})
```

```python
from langchain.schema.messages import HumanMessage, SystemMessage
```

```python
messages = [
    SystemMessage(content="请你作为我的物理课助教，用通俗易懂且间接的语言帮我解释物理概念。"),
    HumanMessage(content="什么是波粒二象性？"),
]
```

```python
response = model.invoke(messages)
```

```python
response
```

```python
AIMessage(content='嗨！波粒二象性是一个非常有趣的物理概念。它指的是，微观粒子（比如电子、光子等）既可以表现出波动性，也可以表现出粒子性。\n\n让我们以光子为例来说明。光子是光的基本粒子，而光又是一种电磁波。当我们将光通过一个狭缝时，它会呈现出波动的特性，产生干涉和衍射现象，就像波一样。但当我们观察光子通过一个光敏探测器时，我们会发现它们的行为更像是粒子，因为它们只在一个点上被探测到。\n\n这就是波粒二象性的精髓所在：微观粒子既可以像波一样传播，又可以像粒子一样被探测到。这种二象性挑战了我们对物质本质的传统观念，但也为我们理解微观世界的奇妙规律提供了新的思路。')
```

```python
print(response.content)
```

```python
嗨！波粒二象性是一个非常有趣的物理概念。它指的是，微观粒子（比如电子、光子等）既可以表现出波动性，也可以表现出粒子性。

让我们以光子为例来说明。光子是光的基本粒子，而光又是一种电磁波。当我们将光通过一个狭缝时，它会呈现出波动的特性，产生干涉和衍射现象，就像波一样。但当我们观察光子通过一个光敏探测器时，我们会发现它们的行为更像是粒子，因为它们只在一个点上被探测到。

这就是波粒二象性的精髓所在：微观粒子既可以像波一样传播，又可以像粒子一样被探测到。这种二象性挑战了我们对物质本质的传统观念，但也为我们理解微观世界的奇妙规律提供了新的思路。
```

## 二、Prompt Template—让模型的输入超级灵活

### 1、提示模板的核心优势

与手动构建提示相比，代码化的提示模板可通过插入变量实现动态调整，能灵活适配不同示例或数据需求，显著提升效率和灵活性。

### 2、聊天模型的提示模板类型

LangChain 的`prompt`模块下，针对不同角色的消息提供了专用模板：

1. **SystemMessagePromptTemplate**：用于系统消息（设定模型行为、背景等）
2. **HumanMessagePromptTemplate**：用于人类消息（用户输入内容）
3. **AIMessagePromptTemplate**：用于 AI 消息（模型回复内容）

### 3、提示模板的创建与变量处理

1. **创建方式**：
   所有模板均通过`from_template`方法创建，传入包含变量的字符串（变量用`{}`包围）。
   示例：`SystemMessagePromptTemplate.from_template("将{input_language}翻译成{output_language}")`
2. **变量识别**：
   花括号包围的内容会被自动识别为变量（如`input_language`、`output_language`），无需额外声明，可通过模板的`input_variables`属性查看变量列表。

### 4、变量填充与消息生成

1. **填充变量**：
   用`format`方法传入变量值，模板会返回对应角色的消息对象：
   * 系统消息模板→`SystemMessage`
   * 人类消息模板→`HumanMessage`
   * AI 消息模板→`AIMessage`
2. **获取模型回应**：
   将填充后的消息对象放入列表，作为参数传入聊天模型的`invoke`方法即可。

### 5、简化方案：ChatPromptTemplate

若需统一管理多角色消息，可使用`ChatPromptTemplate`：

1. **创建方式**：通过`from_messages`方法接收消息模板列表，每个元素为（角色标识，内容字符串）的元组，角色标识可设为`"system"`、`"human"`、`"ai"`。
2. **变量填充**：调用`invoke`方法传入字典（键为变量名，值为变量内容），一次性填充所有角色消息中的变量。
3. **结果使用**：返回`ChatPromptValue`对象，包含填充后的完整消息列表，直接传入模型的`invoke`方法即可获取回应。

### 6、提示模板的核心价值

对于批量处理不同需求（如多语言翻译、不同风格生成等），无需逐个硬编码提示，只需通过循环为同一模板传入不同变量值，即可高效生成多样化结果，大幅提升开发效率。

### 7、Prompt Template使用示例

```python
from langchain.prompts import (
    SystemMessagePromptTemplate,
    AIMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
```

```python
# 提示模板一
system_template_text="你是一位专业的翻译，能够将{input_language}翻译成{output_language}，并且输出文本会根据用户要求的任何语言风格进行调整。请只输出翻译后的文本，不要有任何其它内容。"
system_prompt_template = SystemMessagePromptTemplate.from_template(system_template_text)
```

```python
system_prompt_template
```

```python
SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input_language', 'output_language'], template='你是一位专业的翻译，能够将{input_language}翻译成{output_language}，并且输出文本会根据用户要求的任何语言风格进行调整。请只输出翻译后的文本，不要有任何其它内容。'))
```

```python
system_prompt_template.input_variables
```

```python
['input_language', 'output_language']
```

```python
# 提示模板二
human_template_text="文本：{text}\n语言风格：{style}"
human_prompt_template = HumanMessagePromptTemplate.from_template(human_template_text)
```

```python
human_prompt_template.input_variables
```

```python
['style', 'text']
```

```python
system_prompt = system_prompt_template.format(input_language="英语", output_language="汉语")
system_prompt
```

```python
SystemMessage(content='你是一位专业的翻译，能够将英语翻译成汉语，并且输出文本会根据用户要求的任何语言风格进行调整。请只输出翻译后的文本，不要有任何其它内容。')
```

```python
human_prompt = human_prompt_template.format(text="I'm so hungry I could eat a horse", style="文言文")
human_prompt
```

```python
HumanMessage(content="文本：I'm so hungry I could eat a horse\n语言风格：文言文")
```

```python
from langchain_openai import ChatOpenAI
```

```python
model = ChatOpenAI(model="gpt-3.5-turbo")
response = model.invoke([
    system_prompt,
    human_prompt
])
```

```python
print(response.content)
```

```python
吾飢甚，能食千里馬。
```

## 三、Few Shot Templates—高效往提示里塞示范

### 1、少样本学习（Few-Shot Learning）

在提示词工程中，**少样本提示（Few-Shot Prompting）** 是一种让AI快速适应新任务的高效方法。

* **核心思想**：将几个包含输入（问题）和期望输出（答案）的对话示例（demonstrations）作为上下文，与新的用户提示一起发送给模型。
* **优势**：无需对模型进行任何训练，成本低、灵活性高。

### 2、少样本提示的挑战与优化

#### 2.1、挑战

虽然少样本提示非常有效，但当示例数量较多时，手动编写每个示例的完整消息列表会变得繁琐且容易出错。观察示例可以发现，它们的**格式高度相似**，主要区别仅在于具体的输入和输出值。

#### 2.2、解决方案：使用模板

既然示例结构相似，我们就可以利用**提示模板（Prompt Template）** 来动态生成这些示例，从而大大提高效率。

### 3、`FewShotChatMessagePromptTemplate`：少样本提示的利器

LangChain的 `langchain_core.prompts` 模块提供了一个专门用于构建少样本提示的类：`FewShotChatMessagePromptTemplate`。

### 1. 核心参数

* **`example_prompt`**：一个**提示模板**，用于定义**单个示例**的结构。这个模板可以包含动态变量（用花括号 `{}` 包围），例如 `{input}` 和 `{output}`。
* **`examples`**：一个**示例列表**，列表中的每个元素都是一个**字典**。字典的**键**对应 `example_prompt` 模板中的变量名，字典的**值**则是该变量在具体示例中应填充的实际内容。

### 2. 使用步骤

1. **创建 `example_prompt` 模板** 定义一个包含 `HumanMessage` 和 `AIMessage` 的模板，用于表示一个完整的问答对。

   ```python
   from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate

   # 定义单个示例的模板
   example_prompt = ChatPromptTemplate.from_messages([
       ("human", "{input}"),
       ("ai", "{output}")
   ])
   ```

2. **准备 `examples` 数据** 创建一个包含多个具体示例值的列表。

   ```python
   examples = [
       {"input": "我今年25岁，来自北京。", "output": "年龄：25岁，所在地：北京市"},
       {"input": "我30岁了，住在杭州。", "output": "年龄：30岁，所在地：浙江省杭州市"}
   ]
   ```

3. **构建少样本提示模板** 将 `example_prompt` 和 `examples` 传入 `FewShotChatMessagePromptTemplate` 的构造函数。

   ```python
   few_shot_prompt = FewShotChatMessagePromptTemplate(
       example_prompt=example_prompt,
       examples=examples
   )
   ```

4. **组合完整提示模板** 将少样本提示模板与最终的用户提示模板组合成一个完整的 `ChatPromptTemplate`。`ChatPromptTemplate.from_messages` 方法支持将其他提示模板作为列表元素。

   ```python
   final_prompt = ChatPromptTemplate.from_messages([
       ("system", "你是一个信息提取助手。请按照示例格式提取年龄和所在地信息。"),
       few_shot_prompt,  # 少样本示例会在这里被动态插入
       ("human", "{input}")  # 用户的新输入
   ])
   ```

5. **填充变量并调用模型** 使用 `invoke` 方法，传入包含新用户输入的字典来填充最终的提示。

   ```python
   # 假设 llm 是已创建的聊天模型实例
   chain = final_prompt | llm
   response = chain.invoke({"input": "我今年28岁，来自成都。"})
   print(response.content)  # 输出: 年龄：28岁，所在地：四川省成都市
   ```

### 4、总结与优势

* **`FewShotChatMessagePromptTemplate`** 极大地简化了少样本提示的构建过程。
* 核心优势：
  * **效率高**：只需定义一次模板和提供示例数据，即可自动生成所有示例消息。
  * **易维护**：增加新的示例时，只需向 `examples` 列表中添加新的字典，无需重写整个消息结构。
  * **减少错误**：避免了手动编写大量重复格式消息时可能出现的格式错误。
* **适用场景**：当你需要向模型提供多个结构相似的示例来指导其行为时，使用此模板是最佳实践。

## 四、Output Parser—从模型输出里提取列表

### 1、输出解析器的作用

在代码与 AI 模型交互时，需对模型输出进行后续处理（如提取信息、展示等），但 AI 输出格式存在不确定性。LangChain 的输出解析器主要解决两方面问题：

1. **规范输出格式**：向模型下达指令，要求其按指定格式输出内容。
2. **解析输出内容**：自动处理模型的回应，提取所需信息（如转换为特定数据结构）。

### 2、逗号分隔列表输出解析器（CommaSeparatedListOutputParser）

以提取颜色色号为例，该解析器适用于将输出转换为 Python 列表，具体使用步骤如下：

#### 1. 创建解析器实例

通过`langchain.output_parsers`模块中的`CommaSeparatedListOutputParser`类创建实例，用于后续生成指令和解析结果。

#### 2. 获取格式指令

解析器的`get_format_instructions()`方法会返回规范模型输出的文字指令，例如：

> "你的回应应该是一串以逗号分隔的值，例如 foo, bar, baz"

该指令需嵌入系统提示中，确保模型按要求输出。

#### 3. 构建提示模板并调用模型

* 结合系统提示（包含解析器指令）和用户提示（如 “生成 5 个符合要求的颜色色号”），使用`ChatPromptTemplate`创建提示模板。
* 调用模板的`invoke`方法传入变量值，生成最终提示并传给模型，模型会返回逗号分隔的字符串（如 “#FF0000, #00FF00, #0000FF”）。

#### 4. 解析模型输出

直接调用解析器的`invoke`方法，传入模型的回应，即可自动将逗号分隔的字符串转换为 Python 列表（如`["#FF0000", "#00FF00", "#0000FF"]`），无需手动处理文本。

## 五、Output Parser —从模型输出里提取JSON

### 1、JSON 输出解析的优势与应用场景

JSON 是一种结构化强、易解析的格式，可轻松转换为字典、列表或类实例，适用于需要提取特定字段信息的场景（如从书籍介绍中提取书名、作者、题材等）。

* 核心需求：确保 AI 输出的 JSON 中，字段名、值的类型与预期完全匹配（避免因字段名错误或类型不符导致解析失败）。

### 2、PydanticOutputParser 的作用

LangChain 的`PydanticOutputParser`是处理 JSON 输出的关键工具，依托 Pydantic 库（用于数据解析和验证）实现两大功能：

1. 向 AI 下达指令，要求其输出符合指定数据模式的 JSON。
2. 自动解析 AI 返回的 JSON 字符串，转换为对应的类实例，方便提取信息。

### 3、使用步骤详解

#### 3.1、准备工作：安装并导入依赖

* 安装 Pydantic 库（若未安装）。
* 从pydantic导入核心组件：
  * `BaseModel`：用于定义数据模式（类似 “数据说明书”）。
  * `Field`：为字段提供描述信息和验证条件。

#### 3.2、定义数据模式（类）

创建继承自`BaseModel`的类（如`BookInfo`），明确所需字段的名称、类型及描述：

* **字段类型**：通过类型提示指定（如`bookname: str`、`author: str`表示字符串；`genres: List[str]`表示字符串列表）。

* **字段描述**：使用`Field`函数补充说明（如`Field(description="书籍的标题")`），该描述会传递给 AI，帮助其理解字段含义。

  示例：

  ```python
  from pydantic import BaseModel, Field
  from typing import List

  class BookInfo(BaseModel):
      bookname: str = Field(description="书籍的标题")
      author: str = Field(description="书籍的作者")
      genres: List[str] = Field(description="书籍的题材类别列表")
  ```

#### 3.3、创建 PydanticOutputParser 实例

将定义的数据模式类传入`PydanticOutputParser`，生成解析器：

```python
from langchain.output_parsers import PydanticOutputParser

parser = PydanticOutputParser(pydantic_object=BookInfo)
```

#### 3.4、生成格式指令并嵌入提示

* 调用解析器的`get_format_instructions()`方法，获取规范 AI 输出的指令（包含数据模式要求）。
* 将指令嵌入提示模板（如`ChatPromptTemplate`），确保 AI 按格式输出 JSON。

#### 3.5、调用模型并解析输出

* 模型返回符合要求的 JSON 字符串（字段名、类型与`BookInfo`一致）。

* 调用解析器的invoke方法，传入模型回应，自动将 JSON 字符串转换为BookInfo类实例：

  ```python
  book_data = parser.invoke(model_response)
  ```

* 直接通过实例提取信息（如`book_data.bookname`、`book_data.genres`）。

### 4、核心价值

通过 Pydantic 的类型验证和 LangChain 解析器的协作，既能保证 AI 输出的结构化，又能简化后续信息提取流程，尤其适合需要批量处理结构化数据的场景（如网站信息展示、数据入库等）。

## 六、Chain—串起提示模板-模型-输出解析器

### 1、核心组件的共性：`invoke`方法

LangChain 中的多个核心组件（如提示模板、聊天模型、输出解析器）均实现了`Runnable`接口，因此都具备`invoke`方法，这是 LangChain 表达式语言（LCEL）中统一的调用方式。不同组件的`invoke`方法功能如下：

* **提示模板（如`ChatPromptTemplate`）**：接收含变量值的字典，返回填充后的提示值（`PromptValue`）。
* **聊天模型（如`ChatModel`）**：接收提示值或消息列表，返回模型生成的聊天信息。
* **输出解析器（如`PydanticOutputParser`）**：接收模型的聊天信息，返回解析后的结构化结果。

### 2、组件的串联关系

各组件的`invoke`方法存在 “输入 - 输出” 的上下游依赖：

* 提示模板的输出（提示值）是聊天模型的输入。
* 聊天模型的输出（生成内容）是输出解析器的输入。

因此，可通过连续调用`invoke`方法实现完整流程，例如：

`提示模板.invoke(变量) → 聊天模型.invoke(提示值) → 解析器.invoke(模型输出)`

```
字典（Dictionary） -> 输入 -> 提示模板（Prompt Template） -> 输出 -> 提示值（Prompt Value） -> 输入 -> 聊天模型（Chat Model） -> 输出 -> 聊天消息（Chat Message） -> 输入 -> 输出解析器（Output Parser） -> 输出 -> 解析结果（类型取决于解析器）
```

代码示例

```
```

### 3、链（Chain）与 LangChain 表达式语言（LCEL）

```
(prompt | model | output_parser).invoke()
```

1. **链的定义**：将多个组件按 “上游输出作为下游输入” 的逻辑组合成的流程，称为 “链”。
2. **LCEL 的管道语法**：通过`|`（管道操作符）直观表示组件间的串联关系，例如：
   `prompt | model | parser`
   含义：提示模板的输出传给模型，模型的输出再传给解析器。
3. **链的调用**：对整个链调用`invoke`方法时，只需传入第一个组件所需的参数（因后续组件的输入由上游提供），即可得到最终结果。

### 4、链的灵活性

链的组合方式非常灵活，可根据需求调整组件：

* 中间的聊天模型可替换为其他语言模型（如`LLM`）。
* 提示模板或输出解析器并非必需，可根据场景省略。

通过链，能轻松构建复杂的 AI 交互流程，且组件间的依赖关系清晰易懂。

### 5、Chain使用示例

```
from langchain_openai import ChatOpenAI
from langchain.output_parsers import CommaSeparatedListOutputParser
from langchain.prompts import ChatPromptTemplate
```

```
prompt = ChatPromptTemplate.from_messages([
    ("system", "{parser_instructions}"),
    ("human", "列出5个{subject}色系的十六进制颜色码。")
])
```

```
output_parser = CommaSeparatedListOutputParser()
parser_instructions = output_parser.get_format_instructions()
```

---

---
url: /Python/AI大模型应用开发/17_AI应用部署.md
---

# AI应用部署

## 一、上传项目到GitHub

当你完成 Streamlit AI 网站的代码编写后，默认只能在本地（`localhost`）运行。若想让他人通过互联网访问，需将项目部署到公网服务器。Streamlit 提供了便捷的部署流程，核心步骤包括依赖管理、代码托管（GitHub）和云部署，以下是详细指南：

### 1、准备项目依赖（requirements.txt）

部署前需明确项目依赖的库，确保服务器能正确安装所需环境。

#### 生成依赖文件

在项目终端执行以下命令，自动生成包含所有依赖库的`requirements.txt`：

```
pip freeze > requirements.txt
```

* **说明**：文件中会列出项目中直接或间接使用的所有库（包括依赖的子库），无需手动记忆或填写。

### 2、将项目上传到 GitHub

Streamlit 社区云部署需从 GitHub 仓库获取代码，因此需先将项目托管到 GitHub。

#### 2.1. GitHub 注册与登录

* 访问[GitHub 官网](https://github.com/)，点击右上角 “Sign up” 注册账号（已有账号直接 “Sign in”）。
* 注册流程：输入邮箱→设置密码（8 位以上，含数字 / 字母，或 15 位以上）→设置用户名（支持字母、数字、短横杠）→完成人机验证→输入邮箱验证码。

#### 2.2. 创建代码仓库

代码仓库可理解为 “项目文件夹”，用于存放所有项目文件：

1. 登录后点击右上角 “+”→“New repository”。
2. 填写仓库信息：
   * **Repository name**：仓库名称（支持字母、数字、英文句号、下划线、短横杠，如`ai-chat-app`）。
   * **Description**：项目描述（可选，支持中文，如 “基于 Streamlit 的 AI 对话工具”）。
   * **Visibility**：选择`Public`（公开，便于分享）或`Private`（私有）。
   * **Add a README file**：勾选则自动创建`README.md`（项目说明文档，可选）。
   * **.gitignore**：忽略文件。初期可忽略，后续按需添加。
   * **License**：初期可忽略，后续按需添加。如果想要了解 不同许可证的具体含义，就可以去到[chooselicense.com](https://chooselicense.com/) 这个网站。
3. 点击 “Create repository” 完成创建。

#### 2.3. 上传项目文件

1. 进入新建仓库，点击 “Add file”→“Upload files”。
2. 拖拽项目文件（如`.py`代码、`requirements.txt`、数据文件等）到上传区域，**注意排除虚拟环境文件夹（如`venv`）**（体积大且非必要）。
3. 填写提交信息：
   * **Commit changes**：简短描述本次上传（如 “初始版本提交”）。
   * 可选：填写详细描述。
4. 点击 “Commit changes” 完成上传。

#### 2.4. 后续操作

* **删除文件**：进入文件→点击右上角 “...”→“Delete file”→提交确认。
* **更新文件**：重新上传同名文件，自动覆盖旧版本。
* **版本控制**：GitHub 基于 Git 工具，可记录每次改动历史、追踪修改人，支持回退到旧版本（团队协作必备，建议深入学习 Git）。

## 二、让你的AI应用能用链接访问

将项目上传到 GitHub 后，通过 Streamlit 社区云（`share.streamlit.io`）可免费部署应用，生成公网可访问的链接。以下是详细部署步骤：

### 1、登录 Streamlit 社区云

1. 访问部署平台：打开浏览器，输入网址 [share.streamlit.io](https://share.streamlit.io/)。
2. 登录账号：
   * 推荐点击 “Continue with GitHub”，使用 GitHub 账号登录（无需单独注册）。
   * 首次登录需授权 Streamlit 访问 GitHub 信息，点击绿色授权按钮即可。
3. 完善信息（首次登录）：按提示填写姓名、邮箱、所在国家等基本信息。

### 2、创建并部署应用

1. 进入主页面后，点击 “New App” 按钮，开始部署流程。
   * 若出现 GitHub 仓库访问授权提示，再次点击绿色按钮授权（仅首次需要）。
2. 填写项目信息：
   * **Repository**：从下拉框中选择已上传到 GitHub 的项目仓库（如`your-username/ai-app`）。
   * **Branch**：默认选择`main`或`master`分支（无需修改，除非使用了自定义分支）。
   * **Main file path**：填写项目主文件（即本地运行时执行`streamlit run`后面的文件名，如`main.py`）。
   * **URL**：自定义部署后的网址后缀（自动生成默认值，可修改为更简短的名称，绿色提示表示未被占用）。
3. 启动部署：点击 “Deploy” 按钮，开始部署流程。

### 3、部署过程与完成

* **部署中**：页面会显示加载动画，可点击 “Manage App” 查看实时部署日志（如依赖安装过程，`requirements.txt`中的库会被逐一安装）。
* **完成部署**：等待几分钟后，部署成功，页面会显示应用链接（如`https://your-app-name.streamlit.app`）。

### 4、使用与分享

部署成功后，任何人都可通过生成的链接访问你的应用。你可以将链接分享给他人，无需配置服务器即可实现公网访问。

通过 Streamlit 社区云，无需复杂的服务器配置，几分钟内即可完成应用部署，是快速分享 Streamlit 项目的理想选择。

---

---
url: /常用框架/SpringBoot/SpringBoot与Web应用/8_AOP拦截器.md
---

# AOP拦截器

---

---
url: /Java/系统优化/系统优化/3_API签名认证.md
---

# API签名认证

---

---
url: /Python/AI大模型应用开发/4_API用法示例.md
---

# API用法示例

## 一、文本总结：一建总结用户评价

### 1、大模型的革命性优势

* 从专用模型到通用大模型
  * 过去：需要为总结、提取、翻译等任务分别训练独立模型。
  * 现在：一个大模型即可完成多种自然语言处理任务，极大提升了效率和灵活性。

### 2、文本总结的常见应用场景

* 视频/音频内容摘要
  * 视频总结生成器、一键摘要工具等，通常基于“音频转文字 + 大模型总结”流程实现。
* 用户反馈分析
  * 电商、产品团队可通过AI自动分析大量用户评价，提取产品优缺点，指导市场策略和产品改进。

### 3、使用OpenAI API进行用户评价总结

```python
# 导入库并创建客户端
from openai import OpenAI
client = OpenAI()
# 封装通用请求函数
def get_openai_response(client, prompt, model="gpt-3.5-turbo"):
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}],
    )
    return response.choices[0].message.content
# 构建任务提示词
product_review = """
我上个月买的这个多功能蓝牙耳机。它的连接速度还挺快，而且兼容性强，无论连接手机还是笔记本电脑，基本上都能快速配对上。
音质方面，中高音清晰，低音效果震撼，当然这个价格来说一分钱一分货吧，毕竟也不便宜。
耳机的电池续航能力不错，单次充满电可以连续使用超过8小时。
不过这个耳机也有一些我不太满意的地方。首先是在长时间使用后，耳廓有轻微的压迫感，这可能是因为耳套的材料较硬。总之我感觉戴了超过4小时后耳朵会有点酸痛，需要摘下休息下。
而且耳机的防水性能不是特别理想，在剧烈运动时的汗水防护上有待加强。
最后是耳机盒子的开合机制感觉不够紧致，有时候会不小心打开。
"""
# 动态插入上下文
product_review_prompt = f"""
你的任务是为用户对产品的评价生成简要总结。
请把总结主要分为两个方面，产品的优点，以及产品的缺点，并以Markdown列表形式展示。
用户的评价内容会以三个#符号进行包围。

###
{product_review}
###
"""
```

```python
# 输出
product_review_prompt
```

```python
# 输出结果
'\n你的任务是为用户对产品的评价生成简要总结。\n请把总结主要分为两个方面，产品的优点，以及产品的缺点，并以Markdown列表形式展示。\n用户的评价内容会以三个#符号进行包围。\n\n###\n\n我上个月买的这个多功能蓝牙耳机。它的连接速度还挺快，而且兼容性强，无论连接手机还是笔记本电脑，基本上都能快速配对上。\n音质方面，中高音清晰，低音效果震撼，当然这个价格来说一分钱一分货吧，毕竟也不便宜。\n耳机的电池续航能力不错，单次充满电可以连续使用超过8小时。\n不过这个耳机也有一些我不太满意的地方。首先是在长时间使用后，耳廓有轻微的压迫感，这可能是因为耳套的材料较硬。总之我感觉戴了超过4小时后耳朵会有点酸痛，需要摘下休息下。\n而且耳机的防水性能不是特别理想，在剧烈运动时的汗水防护上有待加强。\n最后是耳机盒子的开合机制感觉不够紧致，有时候会不小心打开。\n\n###\n'
```

```python
# 获取AI响应
response = get_openai_response(client, product_review_prompt)
response
```

```python
# 输出结果
# '产品的优点：\n- 连接速度快，兼容性强\n- 音质中高音清晰，低音效果震撼\n- 电池续航能力强，单次充满电可以连续使用超过8小时\n\n产品的缺点：\n- 长时间使用后耳廓有轻微的压迫感\n- 防水性能不够理想，在剧烈运动时的汗水防护上有待加强\n- 耳机盒子的开合机制不够紧致，有时会不小心打开'
```

```python
# 美化输出显示
print(response)
```

```python
# 输出结果
产品的优点：
- 连接速度快，兼容性强
- 音质中高音清晰，低音效果震撼
- 电池续航能力强，单次充满电可以连续使用超过8小时

产品的缺点：
- 长时间使用后耳廓有轻微的压迫感
- 防水性能不够理想，在剧烈运动时的汗水防护上有待加强
- 耳机盒子的开合机制不够紧致，有时会不小心打开
```

## 二、文本撰写：秒生成小红书爆款文案

````python
# 导入库并创建客户端
from openai import OpenAI
client = OpenAI()
# 封装通用请求函数
def get_openai_response(client, system_prompt, user_prompt, model="gpt-3.5-turbo"):
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
    )
    return response.choices[0].message.content
# 构建任务提示词
xiaohongshu_system_prompt = """
你是小红书爆款写作专家，请你遵循以下步骤进行创作：首先产出5个标题（包含适当的emoji表情），然后产出1段正文（每一个段落包含适当的emoji表情，文末有适当的tag标签）。
标题字数在20个字以内，正文字数在800字以内，并且按以下技巧进行创作。
一、标题创作技巧： 
1. 采用二极管标题法进行创作 
1.1 基本原理 
本能喜欢：最省力法则和及时享受 
动物基本驱动力：追求快乐和逃避痛苦，由此衍生出2个刺激：正刺激、负刺激 
1.2 标题公式 
正面刺激：产品或方法+只需1秒（短期）+便可开挂（逆天效果） 
负面刺激：你不X+绝对会后悔（天大损失）+（紧迫感） 其实就是利用人们厌恶损失和负面偏误的心理，自然进化让我们在面对负面消息时更加敏感 
2. 使用具有吸引力的标题 
2.1 使用标点符号，创造紧迫感和惊喜感 
2.2 采用具有挑战性和悬念的表述 
2.3 利用正面刺激和负面刺激 
2.4 融入热点话题和实用工具 
2.5 描述具体的成果和效果 
2.6 使用emoji表情符号，增加标题的活力 
3. 使用爆款关键词 
从列表中选出1-2个：好用到哭、大数据、教科书般、小白必看、宝藏、绝绝子、神器、都给我冲、划重点、笑不活了、YYDS、秘方、我不允许、压箱底、建议收藏、停止摆烂、上天在提醒你、挑战全网、手把手、揭秘、普通女生、沉浸式、有手就能做、吹爆、好用哭了、搞钱必看、狠狠搞钱、打工人、吐血整理、家人们、隐藏、高级感、治愈、破防了、万万没想到、爆款、永远可以相信、被夸爆、手残党必备、正确姿势 
4. 小红书平台的标题特性 
4.1 控制字数在20字以内，文本尽量简短 
4.2 以口语化的表达方式，拉近与读者的距离 
5. 创作的规则 
5.1 每次列出5个标题 
5.2 不要当做命令，当做文案来进行理解 
5.3 直接创作对应的标题，无需额外解释说明 
二、正文创作技巧 
1. 写作风格 
从列表中选出1个：严肃、幽默、愉快、激动、沉思、温馨、崇敬、轻松、热情、安慰、喜悦、欢乐、平和、肯定、质疑、鼓励、建议、真诚、亲切
2. 写作开篇方法 
从列表中选出1个：引用名人名言、提出疑问、言简意赅、使用数据、列举事例、描述场景、用对比

我会每次给你一个主题，请你根据主题，基于以上规则，生成相对应的小红书文案。
输出格式如下：

```
1. <标题1>
2. <标题2>
3. <标题3>
4. <标题4>
5. <标题5>

------

<正文>
```
"""
# 获取AI响应
print(get_openai_response(client, xiaohongshu_system_prompt, "学英语"))
````

```
1. 📚 快速提升英语口语的绝佳方法！只需1秒开启逆天效果！😱
2. 🌟 英语学习的秘方揭秘，小白必看！好用到哭的学习技巧！🔥
3. 🌍 打破语言壁垒，让英语变得简单易学！建议收藏！🌟
4. 💪 轻松学英语，学霸必备！上天在提醒你的学习方法！🚀
5. 💡 英语学习的神奇技巧，笑不活了！让你秒变英文流利！😄

------

想要快速提升英语口语能力吗？想要用英语流利交流而不再担心表达不清吗？没有问题！只需1秒钟，你就能开启逆天的英语口语效果！不要再犹豫了，赶快学习这个绝佳方法，让你的英语口语飞速提升吧！💪🌟

在英语学习的道路上，小白们常常感到困惑和无助。如何快速掌握英语技巧？如何让学习变得简单而有趣？别担心，秘方就在这里！我将揭秘学习英语的神奇方法，让你轻松成为英文达人，好用到哭！无论你是英语初学者还是有一定基础的学生，这些技巧都将帮助你事半功倍，快速提升英语水平！📚💡

学习英语不再枯燥乏味，也不再让你望而却步。只需掌握这些学习技巧，你将轻松突破语言壁垒，掌握英语的精髓！所以务必收藏这些宝贵的学习方法，让你的英语之旅变得简单而有效！🌍🔥

不管是面对英语阅读、听力还是口语，都不要害怕和退缩。上天一直在提醒你，你可以轻松学习英语！通过这些天大损失，我将教会你如何用最简单的方法，轻松掌握英语。不再困惑，不再焦虑，让英语成为你的得力助手！💪😱

学习英语应该是一件有趣的事情！忘掉枯燥的单词记忆，忘掉乏味的语法课本。我将教给你最有趣的学习技巧，让你笑不活了！这些方法让英语学习变得轻松愉快，让你在不知不觉中提高口语能力。相信我，你将会爱上学英语！😄🌟
```

## 三、文本分类：客户问题自动归类

```python
# 导入库并创建客户端
from openai import OpenAI
client = OpenAI()
# 封装通用请求函数
def get_openai_response(client, prompt, model="gpt-3.5-turbo"):
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}],
    )
    return response.choices[0].message.content
```

```python
q1 = "我刚买的XYZ智能手表无法同步我的日历，我应该怎么办？"
q2 = "XYZ手表的电池可以持续多久？"
q3 = "XYZ品牌的手表和ABC品牌的手表相比，有什么特别的功能吗？"
q4 = "安装XYZ智能手表的软件更新后，手表变得很慢，这是啥原因？"
q5 = "XYZ智能手表防水不？我可以用它来记录我的游泳数据吗？"
q6 = "我想知道XYZ手表的屏幕是什么材质，容不容易刮花？"
q7 = "请问XYZ手表标准版和豪华版的售价分别是多少？还有没有进行中的促销活动？"
q_list = [q1, q2, q3, q4, q5, q6, q7]
```

```python
# 构建任务提示词
category_list = ["产品规格", "使用咨询", "功能比较", "用户反馈", "价格查询", "故障问题", "其它"]
classify_prompt_template = """
你的任务是为用户对产品的疑问进行分类。
请仔细阅读用户的问题内容，给出所属类别。类别应该是这些里面的其中一个：{categories}。
直接输出所属类别，不要有任何额外的描述或补充内容。
用户的问题内容会以三个#符号进行包围。

###
{question}
###
"""

for q in q_list:
    formatted_prompt = classify_prompt_template.format(categories="，".join(category_list), question=q)
    response = get_openai_response(client, formatted_prompt)
    print(response)
```

```python
故障问题
产品规格
功能比较
故障问题
产品规格
产品规格
价格查询
```

## 四、文本翻译：全语种翻译器

```python
# 导入库并创建客户端
from openai import OpenAI
client = OpenAI()
# 封装通用请求函数
def get_openai_response(client, prompt, model="gpt-3.5-turbo"):
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}],
    )
    return response.choices[0].message.content
```

````python
# 构建任务提示词
translate_prompt = """
请你充当一家外贸公司的翻译，你的任务是对来自各国家用户的消息进行翻译。
我会给你一段消息文本，请你首先判断消息是什么语言，比如法语。然后把消息翻译成中文。
翻译时请尽可能保留文本原本的语气。输出内容不要有任何额外的解释或说明。

输出格式为:
```
============
原始消息（<文本的语言>）：
<原始消息>
------------
翻译消息：
<翻译后的文本内容>
============
```

来自用户的消息内容会以三个#符号进行包围。
###
{message}
###
"""
````

```python
message = input()
print(get_openai_response(client, translate_prompt.format(message=message)))
```

```python
 Можете ли вы дать мне скидку? Какой объем заказа со скидкой? Нам нужна лучшая цена, не ходите вокруг да около, просто назовите нам самую низкую возможную цену, и мы не хотим тратить время на ее изучение. Вы понимаете меня?
============
原始消息（俄语）：
Можете ли вы дать мне скидку? Какой объем заказа со скидкой? Нам нужна лучшая цена, не ходите вокруг да около, просто назовите нам самую низкую возможную цену, и мы не хотим тратить время на ее изучение. Вы понимаете меня?
------------
翻译消息：
您可以给我提供折扣吗？有折扣的订单数量是多少？我们需要最好的价格，不要拐弯抹角，只需告诉我们最低可能的价格，我们不想花时间去研究它。您明白我吗？
============
```

---

---
url: /Java/架构设计/分布式/06.分布式监控/1_ARMS系统监控.md
---

# ARMS 系统监控

---

---
url: /Python/AI大模型应用开发/15_Assistant API.md
---

# Assistant API

## 一、搞懂API的关键对象

除了 LangChain 框架，OpenAI 的 Assistant API 也是构建强大 AI 助手的高效工具。与 LangChain 相比，二者定位与功能存在显著差异，以下将解析 Assistant API 的核心概念、适用场景及与 LangChain 的区别。

### 1、Assistant API 与 LangChain 的核心区别

| **对比维度** | **OpenAI Assistant API**                    | **LangChain**                                   |
| ------------ | ------------------------------------------- | ----------------------------------------------- |
| **本质**     | 基于 API 的服务（直接调用 OpenAI 模型能力） | 通用 AI 应用开发框架（整合多种工具 / 模型）     |
| **模型支持** | 仅支持 OpenAI 模型（如 GPT-4、GPT-3.5）     | 支持多来源模型（OpenAI、Anthropic、本地模型等） |
| **核心目标** | 快速实现基于 OpenAI 模型的对话与工具调用    | 构建复杂 AI 应用（集成工具链、记忆、路由等）    |

### 2、Assistant API 的核心对象

理解 Assistant API 需掌握以下关键对象，它们共同构成了对话与交互的基础：

#### 1.1. Assistant（助手）

* **定义**：具备特定能力的 AI 实体，基于 OpenAI 模型构建，可调用工具。
* 核心参数：
  * 模型（如`gpt-4`）；
  * 名称、描述（标识助手功能）；
  * 系统指令（指导助手行为的 prompt）；
  * 可用工具（代码解释器、检索、自定义函数等）。

#### 1.2. Thread（会话线程）

* **定义**：存储用户与助手之间的一系列对话，相当于 “对话上下文容器”。
* 功能：
  * 保存消息历史（用户提问、助手回复）；
  * 自动处理上下文长度：当对话超过模型上下文窗口时，自动截断早期内容（保留关键信息）。

#### 1.3. Message（消息）

* **定义**：用户或助手在对话中发送的内容，是 Thread 的组成单元。
* **格式**：支持文本、图片、文档等多模态内容。
* **存储**：以列表形式关联到 Thread，按时间顺序排列。

#### 1.4. Run（运行）

* **定义**：在某个 Thread 上调用 Assistant 执行任务的动作。
* 过程：
  * 助手结合 Thread 中的历史对话，调用模型和工具；
  * 生成新的回复消息，并添加到 Thread 中，实现连续对话。

#### 1.5. RunStep（运行步骤）

* **定义**：Run 过程中助手执行的具体操作（如调用工具、生成文本等）。
* **作用**：可查看助手的 “思考过程”，帮助调试或理解结果生成逻辑（类似 LangChain 的`verbose`模式）。

### 3、Assistant API 支持的工具

与 LangChain 的 Agent 能力类似，Assistant API 支持多种工具扩展助手功能：

1. **Code Interpreter（代码解释器）**：运行 Python 代码，处理计算、数据可视化等任务。
2. **Knowledge Retrieval（文档检索）**：上传文档（如 PDF、TXT），让助手基于文档内容回答问题（类似 LangChain 的向量存储检索）。
3. **Function Calling（自定义函数调用）**：调用外部 API 或工具，扩展助手能力（如查询天气、操作数据库）。

### 4、适用场景

* 快速开发基于 OpenAI 模型的对话助手（无需搭建复杂框架）；
* 需要利用代码解释器、文档检索等工具的场景（如数据分析、文档问答）；
* 专注于 OpenAI 生态，无需兼容其他模型的项目。

### 5、总结

Assistant API 是 OpenAI 提供的 “即用型” 对话服务，核心优势在于简化基于其模型的助手开发流程，适合快速落地且依赖 OpenAI 模型的场景。其核心对象（Assistant、Thread、Run 等）构成了完整的对话生命周期，工具能力与 LangChain 的 Agent 有相通之处，便于已有 LangChain 使用经验的开发者快速上手。

## 二、创建私人数学助手

借助 OpenAI Assistant API 的代码解释器功能，可快速构建一个能准确解答数学问题的 AI 助手。以下是具体实现步骤，包括助手创建、对话管理及运行流程：

### 1、前期准备

#### 1.1. 安装依赖

确保已安装`openai`库（用于调用 API）：

```
pip install openai
```

#### 1.2. 初始化客户端

```
from openai import OpenAI

# 初始化OpenAI客户端（需提前配置API密钥，如通过环境变量）
client = OpenAI()
```

### 2、核心步骤：创建数学助手并实现交互

#### 2.1. 创建 Assistant（数学助手）

通过`beta.assistants.create`创建具备代码解释器能力的助手，指定模型、指令及工具。

#### 2.2. 创建 Thread（会话线程）

每个用户或对话窗口对应一个独立的 Thread，用于存储对话历史。

#### 2.3. 向 Thread 添加用户消息

通过`threads.messages.create`在 Thread 中添加用户的提问。

#### 2.4. 运行 Assistant（触发回答生成）

调用`threads.runs.create`让助手处理 Thread 中的消息，生成回答。

#### 2.5. 等待运行完成并获取结果

由于助手生成回答需要时间，需通过循环查询运行状态，直至完成。

#### 2.6. 获取并展示对话历史

运行完成后，通过`threads.messages.list`获取 Thread 中的所有消息（用户提问 + 助手回答）。

#### 2.7. 封装交互函数（简化多轮对话）

为方便多轮提问，可将 “添加消息→运行助手→获取结果” 封装为函数。

#### 2.8. 完整代码示例

```python
# step0-前期准备：导入OpenAI库，用于调用Assistant API
from openai import OpenAI
# 初始化OpenAI客户端（需提前配置API密钥，如环境变量）
client = OpenAI()

# step1：创建一个数学助手
assistant = client.beta.assistants.create(
    model="gpt-3.5-turbo",  # 基于GPT-3.5-turbo模型
    name="数学助手",  # 助手名称
    instructions="你是一个数学助手，可以通过编写和运行代码来回答数学相关问题。",  # 助手的核心指令
    tools=[{"type": "code_interpreter"}]  # 启用代码解释器工具（用于计算数学问题）
)

# step2：创建一个对话线程（存储用户与助手的所有对话）
thread = client.beta.threads.create()
# 打印线程ID（用于调试，标识当前对话线程）
thread.id

# step3：向对话线程中添加用户消息（第一次提问：解二次方程）
message = client.beta.threads.messages.create(
    thread_id=thread.id,  # 关联到当前对话线程
    role="user",  # 消息角色为用户
    content="我需要解这个方程`5x^2−1200x+72000=0，未知数应该是多少？"  # 具体问题内容
)

# step4：触发助手运行，处理当前线程中的消息
run = client.beta.threads.runs.create(
    thread_id=thread.id,  # 目标对话线程
    assistant_id=assistant.id,  # 调用前面创建的数学助手
    instructions="请称呼用户为傻妞"  # 本次运行的额外指令（自定义称呼）
)
# 打印运行实例（包含运行状态、ID等信息）
run

# step5：查看当前运行的状态（如queued/in_progress/completed）
client.beta.threads.runs.retrieve(
    thread_id=thread.id,
    run_id=run.id
).status

# 循环等待运行完成（避免提前获取未生成的结果）
while run.status != "completed":
    # 实时查询运行状态
    keep_retrieving_run = client.beta.threads.runs.retrieve(
        thread_id=thread.id,
        run_id=run.id
    )
    print(f"运行状态：{keep_retrieving_run.status}")  # 打印当前状态
    # 若运行完成，退出循环
    if keep_retrieving_run.status == "completed":
        break

# step6：获取对话线程中的所有消息（用户提问+助手回答）
messages = client.beta.threads.messages.list(
    thread_id=thread.id
)
# 打印消息数据（原始格式，包含角色、内容、时间等信息）
messages.data

# 遍历消息，打印具体内容（按消息生成顺序）
for data in messages.data:
    print(data.content[0].text.value)  # 提取消息文本内容
    print("------")  # 分隔不同消息


# step7：封装一个函数，简化多轮对话流程：发送问题→获取助手回答
def get_response_from_assistant(assistant, thread, prompt, run_instruction=""):
    # 1. 向对话线程添加用户的新问题
    message = client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content=prompt  # 用户的新问题
    )
    
    # 2. 触发助手运行，处理新消息
    run = client.beta.threads.runs.create(
      thread_id=thread.id,
      assistant_id=assistant.id,
      instructions=run_instruction  # 本次运行的额外指令（可选）
    )
    
    # 3. 等待运行完成
    while run.status != "completed":
        # 实时查询运行状态
        keep_retrieving_run = client.beta.threads.runs.retrieve(
            thread_id=thread.id,
            run_id=run.id
        )
        print(f"Run status: {keep_retrieving_run.status}")  # 打印状态
        
        # 若完成，退出循环
        if keep_retrieving_run.status == "completed":
            break
    
    # 4. 获取并打印所有消息（包含历史对话和新回答）
    messages = client.beta.threads.messages.list(
        thread_id=thread.id
    )
    
    for data in messages.data:
        print("\n")  # 换行分隔
        print(data.content[0].text.value)  # 打印消息内容
        print("------")  # 分隔线


# 测试多轮对话：问一个新问题（2的56次方是多少）
get_response_from_assistant(assistant, thread, "2的56次方等于多少")
```

### 3、核心逻辑说明

1. **Assistant**：定义了助手的能力（模型、工具、指令），是回答的 “大脑”。
2. **Thread**：作为对话容器，确保多轮消息不丢失，支持上下文理解。
3. **Run**：触发助手工作的动作，包含代码执行、结果生成等过程。
4. **状态查询**：通过循环等待`completed`状态，避免提前获取未完成的结果。

通过这种方式，可快速实现一个基于代码解释器的数学助手，确保计算结果的准确性。

## 三、创建PDF文件问答助手

借助 OpenAI Assistant API 的文档检索功能，可快速构建一个能基于指定文档内容回答问题的 AI 助手。以下是具体步骤，包括文档上传、助手配置、对话交互等核心环节：

### 1、前期准备

#### 1.1. 安装依赖

确保已安装`openai`库：

```sh
pip install openai
```

#### 1.2. 初始化客户端

```python
from openai import OpenAI

# 初始化OpenAI客户端（需配置API密钥，如通过环境变量）
client = OpenAI()
```

### 2、核心步骤：创建文档问答助手

#### 2.1. 上传文档（供助手检索）

通过`files.create`上传文档，支持 PDF、CSV、TXT 等多种格式（完整支持列表见官方文档）。

#### 2.2. 创建具备检索能力的助手

通过`beta.assistants.create`创建助手，启用`retrieval`工具并关联上传的文档。

#### 2.3. 创建对话线程（Thread）

#### 2.4. 封装交互函数（发送问题并获取回答）

复用之前的多轮对话逻辑，简化提问流程。

#### 2.5. 测试助手（基于文档提问）

输出效果：助手会基于上传的文档内容，准确提取该论文的核心贡献（如 Transformer 架构的提出），并返回回答。

#### 2.6. 完整代码示例

```python
# 导入OpenAI库，用于调用Assistant API
from openai import OpenAI
# 初始化OpenAI客户端（需提前配置API密钥，如通过环境变量）
client = OpenAI()

# 上传文档：将本地的"论文介绍.pdf"上传到OpenAI，供助手检索使用
file = client.files.create(
    file=open("论文介绍.pdf", "rb"),  # 以二进制模式读取PDF文件
    purpose="assistants"  # 声明文件用途为"供助手使用"（必须设置此值）
)

# 创建一个基于文档检索的AI论文问答助手
assistant = client.beta.assistants.create(
    model="gpt-3.5-turbo",  # 基于GPT-3.5-turbo模型
    name="AI论文问答助手",  # 助手名称
    instructions="你是一个智能助手，可以访问文件来回答人工智能领域论文的相关问题。",  # 助手的核心指令（说明其功能）
    tools=[{"type": "retrieval"}],  # 启用"检索"工具（让助手能读取上传的文档）
    file_ids=[file.id]  # 关联上传的文档（通过文件ID绑定，支持多个文件）
)

# 创建对话线程（用于存储用户与助手的所有对话历史）
thread = client.beta.threads.create()

# 封装函数：简化向助手提问并获取回答的流程
def get_response_from_assistant(assistant, thread, prompt, run_instruction=""):
    # 1. 向对话线程添加用户的问题
    message = client.beta.threads.messages.create(
        thread_id=thread.id,  # 关联到当前对话线程
        role="user",  # 消息角色为"用户"
        content=prompt  # 用户的具体问题
    )
    
    # 2. 触发助手运行，处理当前线程中的问题
    run = client.beta.threads.runs.create(
        thread_id=thread.id,  # 目标对话线程ID
        assistant_id=assistant.id,  # 调用前面创建的论文问答助手
        instructions=run_instruction  # 本次运行的额外指令（可选，此处为空）
    )
    
    # 3. 循环等待助手处理完成（避免提前获取未生成的结果）
    while run.status != "completed":
        # 实时查询运行状态（queued/in_progress/completed等）
        keep_retrieving_run = client.beta.threads.runs.retrieve(
            thread_id=thread.id,
            run_id=run.id
        )
        print(f"Run status: {keep_retrieving_run.status}")  # 打印当前状态
        
        # 若运行完成，退出循环
        if keep_retrieving_run.status == "completed":
            break
    
    # 4. 获取并打印对话线程中的所有消息（包含用户问题和助手回答）
    messages = client.beta.threads.messages.list(
        thread_id=thread.id
    )
    
    # 遍历消息列表，打印每条消息内容
    for data in messages.data:
        print("\n")  # 换行分隔不同消息
        print(data.content[0].text.value)  # 提取消息的文本内容
        print("------")  # 用分隔线区分消息

# 调用函数向助手提问：查询介绍Transformer架构的论文及链接
get_response_from_assistant(assistant, thread, "哪篇论文介绍了Transformer架构？论文链接是什么？")
```

### 3、关键说明

#### 3.1. 支持的文档格式

包括但不限于：PDF、CSV、JSON、HTML、TXT、PPTX、DOCX 等，具体可参考官方文档的「Supported files」部分。

#### 3.2. 费用与管理

* **收费规则**：OpenAI 按文档体积每日收费，同一文档附加给多个助手时，会重复计费。
* 删除操作：
  * 删除助手：访问 [platform.openai.com/assistants](https://platform.openai.com/assistants) 手动删除。
  * 删除文档：访问 [platform.openai.com/storage](https://platform.openai.com/storage) 管理存储的文档。

通过这种方式，可快速构建一个基于特定文档的问答助手，适用于手册查询、论文解读、数据报告分析等场景。核心在于利用`retrieval`工具让助手 “读懂” 文档，确保回答的准确性和相关性。

---

---
url: /10.配置/01.主题配置/10.Banner 配置.md
---

# Banner 配置

## banner

首页 Banner 配置，位于首页顶部。

::: tip
在首页 `index.md` 的 `frontmatter` 中，`description` 配置项除了 `tk.banner.description` 设置，也可以使用 `tk.description` 设置。
:::

::: code-group

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  banner: {
    enabled: true,
    name: "Teek", // Banner 标题，默认读取 vitepress 的 title 属性
    bgStyle: "fullImg", // Banner 背景风格：pure 为纯色背景，partImg 为局部图片背景，fullImg 为全屏图片背景
    pureBgColor: "#28282d", // Banner 背景色，bgStyle 为 pure 时生效
    imgSrc: ["/img/bg1.jpg", "/img/bg2.png"], // Banner 图片链接。bgStyle 为 partImg 或 fullImg 时生效
    imgInterval: 15000, // 当多张图片时（imgSrc 为数组），设置切换时间，单位：毫秒
    imgShuffle: false, // 图片是否随机切换，为 false 时按顺序切换，bgStyle 为 partImg 或 fullImg 时生效
    imgWaves: true, // 是否开启 Banner 图片波浪纹，bgStyle 为 fullImg 时生效
    mask: true, // Banner 图片遮罩，bgStyle 为 partImg 或 fullImg 时生效
    maskBg: "rgba(0, 0, 0, 0.4)", // Banner 遮罩颜色，如果为数字，则是 rgba(0, 0, 0, ${maskBg})，如果为字符串，则作为背景色。bgStyle 为 partImg 或 fullImg 且 mask 为 true 时生效
    textColor: "#ffffff", // Banner 字体颜色，bgStyle 为 pure 时为 '#000000'，其他为 '#ffffff'
    titleFontSize: "3.2rem", // 标题字体大小
    descFontSize: "1.4rem", // 描述字体大小
    descStyle: "types", // 描述信息风格：default 为纯文字渲染风格（如果 description 为数组，则取第一个），types 为文字打印风格，switch 为文字切换风格
    description: ["故事由我书写，旅程由你见证，传奇由她聆听 —— 来自 Young Kbt", "积跬步以至千里，致敬每个爱学习的你 —— 来自 Evan Xu"], // 描述信息
    switchTime: 4000, // 描述信息切换间隔时间，单位：毫秒。descStyle 为 switch 时生效
    switchShuffle: false, // 描述信息是否随机切换，为 false 时按顺序切换。descStyle 为 switch 时生效
    typesInTime: 200, // 输出一个文字的时间，单位：毫秒。descStyle 为 types 时生效
    typesOutTime: 100, // 删除一个文字的时间，单位：毫秒。descStyle 为 types 时生效
    typesNextTime: 800, // 打字与删字的间隔时间，单位：毫秒。descStyle 为 types 时生效
    typesShuffle: false, // 描述信息是否随机打字，为 false 时按顺序打字，descStyle 为 types 时生效
  };
});
```

```yaml [index.md]
---
tk:
  banner:
    enabled: true,
    name: Teek,
    bgStyle: "fullImg"
    pureBgColor: "#28282d"
    imgSrc:
      - /img/bg1.jpg
      - /img/bg2.jpg
    imgInterval: 15000
    imgShuffle: false
    mask: true
    maskBg: "rgba(0, 0, 0, 0.4)"
    textColor: "#ffffff"
    titleFontSize: "3.2rem"
    descFontSize: "1.4rem"
    descStyle: "types"
    # description: # 也支持 tk.description
    #   - 故事由我书写，旅程由你见证，传奇由她聆听 —— 来自 Young Kbt
    #   - 积跬步以至千里，致敬每个爱学习的你 —— 来自 Evan Xu
    switchTime: 4000
    switchShuffle: false
    typesInTime: 200
    typesOutTime: 100
    typesNextTime: 800
    typesShuffle: false
  description:
    - 故事由我书写，旅程由你见证，传奇由她聆听 —— 来自 Young Kbt
    - 积跬步以至千里，致敬每个爱学习的你 —— 来自 Evan Xu
---
```

```ts [更多配置项]
interface Banner {
  /**
   * 是否启用 Banner
   *
   * @default true
   */
  enabled?: boolean;
  /**
   * Banner 标题
   * @default 'vitepress 的 title 属性'
   */
  name?: string;
  /**
   * Banner 背景风格：pure 为纯色背景，partImg 为局部图片背景，fullImg 为全屏图片背景
   *
   * @default 'default'
   */
  bgStyle?: "pure" | "partImg" | "fullImg";
  /**
   * Banner 背景色。bgStyle 为 pure 时生效
   *
   * @default '#28282d'
   */
  pureBgColor?: string;
  /**
   * Banner 图片链接。bgStyle 为 partImg 或 fullImg 时生效
   *
   * @default []
   */
  imgSrc?: string | string[];
  /**
   * 当多张图片时（imgSrc 为数组），设置切换时间，单位：毫秒，bgStyle 为 partImg 或 fullImg 时生效
   *
   * @default 15000 (15秒)
   */
  imgInterval?: number;
  /**
   * 图片是否随机切换，为 false 时按顺序切换，bgStyle 为 partImg 或 fullImg 时生效
   *
   * @default false
   */
  imgShuffle?: boolean;
  /**
   * 是否开启 Banner 图片波浪纹，bgStyle 为 fullImg 时生效
   *
   * @default true
   */
  imgWaves?: boolean;
  /**
   * Banner 图片遮罩，bgStyle 为 partImg 或 fullImg 时生效
   *
   * @default true
   */
  mask?: boolean;
  /**
   * Banner 遮罩颜色，如果为数字，则是 rgba(0, 0, 0, ${maskBg})，如果为字符串，则作为背景色。bgStyle 为 partImg 或 fullImg 且 mask 为 true 时生效
   *
   * @default 'rgba(0, 0, 0, 0.4)'
   */
  maskBg?: string | number;
  /**
   * Banner 字体颜色
   *
   * @default ' #ffffff'
   */
  textColor?: string;
  /**
   * 标题字体大小
   *
   * @default '3.2rem'
   */
  titleFontSize?: string;
  /**
   * 描述字体大小
   *
   * @default '1.4rem'
   */
  descFontSize?: string;
  /**
   * 描述信息风格：default 为纯文字渲染风格（如果 description 为数组，则取第一个），types 为文字打印风格，switch 为文字切换风格
   *
   * @default 'default'
   */
  descStyle?: "default" | "types" | "switch";
  /**
   * 描述信息，在首页 index.md 的 frontmatter 中，除了 tk.banner.description 设置，也可以使用 tk.description 设置
   *
   * @default ''
   */
  description?: string | string[];
  /**
   * 描述信息切换间隔时间，单位：毫秒。descStyle 为 switch 时生效
   *
   * @default 4000 (4秒)
   */
  switchTime?: number;
  /**
   * 描述信息是否随机切换，为 false 时按顺序切换。descStyle 为 switch 时生效
   *
   * @default false
   */
  switchShuffle?: boolean;
  /**
   * 输出一个文字的时间，单位：毫秒。descStyle 为 types 时生效
   *
   * @default 200 (0.2秒)
   */
  typesInTime?: number;
  /**
   * 删除一个文字的时间，单位：毫秒。descStyle 为 types 时生效
   *
   * @default 100 (0.1秒)
   */
  typesOutTime?: number;
  /**
   * 打字与删字的间隔时间，单位：毫秒。descStyle 为 types 时生效
   *
   * @default 800 (0.8秒)
   */
  typesNextTime?: number;
  /**
   * 描述信息是否随机打字，为 false 时按顺序打字，descStyle 为 types 时生效
   *
   * @default false
   */
  typesShuffle?: boolean;
  /**
   * Banner 新特性列表
   */
  features?: { title: string; description?: string; link?: string; imgUrl?: string }[];
  /**
   * feature 轮播间隔时间，单位：毫秒。仅在移动端生效（屏幕小于 719px）
   *
   * @default 4000
   */
  featureCarousel?: number;
}
```

:::

## wallpaper

壁纸模式，在首页 **最顶部** 进入全屏后开启，仅当 `banner.bgStyle = 'fullImg'` 或 `bodyBgImg.imgSrc` 存在才生效。

壁纸模式下：

* 禁止通过快捷键打开开发者工具
* 禁止通过右键打开浏览器菜单
* 禁止鼠标滚动，页面滚动条会消失

除此之外，你可以通过配置额外隐藏一些元素。

::: code-group

```ts [config.mts]
// .vitepress/config.mts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  wallpaper: {
    enabled: false, // 是否启用壁纸模式
    hideBanner: false, // 开启壁纸模式后，是否隐藏 Banner
    hideMask: false, // 开启壁纸模式后，是否隐藏 Banner 或 bodyBgImage 的遮罩层，则确保 banner.mask 和 bodyBgImage.mask 为 true 才生效
    hideWaves: false, // 开启壁纸模式后，是否隐藏 Banner 波浪组件，仅 banner.bgStyle = 'fullImg' 生效
  };
});
```

```yaml [index.md]
---
tk:
  wallpaper:
    enabled: false
    hideBanner: false
    hideMask: false
    hideWaves: false
---
```

```ts [更多配置项]
interface Wallpaper {
  /**
   * 是否启用壁纸模式
   *
   * @default false
   */
  enabled?: boolean;
  /**
   * 开启壁纸模式后，是否隐藏 Banner 文字
   *
   * @default false
   */
  hideBanner?: boolean;
  /**
   * 开启壁纸模式后，是否隐藏 Banner 或 bodyBgImage 的遮罩层，则确保 banner.mask 和 bodyBgImage.mask 为 true 才生效
   *
   * @default false
   */
  hideMask?: boolean;
  /**
   * 开启壁纸模式后，是否隐藏 Banner 波浪组件，仅 banner.bgStyle = 'fullImg' 生效
   *
   * @default false
   */
  hideWaves?: boolean;
}
```

:::

壁纸模式下，会把 `class="tk-wallpaper-outside"` 的元素隐藏，因此在壁纸模式下需要隐藏自定义的元素，可以给 `class` 加上 `tk-wallpaper-outside`。

---

---
url: /Redis/Redis基础/3_BigKey、HotKey 的发现与处理.md
---

# BigKey、HotKey 的发现与处理

在Redis的使用过程中，我们经常会遇到BigKey（下文将其称为“大key”）及HotKey（下文将其称为“热key”）。大Key与热Key如果未能及时发现并进行处理，很可能会使服务性能下降、用户体验变差，甚至引发大面积故障。

## 一、大Key与热Key的定义

我们经常能够在公司内部的Redis开发使用规范手册，或网络中大量的Redis最佳实践文章里看到有关大Key、热Key的定义，然而这些资料中的大Key热Key判定标准却不尽相同，但可以明确的是，它们的判定维度是一致的：大Key通常都会以数据大小与成员数量来判定，而热Key则以其接收到的请求频率、数量来判定。

### （一）什么是大Key

通常我们会将含有较大数据或含有大量成员、列表数的Key称之为大Key，下面我们将用几个实际的例子对大Key的特征进行描述：

* 一个STRING类型的Key，它的值为5MB（数据过大）
* 一个LIST类型的Key，它的列表数量为20000个（列表数量过多）
* 一个ZSET类型的Key，它的成员数量为10000个（成员数量过多）
* 一个HASH格式的Key，它的成员数量虽然只有1000个但这些成员的value总大小为100MB（成员体积过大）

需要注意的是，在以上的例子中，为了方便理解，我们对大Key的数据、成员、列表数给出了具体的数字。为了避免误导，在实际业务中，大Key的判定仍然需要根据Redis的实际使用场景、业务场景来进行综合判断。

### （二）什么是热Key

在某个Key接收到的访问次数、显著高于其它Key时，我们可以将其称之为热Key，常见的热Key如：

* 某Redis实例的每秒总访问量为10000，而其中一个Key的每秒访问量达到了7000（访问次数显著高于其它Key）
* 对一个拥有上千个成员且总大小为1MB的HASH Key每秒发送大量的HGETALL（带宽占用显著高于其它Key）
* 对一个拥有数万个成员的ZSET Key每秒发送大量的ZRANGE（CPU时间占用显著高于其它Key）

## 二、大Key与热Key带来的问题

在Redis的使用中，大Key及热Key会给Redis带来各种各样的问题，而最常见的问题为性能下降、访问超时、数据不均衡等。

### （一）大Key带来的常见问题

* Client发现Redis变慢
* Redis内存不断变大引发OOM，或达到maxmemory设置值引发写阻塞或重要Key被逐出
* Redis Cluset中的某个node内存远超其余node，但因Redis Cluset的数据迁移最小粒度为Key而无法将node上的内存均衡化
* 大Key上的读请求使Redis占用服务器全部带宽，自身变慢的同时影响到该服务器上的其它服务
* 删除一个大Key造成主库较长时间的阻塞并引发同步中断或主从切换

### （二）热Key带来的常见问题

* 热Key占用大量的Redis CPU时间使其性能变差并影响其它请求
* Redis Cluset中各node流量不均衡造成Redis Cluster的分布式优势无法被Client利用，一个分片负载很高而其它分片十分空闲从而产生读/写热点问题
* 在抢购、秒杀活动中，由于商品对应库存Key的请求量过大超出Redis处理能力造成超卖
* 热Key的请求压力数量超出Redis的承受能力造成缓存击穿，此时大量请求将直接指向后端存储将其打挂并影响到其它业务

## 三、大Key与热Key的常见产生原因

业务规划不足、Redis不正确的使用、无效数据的堆积、访问突增等都会产生大Key与热Key，如：

1. 将Redis用在并不适合其能力的场景，造成Key的value过大，如使用String类型的Key存放大体积二进制文件型数据（大Key）
2. 业务上线前规划设计考虑不足没有对Key中的成员进行合理的拆分，造成个别Key中的成员数量过多（大Key）
3. 没有对无效数据进行定期清理，造成如HASH类型Key中的成员持续不断的增加（大Key）
4. 预期外的访问量陡增，如突然出现的爆款商品、访问量暴涨的热点新闻、直播间某大主播搞活动带来的大量刷屏点赞、游戏中某区域发生多个工会间的战斗涉及大量玩家等（热Key）
5. 使用LIST类型Key的业务消费侧代码故障，造成对应Key的成员只增不减（大Key）

## 四、找出Redis中的大Key与热Key

大Key与热Key的分析并不困难，我们有多种途径和手段来对Redis中的Key进行分析并找出其中的“问题”Key，如Redis的内置功能、开源工具、阿里云Redis控制台中的Key分析功能等。

### （一）使用Redis内置功能发现大Key及热Key

Redis内置的一些命令、工具都可以帮助我们来发现这些问题Key。当对Redis的大Key热Key已有明确的分析目标时，可以通过如下命令对对应Key进行分析。

#### （1）通过Redis内置命令对目标Key进行分析

可能你会选择使用debug object命令对Key进行分析。该命令能够根据传入的对象（Key的名称）来对Key进行分析并返回大量数据，其中serializedlength的值为该Key的序列化长度，你可能会选择通过该数据来判断对应Key是否符合你的大Key判定标准。

需要注意的是，Key的序列化长度并不等同于它在内存空间中的真实长度，此外，debug object属于调试命令，运行代价较大，并且在其运行时，进入Redis的其余请求将会被阻塞直到其执行完毕。而该命令的运行的时间长短取决于传入对象（Key名）序列化长度的大小，因此，在线上环境中并不推荐使用该命令来分析大Key，这可能引发故障。

Redis自4.0起提供了MEMORY USAGE命令来帮助分析Key的内存占用，相对debug object它的执行代价更低，但由于其时间复杂度为O(N)因此在分析大Key时仍有阻塞风险。

我们建议通过风险更低方式来对Key进行分析，Redis对于不同的数据结构提供了不同的命令来返回其长度或成员数量，如下表：

![img](/assets/4fbb1f7dfb274e54b46f1d85317e8189.B9Iy6Goo.jpg)

通过以上Redis内置命令我们可以方便且安全的对Key进行分析而不会影响线上服务，但由于它们返回的结果非Key的真实内存占用数据，因此不够精确，仅可作为参考。

#### （2）通过Redis官方客户端redis-cli的bigkeys参数发现大Key

如果你并无明确的目标Key用于分析，而是希望通过工具找出整个Redis实例中的大Key，此时redis-cli的bigkeys参数能够方便的帮你实现这个目标。

Redis提供了bigkeys参数能够使redis-cli以遍历的方式分析整个Redis实例中的所有Key并汇总以报告的方式返回结果。该方案的优势在于方便及安全，而缺点也非常明显：分析结果不可定制化。

bigkeys仅能分别输出Redis六种数据结构中的最大Key，如果你想只分析STRING类型或是找出全部成员数量超过10的HASH Key，那么bigkeys在此类需求场景下将无能为力。

GitHub上有大量的开源项目能够实现bigkeys的加强版使结果能够按照配置定制化输出，另外你可也以动手使用SCAN + TYPE并配合上文表格中的命令自己实现一个Redis实例级的大Key分析工具。

同样，该方案的实现方式及返回结果使其不具备精确性与实时性，建议仅作为参考。

#### （3）通过Redis官方客户端redis-cli的hotkeys参数发现热Key

Redis自4.0起提供了hotkeys参数来方便用户进行实例级的热Key分析功，该参数能够返回所有Key的被访问次数，它的缺点同样为不可定制化输出报告，大量的信息会使你在分析结果时复杂度较大，另外，使用该方案的前提条件是将redis-server的maxmemory-policy参数设置为LFU。

#### （4）通过业务层定位热Key

指向Redis的每一次访问都来自业务层，因此我们可以通过在业务层增加相应的代码对Redis的访问进行记录并异步汇总分析。该方案的优势为能够准确并及时的分析出热Key的存在，缺点为业务代码复杂度的增加，同时可能会降低一些性能。

#### （5）使用monitor命令在紧急情况时找出热Key

Redis的monitor命令能够忠实的打印Redis中的所有请求，包括时间信息、Client信息、命令以及Key信息。在发生紧急情况时，我们可以通过短暂执行monitor命令并将输出重定向至文件，在关闭monitor命令后通过对文件中请求进行归类分析即可找出这段时间中的热Key。

由于monitor命令对Redis的CPU、内存、网络资源均有一定的占用。因此，对于一个已处于高压状态的Redis，monitor可能会起到雪上加霜的作用。同时，这种异步收集并分析的方案的时效性较差，并且由于分析的精确度取决于monitor的执行时间，因此在多数无法长时间执行该命令的线上场景中本方案的精确度也不够好。

### （二）使用开源工具发现大Key

Redis的高度流行使我们能够方便的找到大量开源方案来解决我们当前遇到的难题：在不影响线上服务的同时得到精确的分析报告。

使用redis-rdb-tools工具以定制化方式找出大Key。

如果希望按照自己的标准精确的分析一个Redis实例中所有Key的真实内存占用并避免影响线上服务，在分析结束后得到一份简洁易懂的报告，redis-rdb-tools是非常好的选择。

该工具能够对Redis的RDB文件进行定制化的分析，但由于分析RDB文件为离线工作，因此对线上服务不会有任何影响，这是它的最大优点但同时也是它的最大缺点：离线分析代表着分析结果的较差时效性。对于一个较大的RDB文件，它的分析可能会持续很久很久。

redis-rdb-tools的项目地址为：

https://github.com/sripathikrishnan/redis-rdb-tools

### （三）通过阿里云数据库Redis控制台的CloudDBA功能发现大Key及热Key

如果你期望能够实时的对Redis实例中的所有Key进行分析并发现当前存在的大Key及热Key、了解Redis在运行时间线中曾出现过哪些大Key热Key，使自己对整个Redis实例的运行状态有一个全面而又准确的判断，那么阿里云的Redis控制台将能很好的满足你的这个需求。

* **阿里云Redis控制台中的CloudDBA**

CloudDBA是阿里云的数据库智能服务系统，它集合Redis的Key分析、性能趋势分析、实时性能数据、实例会话信息、慢请求分析、缓存分析与诊断报告为一体。而中的Key分析功能即可满足大Key与热Key的发现需要。

CloudDBA中的大Key及热Key分析底层为阿里云Redis内核的Key分析功能，阿里云特有的Redis内核能够直接发现并输出大Key热Key的相关信息，因此，CloudDBA的分析结果准确高效且对性能几乎无任何影响，你可以通过点击“Key分析”进入该功能，如下图：

![image-20240530094448684](/assets/image-20240530094448684.Dvj6PpBX.png)

Key分析功能共有两个页面，它们允许在不同的时间维度对对应Redis实例中的Key进行分析：

实时：对当前实例立即开始分析当前实例，展示当前存在的所有大Key及热Key。

历史：展示该实例近期曾出现过的大Key及热Key，在历史页面中，所有出现过的大Key及热Key都会被记录，哪怕这些Key当前已经不存在，该功能能够很好的反映Redis的历史Key状态，帮助追溯过去或现场已遭破坏的问题。

![image-20240530094832113](/assets/image-20240530094832113.BgSVrpKY.png)

更详细的阿里云数据库Redis控制台中的大Key热Key分析功能可查看该文档：[https://help.aliyun.com/document\_detail/279446.html](https://help.aliyun.com/document_detail/279446.html?spm=a2c6h.12873639.article-detail.8.16125a65MSINRE)

## 五、大Key与热Key的处理

现在，我们已经通过多种手段找到了Redis中的问题Key，那么我们应当立即着手对他们进行处理，避免它们在之后的时间中引发问题。

### （一）大Key的常见处理办法

#### （1）对大Key进行拆分

如将一个含有数万成员的HASH Key拆分为多个HASH Key，并确保每个Key的成员数量在合理范围，在Redis Cluster结构中，大Key的拆分对node间的内存平衡能够起到显著作用。

#### （2）对大Key进行清理

将不适合Redis能力的数据存放至其它存储，并在Redis中删除此类数据。需要注意的是，我们已在上文提到一个过大的Key可能引发Redis集群同步的中断，Redis自4.0起提供了UNLINK命令，该命令能够以非阻塞的方式缓慢逐步的清理传入的Key，通过UNLINK，你可以安全的删除大Key甚至特大Key。

#### （3）时刻监控Redis的内存水位

突然出现的大Key问题会让我们措手不及，因此，在大Key产生问题前发现它并进行处理是保持服务稳定的重要手段。我们可以通过监控系统并设置合理的Redis内存报警阈值来提醒我们此时可能有大Key正在产生，如：Redis内存使用率超过70%，Redis内存1小时内增长率超过20%等。

通过此类监控手段我们可以在问题发生前解决问题，如：LIST的消费程序故障造成对应Key的列表数量持续增长，将告警转变为预警从而避免故障的发生。

#### （4）对失效数据进行定期清理

例如我们会在HASH结构中以增量的形式不断写入大量数据而忽略了这些数据的时效性，这些大量堆积的失效数据会造成大Key的产生，可以通过定时任务的方式对失效数据进行清理。在此类场景中，建议使用HSCAN并配合HDEL对失效数据进行清理，这种方式能够在不阻塞的前提下清理无效数据。

#### （5）使用阿里云的Tair(Redis企业版)服务避开失效数据的清理工作

如果你的HASH Key过多，同时存在大量的成员失效需要被清理的问题。由于大量Key与大量失效数据的叠加，在此类场景中定时任务已无法做到对无效数据进行及时的清理，阿里云的Tair服务能够很好的解决此类问题。

Tair是阿里云的Redis企业版，它在具备Redis所有特性（包括Redis的高性能特点）的同时提供了大量额外的高级功能。

TairHash是一种可为field设置过期时间和版本的hash类型数据结构，它不但和Redis Hash一样支持丰富的数据接口和高处理性能，还改变了hash只能为key设置过期时间的限制：TairHash允许为field设置过期时间和版本。这极大地提高了hash数据结构的灵活性，简化了很多场景下的业务开发工作。

TairHash使用高效的Active Expire算法，实现了在对响应时间几乎无影响的前提下，高效完成对field过期判断和删除的功能。此类高级功能的合理使用能够解放大量Redis的运维、故障处理工作并降低业务的代码复杂度，让运维将精力投入到其它更有价值的工作中，让研发有更多的时间来写更有价值的代码。

如果你对TairHash有兴趣，可查看对应文档：

https://help.aliyun.com/document\_detail/145970.html

### （二）热Key的常见处理办法

（1）在Redis Cluster结构中对热Key进行复制

在Redis Cluster中，热Key由于迁移粒度问题造成请求无法打散使单一node的压力无法下降。此时可以将对应热Key进行复制并迁移至其他node，例如为热Key foo复制出3个内容完全一样的Key并名为foo2，foo3，foo4，然后将这三个Key迁移到其他node来解决单一node的热Key压力。

该方案的缺点在于代码需要联动修改，同时，Key一变多带来了数据一致性挑战：由更新一个Key演变为需要同时更新多个Key，在很多时候，该方案仅建议用来临时解决当前的棘手问题。

#### （2）使用读写分离架构

如果热Key的产生来自于读请求，那么读写分离是一个很好的解决方案。在使用读写分离架构时可以通过不断的增加从节点来降低每个Redis实例中的读请求压力。

然而，读写分离架构在业务代码复杂度增加的同时，同样带来了Redis集群架构复杂度的增加：我们不仅要为多个从节点提供转发层（如Proxy，LVS等）来实现负载均衡，还要考虑从节点数量显著增加后带来的故障率增加的问题，Redis集群架构变更的同时为监控、运维、故障处理带来了更大的挑战。

但是，这一切在阿里云Redis服务中显得极为简单，阿里云Redis服务以开箱即用的方式提供服务。同时，在业务的发展发生变化时，阿里云的Redis服务允许用户通过变配的方式调整集群架构来轻松应对，如：主从转变为读写分离，读写分构转变为集群，主从转变为支持读写分离的集群，以及由社区版直接转变为支持大量高级特性的企业版Redis（Tair）。

如果你对阿里云Redis的在线变配功能感兴趣，可查看对应文档：[https://help.aliyun.com/document\_detail/26353.html](https://help.aliyun.com/document_detail/26353.html?spm=a2c6h.12873639.article-detail.10.16125a65MSINRE)

读写分离架构同样存在缺点，在请求量极大的场景下，读写分离架构会产生不可避免的延迟，此时会有读取到脏数据的问题，因此，在读写压力都较大写对数据一致性要求很高的场景下，读写分离架构并不合适。

#### （3）使用阿里云Tair的QueryCache特性

QueryCache是阿里云Tair（Redis企业版）服务的企业级特性之一，它的原理如下图：

![img](/assets/p272452.DFkZmWAj.png)

阿里云数据库Redis会根据高效的排序和统计算法识别出实例中存在的热点Key，开启该功能后，Proxy点会根据设定的规则缓存热点Key的请求和查询结果（仅缓存热点Key的查询结果，无需缓存整个Key），当在缓存有效时间内收到相同的请求时Proxy会直接返回结果至客户端，无需和后端的Redis分片执行交互。在提升读取速度的同时，降低了热点Key对数据分片的性能影响，避免发生请求倾斜。

至此，来自客户端的同样的请求无需再与Proxy后端的Redis进行交互而由Proxy直接返回数据，指向热Key的请求由一个Redis节点承担转为多个Proxy共同承担，能够大幅度降低Redis节点的热Key压力，同时Tair的QueryCache功能还提供了大量的命令来方便用户查看、管理，如通过querycache keys命令查看所有被缓存热Key，通过querycache listall获取所有已缓存的所有命令等。

Tair QueryCache智能化的热Key判定与缓存联动功同样能够降低运维及研发的工作负担，如果你对Tair的QueryCache感兴趣，可查看对应文档：[https://help.aliyun.com/document\_detail/216309.html](https://help.aliyun.com/document_detail/216309.html?spm=a2c6h.12873639.article-detail.11.16125a65MSINRE)

## 参考资料

https://developer.aliyun.com/article/788845

[大Key、热Key的发现与处理-分布式缓存服务Redis版-最佳实践 - 天翼云 (ctyun.cn)](https://www.ctyun.cn/document/10029420/10150059)

---

---
url: /常用框架/Caffeine/0_Caffeine基本概念.md
---

# Caffeine基本概念

Caffeine是基于JDK1.8版本的高性能本地缓存库，它是Guava的增强版，与ConcurrentLinkedHashMap相似，支持并发，并且可以在O(1)的时间复杂度内查找、写入元素。

## 参考资料

https://www.cnblogs.com/MorningBell/p/16659254.html

[(64条消息) Caffeine （史上最全）\_40岁资深老架构师尼恩的博客-CSDN博客](https://blog.csdn.net/crazymakercircle/article/details/113751575)

https://blog.csdn.net/CSDN\_WYL2016/article/details/128258565

https://blog.csdn.net/Listening\_Wind/article/details/110085228

多级缓存优化

[(64条消息) 项目理解（七）多级缓存优化性能\_多级缓存事务性\_lzw2019sun的博客-CSDN博客](https://blog.csdn.net/liuzewei2015/article/details/99706438)

[数据量很大，分页查询很慢，怎么优化？ - 简书 (jianshu.com)](https://www.jianshu.com/p/864d0bd80115)

---

---
url: /Java/解决方案/数据同步/1_Canal.md
---

# Canal

https://github.com/alibaba/canal/wiki

## Canal

> 官方文档：<https://github.com/alibaba/canal/wiki/QuickStart>
>
> 下载版本：[canal.deployer-1.1.8.tar.gz](https://github.com/alibaba/canal/releases/download/canal-1.1.8/canal.deployer-1.1.8.tar.gz)

### MySQL 开启 Binlog

MySQL 的 Binlog（二进制日志）记录了所有对数据库进行更改的操作，是实现主从复制和数据恢复的重要工具。以下是开启 Binlog 的步骤：

**1、修改配置文件**

打开 MySQL 配置文件（通常位于 `/etc/my.cnf` 或 `/etc/mysql/my.cnf`）。

在 *\[mysqld]* 部分添加以下内容

```
[mysqld]
log-bin=mysql-bin # 开启 binlog
binlog-format=ROW # 选择 ROW 模式
server_id=1 # 配置 MySQL replaction 需要定义，不要和 canal 的 slaveId 重复
```

**2、重启 MySQL 服务**

保存配置后，重启 MySQL 服务以使更改生效：

```
sudo service mysql restart
```

**3、验证 Binlog 是否开启**

登录 MySQL，执行以下命令检查 Binlog 状态：

```
SHOW VARIABLES LIKE 'log_bin';
```

如果返回值为 *ON*，表示 Binlog 已成功开启。

```
mysql> SHOW VARIABLES LIKE 'log_bin';
+---------------+-------+
| Variable_name | Value |
+---------------+-------+
| log_bin       | ON    |
+---------------+-------+
1 row in set (0.02 sec)
```

**常用命令**

查看所有 Binlog 文件：

```
SHOW MASTER LOGS;
```

查看当前 Binlog 状态：

```
SHOW MASTER STATUS;
```

手动刷新生成新日志文件：

```
FLUSH LOGS;
```

**注意事项**

1. **Binlog 格式选择**： STATEMENT：基于 SQL 语句，性能较高，但可能导致主从数据不一致。 ROW：基于行的复制，日志更大，但更可靠。 MIXED：结合两者优点，推荐使用。
2. **权限问题**：确保配置的日志路径有 MySQL 用户的写入权限。

## Canal Admin管理页面

> 官方文档：<https://github.com/alibaba/canal/wiki/Canal-Admin-QuickStart>
>
> 下载版本：[canal.admin-1.1.8.tar.gz](https://github.com/alibaba/canal/releases/download/canal-1.1.8/canal.admin-1.1.8.tar.gz)

报错

```
2025-07-29 15:53:06.904 [main] WARN  o.s.b.w.s.c.AnnotationConfigServletWebServerApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'requestMappingHandlerMapping' defined in class path resource [org/springframework/boot/autoconfigure/web/servlet/WebMvcAutoConfiguration$EnableWebMvcConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping]: Factory method 'requestMappingHandlerMapping' threw exception; nested exception is java.lang.IllegalArgumentException: canal.adminPasswd is empty , pls check https://github.com/alibaba/canal/issues/4941
```

解答

https://github.com/alibaba/canal/issues/4941

v1.1.8版本移除自带的password默认值，并在password未传入或非法时阻止节点启动来提醒用户设置自定义password

解决

登录mysql，执行如下密文生成sql即可(记得去掉第一个首字母的星号)

```
select password('admin')

+-------------------------------------------+
| password('admin')                         |
+-------------------------------------------+
| *4ACFE3202A5FF5CF467898FC58AAB1D615029441 |
+-------------------------------------------+

# 如果遇到mysql8.0，可以使用select upper(sha1(unhex(sha1('admin'))))
```

修改`conf/application.yml`中的`canal.adminPasswd`

https://blog.csdn.net/langfeiyes/article/details/130711899

https://blog.csdn.net/qq\_34497272/article/details/117658964

---

---
url: /Java/架构设计/分布式/06.分布式监控/CAT.md
---

# CAT

https://tech.meituan.com/2018/11/01/cat-in-depth-java-application-monitoring.html

https://cloud.tencent.com/developer/article/1642255

---

---
url: /Linux/Shell命令/3_Centos7扩容根目录.md
---

# Centos7扩容根目录（/dev/mapper/centos-root）

## 方式一：命令行

### 1、查看分区状况

根目录在/dev/mapper/centos-root

```sh
[root@git50 ~]# df -h
文件系统                 容量  已用  可用 已用% 挂载点
/dev/mapper/centos-root   17G  9.9G  7.2G   59% /
devtmpfs                 1.9G     0  1.9G    0% /dev
tmpfs                    1.9G   12K  1.9G    1% /dev/shm
tmpfs                    1.9G   15M  1.9G    1% /run
tmpfs                    1.9G     0  1.9G    0% /sys/fs/cgroup
/dev/sda1               1014M  170M  845M   17% /boot
tmpfs                    378M  4.0K  378M    1% /run/user/42
tmpfs                    378M   36K  378M    1% /run/user/0
[root@git50 ~]# lsblk
NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda               8:0    0  200G  0 disk 
├─sda1            8:1    0    1G  0 part /boot
└─sda2            8:2    0   19G  0 part 
  ├─centos-root 253:0    0   17G  0 lvm  /
  └─centos-swap 253:1    0    2G  0 lvm  [SWAP]
sr0              11:0    1 1024M  0 rom 
```

### 2、新建分区，并将id改为8e

```sh
[root@git50 ~]# fdisk /dev/sda
欢迎使用 fdisk (util-linux 2.23.2)。

更改将停留在内存中，直到您决定将更改写入磁盘。
使用写入命令前请三思。


命令(输入 m 获取帮助)：n
Partition type:
   p   primary (2 primary, 0 extended, 2 free)
   e   extended
Select (default p): p
分区号 (3,4，默认 3)：
起始 扇区 (41943040-419430399，默认为 41943040)：
将使用默认值 41943040
Last 扇区, +扇区 or +size{K,M,G} (41943040-419430399，默认为 419430399)：
将使用默认值 419430399
分区 3 已设置为 Linux 类型，大小设为 180 GiB

命令(输入 m 获取帮助)：p

磁盘 /dev/sda：214.7 GB, 214748364800 字节，419430400 个扇区
Units = 扇区 of 1 * 512 = 512 bytes
扇区大小(逻辑/物理)：512 字节 / 512 字节
I/O 大小(最小/最佳)：512 字节 / 512 字节
磁盘标签类型：dos
磁盘标识符：0x000b4e82

   设备 Boot      Start         End      Blocks   Id  System
/dev/sda1   *        2048     2099199     1048576   83  Linux
/dev/sda2         2099200    41943039    19921920   8e  Linux LVM
/dev/sda3        41943040   419430399   188743680   83  Linux

命令(输入 m 获取帮助)：t
分区号 (1-3，默认 3)：
Hex 代码(输入 L 列出所有代码)：l

 0  空              24  NEC DOS         81  Minix / 旧 Linu bf  Solaris        
 1  FAT12           27  隐藏的 NTFS Win 82  Linux 交换 / So c1  DRDOS/sec (FAT-
 2  XENIX root      39  Plan 9          83  Linux           c4  DRDOS/sec (FAT-
 3  XENIX usr       3c  PartitionMagic  84  OS/2 隐藏的 C:  c6  DRDOS/sec (FAT-
 4  FAT16 <32M      40  Venix 80286     85  Linux 扩展      c7  Syrinx         
 5  扩展            41  PPC PReP Boot   86  NTFS 卷集       da  非文件系统数据 
 6  FAT16           42  SFS             87  NTFS 卷集       db  CP/M / CTOS / .
 7  HPFS/NTFS/exFAT 4d  QNX4.x          88  Linux 纯文本    de  Dell 工具      
 8  AIX             4e  QNX4.x 第2部分  8e  Linux LVM       df  BootIt         
 9  AIX 可启动      4f  QNX4.x 第3部分  93  Amoeba          e1  DOS 访问       
 a  OS/2 启动管理器 50  OnTrack DM      94  Amoeba BBT      e3  DOS R/O        
 b  W95 FAT32       51  OnTrack DM6 Aux 9f  BSD/OS          e4  SpeedStor      
 c  W95 FAT32 (LBA) 52  CP/M            a0  IBM Thinkpad 休 eb  BeOS fs        
 e  W95 FAT16 (LBA) 53  OnTrack DM6 Aux a5  FreeBSD         ee  GPT            
 f  W95 扩展 (LBA)  54  OnTrackDM6      a6  OpenBSD         ef  EFI (FAT-12/16/
10  OPUS            55  EZ-Drive        a7  NeXTSTEP        f0  Linux/PA-RISC  
11  隐藏的 FAT12    56  Golden Bow      a8  Darwin UFS      f1  SpeedStor      
12  Compaq 诊断     5c  Priam Edisk     a9  NetBSD          f4  SpeedStor      
14  隐藏的 FAT16 <3 61  SpeedStor       ab  Darwin 启动     f2  DOS 次要       
16  隐藏的 FAT16    63  GNU HURD or Sys af  HFS / HFS+      fb  VMware VMFS    
17  隐藏的 HPFS/NTF 64  Novell Netware  b7  BSDI fs         fc  VMware VMKCORE 
18  AST 智能睡眠    65  Novell Netware  b8  BSDI swap       fd  Linux raid 自动
1b  隐藏的 W95 FAT3 70  DiskSecure 多启 bb  Boot Wizard 隐  fe  LANstep        
1c  隐藏的 W95 FAT3 75  PC/IX           be  Solaris 启动    ff  BBT            
1e  隐藏的 W95 FAT1 80  旧 Minix       
Hex 代码(输入 L 列出所有代码)：8e
已将分区“Linux”的类型更改为“Linux LVM”

命令(输入 m 获取帮助)：w
The partition table has been altered!

Calling ioctl() to re-read partition table.

WARNING: Re-reading the partition table failed with error 16: 设备或资源忙.
The kernel still uses the old table. The new table will be used at
the next reboot or after you run partprobe(8) or kpartx(8)
正在同步磁盘。
```

### 3、刷新并查看sda3是否存在

```sh
[root@git50 ~]# partprobe
[root@git50 ~]# lsblk
NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda               8:0    0  200G  0 disk 
├─sda1            8:1    0    1G  0 part /boot
├─sda2            8:2    0   19G  0 part 
│ ├─centos-root 253:0    0   17G  0 lvm  /
│ └─centos-swap 253:1    0    2G  0 lvm  [SWAP]
└─sda3            8:3    0  180G  0 part 
sr0              11:0    1 1024M  0 rom
```

### 4、使用lvm命令新建卷/dev/sda3,并将其加载到卷组centos中

```sh
[root@git50 ~]# lvm
lvm> pvcreate /dev/sda3
  Physical volume "/dev/sda3" successfully created.
lvm> pvdisplay
  --- Physical volume ---
  PV Name               /dev/sda2
  VG Name               centos
  PV Size               <19.00 GiB / not usable 3.00 MiB
  Allocatable           yes (but full)
  PE Size               4.00 MiB
  Total PE              4863
  Free PE               0
  Allocated PE          4863
  PV UUID               mxA5P7-4vL0-0cOO-0PPy-1Uq3-HRdn-DHRmNV
   
  "/dev/sda3" is a new physical volume of "180.00 GiB"
  --- NEW Physical volume ---
  PV Name               /dev/sda3
  VG Name               
  PV Size               180.00 GiB
  Allocatable           NO
  PE Size               0   
  Total PE              0
  Free PE               0
  Allocated PE          0
  PV UUID               wor8da-PkXI-ghCq-U8sE-vkfq-ZhJo-di78al
   
lvm> vgdisplay
  --- Volume group ---
  VG Name               centos
  System ID             
  Format                lvm2
  Metadata Areas        1
  Metadata Sequence No  3
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                2
  Open LV               2
  Max PV                0
  Cur PV                1
  Act PV                1
  VG Size               <19.00 GiB
  PE Size               4.00 MiB
  Total PE              4863
  Alloc PE / Size       4863 / <19.00 GiB
  Free  PE / Size       0 / 0   
  VG UUID               cDss9h-G3Tk-zTb1-4vsa-lKcs-DX8B-8fDpmv
   
lvm> vgextend centos /dev/sda3
  Volume group "centos" successfully extended
lvm> vgdisplay
  --- Volume group ---
  VG Name               centos
  System ID             
  Format                lvm2
  Metadata Areas        2
  Metadata Sequence No  4
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                2
  Open LV               2
  Max PV                0
  Cur PV                2
  Act PV                2
  VG Size               198.99 GiB
  PE Size               4.00 MiB
  Total PE              50942
  Alloc PE / Size       4863 / <19.00 GiB
  Free  PE / Size       46079 / <180.00 GiB
  VG UUID               cDss9h-G3Tk-zTb1-4vsa-lKcs-DX8B-8fDpmv
   
lvm> lvextend -l +100%FREE /dev/centos/root
  Size of logical volume centos/root changed from <17.00 GiB (4351 extents) to 196.99 GiB (50430 extents).
  Logical volume centos/root successfully resized.
lvm> exit
```

（这里的/dev/centos/root不能随便改成centos-root。LVM逻辑卷管理，根文件系统建立在卷组（VG）centos上的逻辑卷（LV）上，逻辑卷名是root而不是直接建在硬盘分区上）

### 5、之前只是对逻辑卷扩容，还要同步到文件系统，实现对根目录的扩容

```sh
[root@git50 ~]# xfs_growfs /dev/centos/root
meta-data=/dev/mapper/centos-root isize=512    agcount=4, agsize=1113856 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=0 spinodes=0
data     =                       bsize=4096   blocks=4455424, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
log      =internal               bsize=4096   blocks=2560, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0
data blocks changed from 4455424 to 51640320
[root@git50 ~]# df -h
文件系统                 容量  已用  可用 已用% 挂载点
/dev/mapper/centos-root  197G  9.9G  188G    6% /
devtmpfs                 1.9G     0  1.9G    0% /dev
tmpfs                    1.9G   12K  1.9G    1% /dev/shm
tmpfs                    1.9G   15M  1.9G    1% /run
tmpfs                    1.9G     0  1.9G    0% /sys/fs/cgroup
/dev/sda1               1014M  170M  845M   17% /boot
tmpfs                    378M  4.0K  378M    1% /run/user/42
tmpfs                    378M   40K  378M    1% /run/user/0
```

### 参考资料

https://blog.csdn.net/yanchenyu365/article/details/131453446

https://blog.csdn.net/weixin\_40436144/article/details/86235432

https://www.cnblogs.com/leiblog/p/13560070.html

https://zhuanlan.zhihu.com/p/450057653

## 方式二：gparted图形界面

参照如下资料步骤

https://www.cnblogs.com/liulianzhen99/p/17467752.html

Gparted的iso镜像，官网地址：https://sourceforge.net/projects/gparted/files/gparted-live-stable/

下载镜像首次进入时，需要设置CD为第一启动项，BOOT选项卡中需要按（-/+）排序，https://www.ezd.cc/zs/380532.html

下面示例图为具体分区操作

![Snipaste\_2025-06-13\_00-21-14](/assets/Snipaste_2025-06-13_00-21-14.DIYh40SR.png)

![Snipaste\_2025-06-13\_00-23-16](/assets/Snipaste_2025-06-13_00-23-16.BiaI9ii4.png)

![Snipaste\_2025-06-13\_00-23-53](/assets/Snipaste_2025-06-13_00-23-53.CXUX1jTc.png)

---

---
url: /Linux/OpenEuler/3_Cockpit服务器管理工具.md
---

# Cockpit服务器管理工具

## 一、简介

一登录 centos8, 就提示信息

```
Authorized users only. All activities may be monitored and reported.
Activate the web console with: systemctl enable --now cockpit.socket
```

这个是什么意思呢? 其实这个就是想让你开启cockpit服务。这条命令就是要把cockpit设置为开机自启动并且立刻运行起来，Cockpit 是红帽开发的网页版图像化服务管理工具，官方是说“Cockpit 是一个交互式 Linux 服务器管理接口”，说白了就是给你提供个web页面来管理cneots8服务器，听说这个服务还是非常好用的，后面会更新看下这个服务的具体使用。

参考资料：https://zhuanlan.zhihu.com/p/113270502

界面如下：

![image-20250615211823067](/assets/image-20250615211823067.BcB6gFDW.png)

## 二、Cockpit更换端口

修改 Cockpit 端口

**编辑配置文件**： 打开 Cockpit 的配置文件 *cockpit.socket*。

```sh
sudo nano /etc/systemd/system/sockets.target.wants/cockpit.socket
```

**修改端口**： 找到 *\[Socket]* 部分，并将 *ListenStream* 的值修改为所需的端口号，例如 9091。

```sh
 [Socket] ListenStream=9091
```

完整配置如下

```sh
[Unit]
Description=Cockpit Web Service Socket
Documentation=man:cockpit-ws(8)
Wants=cockpit-motd.service

[Socket]
ListenStream=9091
ExecStartPost=-/usr/share/cockpit/motd/update-motd '' localhost
ExecStartPost=-/bin/ln -snf active.motd /run/cockpit/motd
ExecStopPost=-/bin/ln -snf inactive.motd /run/cockpit/motd

[Install]
WantedBy=sockets.target
```

**重新加载 systemd 配置**： 保存并关闭文件后，重新加载 systemd 配置以应用更改。

```sh
sudo systemctl daemon-reload
```

**重启 Cockpit 服务**： 最后，重启 Cockpit 服务以使更改生效。

```sh
sudo systemctl restart cockpit.socket
```

注意事项

**防火墙配置**：确保新的端口在防火墙中已开放，以便外部访问。

**SELinux 配置**：如果启用了 SELinux，可能需要更新 SELinux 策略以允许新的端口。

## 三、更换端口不生效

修改完端口后启动报错

查看日志

```sh
journalctl -u cockpit.socket --since "5 minutes ago"
```

内容如下

```sh
6月 15 17:11:27 192.168.0.105 systemd[1]: cockpit.socket: Failed to create listening socket ([::]:9091): Permission>
6月 15 17:11:27 192.168.0.105 systemd[1]: cockpit.socket: Failed to listen on sockets: Permission denied
6月 15 17:11:27 192.168.0.105 systemd[1]: Starting Cockpit Web Service Socket...
6月 15 17:11:27 192.168.0.105 systemd[1]: cockpit.socket: Failed with result 'resources'.
6月 15 17:11:27 192.168.0.105 systemd[1]: Failed to listen on Cockpit Web Service Socket.
```

关键信息如下：

* **`Permission denied`**：表示 systemd 无法绑定到你指定的端口（如 `9091`）。
* **通常是 SELinux、firewalld 或系统权限限制** 导致。

### 🔒 原因一：SELinux 阻止了绑定低编号端口（<1024）

虽然你使用的是 `9091`（>1024），但有时 SELinux 仍然会阻止非标准服务监听某些端口。

#### 🧪 检查 SELinux 状态：

bash深色版本

```
sestatus
```

如果输出是 `enabled`，说明 SELinux 是开启的。

#### 🛠 解决方法：

你可以临时禁用 SELinux 测试是否与此有关：

bash深色版本

```
setenforce 0
```

然后尝试重启服务：

bash深色版本

```
systemctl daemon-reload
systemctl restart cockpit.socket
```

✅ 如果成功了，说明确实是 SELinux 的问题。

#### 永久解决（推荐）：

创建一个 SELinux 策略允许 `cockpit` 监听新端口，比如 `9091`：

```
# 安装 semanage 工具（如果没有）
dnf install -y policycoreutils-python-utils

# 添加端口到允许列表
semanage port -a -t http_port_t -p tcp 9091

# 再次尝试启动
systemctl restart cockpit.socket
```

### 🧱 原因二：防火墙或 AppArmor（少见）

如果你修改了端口，但没有开放对应防火墙规则，也可能导致失败。

不过这个错误是 `Permission denied`，更可能是 SELinux 或内核权限问题。

### 🗃️ 原因三：系统限制（Capabilities）

Cockpit 使用 systemd socket 激活方式运行，默认不需要 root 权限，但如果绑定某些特殊端口或功能受限，也可能会失败。

检查 `/usr/lib/systemd/system/cockpit.socket` 中的 `[Socket]` 配置是否异常。

修复完整流程

```sh
# 查看 SELinux 状态
sestatus

# 安装 semanage（如未安装）
dnf install -y policycoreutils-python-utils

# 允许 9091 端口
semanage port -a -t http_port_t -p tcp 9091

# 修改 cockpit.socket 文件中的 ListenStream=9091
vim /usr/lib/systemd/system/cockpit.socket

# 重新加载并启动
systemctl daemon-reload
systemctl restart cockpit.socket

# 查看状态
systemctl status cockpit.socket

# 开放防火墙
firewall-cmd --permanent --add-port=9091/tcp
firewall-cmd --reload
```

---

---
url: /StableDiffusion/ComfyUI/1_ComfyUI安装.md
---

# ComfyUI安装

## Windows（N卡）安装流程

如果使用的是Windows平台且为N卡用户，只需要在项目主页的Release页面下载作者提供的“官方整合包”，即可一键开启ComfyUI探索之路：

> 下载地址：https://github.com/comfyanonymous/ComfyUI/releases

![image-20240221005044547](/assets/image-20240221005044547.CX302JU4.png)

进入后，点击“**[Download Link with stable pytorch 2.1 cu121](https://github.com/comfyanonymous/ComfyUI/releases/download/latest/ComfyUI_windows_portable_nvidia_cu121_or_cpu.7z)**”即可下载。

下载完毕后，解压文件夹至任意空白路径。双击“run\_nvdidia\_gpu.bat”，即可开启ComfyUI。

![image-20240225230355012](/assets/image-20240225230355012.C8eGOZn0.png)

启动后的页面

![image-20240225231610282](/assets/image-20240225231610282.Cd07n4Y1.png)

## 其他平台安装流程

**摘录自官方项目页面，待测试**

更详细的安装指引，请参考项目主页的安装部分说明：https://github.com/comfyanonymous/ComfyUI?tab=readme-ov-file#installing

### **手动安装**

1. Git clone 此存储库。

```JSON
git clone https://github.com/comfyanonymous/ComfyUI.git
```

1. 把您的 SD 大模型（Checkpoint，ckpt/safetensors 文件）放在 models/checkpoints 文件夹中；
2. 将您的 VAE 放入 models/vae 文件夹中；

然后按照下面的指引，安装不同平台所需的依赖项：

#### **AMD GPU（Linux）**

AMD users can install rocm and pytorch with pip if you don't have it already installed, this is the command to install the stable version:

如果您还没有安装 rocm 和 pytorch，AMD 用户可以使用 pip 安装 rocm 和 pytorch，这是安装稳定版本的命令：

`pip install torch torchvision torchaudio --index-url ``https://download.pytorch.org/whl/rocm5.7`

Install the dependencies by opening your terminal inside the ComfyUI folder and:

然后，在 ComfyUI 文件夹中打开终端，然后通过下面的命令来安装依赖项：

`pip install -r requirements.txt`

After this you should have everything installed and can proceed to running ComfyUI.

在此之后，您应该已经安装了所有内容，并可以继续运行 ComfyUI。

#### **NVIDIA GPU**

Nvidia users should install stable pytorch using this command:

Nvidia 用户应使用以下命令安装稳定版本的 pytorch：

`pip install torch torchvision torchaudio --extra-index-url ``https://download.pytorch.org/whl/cu121`

If you get the "Torch not compiled with CUDA enabled" error, uninstall torch with:

如果您收到“Torch 未在启用 CUDA 的情况下编译”错误，请使用以下命令卸载 torch：

```
pip uninstall torch
```

And install it again with the command above.

然后使用上面的命令重新安装它。

Install the dependencies by opening your terminal inside the ComfyUI folder and:

然后，在 ComfyUI 文件夹中打开终端，然后通过下面的命令来安装依赖项：

`pip install -r requirements.txt`

After this you should have everything installed and can proceed to running ComfyUI.

在此之后，您应该已经安装了所有内容，并可以继续运行 ComfyUI。

#### Intel GPU

请参考如下链接：**[Intel Arc 英特尔锐炫](https://github.com/comfyanonymous/ComfyUI/discussions/476)**

#### **Apple Mac 芯片**

You can install ComfyUI in Apple Mac silicon (M1 or M2) with any recent macOS version.

您可以在 Apple Mac 芯片（M1 或 M2）中安装任何最新 macOS 版本的 ComfyUI。

1. Install pytorch nightly. For instructions, read the [Accelerated PyTorch training on Mac](https://developer.apple.com/metal/pytorch/) Apple Developer guide (make sure to install the latest pytorch nightly). 安装 pytorch nightly。有关说明，请阅读 Mac 上的加速 PyTorch 培训 Apple 开发人员指南（确保安装最新的 pytorch nightly）。
2. Follow the [ComfyUI manual installation](https://github.com/comfyanonymous/ComfyUI#manual-install-windows-linux) instructions for Windows and Linux. 按照适用于 Windows 和 Linux 的 ComfyUI 手动安装说明进行操作。
3. Install the ComfyUI [dependencies](https://github.com/comfyanonymous/ComfyUI#dependencies). If you have another Stable Diffusion UI [you might be able to reuse the dependencies](https://github.com/comfyanonymous/ComfyUI#i-already-have-another-ui-for-stable-diffusion-installed-do-i-really-have-to-install-all-of-these-dependencies). 安装 ComfyUI 依赖项。如果您有其他 Stable Diffusion UI，则可以重用依赖项。
4. Launch ComfyUI by running `python main.py --force-fp16`. Note that --force-fp16 will only work if you installed the latest pytorch nightly. 通过运行 `python main.py --force-fp16` 启动 ComfyUI。请注意，--force-fp16 只有在安装最新的 pytorch nightly时才有效。

> **Note**: Remember to add your models, VAE, LoRAs etc. to the corresponding Comfy folders, as discussed in [ComfyUI manual installation](https://github.com/comfyanonymous/ComfyUI#manual-install-windows-linux). 注意：请记住将您的模型、VAE、LoRA 等添加到相应的 Comfy 文件夹中，如 ComfyUI 手动安装中所述。

#### **DirectML（Windows 上的 AMD 卡）**

`pip install torch-directml`

Then you can launch ComfyUI with: `python main.py --directml` `pip install torch-directml`

然后，您可以使用以下命令启动ComfyUI： `python main.py --directml`

---

---
url: /StableDiffusion/ComfyUI/3_ComfyUI核心采样器.md
---

# ComfyUI核心采样器

在WebUI里，所看到的“采样方法”是一个单一的参数选项，但在Comfy UI 的核心采样器 KSampler 里，它会被“拆分”成两个单独的选项：**Sampler Name（采样器名称）** 与 **Scheduler （调度器）**

SD WebUI 中的采样方法

![image-20240226002008261](/assets/image-20240226002008261.BrzQASOp.png)

ComfyUI KSampler 中的相关选项

![image-20240226001903135](/assets/image-20240226001903135.DSsstbet.png)

回溯Stable Diffusion生成一张图片的“去噪过程”（参考Nenly同学的[模型训练入门课里的相关内容](https://www.bilibili.com/video/BV1TK411v7Jw?t=285.4)）：SD会经由随机种子生成一张随机噪声图，然后利用训练好的”噪声预测器“（U-Net），结合输入的提示词等“条件”（Conditioning），进行“条件去噪”，在这张噪声图上不断添加一些形象，使之成为一张生成的新图片。

![image-20240226001903135](/assets/c473f5b6-31f3-4f86-b440-e916fd30cbd8.BPv8Q0n0.png)

而广义的**采样方法（Sampler）**，即代表在控制这个“去噪”过程的一种算法。简单地去理解，不同的算法会给你带来不同的采样结果，而不同算法对于采样步数的要求也可能会有些许差异。

![image-20240226001903135](/assets/0A88670A-8000-D4DD-ED5E-2868C733CAB7.boITirVY.png)

![image-20240226001903135](/assets/376DC355-7A0E-2623-D88C-7D7CC05570CA.B6MNOXHc.png)

如果想更深入地挖掘这里面的技术原理，可以在[Reddit上的这一篇回答里](https://www.reddit.com/r/StableDiffusion/comments/zgu6wd/comment/izkhkxc/?utm_source=share\&utm_medium=web2x\&context=3)，找到由 **@ManBearScientist** 撰写的这一篇回答，非常专业地列举了所有采样方法的来源与含义，以及它们是如何一步步积累取得今天的研究成果的。

![image-20240226003008371](/assets/051DD782-C690-20EB-1AEA-047D2B48F6B3.tA_5SuOO.png)

那**调度器（Scheduler）** 呢？你可以把它看做是采样方法的一部分，主要用于控制采样过程中的时间步长。

大部分UI都会习惯将采样方法本身与调度器选项“合并”起来作为一种采样方法呈现给用户，但根据ComfyUI作者 **@ComfyAnonymous** 在[这一个Issue中的答复](https://github.com/comfyanonymous/ComfyUI/discussions/227)：我决定把它作为一个单独的选项，因为它对我来说更有意义。

![image-20240226003008371](/assets/image-20240226003008371.CY2sCcPT.png)

> 这些是调度程序。它们定义采样器采样点的时间步长/西格玛。我决定将它们作为一个单独的选项，与其他 uis 不同，因为它对我来说更有意义。
>
> 在卡拉斯中，采样器花费的时间比正常时间更小的时间步长/西格玛。如果您想要与其他 uis 相同的行为，karras 和 normal 是您应该用于大多数采样器的行为。
>
> Simple 只是我添加的一个，因为我尝试编写[最简单的调度程序](https://github.com/comfyanonymous/ComfyUI/blob/master/comfy/samplers.py#L233)，以便好玩，它实际上在某些情况下运行良好，例如 hiresfix 的第二遍，所以我包含它并将其命名为 simple。
>
> ddim\_uniform 是应该与 ddim 一起使用的那个，如果您希望它的行为与参考 Stable Diffusion 实现中的 ddim 采样器完全相同。

根据他的建议，如果希望各种采样器能与其他UI中保持大致一样的行为，只需要使用**Karras**或**Normal**即可。

---

---
url: /StableDiffusion/ComfyUI/0_ComfyUI简介.md
---

# ComfyUI简介

ComfyUI是一种新型的Stable Diffusion用户界面，因其独树一帜的“节点式”界面，逐渐成为了AI绘画领域进阶玩家的得力武器。搭配各式各样的自定义节点与功能强大的工作流，它得以用更低的配置实现许多在WebUI等常规界面里无法做到的复杂生成任务，并为基于Stable Diffusion搭建各类AIGC应用提供了便利。

> 项目主页：https://github.com/comfyanonymous/ComfyUI

其开发者[@ComfyAnonymous](https://github.com/comfyanonymous/) 在主页提到了制作这个项目的初衷：

> 我想更详细地了解 Stable Diffusion 是如何工作的，并且想要一些干净而强大的东西，让我可以不受限制地尝试Stable Diffusion。

![image-20240225230310894](/assets/image-20240225230310894.C4RFDc7r.png)

## 参考资料

\[1]. Comfy UI入门教程-Nenly同学：https://www.bilibili.com/video/BV1D7421N7xN

---

---
url: /StableDiffusion/ComfyUI/2_ComfyUI配置.md
---

# ComfyUI配置

## 模型路径互通配置

在启动ComfyUI前，如果你有存储于Automatic1111 WebUI项目中的模型文件，则无需额外搬运至ComfyUI的文件夹内，只需通过如下步骤简单配置即可实现模型文件的“互通”：

1. 拷贝WebUI根目录的路径（即包含webui\_user.bat文件的路径）：

   ![image-20240225235820334](/assets/image-20240225235820334.DWyEO1g5.png)

2. 用记事本打开ComfyUI根目录下的extra\_model\_path.yaml.example文件，将路径粘贴至如下位置：

   ![image-20240225235908325](/assets/image-20240225235908325.DySrcBM1.png)

   ![image-20240226000018181](/assets/image-20240226000018181.CntxQEst.png)

   ![image-20240226000055749](/assets/image-20240226000055749.DNHnJB1T.png)

3. 保存文件，并将文件重命名，去除.example的后缀，使文件名变为如下图所示：

4. 正常启动ComfyUI。

## 提高生成速度

Make sure you use the regular loaders/Load Checkpoint node to load checkpoints. It will auto pick the right settings depending on your GPU.

请确保使用常规加载程序/加载检查点节点来加载检查点。它会根据您的 GPU 自动选择正确的设置。

You can set this command line setting to disable the upcasting to fp32 in some cross attention operations which will increase your speed. Note that this will very likely give you black images on SD2.x models. If you use xformers or pytorch attention this option does not do anything.

您可以设置此命令行设置，以在某些交叉注意力操作中禁用向上转换到 fp32，这将提高您的速度。请注意，这很可能会在 SD2.x 型号上为您提供黑色图像。如果使用 xformers 或 pytorch attention，则此选项不会执行任何操作。

```
--dont-upcast-attention
```

## 显示高质量的预览

Use to enable previews.`--preview-method auto`

用于启用预览。`--preview-method auto`

The default installation includes a fast latent preview method that's low-resolution. To enable higher-quality previews with [TAESD](https://github.com/madebyollin/taesd), download the [taesd\_decoder.pth](https://github.com/madebyollin/taesd/raw/main/taesd_decoder.pth) (for SD1.x and SD2.x) and [taesdxl\_decoder.pth](https://github.com/madebyollin/taesd/raw/main/taesdxl_decoder.pth) (for SDXL) models and place them in the `models/vae_approx` folder. Once they're installed, restart ComfyUI to enable high-quality previews.

默认安装包括低分辨率的快速潜伏预览方法。要使用 [TAESD](https://github.com/madebyollin/taesd) 启用更高质量的预览，请下载 taesd\_decoder.pth（适用于 SD1.x 和 SD2.x）和 [taesdxl\_decoder.pth](https://github.com/madebyollin/taesd/raw/main/taesd_decoder.pth)（适用于 SDXL）模型，并将其放在文件夹中。安装完成后，重新启动 ComfyUI 以启用高质量的预览。`models/vae_approx`

## 整合包下载

### 铁锅炖

由@[只剩一瓶辣椒酱](https://space.bilibili.com/35723238) 老师制作的 ComfyUI**铁锅炖启动器及整合包**

视频地址：[国内首个COMFYUI启动器铁锅炖来辣！支持COMFYUI更新/配置/一键优化/中英切换等](https://www.bilibili.com/video/BV1mz4y1K7Jw/)

启动器手册：https://shimo.im/docs/B1AwdxZewWhjwe3m

无限圣杯手册(含满血整合包)：https://shimo.im/docs/Ee32m0w80rfLp4A2

AIGODLIKE社区(支持COMFYUI图像解析)：https://www.aigodlike.com

---

---
url: /Java/容器/Docker/4_Compose脚本.md
---

# Compose脚本

通过`docker-compose`编排一系列环境进行一键快速部署运行，小白运维神器。

### 一、环境准备

```
# 安装git命令： yum install -y git
git clone https://gitee.com/xu_xiaolong/docker-compose.git
cd docker-compose/compose
```

### 二、运行服务

> 环境部署见每个服务下的`run.md`； eg: `docker-compose/compose/portainer/run.md`

---

---
url: /daily/博客文档/VitPress/1_Config配置.md
---

# Config配置

## 站点配置

### 元数据

包含了 `lang`、 `title` 、`description` 信息

```js
import {defineConfig} from 'vitepress'

export default defineConfig({
    lang: 'zh-CN', //语言，可选 en-US
    title: "VitePress", //站点名
    description: "我的vitpress文档教程",  //站点描述
})
```

### 网页标题

使用 `titleTemplate` 自定义整个网页标题，一般不使用，自定义会直接写死

> \[!IMPORTANT]
> 说明
>
> 网页标题随着每个页面的 \`\` 标题而变动，
>
> 如 标题是 `# 页面` ，那么显示的就是 `页面 | VitePress`

```js
export default defineConfig({
    lang: 'zh-CN',
    title: "VitePress",
    description: "我的vitpress文档教程", // 我的文字有下划线，请后期再查看 `组件 - 首页文字下划线`
    titleTemplate: '另起标题会覆盖title', // [!code focus]
    // titleTemplate: ':title - 快速上手', // 完全自定义标题，:title 不要动，改后面的
    // titleTemplate: false, // 关闭标题
})
```

### Fav浏览器标签图标

路径默认`public`目录，在 `docs`目录下新建 `public`目录即可

```js
export default defineConfig({

    //fav图标
    head: [
        ['link', {rel: 'icon', href: '/logo.png'}],
    ],

})
```

### 深色主题

默认是浅色模式，可自行开启或更换

```js
export default defineConfig({

    // appearance: true, // 默认浅色且开启切换
    // 启用深色模式
    appearance: 'dark',
    // appearance: false, // 关闭
    // appearance: "force-dark", // 强制深色主题

})
```

## config配置

### 首页布局

首页进入博客会加载docs/index.md，VitePress默认主题提供了一个主页布局。

```markdown
---
# https://vitepress.dev/reference/default-theme-home-page
layout: home

hero:
  name: "My Awesome Project"
  text: "A VitePress Site"
  tagline: My great project tagline
  actions:
    - theme: brand
      text: Markdown Examples
      link: /markdown-examples
    - theme: alt
      text: API Examples
      link: /api-examples

features:
  - title: Feature A
    details: Lorem ipsum dolor sit amet, consectetur adipiscing elit
  - title: Feature B
    details: Lorem ipsum dolor sit amet, consectetur adipiscing elit
  - title: Feature C
    details: Lorem ipsum dolor sit amet, consectetur adipiscing elit
---
```

`link`说明：

1、如果要跳转项目内的文章，则直接在`link`中写入文件路径，根目录为`docs`文件夹

2、如果要跳转外部链接，则直接在`link`中写入外部链接

### 顶部按钮的跳转栏

顶部按钮在`.viewpress/config.mts`文件中配置，在`config.mts`文件中`nav`则是顶部按钮的配置，例如点击`Examples`跳转，点击配置跳配置文档，此时就可以直接修改顶部按钮的`link`配置，通过路径直接指向对应的文件即可。

```js
themeConfig: {
    // https://vitepress.dev/reference/default-theme-config
    nav: [
        {text: 'Home', link: '/'},
        {text: 'Examples', link: '/markdown-examples'},
    ],
    sidebar: [
        {
            text: 'Examples',
            items: [
                {text: 'Markdown Examples', link: '/markdown-examples'},
                {text: 'Runtime API Examples', link: '/api-examples'}
            ]
        }
    ],
    socialLinks: [
        {icon: 'github', link: 'https://github.com/vuejs/vitepress'}
    ]
}
```

通常一个大型的文档，顶部的按钮会非常多，如果全部写在`config.mts`文件中，随着积累该文件会变得非常臃肿，可以把该文件`nav`配置抽离出一个单独的文件，然后引入到`config.mts`中。

在`.viewpress`中新建`nav.mts`文件，将`nav`的配置写在`nav.mts`文件中，然后导出。

```
```

### 侧边文章的跳转

在`vitepress`中，侧边文章对应的是`.viewpress/config.mts`文件中的`sidebar`字段

`sidebar`中，每一个对象，对应一个`side`，`sidebar`中可以有多个对象，你可以将`sidebar`中的对象想想成一本书，每个对象对应一本书，text对应书名，items是一个数组，对应书内的章节，章节的link就对应的文章路径。

```
```

---

---
url: /常用框架/SpringAIAlibaba/Agent Framework/8_Context Engineering上下文工程.md
---

# Context Engineering 上下文工程

---

---
url: /StableDiffusion/StableDiffusion/3_ControlNet控制网.md
---

# ControlNet控制网

> ControlNet扩展地址：<https://github.com/Mikubill/sd-webui-controlnet>

## ControlNet原理解析

以特定信息进行引导，实现一些我们通过文生图、图生图不好精确控制的特征。

精确控制的意义？

* 如果只能靠“抽卡”产出需要的内容，生成是高度不可控的。

* 面对具体需求，只有“可控”才能成为“生产力”。

基本结构：预处理器 -> 模型

* 预处理器可以从图片里提取特征信息。

* 训练过的ControlNet模型读取这些信息，并引导Stable Diffusion 生成过程。

## ControlNet基本应用

### 安装方式

**1.1版本模型地址：**

（下载地址）<https://huggingface.co/lllyasviel/ControlNet-v1-1/tree/main>

（模型介绍）<https://github.com/lllyasviel/ControlNet-v1-1-nightly>

**1.14版本后更新的新模型（包括社区模型）与XL模型的地址：**

（下载地址）<https://huggingface.co/lllyasviel/sd_control_collection/tree/main>

模型下载后，一般放置于

stable-diffusion-webui（根目录）\extensions\sd-webui-controlnet\models

or

stable-diffusion-webui（根目录）\models\ControlNet

**预处理器下载地址（缺东西在里面找）：**

（下载地址）<https://huggingface.co/lllyasviel/Annotators/tree/main>

预处理器下载后，一般放置于

stable-diffusion-webui（根目录）\extensions\sd-webui-controlnet\annotator\downloads

### 基本使用方式

拖入信息图，选择对应的预处理器和模型组合，一定要点击启用。

对图像进行预处理：点击预处理器旁边的"爆炸"按钮，可以预览预处理结果；预处理的信息图可以保存下来复用，上传信息图时，将预处理设置为“无”（有些情况可能无法准确理解原图，不如使用原图完美）。

参数详解：

* 控制权重（Control Weight）：主要影响控制“力度”。
* 引导时机（Starting Control Step）：生成过程中ControlNet“生效”的时间（从0~1）。
* 控制模式（Control Mode）：更倾向于提示词还是ControlNet。
* 改变控制力度的方式
  * 加大力度：提高权重，降低开始引导步数并提高结束引导步数，选用“ControlNet更重要”模式。
  * 降低力度：减小权重，提高开始引导步数并降低结束引导步数，选用“提示词更重要”模式。

图生图中，不需要另外上传图片，会自动加载图生图原图作为信息图。

![image-20240302143915977](/assets/image-20240302143915977.DHWWmb9Z.png)

对比

| 原图                                                         | 提取的骨骼图                                                 | 改变姿态后                                                   |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| ![img](/assets/eaa760cfe5648f041c3cb49c2d06379c89d314368570e36fd3222a137d3a6c88.BlVedQC6.png) | ![image-20240302144200525](/assets/image-20240302144200525.DQxCZJHI.png) | ![image-20240302144223170](/assets/image-20240302144223170.CNrZoB2c.png) |

## ControlNet模型

ControlNet总模型数量有14个，预处理器有23个。（截止2024-03-02）

五大ControlNet模型

1. Openpose：控制姿势、手部、面部细节。
2. Depth：控制空间组成（深度）。
3. Canny：控制线条轮廓。
4. SoftEdge：控制线条轮廓，但更加柔和、放松。
5. Scribble：涂鸦引导画面生成。

### Openpose：控制姿势、手部、面部细节

几种不同的Openpose预处理

1. Hand：手部骨骼
2. Face （Only）：面部特征点
3. Full：全部加在一起

### Depth：控制空间组成（深度）

深度图：黑色远，白色近。

几种不同的Depth预处理

1. Leres精度高。
2. midas软为泛用。
3. 精度越高的预处理，花费时间一般越久。

推荐使用leress++处理器。

### Canny：控制线条轮廓

预处理时，阔值控制线条的多少，不宜太过密集。
应用：线稿上色。
Tips：白底黑线线稿应使用Invert，反色成黑底白线才可正确识别。

### SoftEdge（原HED）：控制线条轮廓，但更加柔和、放松

几种不同的SoftEdge预处理，没有太大差别。

与Canny对比，SoftEdge对轮廓线条的还原更加"生动"，不会太过于刻板。

Tips：适当“放松”ControlNet的控制力度，有助于AI发挥更多自己的创造力。

### Scribble：涂鸦引导画面生成

可以从图片提取，也可以自己绘制。

应用：灵魂画手。

### 其他

实现创新性局部重绘inPaint模型

放大工作流中用于增加细节的Tile模型

用于固定特征的Reference Only

## 多重ControlNet应用

在设置中，开启多个ControlNet Unit即可使用多重ControlNet。

示例：用Depth+Openpose，让人物手部准确呈现在脸前。

组合逻辑关键：要互补！

1. 正确示范：Canny+Depth，利用线条补足深度里的细节。

2. 错误示范：Canny+SoftEdge,同样是控制边缘，开两个和开一个差不了多少。

## 应用场景

### 运用场景：语义分割

[stable diffusion 大场景构图教程｜语义分割 controlnet seg 快速场景构建｜segment anything 局部修改｜快速提取蒙版\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1xM4y1E7kM/?spm_id_from=333.337.search-card.all.click\&vd_source=cf23a319db3eaab5d4d6aa80ad236bd8)

### 运用场景：线稿上色

[使用SD给线稿上色\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1DN411u7m2/?spm_id_from=333.788.recommend_more_video.1\&vd_source=cf23a319db3eaab5d4d6aa80ad236bd8)

### 运用场景：高清修复

[【Stable Diffusion】图片终极超清化脚本：Stable SR 碾压4xUltraSharp！\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1cs4y1173P/?spm_id_from=333.337.search-card.all.click)

[Multi Diffusion + Tiled VAE + ControlNet Tile模型，低显存打造AI绘画超高清6K分辨率体验！SD扩展插件教程\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1Su4y1d7Dp/?spm_id_from=333.788\&vd_source=cf23a319db3eaab5d4d6aa80ad236bd8)

### 运用场景：AI扩图

[【AI绘画】Stable Diffusion无限扩图，丝滑流畅！无限循环！(附工具）保姆级教程！包教包会！\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV12e411n7RC/?spm_id_from=333.337.search-card.all.click\&vd_source=cf23a319db3eaab5d4d6aa80ad236bd8)

![image-20240303144003700](/assets/image-20240303144003700.CLCKSh04.png)

[【保姆级教程】如何用ComfyUI进行拓图、扩图\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1V64y15722/?spm_id_from=333.337.search-card.all.click\&vd_source=cf23a319db3eaab5d4d6aa80ad236bd8)

### 运用场景：照片上色

[我用AI修复了50年前的亚运会老照片！Stable Diffusion智能上色+高清修复教程！StableSR + ControlNetTile\&Recolor\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1mm4y1575y/?spm_id_from=333.999.0.0\&vd_source=cf23a319db3eaab5d4d6aa80ad236bd8)

### 运用场景：电商换装、换背景

[comfyui 实战 : 真正模特换装\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1rA4m1V7M3/?spm_id_from=333.337.search-card.all.click\&vd_source=cf23a319db3eaab5d4d6aa80ad236bd8)

[AI商用第二弹！首发全网最简单的AI模特换装方案丨人人都能学会 Stable Diffusion 电商AI模特换装\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1Eu4y1h7kY/?spm_id_from=333.337.search-card.all.click\&vd_source=cf23a319db3eaab5d4d6aa80ad236bd8)

### 运用场景：AI换脸

[【Stable Diffusion】Reactor AI换脸详细教程，简单好用，秒杀Roop\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1DQ4y1F7vo/?spm_id_from=333.999.0.0\&vd_source=cf23a319db3eaab5d4d6aa80ad236bd8)

[AI打开新世界大门！立绘一键“真人化”，还能换皮肤！ControlNet LineArt 线稿上色+换装换背景，Stable Diffusion系统教程\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1oB4y1R7Ms/?spm_id_from=333.788\&vd_source=cf23a319db3eaab5d4d6aa80ad236bd8)

### 运用场景：二维码、光影、艺术字

[“牛逼”的教程来了！一次学会AI二维码+艺术字+光影光效+创意Logo生成，绝对是B站最详细的Stable Diffusion特效设计流程教学！AI绘画进阶应用\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1gX4y1J7ei/?spm_id_from=333.788\&vd_source=cf23a319db3eaab5d4d6aa80ad236bd8)

[【Stable Diffusion】AI艺术二维码超详细教程 巧妙运用controlnet解决无法识别问题！\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1kk4y1G7Kd/?spm_id_from=333.999.0.0)

### 运用场景：提炼深度图，转3D模型

[SDXL-DPO-告别lora-线稿生成深度图\_stable diffusion\_SD\_sd\_文生图\_图生图\_AIGC教程\_人工智能\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1Te411U7QD/?spm_id_from=333.337.search-card.all.click\&vd_source=cf23a319db3eaab5d4d6aa80ad236bd8)

运用场景：ai视频（瞬息宇宙）

[【AI视频】 SD Deforum插件详细教学 带你瞬息全宇宙！\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1Y94y1C7Wz/?spm_id_from=333.999.0.0)

### 运用场景：实时渲染

[stablediffusion加速模型！3秒一张图 SD也能实时渲染！LCM加速模型完整教程！附全套安装下载\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1HC4y1y7cL/?spm_id_from=333.999.0.0\&vd_source=cf23a319db3eaab5d4d6aa80ad236bd8)

[(免费) 让任何软件AI实时渲染！打通AI最后500米（一键包2.0版）\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1LH4y1k7js/?spm_id_from=333.788.recommend_more_video.-1\&vd_source=cf23a319db3eaab5d4d6aa80ad236bd8)

https://www.bilibili.com/video/BV17b4y1L7WY/?spm\_id\_from=333.999.0.0

### 运用场景：ps外挂

[一秒打通PS+SD！AI功能免费“平替”，还支持ControlNet？Auto Stable Diffusion Photoshop插件教程，释放轻薄本AI潜力\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1Zj411h7od/?spm_id_from=333.999.0.0)

## 参考资料

[30分钟零基础掌握ControlNet！绝对是你看过最好懂的控制网原理分析 | 基本操作、插件安装与5大模型应用 · Stable Diffusion教程\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1Ds4y1e7ZB/)

---

---
url: /数据库/02.MongoDB/3_Criteria与聚合查询.md
---

# Criteria与聚合查询

## 使用Criteria构造查询条件

请求实体

```java
@Data
public class SysUserRequest {

    private String userName;

    private String phoneNumber;

    private String beginTime;

    private String endTime;

    private Double lowestMoney;

    private Double highestMoney;

    private String TagScope;

    private String[] tags;

    private Integer pageNum = 1;

    private Integer pageSize = 10;

}
```

构造查询条件

```java
SysUserRequest sysUserRequest = new SysUserRequest();
sysUserRequest.setUserName("xxl");
sysUserRequest.setPhoneNumber("15500000000");
// 创建条件对象
Criteria criteria = new Criteria();
// 1. 全等于 (手机号全字匹配)
if (StringUtils.isNotBlank(sysUserRequest.getPhoneNumber())) {
    criteria.and("phone_number").is(sysUserRequest.getPhoneNumber());
}
// 2. 模糊查询 (名称模糊搜索)
if (StringUtils.isNotBlank(sysUserRequest.getUserName())) {
    criteria.and("name").regex(Pattern.compile("^.*" + sysUserRequest.getUserName() + ".*$", Pattern.CASE_INSENSITIVE));
}
// 3. 单个条件查询多个字段
if (StringUtils.isNotEmpty(sysUserRequest.getUserName())) {
    criteria.orOperator(
        Criteria.where("user_name").is(sysUserRequest.getUserName()),
        Criteria.where("nick_name").in(sysUserRequest.getUserName())
    );
}
// 4. 日期范围
if (StringUtils.isNotEmpty(sysUserRequest.getBeginTime()) && StringUtils.isNotEmpty(sysUserRequest.getEndTime())) {
    criteria.andOperator(Criteria.where("birthday").gte(sysUserRequest.getBeginTime()), Criteria.where("birthday").lte(sysUserRequest.getEndTime()));
}
// 5. 数值范围 (存款总金额)
if (sysUserRequest.getLowestMoney() != null && sysUserRequest.getHighestMoney() != null) {
    criteria.and("money").gte(sysUserRequest.getLowestMoney()).lte(sysUserRequest.getHighestMoney());
}
if (sysUserRequest.getTags() != null && !CollectionUtils.isEmpty(Arrays.asList(sysUserRequest.getTags()))) {
    if ("any".equals(sysUserRequest.getTagScope())) {
        // 6. 数组字段满足任一
        criteria.and("tags").in(sysUserRequest.getTags());
    } else if ("all".equals(sysUserRequest.getTagScope())) {
        //  7. 数组字段满足全部 (客户标签)
        criteria.and("tags").all(sysUserRequest.getTags());
    }
}

Query query = new Query();
query.addCriteria(criteria);
// 8. 查询返回指定字段 (自定义列表)
query.fields().include("user_name");
// 10. 分页
query.with(PageRequest.of(sysUserRequest.getPageNum() - 1, sysUserRequest.getPageSize(),
                          // 11. 排序
                          Sort.by(Sort.Order.desc("earliest_add_time"))));
// 分页（方式二，使用skip+limit）
query.with(Sort.by(Sort.Order.desc("birthday")))
    .skip((long) (sysUserRequest.getPageNum() - 1) * sysUserRequest.getPageSize())
    .limit(sysUserRequest.getPageSize());
// 执行查询
List<SysUser> list = mongoTemplate.find(query, SysUser.class);
// 12. 总记录数
long total = mongoTemplate.count(query, SysUser.class);
```

补充Criteria方法说明

| Criteria      | Mongodb | 说明                                                   |
| ------------- | ------- | ------------------------------------------------------ |
| and()         | $and    | 并且                                                   |
| andOperator() | $and    | 并且                                                   |
| orOperator()  | $or     | 或者                                                   |
| is()          | $is     | 等于                                                   |
| in()          | $in     | 是否被包含在数组或者list内                             |
| nin()         | $nin    | 不包含                                                 |
| gt()          | $gt     | 大于                                                   |
| gte()         | $gte    | 大于等于                                               |
| lt()          | $lt     | 小于                                                   |
| lte()         | $lte    | 小于等于                                               |
| regex()       | $regex  | 正则表达式用于模式匹配，基本上是用于文档中的发现字符串 |
| set()         | $set    | 给字段赋值，字段不存在，增加字段并赋值                 |

## Aggregation函数

> Aggregation官方SQL语法：[Aggregation Pipeline Stages — MongoDB Manual](https://www.mongodb.com/docs/v4.4/reference/operator/aggregation-pipeline/)

### 常用函数

1. Aggregation.group() : 聚合函数，将某个字段或者某个数组作为分组统计的依据,在group的基础上又扩展出以下函数：
   * sum() : 求和
   * max() : 获取最大值
   * min() : 获取最小值
   * avg() : 获取平均值
   * count() : 统计条目数
   * first () : 获取group by 后的某个字段的首个值
   * last() : 获取 group by 后的某个字段的最后一个值
   * push() : 在结果文档中插入值到一个数组中
   * addToSet() : 在结果文档中插入值到一个数组中，但不创建副本(作为集合)。
2. Aggregation.match() : 过滤函数，主要存储过滤数据的条件，输出符合条件的记录，相当于where条件。
   * is()：==相等
3. Aggregation.project(): 修改数据结构函数,将前面管道中的获取的字段进行重名,增加，修改字段等操作。
4. Aggregation.unwind()：将文档中的某一个数组类型字段拆分成多条，每条包含数组中的一个值。当preserveNullAndEmptyArrays为true时，将包括字段为null，空，或者缺失的数据;
5. Aggregation.sort(): 排序函数，将上级管道的内容按照某个字段进行排序并且输出。值为1升、-1降。sort一般放在group后,也就是说得到结果后再排序，如果先排序再分组没什么意义;
6. Aggregation.limit(): 限制输出函数，将聚合返回的内容限定在某个条目之内。通常作为页面大小
7. Aggregation.skip(): 跳过指定数量的条目再开始返回数据的函数，**通常和sort()，limit()配合，实现数据翻页查询等操作**。
8. Aggregation.lookup(): 连表查询，将被关联集合添加到执行操作的集合中。

### group、match

#### mongo语句

```sql
db.getCollection("sys_user").aggregate([
    {
        $match: {
            birthday: {
                $gte: ISODate('1900-01-01 00:00:00.014'),
                $lte: ISODate('2023-10-23 15:30:00.014')
            },
            userName: 'xxl'
        }
    },
    {
        $group: {
            _id: "$idNumber",
            sum: { $sum: 1 },
            userName: { $first: "$userName" },
            phoneNumber: { $first: "$phoneNumber" },
            birthday: { $first: "$birthday" }
        }
    },
    {
        $sort: { sum: -1 }
    },
		{
        $skip: 0
    },
    {
        $limit: 10
    }
])
```

#### java代码

```java
int page = 1;
int size = 10;
Date startTime = DateUtil.parse("1900-01-01 00:00:00");
Date endTime = new Date();
String userName = "xxl";

Aggregation aggregation = Aggregation.newAggregation(
    Aggregation.match(Criteria.where("birthday").gte(startTime).lte(endTime)),
    Aggregation.match(Criteria.where("userName").is(userName)),
    Aggregation.group("idNumber").count().as("sum")
    .first("userName").as("userName")
    .first("phoneNumber").as("phoneNumber")
    .first("birthday").as("birthday"),
    Aggregation.sort(Sort.by("sum").descending()),
    Aggregation.skip(page > 1 ? (page - 1) * size : 0),
    Aggregation.limit(size)
);

List<SysUser> results = mongoTemplate.aggregate(aggregation, "sys_user", SysUser.class).getMappedResults();
System.out.println(results);
```

### project筛选字段

#### mongo语句

```sql
db.getCollection("sys_user").aggregate([
    {
        $group: {
            _id: "$idNumber",
            sum: {
                $sum: "$money"
            },
            userName: {
                $first: "$userName"
            },
            phoneNumber: {
                $first: "$phoneNumber"
            },
            birthday: {
                $last: "$birthday"
            }
        }
    },
    {
        "$project": {
            "_id": 1,
            "sum": 1,
            "userName": 1,
            "phoneNumber": 1,
            "birth": "$birthday"
        }
    }
])
```

#### java代码

```java
Aggregation aggregation = Aggregation.newAggregation(
    Aggregation.group(new String[]{"_id"})
    .sum("money").as("sum")
    .first("userName").as("userName")
    .first("phoneNumber").as("phoneNumber")
    .last("birthday").as("birthday"),
    Aggregation.project("_id", "sum", "userName", "phoneNumber")
    .and("birthday").as("birth") // 重新命名字段
);
List<SysUser> results = mongoTemplate.aggregate(aggregation, "sys_user", SysUser.class).getMappedResults();
System.out.println(results);
```

### unwind拆分数组

#### mongo语句

```sql
db.getCollection('sys_user').aggregate([
    {
        $match: {
            userName: "xxl"
        }
    },
    {
        $unwind: {
            path: "$tags",
            includeArrayIndex: "arrayIndex"
        }
    }
])

# 原始数据
{
    "_id": ObjectId("658712b96a3c742d4070f6ca"),
    "userName": "xxl",
    "phoneNumber": "15285602889",
    "address": "北州市戴栋25802号",
    "idNumber": "790324-1128",
    "birthday": ISODate("1961-12-16T06:03:38.014Z"),
    "money": NumberInt("204"),
    "_class": "com.xxl.mongodb.result.SysUser",
    "tags": [
        "python",
        "c",
        "c#",
        "java"
    ],
    "child": {
        "userName": "xxl2",
        "phoneNumber": "110",
        "address": "洛杉矶",
        "idNumber": "911",
        "birthday": ISODate("2023-12-24T16:31:40.753Z"),
        "money": NumberInt("9999"),
        "_class": "com.xxl.mongodb.result.SysUser"
    }
}

# 查询出的数据
_id	userName	phoneNumber	address	idNumber	birthday	money	_class	tags	child	arrayIndex
658712b96a3c742d4070f6ca	xxl	15285602889	北州市戴栋25802号	790324-1128	1961-12-16 06:03:38.014	204	com.xxl.mongodb.result.SysUser	python	(Document) 7 Fields	0
658712b96a3c742d4070f6ca	xxl	15285602889	北州市戴栋25802号	790324-1128	1961-12-16 06:03:38.014	204	com.xxl.mongodb.result.SysUser	c	(Document) 7 Fields	1
658712b96a3c742d4070f6ca	xxl	15285602889	北州市戴栋25802号	790324-1128	1961-12-16 06:03:38.014	204	com.xxl.mongodb.result.SysUser	c#	(Document) 7 Fields	2
658712b96a3c742d4070f6ca	xxl	15285602889	北州市戴栋25802号	790324-1128	1961-12-16 06:03:38.014	204	com.xxl.mongodb.result.SysUser	java	(Document) 7 Fields	3
```

#### java代码

```java
Aggregation aggregation = Aggregation.newAggregation(
    Aggregation.match(new Criteria().and("userName").is("xxl")),
    Aggregation.unwind("tags", true)
);
List<SysUser> results = mongoTemplate.aggregate(aggregation, "sys_user", SysUser.class).getMappedResults();
System.out.println(results);
```

### lookup多表关联查询

```sql
// 创建新集合，增加关联数据
db.sys_user_label.insert({"user_name" : "xxl", "label_name" : "唱"})
db.sys_user_label.insert({"user_name" : "xxl", "label_name" : "跳"})
db.sys_user_label.insert({"user_name" : "xxl", "label_name" : "Rap"})
```

#### mongo语句

```sql
db.getCollection('sys_user').aggregate([
    {
        $lookup: {
            from: "sys_user_label",    // 被关联表名
            localField: "userName",    // 主表（mro_accounts）中用于关联的字段
            foreignField: "user_name", // 被关联表（mro_profiles）中用于关联的字段
            as: "label" 			   // 被关联的表的别名
        }
    }
])
```

#### java代码

```java
Aggregation aggregation = Aggregation.newAggregation(
    //分别对应from, localField, foreignField, as
    Aggregation.lookup("sys_user_label", "userName", "user_name", "label")
);
List<SysUser> results = mongoTemplate.aggregate(aggregation, "sys_user", SysUser.class).getMappedResults();
System.out.println(results);
```

## 参考资料

<https://blog.csdn.net/Java_Rookie_Xiao/article/details/125602833>

<https://www.programcreek.com/java-api-examples/index.php?api=org.springframework.data.mongodb.core.aggregation.Aggregation>

mongodb聚合在Java中的使用（包含mongo多表关联查询）：<https://www.cnblogs.com/wangzhebin/p/16494929.html>

---

---
url: /Python/AI大模型应用开发/14_CSV数据分析智能工具.md
---

# CSV数据分析智能工具

## 一、项目介绍

本次项目是开发一款基于 Agent 的 CSV 数据分析智能工具，通过整合大语言模型与数据处理能力，实现对 CSV 文档的精准分析、可视化展示及数据提取，确保回答基于实际数据而非 “编造”。以下是其核心功能、使用流程及优势：

### 1、核心功能与使用流程

#### 1. 前期准备：API 密钥与文档上传

* **API 密钥配置**：左侧侧边栏提供 API 密钥输入框，用户需填写自有密钥以驱动模型（如 OpenAI API）。
* **CSV 格式限制**：仅支持`.csv`后缀文件上传，自动过滤其他格式（如 Excel、TXT），避免解析错误。

#### 2. 数据预览：交互式表格展示

* 上传 CSV 后，工具自动加载数据并以**交互式表格**展示（支持排序、搜索、全屏预览），方便用户快速了解数据结构（如列名、部分行内容）。

#### 3. 核心功能：问答、可视化与数据提取

##### （1）精准回答数据问题

* **示例**：提问 “数据集里所有房子卧室数的平均值是多少？”，工具返回准确结果（基于 500 + 行数据计算）。
* 原理：Agent 不直接传递全量数据，而是：
  1. 向模型发送数据前几行（帮助理解结构）；
  2. 模型生成计算所需的 Python 代码（如`df['卧室数'].mean()`）；
  3. 执行代码并返回结果，确保准确性。

#### （2）数据可视化展示

* 支持生成**散点图、折线图、条形图**等，直观呈现数据特征。
* 示例：“用条形图展示所有房子的装修状态”，工具自动：
  1. 生成提取 “装修状态” 数据的代码；
  2. 统计各状态的数量；
  3. 绘制条形图（支持悬停查看具体数值）。

#### （3）精准数据提取

* 支持按条件筛选数据，如 “提取所有价格高于 1000 万的房子”。
* **效果**：工具返回符合条件的记录（如示例中 8 套房子），并以表格形式展示，方便用户进一步分析。

### 2、技术优势

1. **准确性保障**
   基于 Agent 执行 Python 代码（而非模型直接猜测），确保计算结果、筛选数据与原始 CSV 一致。
2. **高效处理大量数据**
   无需传递全量数据给模型，仅通过代码逻辑处理，解决 “数据量过大超出模型上下文” 的问题，同时节省 Token 成本。
3. **交互友好**
   表格与图表均支持交互（排序、搜索、悬停查看），降低非技术用户的使用门槛。
4. **可追溯性**
   后台打印 Agent 执行流程（代码生成、执行步骤），方便用户验证结果可靠性。

### 3、适用场景

* 数据分析初学者：无需编写代码，通过自然语言获取数据洞察；
* 业务人员：快速提取关键数据（如高价值客户、热销产品）；
* 研究者：可视化数据分布（如变量相关性、分类统计）。

该工具通过 Agent 将 “自然语言提问” 转化为 “代码自动执行”，实现了 CSV 数据的 “零代码” 分析，兼顾准确性与易用性。

## 二、创建AI请求

工具的核心是通过封装`DataFrame Agent`函数，实现对用户 CSV 数据的提问、提取、可视化等需求的精准响应。该函数将用户请求转化为 AI 可执行的任务，并规范返回格式，以便前端根据内容类型（文本、表格、图表）进行差异化展示。以下是具体实现步骤与逻辑：

### 1、前期准备：项目初始化与依赖安装

#### 1. 项目结构

```markdown
CSV数据分析工具/
├─ data_agent.py  # 核心逻辑：与AI交互的DataFrame Agent函数
├─ requirements.txt  # 依赖清单
└─ sample_data.csv  # 示例CSV数据（测试用）
```

#### 2. 依赖安装

通过`requirements.txt`安装所需库（终端执行）：

```sh
pip install -r requirements.txt
```

关键依赖：`langchain`（Agent 框架）、`langchain-experimental`（DataFrame Agent 工具）、`pandas`（数据处理）、`openai`（模型调用）、`streamlit`（前端基础，后续用）。

### 2、核心函数`dataframe_agent`实现

该函数接收用户 API 密钥、DataFrame（表格数据）、用户请求，返回规范格式的响应（文本、表格或图表数据），供前端展示。

【utils.py】完整代码

```python
import json
from langchain_openai import ChatOpenAI
from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent

# 定义响应格式提示词
# 为确保 Agent 返回的内容可被前端正确解析，需规范输出格式（文本→answer键，表格→table键，图表→bar/line/scatter键）：
PROMPT_TEMPLATE = """
你是一位数据分析助手，你的回应内容取决于用户的请求内容。

1. 对于文字回答的问题，按照这样的格式回答：
   {"answer": "<你的答案写在这里>"}
例如：
   {"answer": "订单量最高的产品ID是'MNWC3-067'"}

2. 如果用户需要一个表格，按照这样的格式回答：
   {"table": {"columns": ["column1", "column2", ...], "data": [[value1, value2, ...], [value1, value2, ...], ...]}}

3. 如果用户的请求适合返回条形图，按照这样的格式回答：
   {"bar": {"columns": ["A", "B", "C", ...], "data": [34, 21, 91, ...]}}

4. 如果用户的请求适合返回折线图，按照这样的格式回答：
   {"line": {"columns": ["A", "B", "C", ...], "data": [34, 21, 91, ...]}}

5. 如果用户的请求适合返回散点图，按照这样的格式回答：
   {"scatter": {"columns": ["A", "B", "C", ...], "data": [34, 21, 91, ...]}}
注意：我们只支持三种类型的图表："bar", "line" 和 "scatter"。


请将所有输出作为JSON字符串返回。请注意要将"columns"列表和数据列表中的所有字符串都用双引号包围。
例如：{"columns": ["Products", "Orders"], "data": [["32085Lip", 245], ["76439Eye", 178]]}

你要处理的用户请求如下： 
"""

# 参数说明：
# openai_api_key：用户提供的OpenAI API密钥
# df：pandas.DataFrame对象（用户上传的CSV数据）
# user_query：用户的问题或需求（如“计算平均值”“提取高价房源”“画条形图”）
# 返回值：字典（含answer/table/bar/line/scatter等键，对应不同内容类型）
def dataframe_agent(openai_api_key, df, query):
    # 初始化模型（低随机性，确保遵循REACT框架和格式要求）
    model = ChatOpenAI(
        model="gpt-4-turbo",
        openai_api_key=openai_api_key,
        temperature=0 # 温度设为0，避免模型“自由发挥”导致格式错误
    )
    # 创建Agent执行器（整合模型与DataFrame）
    agent = create_pandas_dataframe_agent(
        llm=model,
        df=df, # 传入待分析的DataFrame
        agent_executor_kwargs={"handle_parsing_errors": True}, # 让模型自行处理解析错误
        verbose=True # 打印思考过程（便于调试）
    )
    prompt = PROMPT_TEMPLATE + query
    response = agent.invoke({"input": prompt})
    response_dict = json.loads(response["output"])
    return response_dict
```

### 3、函数测试：验证核心功能

以 “分析职业数据” 为例，测试`dataframe_agent`的有效性：

```python
import pandas as pd
import os

# 加载示例CSV数据（含“职业”列）
df = pd.read_csv("sample_data.csv")

# 从环境变量获取API密钥（实际使用中由用户输入）
api_key = os.getenv("OPENAI_API_KEY")

# 测试：查询数据中出现最多的职业
result = dataframe_agent(
    openai_api_key=api_key,
    df=df,
    user_query="数据中出现次数最多的职业是什么？"
)

print(result)  # 输出：{"answer": "数据中出现次数最多的职业是healthcare"}
```

执行过程解析（`verbose=True`时打印）

```sh
> 推理：需统计“职业”列的出现次数，找到最大值。
> 行动：生成代码`df['职业'].value_counts().idxmax()`，执行后得到结果“healthcare”。
> 观察：代码返回“healthcare”。
> 推理：符合文本回答需求，按格式返回{"answer": "..."}。
> 输出：{"answer": "数据中出现次数最多的职业是healthcare"}
```

### 4、关键逻辑说明

1. **格式约束**：通过提示词强制 Agent 返回 JSON 格式，确保前端能根据键（`answer`/`table`等）判断展示方式。
2. **错误处理**：`handle_parsing_errors=True`让模型自行修正格式错误，降低解析失败概率。
3. **效率优化**：Agent 仅通过代码处理 DataFrame（而非传递全量数据），节省 Token 并避免模型 “记忆过载”。

## 三、创建网站页面

### 1、前期准备

#### 1. 新建前端文件

创建`main.py`作为前端主文件，导入核心依赖：

```python
import pandas as pd
import streamlit as st
from utils import dataframe_agent # 导入后端交互函数
```

#### 2. 页面基础配置

1. 设置页面标题
2. 侧边栏：API密钥输入

### 2、核心功能实现

1. CSV 文件上传与数据预览
2. 用户输入与请求提交
3. 后端交互与结果展示：根据用户点击事件，调用`dataframe_agent`并处理返回结果（文本、表格、图表）。

【main.py】完整代码示例

```python
# 导入所需库：pandas处理数据，streamlit构建网页，dataframe_agent处理AI交互
import pandas as pd
import streamlit as st
from utils import dataframe_agent


# 定义图表创建函数：根据数据和类型生成对应图表
def create_chart(input_data, chart_type):
    # 将输入数据转换为DataFrame（表格形式）
    df_data = pd.DataFrame(input_data["data"], columns=input_data["columns"])
    # 把第一列设为索引（作为图表的横轴）
    df_data.set_index(input_data["columns"][0], inplace=True)
    # 根据图表类型绘制对应图表
    if chart_type == "bar":
        st.bar_chart(df_data)  # 条形图
    elif chart_type == "line":
        st.line_chart(df_data)  # 折线图
    elif chart_type == "scatter":
        st.scatter_chart(df_data)  # 散点图

# 设置网页标题
st.title("💡 CSV数据分析智能工具")

# 侧边栏：用于输入OpenAI API密钥
with st.sidebar:
    openai_api_key = st.text_input("请输入OpenAI API密钥：", type="password")
    # 显示获取API密钥的链接
    st.markdown("[获取OpenAI API key](https://platform.openai.com/account/api-keys)")

# 文件上传器：仅允许上传CSV文件
data = st.file_uploader("上传你的数据文件（CSV格式）：", type="csv")
# 如果用户上传了文件
if data:
    # 读取CSV数据并保存到会话状态（避免重复读取）
    st.session_state["df"] = pd.read_csv(data)
    # 折叠面板：展示原始数据表格
    with st.expander("原始数据"):
        st.dataframe(st.session_state["df"])

# 文本输入框：用户输入问题、数据提取或可视化请求
query = st.text_area("请输入你关于以上表格的问题，或数据提取请求，或可视化要求（支持散点图、折线图、条形图）：")
# 生成回答按钮
button = st.button("生成回答")

# 按钮点击后的逻辑处理
# 情况1：点击了按钮但未输入API密钥
if button and not openai_api_key:
    st.info("请输入你的OpenAI API密钥")
# 情况2：点击了按钮但未上传数据
if button and "df" not in st.session_state:
    st.info("请先上传数据文件")
# 情况3：点击了按钮，且密钥和数据都已提供
if button and openai_api_key and "df" in st.session_state:
    # 显示加载状态，提示用户等待
    with st.spinner("AI正在思考中，请稍等..."):
        # 调用后端函数，获取AI的响应（字典形式）
        response_dict = dataframe_agent(openai_api_key, st.session_state["df"], query)
        # 如果响应是文本回答，直接展示
        if "answer" in response_dict:
            st.write(response_dict["answer"])
        # 如果响应是表格数据，用表格展示
        if "table" in response_dict:
            st.table(pd.DataFrame(
                response_dict["table"]["data"],  # 表格内容
                columns=response_dict["table"]["columns"]  # 列名
            ))
        # 如果响应是条形图数据，调用函数生成条形图
        if "bar" in response_dict:
            create_chart(response_dict["bar"], "bar")
        # 如果响应是折线图数据，调用函数生成折线图
        if "line" in response_dict:
            create_chart(response_dict["line"], "line")
        # 如果响应是散点图数据，调用函数生成散点图
        if "scatter" in response_dict:
            create_chart(response_dict["scatter"], "scatter")
```

### 3、功能测试与效果

#### 1. 测试场景 1：数据提问

* **用户输入**：“所有用户的平均年龄是多少？”
* **结果**：AI 返回文本回答（如 “平均年龄为 35.2 岁”），通过`answer`键展示。

#### 2. 测试场景 2：数据提取

* **用户输入**：“提取年龄大于 30 的客户数据”
* **结果**：AI 返回表格数据（含列名和符合条件的行），通过`table`键以交互式表格展示。

#### 3. 测试场景 3：图表可视化

* **用户输入**：“用条形图展示各职业的人数分布”
* **结果**：AI 返回图表数据，通过`bar`键调用`st.bar_chart`展示，支持悬停查看具体数值。

---

---
url: /daily/开源项目/DataEase可视化分析.md
---

# DataEase可视化分析

https://dataease.io/docs/v2/installation/offline\_INSTL\_and\_UPG/

```
系统登录信息如下:
        访问地址: http://服务器IP:8100
        用户名: admin
        初始密码: DataEase@123456
root@qydy:/tmp/dataease-offline-installer-v2.7.0-ce# ./install.sh
当前时间 : 2024年 06月 13日 星期四 09:32:39 CST
1. 检查安装环境并初始化环境变量
        停止 DataEase 服务
        升级安装
        [警告] DataEase 运行目录所在磁盘剩余空间不足 20G 可能无法正常启动!
2. 设置运行目录
        运行目录 /opt/dataease2.0
        配置文件目录 /opt/dataease2.0/conf
3. 初始化运行目录
        复制安装文件到运行目录
        调整配置文件参数
4. 安装 dectl 命令行工具
        安装至 /usr/local/bin/dectl & /usr/bin/dectl
5. 修改操作系统相关设置
        防火墙未开启，忽略端口开放
6. 安装 docker
        检测到 Docker 已安装，跳过安装步骤
        启动 Docker
7. 安装 docker-compose
        检测到 Docker Compose 已安装，跳过安装步骤
8. 加载 DataEase 镜像
        已存在镜像 dataease:v2.7.0
        已存在镜像 mysql:8.3.0
9. 配置 DataEase 服务
10. 启动 DataEase 服务
Job for dataease.service failed because the control process exited with error code.
See "systemctl status dataease.service" and "journalctl -xeu dataease.service" for details.

```

https://blog.csdn.net/XiaoBei\_BI/article/details/128854470

```

root@qydy:/tmp/dataease-offline-installer-v2.7.0-ce# ./install.sh
当前时间 : 2024年 06月 13日 星期四 09:45:08 CST
1. 检查安装环境并初始化环境变量
        停止 DataEase 服务
        升级安装
        [警告] DataEase 运行目录所在磁盘剩余空间不足 20G 可能无法正常启动!
2. 设置运行目录
        运行目录 /opt/dataease2.0
        配置文件目录 /opt/dataease2.0/conf
3. 初始化运行目录
        复制安装文件到运行目录
        调整配置文件参数
4. 安装 dectl 命令行工具
        安装至 /usr/local/bin/dectl & /usr/bin/dectl
5. 修改操作系统相关设置
        防火墙未开启，忽略端口开放
6. 安装 docker
        检测到 Docker 已安装，跳过安装步骤
        启动 Docker
7. 安装 docker-compose
        检测到 Docker Compose 已安装，跳过安装步骤
8. 加载 DataEase 镜像
        已存在镜像 dataease:v2.7.0
        已存在镜像 mysql:8.3.0
9. 配置 DataEase 服务
10. 启动 DataEase 服务

======================= 安装完成 =======================

系统登录信息如下:
        访问地址: http://服务器IP:8100
        用户名: admin
        初始密码: DataEase@123456

```

---

---
url: /daily/开源项目/RestCloud.md
---

# DataEase可视化分析

restcloud.zip

```
ROOT
apache-tomcat-9.0.68.tar.gz
jdk-8u333-linux-x64.tar.gz
mongo_init.js
mongodb-linux-x86_64-rhel70-4.2.24.tgz
restcloud_install.sh
```

mongo\_init.js

```js
try{

    if(db.system.users.find({'user':'admin'}).count() == 0){
        print('准备添加admin用户');
        db.createUser(
            {
            user: "admin",
            pwd: "pass@2022",
            roles:['readWriteAnyDatabase','dbAdminAnyDatabase']
            }
        ) ;
    }else{
        print('已经存在admin用户')
    }
	
	if(db.system.users.find({'user':'root'}).count() == 0){
        print('准备添加root用户');
        db.createUser(
            {
            user: "root",
            pwd: "root@2022",
            roles:['root']
            }
        ) ;
    }else{
        print('已经存在root用户')
    }
}catch(err){
    print('创建用户错误：'+err);
}
```

restcloud\_install.sh

```sh
#! /bin/bash
set -e
#将jdk、tomcat、ROOT.war和mongo的安装包和此脚本文件放在同一个目录下
jdk_name=jdk-8u333-linux-x64.tar.gz
tomcat_name=apache-tomcat-9.0.68.tar.gz
mongo_name=mongodb-linux-x86_64-rhel70-4.2.24.tgz
jdk_dir=jdk1.8.0_333
tomcat_dir=apache-tomcat-9.0.68
mongo_dir=mongodb-linux-x86_64-rhel70-4.2.24
jdk_path=/usr/jdk
tomcat_path=/usr/tomcat
mongo_path=/data/mongodb


#安装Mongo
echo -e "安装mongo-------------"
if [ -e $mongo_name ];then
    if [ -d $mongo_path ];then
        echo "已存在mongo目录${mongo_path},选择执行后续操作： 1、跳过安装包解压 2、退出安装"
        read mongo_option
        if [ $mongo_option -eq 1 ];then
            echo "跳过mongo的解压安装"
        elif [ $mongo_option -eq 2 ];then
            exit            
        else
            echo "所选操作未能识别，退出安装。"
            exit
        fi
    else
        echo "解压安装包-----"
        tar -zxvf $mongo_name $1> /dev/null
        echo "开始安装-----"
        mkdir -p $mongo_path
        mv $mongo_dir/* $mongo_path
        mkdir -p $mongo_path/db
        mkdir -p $mongo_path/logs
        touch $mongo_path/logs/mongodb.log
        touch $mongo_path/bin/mongodb.conf
        echo "开始导入配置----"
cat > $mongo_path/bin/mongodb.conf << EOF
bind_ip=0.0.0.0
port=27017
logappend=true
logpath=$mongo_path/logs/mongodb.log
dbpath=$mongo_path/db
fork=true
#auth=true
EOF
    fi

if [ -e /usr/lib/systemd/system/mongodb.service ];then
    rm -f /usr/lib/systemd/system/mongodb.service
else
cat > /usr/lib/systemd/system/mongodb.service << EOF
[Unit]
Description=mongodb
After=network.target remote-fs.target nss-lookup.target

[Service]
Type=forking
ExecStart=$mongo_path/bin/mongod --config $mongo_path/bin/mongodb.conf
ExecReload=/bin/kill -s HUP \$MAINPID
ExecStop=$mongo_path/bin/mongod --shutdown --config $mongo_path/bin/mongodb.conf
PrivateTmp=true

[Install]
WantedBy=multi-user.target
EOF

chmod 754 /usr/lib/systemd/system/mongodb.service

echo "mongo加入开机自启"
systemctl daemon-reload
systemctl enable mongodb
echo "启动mongodb-------"
systemctl start mongodb
fi

echo "添加mongo用户"
$mongo_path/bin/mongo 127.0.0.1:27017/admin mongo_init.js

authline=$(grep -n "auth" ${mongo_path}/bin/mongodb.conf |cut -f1 -d":")
sed -i "${authline}c auth=true" ${mongo_path}/bin/mongodb.conf

echo "重启mongo"
systemctl restart mongodb
echo "mongo安装完成------"
else
    echo "未检测到mongo安装包，请把安装到放到安装脚本目录下"
fi



#安装jdk
echo -e "准备安装jdk-------------"
if [ -e $jdk_name ];then

    if [ -d $jdk_path ];then
        echo "已存在jdk目录${jdk_path}，选择后续执行操作：1、删除后继续安装（注意删除后不可恢复！！！） 2、跳过安装包解压 3、退出安装"
        read jdk_option
        if [ $jdk_option -eq 1 ];then
            echo "删除文件重新安装-----"
            rm -rf $jdk_path
            echo "解压安装包-----"
            tar -zxvf $jdk_name $1> /dev/null
            mv $jdk_dir $jdk_path
        elif [ $jdk_option -eq 2 ];then
            echo "跳过jdk的解压安装"
        elif [ $jdk_option -eq 3];then
            exit
        else
            echo "所选操作未能识别，退出安装。"
            exit
        fi
    else
        echo "解压安装包-----"
        tar -zxvf $jdk_name $1> /dev/null
        mv $jdk_dir $jdk_path
    fi
    
#关闭错误退出，不然无法获取命令执行结果
set +e
echo "正在配置jdk环境变量-----"
jdk_env=$(grep "JAVA_HOME=${jdk_path}" /etc/profile)
if [ "$jdk_env" = "JAVA_HOME=${jdk_path}" ];then
    echo "在/etc/profile已存在jdk环境变量，跳过配置"
else
cat >> /etc/profile << EOF
JAVA_HOME=$jdk_path
PATH=\$JAVA_HOME/bin:\$PATH
CLASSPATH=:.:\$JAVA_HOME/lib/dt.jar:\$JAVA_HOME/lib/tools.jar
export JAVA_HOME
export PATH
export CLASSPATH
EOF

set -e
source /etc/profile
fi
    java -version
    if [ $? -eq 0 ];then
        echo -e "jdk安装完成。"
    else
        echo -e "jdk安装失败，请检查原因并把$jdk_path的文件进行删除后重试"
    fi
else
    echo "未检测到JDK安装包，请把安装包放到安装脚本目录下！！"
fi

#安装tomcat
echo "准备安装tomcat----------"
if [ -e $tomcat_name ];then
    if [ -d $tomcat_path ];then
        echo "已存在tomcat目录${tomcat_path}，选择后续执行操作：1、删除后继续安装（注意删除后不可恢复！！！） 2、跳过安装包解压 3、退出安装"
        read tomcat_option
        if [ $tomcat_option -eq 1 ];then
            echo "删除文件重新安装-----"
            rm -rf $tomcat_path
            echo "解压安装包-----"
            tar -zxvf $tomcat_name $1> /dev/null
            mv $tomcat_dir $tomcat_path
            rm -rf $tomcat_path/webapps/*
        elif [ $tomcat_option -eq 2 ];then
            echo "跳过tomcat的解压安装"
        elif [ $tomcat_option -eq 3];then
            exit
        else
            echo "所选操作未能识别，退出安装。"
            exit
        fi
    else
        tar -zxvf $tomcat_name $1> /dev/null
        mv $tomcat_dir $tomcat_path
        rm -rf $tomcat_path/webapps/*
    fi

sed -i '2a #set java environment\
export JAVA_HOME='${jdk_path}'\
export JRE_HOME=${JAVA_HOME}/jre\
export CLASSPATH=.:%{JAVA_HOME}/lib:%{JRE_HOME}/lib\
export PATH=${JAVA_HOME}/bin:$PATH\
#tomcat\
export TOMCAT_HOME=/usr/tomcat\
' $tomcat_path/bin/startup.sh
#设置tomcat内存和时间
sed -i '2a JAVA_OPTS="${JAVA_OPTS} -server -Xms2048M -Xmx2048M -XX:PermSize=256M -XX:MaxPermSize=256M"\nJAVA_OPTS="${JAVA_OPTS} -Duser.timezone=GMT+08"' $tomcat_path/bin/catalina.sh

echo "tomcat安装完成"
else
    echo "未检测到tomcat安装包，请把安装包放到安装脚本目录下！！"
fi

echo "准备安装RestCloud iPaaS集成平台-------"
if [ -d ROOT ];then
    mv ROOT $tomcat_path/webapps/
    set +e
    $tomcat_path/bin/startup.sh && tail -f $tomcat_path/logs/catalina.out
    #判断tomcat进程是否启动
    PID=$(ps -ef | grep "tomcat" | grep -v grep | awk '{print $2}')
    if [ -n "$PID" ]; then
        echo "RestCloud iPaaS集成平台安装完成"
    else
        echo "RestCloud iPaaS集成平台安装失败，请查看日志并检查各项属性配置是否正确。"
    fi
else
    echo "未检测到ROOT文件，请把平台安装文件解压为ROOT文件夹放到脚本目录下"
fi

```

---

---
url: /Java/解决方案/数据同步/5_DataX.md
---

# DataX

因为最近使用到了DataX，所以接下来需要来个系统的学习，并以博客的形式记录。

## 一. DataX3.0概览

DataX 是一个异构数据源离线同步工具，致力于实现包括关系型数据库(MySQL、Oracle等)、HDFS、Hive、ODPS、HBase、FTP等各种异构数据源之间稳定高效的数据同步功能。

![datax\_why\_new](/assets/93b7fc1c-6927-11e6-8cda-7cf8420fc65f.D42m3-tE.png)

* #### 设计理念

  为了解决异构数据源同步问题，DataX将复杂的网状的同步链路变成了星型数据链路，DataX作为中间传输载体负责连接各种数据源。当需要接入一个新的数据源的时候，只需要将此数据源对接到DataX，便能跟已有的数据源做到无缝数据同步。

* #### 当前使用现状

  DataX在阿里巴巴集团内被广泛使用，承担了所有大数据的离线同步业务，并已持续稳定运行了6年之久。目前每天完成同步8w多道作业，每日传输数据量超过300TB。

目前最新版本是DataX3.0，有了更多更强大的功能和更好的使用体验。

## 二、DataX3.0框架设计

![datax\_framework\_new](/assets/ec7e36f4-6927-11e6-8f5f-ffc43d6a468b.ceo6Jspz.png)

DataX本身作为离线数据同步框架，采用Framework + plugin架构构建。将数据源读取和写入抽象成为Reader/Writer插件，纳入到整个同步框架中。

* Reader：Reader为数据采集模块，负责采集数据源的数据，将数据发送给Framework。
* Writer： Writer为数据写入模块，负责不断向Framework取数据，并将数据写入到目的端。
* Framework：Framework用于连接reader和writer，作为两者的数据传输通道，并处理缓冲，流控，并发，数据转换等核心技术问题。

## 三. DataX3.0插件体系

经过几年积累，DataX目前已经有了比较全面的插件体系，主流的RDBMS数据库、NOSQL、大数据计算系统都已经接入。DataX目前支持数据如下：

| 类型               | 数据源                          | Reader(读) | Writer(写) |                             文档                             |
| ------------------ | ------------------------------- | :--------: | :--------: | :----------------------------------------------------------: |
| RDBMS 关系型数据库 | MySQL                           |     √      |     √      | [读](https://github.com/alibaba/DataX/blob/master/mysqlreader/doc/mysqlreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/mysqlwriter/doc/mysqlwriter.md) |
|                    | Oracle                          |     √      |     √      | [读](https://github.com/alibaba/DataX/blob/master/oraclereader/doc/oraclereader.md) 、[写](https://github.com/alibaba/DataX/blob/master/oraclewriter/doc/oraclewriter.md) |
|                    | OceanBase                       |     √      |     √      | [读](https://open.oceanbase.com/docs/community/oceanbase-database/V3.1.0/use-datax-to-full-migration-data-to-oceanbase) 、[写](https://open.oceanbase.com/docs/community/oceanbase-database/V3.1.0/use-datax-to-full-migration-data-to-oceanbase) |
|                    | SQLServer                       |     √      |     √      | [读](https://github.com/alibaba/DataX/blob/master/sqlserverreader/doc/sqlserverreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/sqlserverwriter/doc/sqlserverwriter.md) |
|                    | PostgreSQL                      |     √      |     √      | [读](https://github.com/alibaba/DataX/blob/master/postgresqlreader/doc/postgresqlreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/postgresqlwriter/doc/postgresqlwriter.md) |
|                    | DRDS                            |     √      |     √      | [读](https://github.com/alibaba/DataX/blob/master/drdsreader/doc/drdsreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/drdswriter/doc/drdswriter.md) |
|                    | 达梦                            |     √      |     √      |                       [读]() 、[写]()                        |
|                    | 通用RDBMS(支持所有关系型数据库) |     √      |     √      |                       [读]() 、[写]()                        |
| 阿里云数仓数据存储 | ODPS                            |     √      |     √      | [读](https://github.com/alibaba/DataX/blob/master/odpsreader/doc/odpsreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/odpsswriter/doc/odpswriter.md) |
|                    | ADS                             |            |     √      | [写](https://github.com/alibaba/DataX/blob/master/adswriter/doc/adswriter.md) |
|                    | OSS                             |     √      |     √      | [读](https://github.com/alibaba/DataX/blob/master/ossreader/doc/ossreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/osswriter/doc/osswriter.md) |
|                    | OCS                             |     √      |     √      | [读](https://github.com/alibaba/DataX/blob/master/ocsreader/doc/ocsreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/ocswriter/doc/ocswriter.md) |
| NoSQL数据存储      | OTS                             |     √      |     √      | [读](https://github.com/alibaba/DataX/blob/master/otsreader/doc/otsreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/otswriter/doc/otswriter.md) |
|                    | Hbase0.94                       |     √      |     √      | [读](https://github.com/alibaba/DataX/blob/master/hbase094xreader/doc/hbase094xreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/hbase094xwriter/doc/hbase094xwriter.md) |
|                    | Hbase1.1                        |     √      |     √      | [读](https://github.com/alibaba/DataX/blob/master/hbase11xreader/doc/hbase11xreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/hbase11xwriter/doc/hbase11xwriter.md) |
|                    | MongoDB                         |     √      |     √      | [读](https://github.com/alibaba/DataX/blob/master/mongoreader/doc/mongoreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/mongowriter/doc/mongowriter.md) |
|                    | Hive                            |     √      |     √      | [读](https://github.com/alibaba/DataX/blob/master/hdfsreader/doc/hdfsreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/hdfswriter/doc/hdfswriter.md) |
| 无结构化数据存储   | TxtFile                         |     √      |     √      | [读](https://github.com/alibaba/DataX/blob/master/txtfilereader/doc/txtfilereader.md) 、[写](https://github.com/alibaba/DataX/blob/master/txtfilewriter/doc/txtfilewriter.md) |
|                    | FTP                             |     √      |     √      | [读](https://github.com/alibaba/DataX/blob/master/ftpreader/doc/ftpreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/ftpwriter/doc/ftpwriter.md) |
|                    | HDFS                            |     √      |     √      | [读](https://github.com/alibaba/DataX/blob/master/hdfsreader/doc/hdfsreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/hdfswriter/doc/hdfswriter.md) |
|                    | Elasticsearch                   |            |     √      | [写](https://github.com/alibaba/DataX/blob/master/elasticsearchwriter/doc/elasticsearchwriter.md) |

DataX Framework提供了简单的接口与插件交互，提供简单的插件接入机制，只需要任意加上一种插件，就能无缝对接其他数据源。

详情请看：[DataX数据源指南](https://github.com/alibaba/DataX/wiki/DataX-all-data-channels)

## 四、DataX3.0核心架构

DataX 3.0 开源版本支持单机多线程模式完成同步作业运行，本小节按一个DataX作业生命周期的时序图，从整体架构设计非常简要说明DataX各个模块相互关系。

![datax\_arch](/assets/aa6c95a8-6891-11e6-94b7-39f0ab5af3b4.6vpokrve.png)

#### 核心模块介绍：

1. DataX完成单个数据同步的作业，我们称之为Job，DataX接受到一个Job之后，将启动一个进程来完成整个作业同步过程。DataX Job模块是单个作业的中枢管理节点，承担了数据清理、子任务切分(将单一作业计算转化为多个子Task)、TaskGroup管理等功能。
2. DataXJob启动后，会根据不同的源端切分策略，将Job切分成多个小的Task(子任务)，以便于并发执行。Task便是DataX作业的最小单元，每一个Task都会负责一部分数据的同步工作。
3. 切分多个Task之后，DataX Job会调用Scheduler模块，根据配置的并发数据量，将拆分成的Task重新组合，组装成TaskGroup(任务组)。每一个TaskGroup负责以一定的并发运行完毕分配好的所有Task，默认单个任务组的并发数量为5。
4. 每一个Task都由TaskGroup负责启动，Task启动后，会固定启动Reader—>Channel—>Writer的线程来完成任务同步工作。
5. DataX作业运行起来之后， Job监控并等待多个TaskGroup模块任务完成，等待所有TaskGroup任务完成后Job成功退出。否则，异常退出，进程退出值非0

#### DataX调度流程：

举例来说，用户提交了一个DataX作业，并且配置了20个并发，目的是将一个100张分表的mysql数据同步到odps里面。	DataX的调度决策思路是：

1. DataXJob根据分库分表切分成了100个Task。
2. 根据20个并发，DataX计算共需要分配4个TaskGroup。
3. 4个TaskGroup平分切分好的100个Task，每一个TaskGroup负责以5个并发共计运行25个Task。

## 五、DataX 3.0六大核心优势

* #### 可靠的数据质量监控

  * 完美解决数据传输个别类型失真问题

    DataX旧版对于部分数据类型(比如时间戳)传输一直存在毫秒阶段等数据失真情况，新版本DataX3.0已经做到支持所有的强数据类型，每一种插件都有自己的数据类型转换策略，让数据可以完整无损的传输到目的端。

  * 提供作业全链路的流量、数据量运行时监控

    DataX3.0运行过程中可以将作业本身状态、数据流量、数据速度、执行进度等信息进行全面的展示，让用户可以实时了解作业状态。并可在作业执行过程中智能判断源端和目的端的速度对比情况，给予用户更多性能排查信息。

  * 提供脏数据探测

    在大量数据的传输过程中，必定会由于各种原因导致很多数据传输报错(比如类型转换错误)，这种数据DataX认为就是脏数据。DataX目前可以实现脏数据精确过滤、识别、采集、展示，为用户提供多种的脏数据处理模式，让用户准确把控数据质量大关！

* #### 丰富的数据转换功能

  DataX作为一个服务于大数据的ETL工具，除了提供数据快照搬迁功能之外，还提供了丰富数据转换的功能，让数据在传输过程中可以轻松完成数据脱敏，补全，过滤等数据转换功能，另外还提供了自动groovy函数，让用户自定义转换函数。详情请看DataX3的transformer详细介绍。

* #### 精准的速度控制

  还在为同步过程对在线存储压力影响而担心吗？新版本DataX3.0提供了包括通道(并发)、记录流、字节流三种流控模式，可以随意控制你的作业速度，让你的作业在库可以承受的范围内达到最佳的同步速度。

  ```json
  "speed": {
     "channel": 5,
     "byte": 1048576,
     "record": 10000
  }
  ```

* #### 强劲的同步性能

  DataX3.0每一种读插件都有一种或多种切分策略，都能将作业合理切分成多个Task并行执行，单机多线程执行模型可以让DataX速度随并发成线性增长。在源端和目的端性能都足够的情况下，单个作业一定可以打满网卡。另外，DataX团队对所有的已经接入的插件都做了极致的性能优化，并且做了完整的性能测试。性能测试相关详情可以参照每单个数据源的详细介绍：[DataX数据源指南](https://github.com/alibaba/DataX/wiki/DataX-all-data-channels)

* #### 健壮的容错机制

  DataX作业是极易受外部因素的干扰，网络闪断、数据源不稳定等因素很容易让同步到一半的作业报错停止。因此稳定性是DataX的基本要求，在DataX 3.0的设计中，重点完善了框架和插件的稳定性。目前DataX3.0可以做到线程级别、进程级别(暂时未开放)、作业级别多层次局部/全局的重试，保证用户的作业稳定运行。

  * 线程内部重试

    DataX的核心插件都经过团队的全盘review，不同的网络交互方式都有不同的重试策略。

  * 线程级别重试

    目前DataX已经可以实现TaskFailover，针对于中间失败的Task，DataX框架可以做到整个Task级别的重新调度。

* #### 极简的使用体验

  * 易用

    下载即可用，支持linux和windows，只需要短短几步骤就可以完成数据的传输。请点击：[Quick Start](https://github.com/alibaba/DataX/wiki/Quick-Start)

  * 详细

    DataX在运行日志中打印了大量信息，其中包括传输速度，Reader、Writer性能，进程CPU，JVM和GC情况等等。

    * 传输过程中打印传输速度、进度等

      ![datax\_run\_speed](/assets/d1612c0a-6891-11e6-9970-d6693c15ef24.C7V7WGsF.png)

    * 传输过程中会打印进程相关的CPU、JVM等

      ![datax\_run\_cpu](/assets/ee63c2fe-6891-11e6-9056-97d7e3d13d8d.EFRXfssa.png)

    * 在任务结束之后，打印总体运行情况

      ![datax\_end\_info](/assets/0484d3ac-6892-11e6-9c1d-b102ad210a32.BDQ3ud6Z.png)

https://developer.aliyun.com/article/1045779

---

---
url: /Java/容器/Docker/镜像安装-Docker安装Graylog.md
---

# Docker安装Graylog

前期准备

安装docker

安装docker-compose

## 简单安装

二、mongo安装

```shell
docker pull mongo:4.4.6                      #拉取镜像
docker run --name mongo -d mongo:4.4.6       #启动
#
--name   容器名称
-d       后台运行
```

三、elasticsearch安装

```shell
docker pull elasticsearch:7.1.1              
docker run --name elasticsearch -d -e ES_JAVA_OPTS="-Xms512m -Xmx512m"  -e xpack.security.enabled=false -e transport.host=localhost 
#
-e network.host=0.0.0.0 -e http.port:9200 -e "discovery.type=single-node" -p 9200:9200 -p 9300:9300 elasticsearch:7.1.1    
-e 指定环境变量，容器中可以使用该环境变量
-p 指定容器暴露的端口
```

四、graylog安装

```shell
docker pull graylog/graylog:4.3              
docker run --name graylog --link mongo --link elasticsearch -p 9000:9000 -p 12201:12201/udp -p 1514:1514/udp -p 5555:5555
-e GRAYLOG_PASSWORD_SECRET=somepasswordpepper -e GRAYLOG_ROOT_TIMEZONE=Asia/Shanghai -e GRAYLOG_ELASTICSEARCH_HOSTS=http://192.168.1.10:9200 -e GRAYLOG_ROOT_PASSWORD_SHA2=8c6976e5b5410415bde908bd4dee15dfb167a9c873fc4bb8a81f6f2ab448a918 -e GRAYLOG_HTTP_EXTERNAL_URI="http://192.168.1.10:9000/" -d graylog/graylog:4.3         
#     
-e GRAYLOG_ROOT_PASSWORD_SHA2    为SHA256加密的密文，使用的是admin
```

五、测试

服务器IP地址:9000访问，账号密码均为admin

点击System➡点击Inputs➡选择gelf-http➡点击Launch new input

填写名称和端口，往下拉保存

## docker compose安装

[docker安装graylog\_docker graylog\_黑色鲸鱼的博客-CSDN博客](https://blog.csdn.net/heisejingyu/article/details/128102560)

https://blog.csdn.net/u012954706/article/details/79592060

https://blog.csdn.net/qq\_36120342/article/details/123729329

https://gitee.com/zhengqingya/docker-compose/blob/master/Linux/graylog/docker-compose-graylog.yml#

graylog查询语法

https://blog.csdn.net/weixin\_43066697/article/details/126303133

https://archivedocs.graylog.org/en/latest/pages/searching/query\_language.html

---

---
url: /Java/容器/Docker/镜像安装-Docker安装MongoDB.md
---

# Docker安装MongoDB

### 1、查看可用的 MongoDB 版本

1. 访问 MongoDB 镜像库地址： https://hub.docker.com/\_/mongo?tab=tags\&page=1。可以通过 Sort by 查看其他版本的 MongoDB，默认是最新版本 **mongo:latest**。也可以在下拉列表中找到想要的版本。
2. 使用 **docker search mongo** 命令来查看可用版本。

### 2、拉取最新版的 MongoDB 镜像

拉取官方的最新版本的镜像：

```sh
docker pull mongo:latest
```

### 3、查看本地镜像

使用以下命令来查看是否已安装了 mongo：

```sh
docker images
```

TAG显示（latest）证明已安装最新版本 mongo 镜像。

### 4、运行容器

安装完成后，使用以下命令来运行 mongo 容器：

```sh
docker run -d \
 -p 27017:27017 \
 --name \
 my-mongo-container mongo
```

参数说明：

* `-d`: 后台运行容器。
* `-p 27017:27017`: 将主机的27017端口映射到容器的27017端口。
* `--name my-mongo-container`: 为容器指定一个名字，这里是`my-mongo-container`，你可以根据需要更改。

### 5、安装成功

最后我们可以通过 **docker ps** 命令查看容器的运行信息：

```sh
$ docker ps
CONTAINER ID   IMAGE                                      COMMAND                   CREATED          STATUS                          PORTS                                                                                            NAMES
1e29386a1343   mongo                                      "docker-entrypoint.s…"   10 seconds ago   Up 9 seconds                    0.0.0.0:27017->27017/tcp, :::27017->27017/tcp                                                    my-mongo-container
```

能够看到名为 **my-mongo-container** 的 MongoDB 容器正在运行。

接下来可以使用 MongoDB 客户端（例如 mongo shell）连接到运行中的 MongoDB 容器。

你可以使用以下命令连接到 MongoDB：

```sh
$ docker exec -it my-mongo-container bash
root@1e29386a1343:/# mongosh --host 127.0.0.1 --port 27017
Current Mongosh Log ID: 659056f433cd505b37dee3ed
Connecting to:          mongodb://127.0.0.1:27017/?directConnection=true&serverSelectionTimeoutMS=2000
Using MongoDB:          5.0.5
Using Mongosh:          1.1.6

For mongosh info see: https://docs.mongodb.com/mongodb-shell/


To help improve our products, anonymous usage data is collected and sent to MongoDB periodically (https://www.mongodb.com/legal/privacy-policy).
You can opt-out by running the disableTelemetry() command.

------
   The server generated these startup warnings when booting:
   2023-12-30T17:42:24.704+00:00: Access control is not enabled for the database. Read and write access to data and configuration is unrestricted
   2023-12-30T17:42:24.704+00:00: /sys/kernel/mm/transparent_hugepage/enabled is 'always'. We suggest setting it to 'never'
   2023-12-30T17:42:24.705+00:00: /sys/kernel/mm/transparent_hugepage/defrag is 'always'. We suggest setting it to 'never'
------

test> exit
root@1e29386a1343:/# exit
exit
```

这将连接到本地主机的 27017 端口，你可以根据之前映射的端口进行调整。

进入 MongoDB 容器的 bash shell 命令如下：

```sh
docker exec -it my-mongo-container bash
```

记得在不再需要时停止和删除容器，可以使用以下命令：

```sh
docker stop my-mongo-container
docker rm my-mongo-container
```

---

---
url: /Java/容器/Docker/镜像安装-Docker安装MySQL.md
---

# Docker安装MySQL

### 1、查看可用的 MySQL 版本

1. 访问 MySQL 镜像库地址：https://hub.docker.com/\_/mysql?tab=tags 。可以通过 Sort by 查看其他版本的 MySQL，默认是最新版本 **mysql:latest** 。在下拉列表中找到其他你想要的版本。
2. 使用用 **docker search mysql** 命令来查看可用版本。

### 2、拉取 MySQL 镜像

这里我们拉取官方的最新版本的镜像：

```sh
docker pull mysql:latest
```

### 3、查看本地镜像

使用以下命令来查看是否已安装了 mysql：

```sh
docker images
```

### 4、运行容器

安装完成后，我们可以使用以下命令来运行 mysql 容器：

```sh
docker run -itd \
  -p 3306:3306 \
  --name mysql-test \
  -e MYSQL_ROOT_PASSWORD=123456 \
  mysql
```

参数说明：

* **-p 3306:3306** ：映射容器服务的 3306 端口到宿主机的 3306 端口，外部主机可以直接通过 **宿主机ip:3306** 访问到 MySQL 的服务。
* **MYSQL\_ROOT\_PASSWORD=123456**：设置 MySQL 服务 root 用户的密码。

### 5、安装成功

通过 **docker ps** 命令查看是否安装成功。

本机可以通过 root 和密码 123456 访问 MySQL 服务。

```sh
# 进入 MySQL 容器
docker exec -it mysql-test bash
# 登录 MySQL
mysql -h localhost -u root -p、
# 修改用户密码
ALTER USER 'root'@'localhost' IDENTIFIED BY 'xxl666';
# 添加远程登录用户
CREATE USER 'xxl'@'%' IDENTIFIED WITH mysql_native_password BY 'xxl666';
GRANT ALL PRIVILEGES ON *.* TO 'xxl'@'%';
```

---

---
url: /Java/容器/Docker/镜像安装-Docker安装Nginx.md
---

# Docker安装Nginx

### 1、查看可用的 Nginx 版本

1. 访问 Nginx 镜像库地址： https://hub.docker.com/\_/nginx?tab=tags。可以通过 Sort by 查看其他版本的 Nginx，默认是最新版本 **nginx:latest**。在下拉列表中找到其他你想要的版本。
2. 使用用 **docker search nginx** 命令来查看可用版本。

### 2、取最新版的 Nginx 镜像

这里我们拉取官方的最新版本的镜像：

```sh
docker pull nginx:latest
```

### 3、查看本地镜像

使用以下命令来查看是否已安装了 nginx：

```sh
docker images
```

TAG显示（latest）证明已安装最新版本 nginx 镜像。

```sh
[root@192 ~]# docker images
REPOSITORY                            TAG       IMAGE ID       CREATED         SIZE
nginx                                 latest    605c77e624dd   2 years ago     141MB
```

### 4、运行容器

安装完成后，我们可以使用以下命令来运行 nginx 容器：

```sh
docker run --rm \
-p 8080:80 \
--name nginx-test \
-v /dir/dist/:/usr/share/nginx/html \
-d nginx
```

参数说明：

* `--rm`: 这是一个标志，意味着当容器停止运行时，自动删除该容器。这对于临时任务或一次性任务很有用，因为不需要手动清理。
* **-p 8080:80**： 端口进行映射，将本地 8080 端口映射到容器内部的 80 端口。
* **--name nginx-test**：容器名称。
* `-v /dir/dist/:/usr/share/nginx/html`: 这是一个卷映射。它将宿主机上的`/dir/dist/`目录映射到容器内的`/usr/share/nginx/html`目录。这意味着，如果在宿主机上的`/dir/dist/`目录中有所有的文件和文件夹，它们将出现在容器内的相应位置。这对于配置静态网站或应用程序的部署非常有用。
* **-d nginx**： 设置容器在在后台一直运行。

### 5、安装成功

最后我们可以通过浏览器可以直接访问 8080 端口的 nginx 服务。

---

---
url: /Java/容器/Docker/镜像安装-Docker安装Redis.md
---

# Docker安装Redis

### 1、查看可用的 Redis 版本

1. 访问 Redis 镜像库地址： https://hub.docker.com/\_/redis?tab=tags。可以通过 Sort by 查看其他版本的 Redis，默认是最新版本 **redis:latest**。也可以在下拉列表中找到想要的版本。
2. 使用用 **docker search redis** 命令来查看可用版本：

### 2、取最新版的 Redis 镜像

这里我们拉取官方的最新版本的镜像：

```sh
docker pull redis:latest
```

### 3、查看本地镜像

使用以下命令来查看是否已安装了 redis：

```sh
docker images
```

### 4、运行容器

安装完成后，我们可以使用以下命令来运行 redis 容器：

```sh
docker run -itd \
  -p 6379:6379 \
  --name redis-test \
  --requirepass "redispwd" \
  redis
```

参数说明：

* **-p 6379:6379**：映射容器服务的 6379 端口到宿主机的 6379 端口。外部可以直接通过宿主机ip:6379 访问到 Redis 的服务。
* \--requirepass：指定密码

### 5、安装成功

最后我们可以通过 **docker ps** 命令查看容器的运行信息：

接着我们通过 redis-cli 连接测试使用 redis 服务。

```sh
docker exec -it redis-test /bin/bash
```

---

---
url: /Java/容器/Docker/5_Docker部署jar包.md
---

# Docker部署jar包

https://blog.csdn.net/wanshuai12138/article/details/123376307

https://blog.csdn.net/weixin\_49171365/article/details/131031451

https://www.cnblogs.com/Howinfun/p/11658516.html

https://blog.csdn.net/y1391625461/article/details/119803859

https://blog.csdn.net/chengxuyuan316/article/details/125773171

---

---
url: /Java/容器/Docker/1_Docker常用命令合集.md
---

# Docker常用命令合集

* 搜索镜像：docker search name

  镜像官网：https://hub.docker.com

* 构建容器：docker run -itd --name=mycentos centos:7
  * **-i** ：表示以交互模式运行容器（让容器的标准输入保持打开）
  * **-d**：表示后台运行容器，并返回容器ID
  * **-t**：为容器重新分配一个伪输入终端 { "registry-mirrors": \["https://5xok66d4.mirror.aliyuncs.com"] }
  * **--name**：为容器指定名称 查看本地所有的容器：docker ps -a

* 查看本地所有的容器：**docker ps -a**

* 查看本地正在运行的容器：**docker ps**

* 停止容器：**docker stop id或name**

* 一次性停止所有容器：**docker stop $(docker ps -a -q)**

* 启动容器：**docker start id或name**\*\*

* 重启容器：**docker restart id或name**

* 删除容器：**docker rm id或name**

* 强制删除容器：**docker rmi -f id或name**

* 查看容器详细信息：**docker inspect id或name**

* 进入容器：**docker exec -it id /bin/bash**

* 获取所有容器的id：**docker ps -a -q** 或 **docker ps -aq**

* 查看容器日志：**docker ps logs id/name**

* 动态查看日志：**docker ps logs -f id/name**

* 查看容器创建的网络：**docker network ls**

* 删除网络：**docker network rm container\_name\_net**

容器启动之attached和detached模式

两种模式最简单的对比理解就是：attached模式在前台运行，detached模式在后台运行。

detached模式的开启方法，就是加一个参数`-d`或者`--detach`，一般我们采用的都是这种方式，命令如下：

```shell
docker run -d -p 80:80 nginx
```

attached模式可能调试起来更为方便，因此Docker也提供了detached模式转换attached模式：`docker attach id/name`

镜像的导入与导出

* 镜像的导出：**docker image save 镜像名称:版本 -o 导出的文件名**
* 镜像的导入：**docker image load -i 镜像地址+名称**

---

---
url: /Java/容器/Docker/0_Docker介绍及使用.md
---

# 初识Docker

## 一、初识Docker

### 1.1.什么是Docker

微服务虽然具备各种各样的优势，但服务的拆分通用给部署带来了很大的麻烦。

* 分布式系统中，依赖的组件非常多，不同组件之间部署时往往会产生一些冲突。
* 在数百上千台服务中重复部署，环境不一定一致，会遇到各种问题

#### 1.1.1.应用部署的环境问题

大型项目组件较多，运行环境也较为复杂，部署时会碰到一些问题：

* 依赖关系复杂，容易出现兼容性问题

* 开发、测试、生产环境有差异

![image-20210731141907366](/assets/image-20210731141907366.bz1NiLB1.png)

例如一个项目中，部署时需要依赖于node.js、Redis、RabbitMQ、MySQL等，这些服务部署时所需要的函数库、依赖项各不相同，甚至会有冲突。给部署带来了极大的困难。

#### 1.1.2.Docker解决依赖兼容问题

而Docker确巧妙的解决了这些问题，Docker是如何实现的呢？

Docker为了解决依赖的兼容问题的，采用了两个手段：

* 将应用的Libs（函数库）、Deps（依赖）、配置与应用一起打包

* 将每个应用放到一个隔离**容器**去运行，避免互相干扰

![image-20210731142219735](/assets/image-20210731142219735.BeZY24_X.png)

这样打包好的应用包中，既包含应用本身，也保护应用所需要的Libs、Deps，无需再操作系统上安装这些，自然就不存在不同应用之间的兼容问题了。

虽然解决了不同应用的兼容问题，但是开发、测试等环境会存在差异，操作系统版本也会有差异，怎么解决这些问题呢？

#### 1.1.3.Docker解决操作系统环境差异

要解决不同操作系统环境差异问题，必须先了解操作系统结构。以一个Ubuntu操作系统为例，结构如下：

![image-20210731143401460](/assets/image-20210731143401460.6BUYamft.png)

结构包括：

* 计算机硬件：例如CPU、内存、磁盘等
* 系统内核：所有Linux发行版的内核都是Linux，例如CentOS、Ubuntu、Fedora等。内核可以与计算机硬件交互，对外提供**内核指令**，用于操作计算机硬件。
* 系统应用：操作系统本身提供的应用、函数库。这些函数库是对内核指令的封装，使用更加方便。

应用于计算机交互的流程如下：

1）应用调用操作系统应用（函数库），实现各种功能

2）系统函数库是对内核指令集的封装，会调用内核指令

3）内核指令操作计算机硬件

Ubuntu和CentOSpringBoot都是基于Linux内核，无非是系统应用不同，提供的函数库有差异：

![image-20210731144304990](/assets/image-20210731144304990.DGxh3tPs.png)

此时，如果将一个Ubuntu版本的MySQL应用安装到CentOS系统，MySQL在调用Ubuntu函数库时，会发现找不到或者不匹配，就会报错了：

![image-20210731144458680](/assets/image-20210731144458680.DhfRRG8n.png)

Docker如何解决不同系统环境的问题？

* Docker将用户程序与所需要调用的系统(比如Ubuntu)函数库一起打包
* Docker运行到不同操作系统时，直接基于打包的函数库，借助于操作系统的Linux内核来运行

如图：

![image-20210731144820638](/assets/image-20210731144820638.DNhGmled.png)

#### 1.1.4.小结

Docker如何解决大型项目依赖关系复杂，不同组件依赖的兼容性问题？

* Docker允许开发中将应用、依赖、函数库、配置一起**打包**，形成可移植镜像
* Docker应用运行在容器中，使用沙箱机制，相互**隔离**

Docker如何解决开发、测试、生产环境有差异的问题？

* Docker镜像中包含完整运行环境，包括系统函数库，仅依赖系统的Linux内核，因此可以在任意Linux操作系统上运行

Docker是一个快速交付应用、运行应用的技术，具备下列优势：

* 可以将程序及其依赖、运行环境一起打包为一个镜像，可以迁移到任意Linux操作系统
* 运行时利用沙箱机制形成隔离容器，各个应用互不干扰
* 启动、移除都可以通过一行命令完成，方便快捷

### 1.2.Docker和虚拟机的区别

Docker可以让一个应用在任何操作系统中非常方便的运行。而以前我们接触的虚拟机，也能在一个操作系统中，运行另外一个操作系统，保护系统中的任何应用。

两者有什么差异呢？

**虚拟机**（virtual machine）是在操作系统中**模拟**硬件设备，然后运行另一个操作系统，比如在 Windows 系统里面运行 Ubuntu 系统，这样就可以运行任意的Ubuntu应用了。

**Docker**仅仅是封装函数库，并没有模拟完整的操作系统，如图：

![image-20210731145914960](/assets/image-20210731145914960.DHPFwEBe.png)

对比来看：

![image-20210731152243765](/assets/image-20210731152243765.Duv1P9Eh.png)

小结：

Docker和虚拟机的差异：

* docker是一个系统进程；虚拟机是在操作系统中的操作系统

* docker体积小、启动速度快、性能好；虚拟机体积大、启动速度慢、性能一般

### 1.3.Docker架构

#### 1.3.1.镜像和容器

Docker中有几个重要的概念：

**镜像（Image）**：Docker将应用程序及其所需的依赖、函数库、环境、配置等文件打包在一起，称为镜像。

**容器（Container）**：镜像中的应用程序运行后形成的进程就是**容器**，只是Docker会给容器进程做隔离，对外不可见。

一切应用最终都是代码组成，都是硬盘中的一个个的字节形成的**文件**。只有运行时，才会加载到内存，形成进程。

而**镜像**，就是把一个应用在硬盘上的文件、及其运行环境、部分系统函数库文件一起打包形成的文件包。这个文件包是只读的。

**容器**呢，就是将这些文件中编写的程序、函数加载到内存中允许，形成进程，只不过要隔离起来。因此一个镜像可以启动多次，形成多个容器进程。

![image-20210731153059464](/assets/image-20210731153059464.DD4FQgQg.png)

例如你下载了一个QQ，如果我们将QQ在磁盘上的运行**文件**及其运行的操作系统依赖打包，形成QQ镜像。然后你可以启动多次，双开、甚至三开QQ，跟多个妹子聊天。

#### 1.3.2.DockerHub

开源应用程序非常多，打包这些应用往往是重复的劳动。为了避免这些重复劳动，人们就会将自己打包的应用镜像，例如Redis、MySQL镜像放到网络上，共享使用，就像GitHub的代码共享一样。

* DockerHub：DockerHub是一个官方的Docker镜像的托管平台。这样的平台称为Docker Registry。

* 国内也有类似于DockerHub 的公开服务，比如 [网易云镜像服务](https://c.163yun.com/hub)、[阿里云镜像库](https://cr.console.aliyun.com/)等。

我们一方面可以将自己的镜像共享到DockerHub，另一方面也可以从DockerHub拉取镜像：

![image-20210731153743354](/assets/image-20210731153743354._vBA-4CV.png)

#### 1.3.3.Docker架构

我们要使用Docker来操作镜像、容器，就必须要安装Docker。

Docker是一个CS架构的程序，由两部分组成：

* 服务端(server)：Docker守护进程，负责处理Docker指令，管理镜像、容器等

* 客户端(client)：通过命令或RestAPI向Docker服务端发送指令。可以在本地或远程向服务端发送指令。

如图：

![image-20210731154257653](/assets/image-20210731154257653.DWoqGEkZ.png)

#### 1.3.4.小结

镜像：

* 将应用程序及其依赖、环境、配置打包在一起

容器：

* 镜像运行起来就是容器，一个镜像可以运行多个容器

Docker结构：

* 服务端：接收命令或远程请求，操作镜像或容器

* 客户端：发送命令或者请求到Docker服务端

DockerHub：

* 一个镜像托管的服务器，类似的还有阿里云镜像服务，统称为DockerRegistry

## 二、Docker的基本操作

### 2.1.镜像操作

#### 2.1.1.镜像名称

首先来看下镜像的名称组成：

* 镜名称一般分两部分组成：\[repository]:\[tag]。
* 在没有指定tag时，默认是latest，代表最新版本的镜像

如图：

![image-20210731155141362](/assets/image-20210731155141362.CM_szGpR.png)

这里的mysql就是repository，5.7就是tag，合一起就是镜像名称，代表5.7版本的MySQL镜像。

#### 2.1.2.镜像命令

常见的镜像操作命令如图：

![image-20210731155649535](/assets/image-20210731155649535.BEeuld5s.png)

#### 2.1.3.案例1-拉取、查看镜像

需求：从DockerHub中拉取一个nginx镜像并查看

1）首先去镜像仓库搜索nginx镜像，比如[DockerHub](https://hub.docker.com/):

![image-20210731155844368](/assets/image-20210731155844368.D5CfaYkH.png)

2）根据查看到的镜像名称，拉取自己需要的镜像，通过命令：docker pull nginx

![image-20210731155856199](/assets/image-20210731155856199.CheEEWJE.png)

3）通过命令：docker images 查看拉取到的镜像

![image-20210731155903037](/assets/image-20210731155903037.Vie9MmWd.png)

#### 2.1.4.案例2-保存、导入镜像

需求：利用docker save将nginx镜像导出磁盘，然后再通过load加载回来

1）利用docker xx --help命令查看docker save和docker load的语法

例如，查看save命令用法，可以输入命令：

```sh
docker save --help
```

结果：

![image-20210731161104732](/assets/image-20210731161104732.DDZcorus.png)

命令格式：

```shell
docker save -o [保存的目标文件名称] [镜像名称]
```

2）使用docker save导出镜像到磁盘

运行命令：

```sh
docker save -o nginx.tar nginx:latest
```

结果如图：

![image-20210731161354344](/assets/image-20210731161354344.BSxjIMrI.png)

3）使用docker load加载镜像

先删除本地的nginx镜像：

```sh
docker rmi nginx:latest
```

然后运行命令，加载本地文件：

```sh
docker load -i nginx.tar
```

结果：

![image-20210731161746245](/assets/image-20210731161746245.C5RTvP0b.png)

#### 2.1.5.练习

需求：去DockerHub搜索并拉取一个Redis镜像

步骤：

1）去DockerHub搜索Redis镜像

2）查看Redis镜像的名称和版本

3）利用docker pull命令拉取镜像

4）利用docker save命令将 redis:latest打包为一个redis.tar包

5）利用docker rmi 删除本地的redis:latest

6）利用docker load 重新加载 redis.tar文件

### 2.2.容器操作

#### 2.2.1.容器相关命令

容器操作的命令如图：

![image-20210731161950495](/assets/image-20210731161950495.C32wK6d5.png)

容器保护三个状态：

* 运行：进程正常运行
* 暂停：进程暂停，CPU不再运行，并不释放内存
* 停止：进程终止，回收进程占用的内存、CPU等资源

其中：

* docker run：创建并运行一个容器，处于运行状态

* docker pause：让一个运行的容器暂停

* docker unpause：让一个容器从暂停状态恢复运行

* docker stop：停止一个运行的容器

* docker start：让一个停止的容器再次运行

* docker rm：删除一个容器

#### 2.2.2.案例-创建并运行一个容器

创建并运行nginx容器的命令：

```sh
docker run --name containerName -p 80:80 -d nginx
```

命令解读：

* docker run ：创建并运行一个容器
* \--name : 给容器起一个名字，比如叫做mn
* -p ：将宿主机端口与容器端口映射，冒号左侧是宿主机端口，右侧是容器端口
* -d：后台运行容器
* nginx：镜像名称，例如nginx

这里的`-p`参数，是将容器端口映射到宿主机端口。

默认情况下，容器是隔离环境，我们直接访问宿主机的80端口，肯定访问不到容器中的nginx。

现在，将容器的80与宿主机的80关联起来，当我们访问宿主机的80端口时，就会被映射到容器的80，这样就能访问到nginx了：

![image-20210731163255863](/assets/image-20210731163255863.Bk1BjPAo.png)

#### 2.2.3.案例-进入容器，修改文件

**需求**：进入Nginx容器，修改HTML文件内容，添加“传智教育欢迎您”

**提示**：进入容器要用到docker exec命令。

**步骤**：

1）进入容器。进入我们刚刚创建的nginx容器的命令为：

```sh
docker exec -it mn bash
```

命令解读：

* docker exec ：进入容器内部，执行一个命令

* -it : 给当前进入的容器创建一个标准输入、输出终端，允许我们与容器交互

* mn ：要进入的容器的名称

* bash：进入容器后执行的命令，bash是一个linux终端交互命令

2）进入nginx的HTML所在目录 /usr/share/nginx/html

容器内部会模拟一个独立的Linux文件系统，看起来如同一个linux服务器一样：

![image-20210731164159811](/assets/image-20210731164159811.CStElVgm.png)

nginx的环境、配置、运行文件全部都在这个文件系统中，包括我们要修改的html文件。

查看DockerHub网站中的nginx页面，可以知道nginx的html目录位置在`/usr/share/nginx/html`

我们执行命令，进入该目录：

```sh
cd /usr/share/nginx/html
```

查看目录下文件：

![image-20210731164455818](/assets/image-20210731164455818.Cjjww6EL.png)

3）修改index.html的内容

容器内没有vi命令，无法直接修改，我们用下面的命令来修改：

```sh
sed -i -e 's#Welcome to nginx#传智教育欢迎您#g' -e 's#<head>#<head><meta charset="utf-8">#g' index.html
```

在浏览器访问自己的虚拟机地址，例如我的是：http://192.168.150.101，即可看到结果：

![image-20210731164717604](/assets/image-20210731164717604.CEOl8RSh.png)

#### 2.2.4.小结

docker run命令的常见参数有哪些？

* \--name：指定容器名称
* -p：指定端口映射
* -d：让容器后台运行

查看容器日志的命令：

* docker logs
* 添加 -f 参数可以持续查看日志

查看容器状态：

* docker ps
* docker ps -a 查看所有容器，包括已经停止的

### 2.3.数据卷（容器数据管理）

在之前的nginx案例中，修改nginx的html页面时，需要进入nginx内部。并且因为没有编辑器，修改文件也很麻烦。

这就是因为容器与数据（容器内文件）耦合带来的后果。

![image-20210731172440275](/assets/image-20210731172440275.BqipVZjS.png)

要解决这个问题，必须将数据与容器解耦，这就要用到数据卷了。

#### 2.3.1.什么是数据卷

\*\*数据卷（volume）\*\*是一个虚拟目录，指向宿主机文件系统中的某个目录。

![image-20210731173541846](/assets/image-20210731173541846.BSs8iNsM.png)

一旦完成数据卷挂载，对容器的一切操作都会作用在数据卷对应的宿主机目录了。

这样，我们操作宿主机的/var/lib/docker/volumes/html目录，就等于操作容器内的/usr/share/nginx/html目录了

#### 2.3.2.数据集操作命令

数据卷操作的基本语法如下：

```sh
docker volume [COMMAND]
```

docker volume命令是数据卷操作，根据命令后跟随的command来确定下一步的操作：

* create 创建一个volume
* inspect 显示一个或多个volume的信息
* ls 列出所有的volume
* prune 删除未使用的volume
* rm 删除一个或多个指定的volume

#### 2.3.3.创建和查看数据卷

**需求**：创建一个数据卷，并查看数据卷在宿主机的目录位置

① 创建数据卷

```sh
docker volume create html
```

② 查看所有数据

```sh
docker volume ls
```

结果：

![image-20210731173746910](/assets/image-20210731173746910.BC6oDeU2.png)

③ 查看数据卷详细信息卷

```sh
docker volume inspect html
```

结果：

![image-20210731173809877](/assets/image-20210731173809877.BuXU9ut3.png)

可以看到，我们创建的html这个数据卷关联的宿主机目录为`/var/lib/docker/volumes/html/_data`目录。

**小结**：

数据卷的作用：

* 将容器与数据分离，解耦合，方便操作容器内数据，保证数据安全

数据卷操作：

* docker volume create：创建数据卷
* docker volume ls：查看所有数据卷
* docker volume inspect：查看数据卷详细信息，包括关联的宿主机目录位置
* docker volume rm：删除指定数据卷
* docker volume prune：删除所有未使用的数据卷

#### 2.3.4.挂载数据卷

我们在创建容器时，可以通过 -v 参数来挂载一个数据卷到某个容器内目录，命令格式如下：

```sh
docker run \
  --name mn \
  -v html:/root/html \
  -p 8080:80
  nginx \
```

这里的-v就是挂载数据卷的命令：

* `-v html:/root/htm` ：把html数据卷挂载到容器内的/root/html这个目录中

#### 2.3.5.案例-给nginx挂载数据卷

**需求**：创建一个nginx容器，修改容器内的html目录内的index.html内容

**分析**：上个案例中，我们进入nginx容器内部，已经知道nginx的html目录所在位置/usr/share/nginx/html ，我们需要把这个目录挂载到html这个数据卷上，方便操作其中的内容。

**提示**：运行容器时使用 -v 参数挂载数据卷

步骤：

① 创建容器并挂载数据卷到容器内的HTML目录

```sh
docker run --name mn -v html:/usr/share/nginx/html -p 80:80 -d nginx
```

② 进入html数据卷所在位置，并修改HTML内容

```sh
# 查看html数据卷的位置
docker volume inspect html
# 进入该目录
cd /var/lib/docker/volumes/html/_data
# 修改文件
vi index.html
```

#### 2.3.6.案例-给MySQL挂载本地目录

容器不仅仅可以挂载数据卷，也可以直接挂载到宿主机目录上。关联关系如下：

* 带数据卷模式：宿主机目录 --> 数据卷 ---> 容器内目录
* 直接挂载模式：宿主机目录 ---> 容器内目录

如图：

![image-20210731175155453](/assets/image-20210731175155453.6uFQ1-rn.png)

**语法**：

目录挂载与数据卷挂载的语法是类似的：

* -v \[宿主机目录]:\[容器内目录]
* -v \[宿主机文件]:\[容器内文件]

**需求**：创建并运行一个MySQL容器，将宿主机目录直接挂载到容器

实现思路如下：

1）在将课前资料中的mysql.tar文件上传到虚拟机，通过load命令加载为镜像

2）创建目录/tmp/mysql/data

3）创建目录/tmp/mysql/conf，将课前资料提供的hmy.cnf文件上传到/tmp/mysql/conf

4）去DockerHub查阅资料，创建并运行MySQL容器，要求：

① 挂载/tmp/mysql/data到mysql容器内数据存储目录

② 挂载/tmp/mysql/conf/hmy.cnf到mysql容器的配置文件

③ 设置MySQL密码

#### 2.3.7.小结

docker run的命令中通过 -v 参数挂载文件或目录到容器中：

* -v volume名称:容器内目录
* -v 宿主机文件:容器内文
* -v 宿主机目录:容器内目录

数据卷挂载与目录直接挂载的

* 数据卷挂载耦合度低，由docker来管理目录，但是目录较深，不好找
* 目录挂载耦合度高，需要我们自己管理目录，不过目录容易寻找查看

## 三、Dockerfile自定义镜像

常见的镜像在DockerHub就能找到，但是我们自己写的项目就必须自己构建镜像了。

而要自定义镜像，就必须先了解镜像的结构才行。

### 3.1.镜像结构

镜像是将应用程序及其需要的系统函数库、环境、配置、依赖打包而成。

我们以MySQL为例，来看看镜像的组成结构：

![image-20210731175806273](/assets/image-20210731175806273.Dgfyczup.png)

简单来说，镜像就是在系统函数库、运行环境基础上，添加应用程序文件、配置文件、依赖文件等组合，然后编写好启动脚本打包在一起形成的文件。

我们要构建镜像，其实就是实现上述打包的过程。

### 3.2.Dockerfile语法

构建自定义的镜像时，并不需要一个个文件去拷贝，打包。

我们只需要告诉Docker，我们的镜像的组成，需要哪些BaseImage、需要拷贝什么文件、需要安装什么依赖、启动脚本是什么，将来Docker会帮助我们构建镜像。

而描述上述信息的文件就是Dockerfile文件。

**Dockerfile**就是一个文本文件，其中包含一个个的**指令(Instruction)**，用指令来说明要执行什么操作来构建镜像。每一个指令都会形成一层Layer。

![image-20210731180321133](/assets/image-20210731180321133.La_eZCKP.png)

更新详细语法说明，请参考官网文档： https://docs.docker.com/engine/reference/builder

### 3.3.构建Java项目

#### 3.3.1.基于Ubuntu构建Java项目

需求：基于Ubuntu镜像构建一个新镜像，运行一个java项目

* 步骤1：新建一个空文件夹docker-demo

  ![image-20210801101207444](/assets/image-20210801101207444.C-5C4OTb.png)

* 步骤2：拷贝课前资料中的docker-demo.jar文件到docker-demo这个目录

  ![image-20210801101314816](/assets/image-20210801101314816.CazyYwLi.png)

* 步骤3：拷贝课前资料中的jdk8.tar.gz文件到docker-demo这个目录

  ![image-20210801101410200](/assets/image-20210801101410200.ClaTKsy7.png)

* 步骤4：拷贝课前资料提供的Dockerfile到docker-demo这个目录

  ![image-20210801101455590](/assets/image-20210801101455590.CO7nVzkf.png)

  其中的内容如下：

  ```dockerfile
  # 指定基础镜像
  FROM ubuntu:16.04
  # 配置环境变量，JDK的安装目录
  ENV JAVA_DIR=/usr/local

  # 拷贝jdk和java项目的包
  COPY ./jdk8.tar.gz $JAVA_DIR/
  COPY ./docker-demo.jar /tmp/app.jar

  # 安装JDK
  RUN cd $JAVA_DIR \
   && tar -xf ./jdk8.tar.gz \
   && mv ./jdk1.8.0_144 ./java8

  # 配置环境变量
  ENV JAVA_HOME=$JAVA_DIR/java8
  ENV PATH=$PATH:$JAVA_HOME/bin

  # 暴露端口
  EXPOSE 8090
  # 入口，java项目的启动命令
  ENTRYPOINT java -jar /tmp/app.jar
  ```

* 步骤5：进入docker-demo

  将准备好的docker-demo上传到虚拟机任意目录，然后进入docker-demo目录下

* 步骤6：运行命令：

  ```sh
  docker build -t javaweb:1.0 .
  ```

最后访问 http://192.168.150.101:8090/hello/count，其中的ip改成你的虚拟机ip

#### 3.3.2.基于java8构建Java项目

虽然我们可以基于Ubuntu基础镜像，添加任意自己需要的安装包，构建镜像，但是却比较麻烦。所以大多数情况下，我们都可以在一些安装了部分软件的基础镜像上做改造。

例如，构建java项目的镜像，可以在已经准备了JDK的基础镜像基础上构建。

需求：基于java:8-alpine镜像，将一个Java项目构建为镜像

实现思路如下：

* ① 新建一个空的目录，然后在目录中新建一个文件，命名为Dockerfile

* ② 拷贝课前资料提供的docker-demo.jar到这个目录中

* ③ 编写Dockerfile文件：

  * a ）基于java:8-alpine作为基础镜像

  * b ）将app.jar拷贝到镜像中

  * c ）暴露端口

  * d ）编写入口ENTRYPOINT

    内容如下：

    ```dockerfile
    FROM java:8-alpine
    COPY ./app.jar /tmp/app.jar
    EXPOSE 8090
    ENTRYPOINT java -jar /tmp/app.jar
    ```

* ④ 使用docker build命令构建镜像

* ⑤ 使用docker run创建容器并运行

### 3.4.小结

小结：

1. Dockerfile的本质是一个文件，通过指令描述镜像的构建过程

2. Dockerfile的第一行必须是FROM，从一个基础镜像来构建

3. 基础镜像可以是基本操作系统，如Ubuntu。也可以是其他人制作好的镜像，例如：java:8-alpine

## 四、Docker-Compose

Docker Compose可以基于Compose文件帮我们快速的部署分布式应用，而无需手动一个个创建和运行容器！

![image-20210731180921742](/assets/image-20210731180921742.DT3f5Foc.png)

### 4.1.初识DockerCompose

Compose文件是一个文本文件，通过指令定义集群中的每个容器如何运行。格式如下：

```json
version: "3.8"
 services:
  mysql:
    image: mysql:5.7.25
    environment:
     MYSQL_ROOT_PASSWORD: 123 
    volumes:
     - "/tmp/mysql/data:/var/lib/mysql"
     - "/tmp/mysql/conf/hmy.cnf:/etc/mysql/conf.d/hmy.cnf"
  web:
    build: .
    ports:
     - "8090:8090"

```

上面的Compose文件就描述一个项目，其中包含两个容器：

* mysql：一个基于`mysql:5.7.25`镜像构建的容器，并且挂载了两个目录
* web：一个基于`docker build`临时构建的镜像容器，映射端口时8090

DockerCompose的详细语法参考官网：https://docs.docker.com/compose/compose-file/

其实DockerCompose文件可以看做是将多个docker run命令写到一个文件，只是语法稍有差异。

### 4.3.部署微服务集群

**需求**：将之前学习的cloud-demo微服务集群利用DockerCompose部署

**实现思路**：

① 查看课前资料提供的cloud-demo文件夹，里面已经编写好了docker-compose文件

② 修改自己的cloud-demo项目，将数据库、nacos地址都命名为docker-compose中的服务名

③ 使用maven打包工具，将项目中的每个微服务都打包为app.jar

④ 将打包好的app.jar拷贝到cloud-demo中的每一个对应的子目录中

⑤ 将cloud-demo上传至虚拟机，利用 docker-compose up -d 来部署

#### 4.3.1.compose文件

查看课前资料提供的cloud-demo文件夹，里面已经编写好了docker-compose文件，而且每个微服务都准备了一个独立的目录：

![image-20210731181341330](/assets/image-20210731181341330.DYamaTtf.png)

内容如下：

```yaml
version: "3.2"

services:
  nacos:
    image: nacos/nacos-server
    environment:
      MODE: standalone
    ports:
      - "8848:8848"
  mysql:
    image: mysql:5.7.25
    environment:
      MYSQL_ROOT_PASSWORD: 123
    volumes:
      - "$PWD/mysql/data:/var/lib/mysql"
      - "$PWD/mysql/conf:/etc/mysql/conf.d/"
  userservice:
    build: ./user-service
  orderservice:
    build: ./order-service
  gateway:
    build: ./gateway
    ports:
      - "10010:10010"
```

可以看到，其中包含5个service服务：

* `nacos`：作为注册中心和配置中心
  * `image: nacos/nacos-server`： 基于nacos/nacos-server镜像构建
  * `environment`：环境变量
    * `MODE: standalone`：单点模式启动
  * `ports`：端口映射，这里暴露了8848端口
* `mysql`：数据库
  * `image: mysql:5.7.25`：镜像版本是mysql:5.7.25
  * `environment`：环境变量
    * `MYSQL_ROOT_PASSWORD: 123`：设置数据库root账户的密码为123
  * `volumes`：数据卷挂载，这里挂载了mysql的data、conf目录，其中有我提前准备好的数据
* `userservice`、`orderservice`、`gateway`：都是基于Dockerfile临时构建的

查看mysql目录，可以看到其中已经准备好了cloud\_order、cloud\_user表：

![image-20210801095205034](/assets/image-20210801095205034.BpYFABAJ.png)

查看微服务目录，可以看到都包含Dockerfile文件：

![image-20210801095320586](/assets/image-20210801095320586.BlI4pOFq.png)

内容如下：

```dockerfile
FROM java:8-alpine
COPY ./app.jar /tmp/app.jar
ENTRYPOINT java -jar /tmp/app.jar
```

#### 4.3.2.修改微服务配置

因为微服务将来要部署为docker容器，而容器之间互联不是通过IP地址，而是通过容器名。这里我们将order-service、user-service、gateway服务的mysql、nacos地址都修改为基于容器名的访问。

如下所示：

```yaml
spring:
  datasource:
    url: jdbc:mysql://mysql:3306/cloud_order?useSSL=false
    username: root
    password: 123
    driver-class-name: com.mysql.jdbc.Driver
  application:
    name: orderservice
  cloud:
    nacos:
      server-addr: nacos:8848 # nacos服务地址
```

#### 4.3.3.打包

接下来需要将我们的每个微服务都打包。因为之前查看到Dockerfile中的jar包名称都是app.jar，因此我们的每个微服务都需要用这个名称。

可以通过修改pom.xml中的打包名称来实现，每个微服务都需要修改：

```xml
<build>
  <!-- 服务打包的最终名称 -->
  <finalName>app</finalName>
  <plugins>
    <plugin>
      <groupId>org.springframework.boot</groupId>
      <artifactId>spring-boot-maven-plugin</artifactId>
    </plugin>
  </plugins>
</build>
```

打包后：

![image-20210801095951030](/assets/image-20210801095951030.ChTIDgaC.png)

#### 4.3.4.拷贝jar包到部署目录

编译打包好的app.jar文件，需要放到Dockerfile的同级目录中。注意：每个微服务的app.jar放到与服务名称对应的目录，别搞错了。

user-service：

![image-20210801100201253](/assets/image-20210801100201253.DUuQU8gm.png)

order-service：

![image-20210801100231495](/assets/image-20210801100231495.-dhSm7WT.png)

gateway：

![image-20210801100308102](/assets/image-20210801100308102.DafGNR6_.png)

#### 4.3.5.部署

最后，我们需要将文件整个cloud-demo文件夹上传到虚拟机中，理由DockerCompose部署。

上传到任意目录：

![image-20210801100955653](/assets/image-20210801100955653.DMZWbsG4.png)

部署：

进入cloud-demo目录，然后运行下面的命令：

```sh
docker-compose up -d
```

## 五、Docker镜像仓库

### 5.1.搭建私有镜像仓库

### 5.2.推送、拉取镜像

推送镜像到私有镜像服务必须先tag，步骤如下：

① 重新tag本地镜像，名称前缀为私有仓库的地址：192.168.150.101:8080/

```sh
docker tag nginx:latest 192.168.150.101:8080/nginx:1.0 
```

② 推送镜像

```sh
docker push 192.168.150.101:8080/nginx:1.0 
```

③ 拉取镜像

```sh
docker pull 192.168.150.101:8080/nginx:1.0 
```

---

---
url: /Java/解决方案/数据同步/4_DolphinScheduler.md
---

# DolphinScheduler

https://juejin.cn/post/7086854492016082980

https://www.cnblogs.com/DolphinScheduler/p/18989962

---

---
url: /常用框架/EasyExcel/0_EasyExcel概述.md
---

# EasyExcel概述

> 官方文档：https://easyexcel.opensource.alibaba.com

### 什么是Alibaba-EasyExcel

EasyExcel是阿里巴巴开源的一个excel处理框架，**以使用简单、节省内存著称**。EasyExcel能大大减少占用内存的主要原因是在解析Excel时没有将文件数据一次性全部加载到内存中，而是从磁盘上一行行读取数据，逐个解析。

### EasyExcel与其它框架的区别

Apache poi、jxl等处理Excel的框架，他们都存在一个严重的问题就是非常的耗内存。如果你的系统并发量不大的话可能还行，但是一旦并发上来后一定会OOM或者JVM频繁的full gc。而EasyExcel采用一行一行的解析模式，并将一行的解析结果以观察者的模式通知处理（AnalysisEventListener）。

### 参考资料

https://zhuanlan.zhihu.com/p/471712987

https://blog.csdn.net/chire\_jr/article/details/106492700

https://max.book118.com/html/2023/0510/8133104073005065.shtm

https://www.lsjlt.com/news/178303.html

https://blog.csdn.net/weixin\_43949154/article/details/122071510

---

---
url: /常用框架/EasyExcel/2_EasyExcel入门之导入Excel.md
---

# EasyExcel入门之导出Excel

### 一、创建监听器

```java
/**
 * 监听器
 * 有个很重要的点 DemoDataListener 不能被spring管理，要每次读取excel都要new,然后里面用到spring可以构造方法传进去
 *
 * @author xxl
 * @date 2024/5/27 23:23
 */
@Slf4j
public class DemoDataListener implements ReadListener<ExportExcelEntity> {

    /**
     * 每隔5条存储数据库，实际使用中可以100条，然后清理list ，方便内存回收
     */
    private static final int BATCH_COUNT = 100;
    /**
     * 缓存的数据
     */
    private List<ExportExcelEntity> cachedDataList = ListUtils.newArrayListWithExpectedSize(BATCH_COUNT);

    /**
     * 假设这个是一个DAO，当然有业务逻辑这个也可以是一个service。当然如果不用存储这个对象没用。
     */
    //private DemoDAO demoDAO;
    public DemoDataListener() {
        // 这里是demo，所以随便new一个。实际使用如果到了spring,请使用下面的有参构造函数
        //demoDAO = new DemoDAO();
    }

    /**
     * 如果使用了spring,请使用这个构造方法。每次创建Listener的时候需要把spring管理的类传进来
     *
     * @param demoDAO
     */
//    public DemoDataListener(DemoDAO demoDAO) {
//        this.demoDAO = demoDAO;
//    }

    /**
     * 这个每一条数据解析都会来调用
     *
     * @param data    one row value. Is is same as {@link AnalysisContext#readRowHolder()}
     * @param context
     */
    @Override
    public void invoke(ExportExcelEntity data, AnalysisContext context) {
//        log.info("解析到一条数据:{}", JSON.toJSONString(data));
        log.info("解析到一条数据:{}", data);
        cachedDataList.add(data);
        // 达到BATCH_COUNT了，需要去存储一次数据库，防止数据几万条数据在内存，容易OOM
        if (cachedDataList.size() >= BATCH_COUNT) {
            saveData();
            // 存储完成清理 list
            cachedDataList = ListUtils.newArrayListWithExpectedSize(BATCH_COUNT);
        }
    }

    /**
     * 所有数据解析完成了 都会来调用
     *
     * @param context
     */
    @Override
    public void doAfterAllAnalysed(AnalysisContext context) {
        // 这里也要保存数据，确保最后遗留的数据也存储到数据库
        saveData();
        log.info("所有数据解析完成！");
    }

    /**
     * 加上存储数据库
     */
    private void saveData() {
        log.info("{}条数据，开始存储数据库！", cachedDataList.size());
//        demoDAO.save(cachedDataList);
        log.info("存储数据库成功！");
    }

    @Override
    public void onException(Exception exception, AnalysisContext context) {
        log.error("解析失败，但是继续解析下一行:{}", exception.getMessage());
        // 如果是某一个单元格的转换异常 能获取到具体行号
        // 如果要获取头的信息 配合invokeHeadMap使用
        if (exception instanceof ExcelDataConvertException) {
            ExcelDataConvertException excelDataConvertException = (ExcelDataConvertException) exception;
            log.error("第{}行，第{}列解析异常，数据为:{}", excelDataConvertException.getRowIndex(), excelDataConvertException.getColumnIndex(), excelDataConvertException.getCellData());
            // TODO 实际业务处理参考：将异常保存到数据库后配合消息队列或其他方法进行消息投递，告知用户哪里错了。
        }
    }

}

```

### 二、Controller层

```java
/**
 * 上传、导入
 *
 * @param file
 * @return
 * @throws IOException
 */
@PostMapping("/upload")
@ResponseBody
public String upload(MultipartFile file) throws IOException {
    EasyExcel.read(file.getInputStream(), ExportExcelEntity.class, new DemoDataListener()).sheet().doRead();
    // 如果配置了数据持久层
    //EasyExcel.read(file.getInputStream(), ExportExcelEntity.class, new DemoDataListener(demoService)).sheet().doRead();
    return "success";
}
```

---

---
url: /常用框架/EasyExcel/3_EasyExcel入门之导出Excel.md
---

# EasyExcel入门之导入Excel

### 简单导出

```java
/**
 * 最简单的写
 * <p>
 * 1. 创建excel对应的实体对象 参照{@link DemoData}
 * <p>
 * 2. 直接写即可
 */
@Test
public void simpleWrite() {
    // 注意 simpleWrite在数据量不大的情况下可以使用（5000以内，具体也要看实际情况），数据量大参照 重复多次写入

    // 写法1 JDK8+
    // since: 3.0.0-beta1
    String fileName = TestFileUtil.getPath() + "simpleWrite" + System.currentTimeMillis() + ".xlsx";
    // 这里 需要指定写用哪个class去写，然后写到第一个sheet，名字为模板 然后文件流会自动关闭
    // 如果这里想使用03 则 传入excelType参数即可
    EasyExcel.write(fileName, ExportExcelEntity.class)
        .sheet("模板")
        .doWrite(() -> {
            // 分页查询数据
            return data();
        });

    // 写法2
    fileName = TestFileUtil.getPath() + "simpleWrite" + System.currentTimeMillis() + ".xlsx";
    // 这里 需要指定写用哪个class去写，然后写到第一个sheet，名字为模板 然后文件流会自动关闭
    // 如果这里想使用03 则 传入excelType参数即可
    EasyExcel.write(fileName, ExportExcelEntity.class).sheet("模板").doWrite(data());

    // 写法3
    fileName = TestFileUtil.getPath() + "simpleWrite" + System.currentTimeMillis() + ".xlsx";
    // 这里 需要指定写用哪个class去写
    try (ExcelWriter excelWriter = EasyExcel.write(fileName, ExportExcelEntity.class).build()) {
        WriteSheet writeSheet = EasyExcel.writerSheet("模板").build();
        excelWriter.write(data(), writeSheet);
    }
}
```

### 重复多次写入(写到单个或者多个Sheet)

```java
/**
 * 重复多次写入
 * <p>
 * 1. 创建excel对应的实体对象 参照{@link ComplexHeadData}
 * <p>
 * 2. 使用{@link ExcelProperty}注解指定复杂的头
 * <p>
 * 3. 直接调用二次写入即可
 */
@Test
public void repeatedWrite() {
    // 方法1: 如果写到同一个sheet
    String fileName = TestFileUtil.getPath() + "repeatedWrite" + System.currentTimeMillis() + ".xlsx";
    // 这里 需要指定写用哪个class去写
    try (ExcelWriter excelWriter = EasyExcel.write(fileName, ExportExcelEntity.class).build()) {
        // 这里注意 如果同一个sheet只要创建一次
        WriteSheet writeSheet = EasyExcel.writerSheet("模板").build();
        // 去调用写入,这里我调用了五次，实际使用时根据数据库分页的总的页数来
        for (int i = 0; i < 5; i++) {
            // 分页去数据库查询数据 这里可以去数据库查询每一页的数据
            List<ExportExcelEntity> data = data();
            excelWriter.write(data, writeSheet);
        }
    }

    // 方法2: 如果写到不同的sheet 同一个对象
    fileName = TestFileUtil.getPath() + "repeatedWrite" + System.currentTimeMillis() + ".xlsx";
    // 这里 指定文件
    try (ExcelWriter excelWriter = EasyExcel.write(fileName, ExportExcelEntity.class).build()) {
        // 去调用写入,这里我调用了五次，实际使用时根据数据库分页的总的页数来。这里最终会写到5个sheet里面
        for (int i = 0; i < 5; i++) {
            // 每次都要创建writeSheet 这里注意必须指定sheetNo 而且sheetName必须不一样
            WriteSheet writeSheet = EasyExcel.writerSheet(i, "模板" + i).build();
            // 分页去数据库查询数据 这里可以去数据库查询每一页的数据
            List<ExportExcelEntity> data = data();
            excelWriter.write(data, writeSheet);
        }
    }

    // 方法3 如果写到不同的sheet 不同的对象
    fileName = TestFileUtil.getPath() + "repeatedWrite" + System.currentTimeMillis() + ".xlsx";
    // 这里 指定文件
    try (ExcelWriter excelWriter = EasyExcel.write(fileName).build()) {
        // 去调用写入,这里我调用了五次，实际使用时根据数据库分页的总的页数来。这里最终会写到5个sheet里面
        for (int i = 0; i < 5; i++) {
            // 每次都要创建writeSheet 这里注意必须指定sheetNo 而且sheetName必须不一样。这里注意DemoData.class 可以每次都变，我这里为了方便 所以用的同一个class
            // 实际上可以一直变
            WriteSheet writeSheet = EasyExcel.writerSheet(i, "模板" + i).head(ExportExcelEntity.class).build();
            // 分页去数据库查询数据 这里可以去数据库查询每一页的数据
            List<ExportExcelEntity> data = data();
            excelWriter.write(data, writeSheet);
        }
    }

}
```

### 指定列或排除列

```Java
@GetMapping("download/exclude")
public void downloadExcludeColumn(HttpServletResponse response) throws IOException {
    response.setContentType("application/vnd.openxmlformats-officedocument.spreadsheetml.sheet");
    response.setCharacterEncoding("utf-8");
    String fileName = URLEncoder.encode("测试", StandardCharsets.UTF_8).replaceAll("\\+", "%20");
    response.setHeader("Content-disposition", "attachment;filename*=utf-8''" + fileName + ".xlsx");

    // 根据用户传入字段，假设我们要忽略 userName
    Set<String> excludeColumnFiledNames = new HashSet<>();
    excludeColumnFiledNames.add("userName");
    // 忽略某些列，使其不导出 excludeColumnFiledNames
    EasyExcel.write(response.getOutputStream(), ExportExcelEntity.class).excludeColumnFiledNames(excludeColumnFiledNames).sheet("模板").doWrite(FakerUtil.generateStudentList(1000));
    
    // 根据用户传入字段，例如只导出userName和mobile列
    Set<String> includeColumnFiledNames = new HashSet<>();
    includeColumnFiledNames.add("userName");
    includeColumnFiledNames.add("mobile");
    // 只导出选中列 includeColumnFiledNames
    EasyExcel.write(response.getOutputStream(), ExportExcelEntity.class).includeColumnFiledNames(includeColumnFiledNames).sheet("模板").doWrite(FakerUtil.generateStudentList(1000));
}
```

其他涉及导出格式问题参考官方文档

复杂导出

https://segmentfault.com/a/1190000041261830#item-3-3

---

---
url: /常用框架/EasyExcel/4_EasyExcel入门之填充Excel.md
---

# EasyExcel入门之填充Excel

### 一、简单填充

```java
/**
 * 简单的填充
 */
@GetMapping("/simpleFill")
public void simpleFill() {
    // 模板注意 用{} 来表示你要用的变量 如果本来就有"{","}" 特殊字符 用"\{","\}"代替
    String templateFileName =
            TestFileUtil.getPath() + "demo" + File.separator + "fill" + File.separator + "simple.xlsx";

    // 方案1 根据对象填充
    String fileName = TestFileUtil.getPath() + "simpleFill" + System.currentTimeMillis() + ".xlsx";
    // 这里 会填充到第一个sheet， 然后文件流会自动关闭
    ExportExcelEntity fillData = new ExportExcelEntity();
    fillData.setUserName("张三");
    fillData.setMobile("13999999999");
    EasyExcel.write(fileName).withTemplate(templateFileName).sheet().doFill(fillData);

    // 方案2 根据Map填充
    fileName = TestFileUtil.getPath() + "simpleFill" + System.currentTimeMillis() + ".xlsx";
    // 这里 会填充到第一个sheet， 然后文件流会自动关闭
    Map<String, Object> map = MapUtils.newHashMap();
    map.put("name", "张三");
    map.put("number", "13999999999");
    EasyExcel.write(fileName).withTemplate(templateFileName).sheet().doFill(map);
}
```

### 二、列表填充

```java
/**
 * 列表填充
 */
@GetMapping("/listFill")
public void listFill() {
    // 模板注意 用{} 来表示你要用的变量 如果本来就有"{","}" 特殊字符 用"\{","\}"代替
    // 填充list 的时候还要注意 模板中{.} 多了个点 表示list
    // 如果填充list的对象是map,必须包涵所有list的key,哪怕数据为null，必须使用map.put(key,null)
    String templateFileName =
            TestFileUtil.getPath() + "demo" + File.separator + "fill" + File.separator + "list.xlsx";

    // 方案1 一下子全部放到内存里面 并填充
    String fileName = TestFileUtil.getPath() + "listFill" + System.currentTimeMillis() + ".xlsx";
    // 这里 会填充到第一个sheet， 然后文件流会自动关闭
    EasyExcel.write(fileName).withTemplate(templateFileName).sheet().doFill(FakerUtil.generateStudentList(1000));

    // 方案2 分多次 填充 会使用文件缓存（省内存）
    fileName = TestFileUtil.getPath() + "listFill" + System.currentTimeMillis() + ".xlsx";
    ExcelWriter excelWriter = EasyExcel.write(fileName).withTemplate(templateFileName).build();
    WriteSheet writeSheet = EasyExcel.writerSheet().build();
    excelWriter.fill(FakerUtil.generateStudentList(1000), writeSheet);
    excelWriter.fill(FakerUtil.generateStudentList(1000), writeSheet);
}
```

其他填充方式参考官方文档

---

---
url: /常用框架/EasyExcel/1_EasyExcel准备操作.md
---

# EasyExcel准备操作

## EasyExcel准备操作

### 引入EasyExcel依赖

```xml
<dependency>
    <groupId>com.alibaba</groupId>
    <artifactId>easyexcel</artifactId>
    <version>3.0.1</version>
</dependency>
```

### 创建对象实体类

```java
@Data
@ContentRowHeight(20)
@HeadRowHeight(40)
@ColumnWidth(40)
public class ExportExcelEntity {

    @ExcelProperty(value = "用户名", index = 0)
    private String userName;

    @ColumnWidth(30)
    @ExcelProperty(value = "手机号", index = 1)
    private String mobile;

    @ColumnWidth(60)
    @ExcelProperty(value = "地址", index = 2)
    private String place;

    @ColumnWidth(60)
    @ExcelProperty(value = "身份证", index = 3)
    private String idCard;

    public ExportExcelEntity(String userName, String mobile, String place) {
        this.userName = userName;
        this.mobile = mobile;
        this.place = place;
    }

    public ExportExcelEntity(String userName, String mobile, String place, String idCard) {
        this.userName = userName;
        this.mobile = mobile;
        this.place = place;
        this.idCard = idCard;
    }
}
```

## 生成测试数据

为了后面方面测试导出性能，使用 javafaker 生成大量测试数据。

### 引入javafaker依赖

```xml
<dependency>
    <groupId>com.github.javafaker</groupId>
    <artifactId>javafaker</artifactId>
    <version>0.17.2</version>
</dependency>
```

### 增加配置

`resources`下新建`zh-CN.yml`配置文件

```yaml
# coding: utf-8
zh-CN:
  faker:
    address:
      city: [海门, 鄂尔多斯, 招远, 舟山, 齐齐哈尔, 盐城, 赤峰, 青岛, 乳山, 金昌, 泉州, 莱西, 日照, 胶南, 南通, 拉萨, 云浮, 梅州, 文登, 上海, 攀枝花, 威海, 承德, 厦门, 汕尾, 潮州, 丹东, 太仓, 曲靖, 烟台, 福州, 瓦房店, 即墨, 抚顺, 玉溪, 张家口, 阳泉, 莱州, 湖州, 汕头, 昆山, 宁波, 湛江, 揭阳, 荣成, 连云港, 葫芦岛, 常熟, 东莞, 河源, 淮安, 泰州, 南宁, 营口, 惠州, 江阴, 蓬莱, 韶关, 嘉峪关, 广州, 延安, 太原, 清远, 中山, 昆明, 寿光, 盘锦, 长治, 深圳, 珠海, 宿迁, 咸阳, 铜川, 平度, 佛山, 海口, 江门, 章丘, 肇庆, 大连, 临汾, 吴江, 石嘴山, 沈阳, 苏州, 茂名, 嘉兴, 长春, 胶州, 银川, 张家港, 三门峡, 锦州, 南昌, 柳州, 三亚, 自贡, 吉林, 阳江, 泸州, 西宁, 宜宾, 呼和浩特, 成都, 大同, 镇江, 桂林, 张家界, 宜兴, 北海, 西安, 金坛, 东营, 牡丹江, 遵义, 绍兴, 扬州, 常州, 潍坊, 重庆, 台州, 南京, 滨州, 贵阳, 无锡, 本溪, 克拉玛依, 渭南, 马鞍山, 宝鸡, 焦作, 句容, 北京, 徐州, 衡水, 包头, 绵阳, 乌鲁木齐, 枣庄, 杭州, 淄博, 鞍山, 溧阳, 库尔勒, 安阳, 开封, 济南, 德阳, 温州, 九江, 邯郸, 临安, 兰州, 沧州, 临沂, 南充, 天津, 富阳, 泰安, 诸暨, 郑州, 哈尔滨, 聊城, 芜湖, 唐山, 平顶山, 邢台, 德州, 济宁, 荆州, 宜昌, 义乌, 丽水, 洛阳, 秦皇岛, 株洲, 石家庄, 莱芜, 常德, 保定, 湘潭, 金华, 岳阳, 长沙, 衢州, 廊坊, 菏泽, 合肥, 武汉, 大庆]
      building_number: ['#####', '####', '###', '##', '#']
      street_suffix: [巷, 街, 路, 桥, 侬, 旁, 中心, 栋]
      postcode: ['######']
      state: [北京市, 上海市, 天津市, 重庆市, 黑龙江省, 吉林省, 辽宁省, 内蒙古, 河北省, 新疆, 甘肃省, 青海省, 陕西省, 宁夏, 河南省, 山东省, 山西省, 安徽省, 湖北省, 湖南省, 江苏省, 四川省, 贵州省, 云南省, 广西省, 西藏, 浙江省, 江西省, 广东省, 福建省, 海南省, 香港, 澳门]
      state_abbr: [京, 沪, 津, 渝, 黑, 吉, 辽, 蒙, 冀, 新, 甘, 青, 陕, 宁, 豫, 鲁, 晋, 皖, 鄂, 湘, 苏, 川, 黔, 滇, 桂, 藏, 浙, 赣, 粤, 闽, 琼, 港, 澳]
      street_name:
        - "#{Name.last_name}#{street_suffix}"
      street_address:
        - "#{street_name}#{building_number}号"
      default_country: [中国]


    name:
      last_name: [徐, 王, 曲, 解, 张, 陈, 刑, 贾, 胡, 岳]
      first_name: [绍齐, 博文, 梓晨, 胤祥, 瑞霖, 明哲, 天翊, 凯瑞, 健雄, 耀杰, 潇然, 子涵, 越彬, 钰轩, 智辉, 致远, 俊驰, 雨泽, 烨磊, 晟睿, 文昊, 修洁, 黎昕, 远航, 旭尧, 鸿涛, 伟祺, 荣轩, 越泽, 浩宇, 瑾瑜, 皓轩, 擎苍, 擎宇, 志泽, 子轩, 睿渊, 弘文, 哲瀚, 雨泽, 楷瑞, 建辉, 晋鹏, 天磊, 绍辉, 泽洋, 鑫磊, 鹏煊, 昊强, 伟宸, 博超, 君浩, 子骞, 鹏涛, 炎彬, 鹤轩, 越彬, 风华, 靖琪, 明辉, 伟诚, 明轩, 健柏, 修杰, 志泽, 弘文, 峻熙, 嘉懿, 煜城, 懿轩, 烨伟, 苑博, 伟泽, 熠彤, 鸿煊, 博涛, 烨霖, 烨华, 煜祺, 智宸, 正豪, 昊然, 明杰, 立诚, 立轩, 立辉, 峻熙, 弘文, 熠彤, 鸿煊, 烨霖, 哲瀚, 鑫鹏, 昊天, 思聪, 展鹏, 笑愚, 志强, 炫明, 雪松, 思源, 智渊, 思淼, 晓啸, 天宇, 浩然, 文轩, 鹭洋, 振家, 乐驹, 晓博, 文博, 昊焱, 立果, 金鑫, 锦程, 嘉熙, 鹏飞, 子默, 思远, 浩轩, 语堂, 聪健, 明, 文, 果, 思, 鹏, 驰, 涛, 琪, 浩, 航, 彬]
      name:
        - "#{last_name}#{first_name}"
      name_with_middle: # Chinese names don't have middle names
        - "#{name}"

    phone_number:
      formats: ['###-########', '####-########', '###########']
    cell_phone:
      formats:
        - '13#########'
        - '145########'
        - '147########'
        - '150########'
        - '151########'
        - '152########'
        - '153########'
        - '155########'
        - '156########'
        - '157########'
        - '158########'
        - '159########'
        - '170########'
        - '171########'
        - '172########'
        - '173########'
        - '175########'
        - '176########'
        - '177########'
        - '178########'
        - '18#########'

    university:
      prefix: ["东", "南", "西", "北", "东南", "东北", "西南", "西北", "中国"]
      suffix: ["理工大学", "技术大学", "艺术大学", "体育大学", "经贸大学", "农业大学", "科技大学", "大学"]
      name:
        - "#{University.prefix}#{University.suffix}"

```

生成数据方法

```java
    /**
     * faker 指定汉语，默认英语
     */
    private static Faker FAKER = new Faker(Locale.CHINA);

    /**
     * 随机生成一定数量学生
     *
     * @param number 数量
     * @return 学生
     */
    public static List<ExportExcelEntity> listStudentList(final int number) {
        return Stream.generate(() -> new ExportExcelEntity(
                        FAKER.name().fullName(),
                        FAKER.phoneNumber().cellPhone(),
                        FAKER.address().city() + FAKER.address().streetAddress(),
                        FAKER.idNumber().validSvSeSsn()))
                .limit(number)
                .collect(Collectors.toList());
    }
```

---

---
url: /Java/架构设计/分布式/07.分布式日志收集/4_EFK篇之Filebeat使用.md
---

# EFK篇之Filebeat使用

## **EFK**概述

主流的 ELK (Elasticsearch, Logstash, Kibana) 目前已经转变为 EFK (Elasticsearch, Filebeat or Fluentd, Kibana) 比较重，对于容器云的日志方案业内也普遍推荐采用 Fluentd。

一个典型的 ELK 方案下的日志收集处理流程：

* Logstash 从各个 Docker 容器中提取日志信息
* Logstash 将日志转发到 Elasticsearch 进行索引和保存
* Kibana 负责分析和可视化日志信息

由于 Logstash 在数据收集上并不出色，而且作为 Agent，其性能并不达标。基于此，Elastic 发布了 beats 系列轻量级采集组件。

这里我们要实践的 Beat 组件是 Filebeat，Filebeat 是构建于 beats 之上的，应用于日志收集场景的实现，用来替代 Logstash Forwarder 的下一代 Logstash 收集器，是为了更快速稳定轻量低耗地进行收集工作，它可以很方便地与 Logstash 还有直接与 Elasticsearch 进行对接。

Filebeat 作为 Agent，它会收集的 log 文件中的记录变动，并直接将日志发给 Elasticsearch 进行索引和保存。

## **Filebeat、Logstash、Fluentd 三者的区别和联系**

Filebeat 是一个轻量级的收集本地 log 数据的方案，官方对 Filebeats 的说明如下。

```
Filebeat is a log data shipper for local files. Installed as an agent on your servers, Filebeat monitors the log directories or specific log files, tails the files, and forwards them either to Elasticsearch or Logstash for indexing
```

可以看出 Filebeat 功能比较单一，它仅仅只能收集本地的 log，但并不能对收集到的 Log 做什么处理，所以通常 Filebeat 通常需要将收集到的 log 发送到 Logstash 做进一步的处理。相比Logstash用Filebeats只需要一步搞定，而且不需要考虑代码侵入。

Logstash 和 Fluentd 都具有收集并处理 log 的能力，功能上二者旗鼓相当，但 Logstash 消耗更多的 memory，对此 Logstash 的解决方案是使用 Filebeats 从各个叶子节点上收集 log，当然 Fluentd 也有对应的 Fluent Bit。

另外一个重要的区别是 Fluentd 抽象性做得更好，对用户屏蔽了底层细节的繁琐。作者的原话如下：

```
Fluentd’s approach is more declarative whereas Logstash’s method is procedural. For programmers trained in procedural programming, Logstash’s configuration can be easier to get started. On the other hand, Fluentd’s tag-based routing allows complex routing to be expressed cleanly.
```

虽然作者说是要中立的对二者（Logstash 和 Fluentd）进行对比，但实际上偏向性很明显了：）。

Filebeat、Logstash、Elasticsearch 和 Kibana 是属于同一家公司的开源项目，官方文档如下：

> https://www.elastic.co/guide/index.html

Fluentd 则是另一家公司的开源项目，官方文档如下：

> https://docs.fluentd.org

## **Filebeat 配置**

> 下载地址：https://www.elastic.co/cn/downloads/beats/filebeat

下载

```sh
docker pull docker.elastic.co/beats/filebeat:7.17.15
```

新建配置文件`filebeat.docker.yml`，路径`/home/tools/filebeat`

> 官方默认配置文件：https://raw.githubusercontent.com/elastic/beats/7.17/deploy/docker/filebeat.docker.yml

```yml
# 日志输入配置
filebeat.inputs:
- type: log
  enabled: true
  paths:
  # 需要收集的日志所在的位置，可使用通配符进行配置
  #- /data/elk/*.log
  - /home/springboot/*/*.log

#日志输出配置(采用 logstash 收集日志，5044为logstash端口)
output.logstash:
  hosts: ['192.168.64.128:5044']
```

启动

```sh
docker run --restart=always \
--log-driver json-file \
--log-opt max-size=100m \
--log-opt max-file=2 \
--name filebeat \
--user=root -d \
-v /logs/:/logs/ \
-v /home/tools/filebeat/filebeat.docker.yml:/usr/share/filebeat/filebeat.yml \
docker.elastic.co/beats/filebeat:7.17.15
```

重启容器

```
systemctl restart filebeat
```

进入 Kibana 中查看日志信息，便可以看到刚刚添加的容器的日志信息了。

## 参考资料

\[1]. 公众号【奇妙的Linux世界】一文读懂开源日志管理方案 ELK 和 EFK 的区别

\[2]. https://zhuanlan.zhihu.com/p/141439013

\[3]. https://blog.csdn.net/zyxwvuuvwxyz/article/details/108831962

\[4]. https://www.bilibili.com/video/BV18u4y1e7rs

---

---
url: /Java/架构设计/分布式/02.分布式搜索/3_搜索引擎Elasticsearch03.md
---

# Elasticsearch高级功能

# 1.数据聚合

**[聚合（](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations.html)[aggregations](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations.html)[）](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations.html)** 可以让我们极其方便的实现对数据的统计、分析、运算。例如：

* 什么品牌的手机最受欢迎？
* 这些手机的平均价格、最高价格、最低价格？
* 这些手机每月的销售情况如何？

实现这些统计功能的比数据库的sql要方便的多，而且查询速度非常快，可以实现近实时搜索效果。

## 1.1.聚合的种类

聚合常见的有三类：

* **桶（Bucket）** 聚合：用来对文档做分组
  * TermAggregation：按照文档字段值分组，例如按照品牌值分组、按照国家分组
  * Date Histogram：按照日期阶梯分组，例如一周为一组，或者一月为一组

* **度量（Metric）** 聚合：用以计算一些值，比如：最大值、最小值、平均值等
  * Avg：求平均值
  * Max：求最大值
  * Min：求最小值
  * Stats：同时求max、min、avg、sum等

* **管道（pipeline）** 聚合：其它聚合的结果为基础做聚合

> \*\*注意：\*\*参加聚合的字段必须是keyword、日期、数值、布尔类型

## 1.2.DSL实现聚合

现在，我们要统计所有数据中的酒店品牌有几种，其实就是按照品牌对数据分组。此时可以根据酒店品牌的名称做聚合，也就是Bucket聚合。

### 1.2.1.Bucket聚合语法

语法如下：

```json
GET /hotel/_search
{
  "size": 0,  // 设置size为0，结果中不包含文档，只包含聚合结果
  "aggs": { // 定义聚合
    "brandAgg": { //给聚合起个名字
      "terms": { // 聚合的类型，按照品牌值聚合，所以选择term
        "field": "brand", // 参与聚合的字段
        "size": 20 // 希望获取的聚合结果数量
      }
    }
  }
}
```

结果如图：

![image-20210723171948228](/assets/image-20210723171948228.B5Yp5fz0.png)

### 1.2.2.聚合结果排序

默认情况下，Bucket聚合会统计Bucket内的文档数量，记为\_count，并且按照\_count降序排序。

我们可以指定order属性，自定义聚合的排序方式：

```json
GET /hotel/_search
{
  "size": 0, 
  "aggs": {
    "brandAgg": {
      "terms": {
        "field": "brand",
        "order": {
          "_count": "asc" // 按照_count升序排列
        },
        "size": 20
      }
    }
  }
}
```

### 1.2.3.限定聚合范围

默认情况下，Bucket聚合是对索引库的所有文档做聚合，但真实场景下，用户会输入搜索条件，因此聚合必须是对搜索结果聚合。那么聚合必须添加限定条件。

我们可以限定要聚合的文档范围，只要添加query条件即可：

```json
GET /hotel/_search
{
  "query": {
    "range": {
      "price": {
        "lte": 200 // 只对200元以下的文档聚合
      }
    }
  }, 
  "size": 0, 
  "aggs": {
    "brandAgg": {
      "terms": {
        "field": "brand",
        "size": 20
      }
    }
  }
}
```

这次，聚合得到的品牌明显变少了：

![image-20210723172404836](/assets/image-20210723172404836.CDGbWmlX.png)

### 1.2.4.Metric聚合语法

上节课，我们对酒店按照品牌分组，形成了一个个桶。现在我们需要对桶内的酒店做运算，获取每个品牌的用户评分的min、max、avg等值。

这就要用到Metric聚合了，例如stat聚合：就可以获取min、max、avg等结果。

语法如下：

```json
GET /hotel/_search
{
  "size": 0, 
  "aggs": {
    "brandAgg": { 
      "terms": { 
        "field": "brand", 
        "size": 20
      },
      "aggs": { // 是brands聚合的子聚合，也就是分组后对每组分别计算
        "score_stats": { // 聚合名称
          "stats": { // 聚合类型，这里stats可以计算min、max、avg等
            "field": "score" // 聚合字段，这里是score
          }
        }
      }
    }
  }
}
```

这次的score\_stats聚合是在brandAgg的聚合内部嵌套的子聚合。因为我们需要在每个桶分别计算。

另外，我们还可以给聚合结果做个排序，例如按照每个桶的酒店平均分做排序：

![image-20210723172917636](/assets/image-20210723172917636.Bo26ofTG.png)

### 1.2.5.小结

aggs代表聚合，与query同级，此时query的作用是？

* 限定聚合的的文档范围

聚合必须的三要素：

* 聚合名称
* 聚合类型
* 聚合字段

聚合可配置属性有：

* size：指定聚合结果数量
* order：指定聚合结果排序方式
* field：指定聚合字段

## 1.3.RestAPI实现聚合

### 1.3.1.API语法

聚合条件与query条件同级别，因此需要使用request.source()来指定聚合条件。

聚合条件的语法：

![image-20210723173057733](/assets/image-20210723173057733.CFESkWca.png)

聚合的结果也与查询结果不同，API也比较特殊。不过同样是JSON逐层解析：

![image-20210723173215728](/assets/image-20210723173215728.wUZwoMgY.png)

### 1.3.2.业务需求

需求：搜索页面的品牌、城市等信息不应该是在页面写死，而是通过聚合索引库中的酒店数据得来的：

![image-20210723192605566](/assets/image-20210723192605566.C7A0SkFK.png)

分析：

目前，页面的城市列表、星级列表、品牌列表都是写死的，并不会随着搜索结果的变化而变化。但是用户搜索条件改变时，搜索结果会跟着变化。

例如：用户搜索“东方明珠”，那搜索的酒店肯定是在上海东方明珠附近，因此，城市只能是上海，此时城市列表中就不应该显示北京、深圳、杭州这些信息了。

也就是说，搜索结果中包含哪些城市，页面就应该列出哪些城市；搜索结果中包含哪些品牌，页面就应该列出哪些品牌。

如何得知搜索结果中包含哪些品牌？如何得知搜索结果中包含哪些城市？

使用聚合功能，利用Bucket聚合，对搜索结果中的文档基于品牌分组、基于城市分组，就能得知包含哪些品牌、哪些城市了。

因为是对搜索结果聚合，因此聚合是**限定范围的聚合**，也就是说聚合的限定条件跟搜索文档的条件一致。

查看浏览器可以发现，前端其实已经发出了这样的一个请求：

![image-20210723193730799](/assets/image-20210723193730799.DAbCQDGC.png)

请求**参数与搜索文档的参数完全一致**。

返回值类型就是页面要展示的最终结果：

![image-20210723203915982](/assets/image-20210723203915982.CzSdGGuo.png)

结果是一个Map结构：

* key是字符串，城市、星级、品牌、价格
* value是集合，例如多个城市的名称

### 1.3.3.业务实现

在`cn.itcast.hotel.web`包的`HotelController`中添加一个方法，遵循下面的要求：

* 请求方式：`POST`
* 请求路径：`/hotel/filters`
* 请求参数：`RequestParams`，与搜索文档的参数一致
* 返回值类型：`Map<String, List<String>>`

代码：

```java
    @PostMapping("filters")
    public Map<String, List<String>> getFilters(@RequestBody RequestParams params){
        return hotelService.getFilters(params);
    }
```

这里调用了IHotelService中的getFilters方法，尚未实现。

在`cn.itcast.hotel.service.IHotelService`中定义新方法：

```java
Map<String, List<String>> filters(RequestParams params);
```

在`cn.itcast.hotel.service.impl.HotelService`中实现该方法：

```java
@Override
public Map<String, List<String>> filters(RequestParams params) {
    try {
        // 1.准备Request
        SearchRequest request = new SearchRequest("hotel");
        // 2.准备DSL
        // 2.1.query
        buildBasicQuery(params, request);
        // 2.2.设置size
        request.source().size(0);
        // 2.3.聚合
        buildAggregation(request);
        // 3.发出请求
        SearchResponse response = client.search(request, RequestOptions.DEFAULT);
        // 4.解析结果
        Map<String, List<String>> result = new HashMap<>();
        Aggregations aggregations = response.getAggregations();
        // 4.1.根据品牌名称，获取品牌结果
        List<String> brandList = getAggByName(aggregations, "brandAgg");
        result.put("品牌", brandList);
        // 4.2.根据品牌名称，获取品牌结果
        List<String> cityList = getAggByName(aggregations, "cityAgg");
        result.put("城市", cityList);
        // 4.3.根据品牌名称，获取品牌结果
        List<String> starList = getAggByName(aggregations, "starAgg");
        result.put("星级", starList);

        return result;
    } catch (IOException e) {
        throw new RuntimeException(e);
    }
}

private void buildAggregation(SearchRequest request) {
    request.source().aggregation(AggregationBuilders
                                 .terms("brandAgg")
                                 .field("brand")
                                 .size(100)
                                );
    request.source().aggregation(AggregationBuilders
                                 .terms("cityAgg")
                                 .field("city")
                                 .size(100)
                                );
    request.source().aggregation(AggregationBuilders
                                 .terms("starAgg")
                                 .field("starName")
                                 .size(100)
                                );
}

private List<String> getAggByName(Aggregations aggregations, String aggName) {
    // 4.1.根据聚合名称获取聚合结果
    Terms brandTerms = aggregations.get(aggName);
    // 4.2.获取buckets
    List<? extends Terms.Bucket> buckets = brandTerms.getBuckets();
    // 4.3.遍历
    List<String> brandList = new ArrayList<>();
    for (Terms.Bucket bucket : buckets) {
        // 4.4.获取key
        String key = bucket.getKeyAsString();
        brandList.add(key);
    }
    return brandList;
}
```

# 2.自动补全

当用户在搜索框输入字符时，我们应该提示出与该字符有关的搜索项，如图：

![image-20210723204936367](/assets/image-20210723204936367.BzQgboI5.png)

这种根据用户输入的字母，提示完整词条的功能，就是自动补全了。

因为需要根据拼音字母来推断，因此要用到拼音分词功能。

## 2.1.拼音分词器

要实现根据字母做补全，就必须对文档按照拼音分词。在GitHub上恰好有elasticsearch的拼音分词插件。地址：https://github.com/medcl/elasticsearch-analysis-pinyin

![image-20210723205932746](/assets/image-20210723205932746.BQCf1c5q.png)

课前资料中也提供了拼音分词器的安装包：

![image-20210723205722303](/assets/image-20210723205722303.ChFn7bDO.png)

安装方式与IK分词器一样，分三步：

​	①解压

​	②上传到虚拟机中，elasticsearch的plugin目录

​	③重启elasticsearch

​	④测试

详细安装步骤可以参考IK分词器的安装过程。

测试用法如下：

```json
POST /_analyze
{
  "text": "如家酒店还不错",
  "analyzer": "pinyin"
}
```

结果：

![image-20210723210126506](/assets/image-20210723210126506.Dsh7Q3dJ.png)

## 2.2.自定义分词器

默认的拼音分词器会将每个汉字单独分为拼音，而我们希望的是每个词条形成一组拼音，需要对拼音分词器做个性化定制，形成自定义分词器。

elasticsearch中分词器（analyzer）的组成包含三部分：

* character filters：在tokenizer之前对文本进行处理。例如删除字符、替换字符
* tokenizer：将文本按照一定的规则切割成词条（term）。例如keyword，就是不分词；还有ik\_smart
* tokenizer filter：将tokenizer输出的词条做进一步处理。例如大小写转换、同义词处理、拼音处理等

文档分词时会依次由这三部分来处理文档：

![image-20210723210427878](/assets/image-20210723210427878.DpNWakJh.png)

声明自定义分词器的语法如下：

```json
PUT /test
{
  "settings": {
    "analysis": {
      "analyzer": { // 自定义分词器
        "my_analyzer": {  // 分词器名称
          "tokenizer": "ik_max_word",
          "filter": "py"
        }
      },
      "filter": { // 自定义tokenizer filter
        "py": { // 过滤器名称
          "type": "pinyin", // 过滤器类型，这里是pinyin
		  "keep_full_pinyin": false,
          "keep_joined_full_pinyin": true,
          "keep_original": true,
          "limit_first_letter_length": 16,
          "remove_duplicated_term": true,
          "none_chinese_pinyin_tokenize": false
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "name": {
        "type": "text",
        "analyzer": "my_analyzer",
        "search_analyzer": "ik_smart"
      }
    }
  }
}
```

测试：

![image-20210723211829150](/assets/image-20210723211829150.CyKe6ieT.png)

总结：

如何使用拼音分词器？

* ①下载pinyin分词器

* ②解压并放到elasticsearch的plugin目录

* ③重启即可

如何自定义分词器？

* ①创建索引库时，在settings中配置，可以包含三部分

* ②character filter

* ③tokenizer

* ④filter

拼音分词器注意事项？

* 为了避免搜索到同音字，搜索时不要使用拼音分词器

## 2.3.自动补全查询

elasticsearch提供了[Completion Suggester](https://www.elastic.co/guide/en/elasticsearch/reference/7.6/search-suggesters.html)查询来实现自动补全功能。这个查询会匹配以用户输入内容开头的词条并返回。为了提高补全查询的效率，对于文档中字段的类型有一些约束：

* 参与补全查询的字段必须是completion类型。

* 字段的内容一般是用来补全的多个词条形成的数组。

比如，一个这样的索引库：

```json
// 创建索引库
PUT test
{
  "mappings": {
    "properties": {
      "title":{
        "type": "completion"
      }
    }
  }
}
```

然后插入下面的数据：

```json
// 示例数据
POST test/_doc
{
  "title": ["Sony", "WH-1000XM3"]
}
POST test/_doc
{
  "title": ["SK-II", "PITERA"]
}
POST test/_doc
{
  "title": ["Nintendo", "switch"]
}
```

查询的DSL语句如下：

```json
// 自动补全查询
GET /test/_search
{
  "suggest": {
    "title_suggest": {
      "text": "s", // 关键字
      "completion": {
        "field": "title", // 补全查询的字段
        "skip_duplicates": true, // 跳过重复的
        "size": 10 // 获取前10条结果
      }
    }
  }
}
```

## 2.4.实现酒店搜索框自动补全

现在，我们的hotel索引库还没有设置拼音分词器，需要修改索引库中的配置。但是我们知道索引库是无法修改的，只能删除然后重新创建。

另外，我们需要添加一个字段，用来做自动补全，将brand、suggestion、city等都放进去，作为自动补全的提示。

因此，总结一下，我们需要做的事情包括：

1. 修改hotel索引库结构，设置自定义拼音分词器

2. 修改索引库的name、all字段，使用自定义分词器

3. 索引库添加一个新字段suggestion，类型为completion类型，使用自定义的分词器

4. 给HotelDoc类添加suggestion字段，内容包含brand、business

5. 重新导入数据到hotel库

### 2.4.1.修改酒店映射结构

代码如下：

```json
// 酒店数据索引库
PUT /hotel
{
  "settings": {
    "analysis": {
      "analyzer": {
        "text_anlyzer": {
          "tokenizer": "ik_max_word",
          "filter": "py"
        },
        "completion_analyzer": {
          "tokenizer": "keyword",
          "filter": "py"
        }
      },
      "filter": {
        "py": {
          "type": "pinyin",
          "keep_full_pinyin": false,
          "keep_joined_full_pinyin": true,
          "keep_original": true,
          "limit_first_letter_length": 16,
          "remove_duplicated_term": true,
          "none_chinese_pinyin_tokenize": false
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "id":{
        "type": "keyword"
      },
      "name":{
        "type": "text",
        "analyzer": "text_anlyzer",
        "search_analyzer": "ik_smart",
        "copy_to": "all"
      },
      "address":{
        "type": "keyword",
        "index": false
      },
      "price":{
        "type": "integer"
      },
      "score":{
        "type": "integer"
      },
      "brand":{
        "type": "keyword",
        "copy_to": "all"
      },
      "city":{
        "type": "keyword"
      },
      "starName":{
        "type": "keyword"
      },
      "business":{
        "type": "keyword",
        "copy_to": "all"
      },
      "location":{
        "type": "geo_point"
      },
      "pic":{
        "type": "keyword",
        "index": false
      },
      "all":{
        "type": "text",
        "analyzer": "text_anlyzer",
        "search_analyzer": "ik_smart"
      },
      "suggestion":{
          "type": "completion",
          "analyzer": "completion_analyzer"
      }
    }
  }
}
```

### 2.4.2.修改HotelDoc实体

HotelDoc中要添加一个字段，用来做自动补全，内容可以是酒店品牌、城市、商圈等信息。按照自动补全字段的要求，最好是这些字段的数组。

因此我们在HotelDoc中添加一个suggestion字段，类型为`List<String>`，然后将brand、city、business等信息放到里面。

代码如下：

```java
package cn.itcast.hotel.pojo;

import lombok.Data;
import lombok.NoArgsConstructor;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collections;
import java.util.List;

@Data
@NoArgsConstructor
public class HotelDoc {
    private Long id;
    private String name;
    private String address;
    private Integer price;
    private Integer score;
    private String brand;
    private String city;
    private String starName;
    private String business;
    private String location;
    private String pic;
    private Object distance;
    private Boolean isAD;
    private List<String> suggestion;

    public HotelDoc(Hotel hotel) {
        this.id = hotel.getId();
        this.name = hotel.getName();
        this.address = hotel.getAddress();
        this.price = hotel.getPrice();
        this.score = hotel.getScore();
        this.brand = hotel.getBrand();
        this.city = hotel.getCity();
        this.starName = hotel.getStarName();
        this.business = hotel.getBusiness();
        this.location = hotel.getLatitude() + ", " + hotel.getLongitude();
        this.pic = hotel.getPic();
        // 组装suggestion
        if(this.business.contains("/")){
            // business有多个值，需要切割
            String[] arr = this.business.split("/");
            // 添加元素
            this.suggestion = new ArrayList<>();
            this.suggestion.add(this.brand);
            Collections.addAll(this.suggestion, arr);
        }else {
            this.suggestion = Arrays.asList(this.brand, this.business);
        }
    }
}
```

### 2.4.3.重新导入

重新执行之前编写的导入数据功能，可以看到新的酒店数据中包含了suggestion：

![image-20210723213546183](/assets/image-20210723213546183.BkyZJ5HG.png)

### 2.4.4.自动补全查询的JavaAPI

之前我们学习了自动补全查询的DSL，而没有学习对应的JavaAPI，这里给出一个示例：

![image-20210723213759922](/assets/image-20210723213759922.CYs0dflY.png)

而自动补全的结果也比较特殊，解析的代码如下：

![image-20210723213917524](/assets/image-20210723213917524.BI0M9kyG.png)

### 2.4.5.实现搜索框自动补全

查看前端页面，可以发现当我们在输入框键入时，前端会发起ajax请求：

![image-20210723214021062](/assets/image-20210723214021062.B3A0rks0.png)

返回值是补全词条的集合，类型为`List<String>`

1）在`cn.itcast.hotel.web`包下的`HotelController`中添加新接口，接收新的请求：

```java
@GetMapping("suggestion")
public List<String> getSuggestions(@RequestParam("key") String prefix) {
    return hotelService.getSuggestions(prefix);
}
```

2）在`cn.itcast.hotel.service`包下的`IhotelService`中添加方法：

```java
List<String> getSuggestions(String prefix);
```

3）在`cn.itcast.hotel.service.impl.HotelService`中实现该方法：

```java
@Override
public List<String> getSuggestions(String prefix) {
    try {
        // 1.准备Request
        SearchRequest request = new SearchRequest("hotel");
        // 2.准备DSL
        request.source().suggest(new SuggestBuilder().addSuggestion(
            "suggestions",
            SuggestBuilders.completionSuggestion("suggestion")
            .prefix(prefix)
            .skipDuplicates(true)
            .size(10)
        ));
        // 3.发起请求
        SearchResponse response = client.search(request, RequestOptions.DEFAULT);
        // 4.解析结果
        Suggest suggest = response.getSuggest();
        // 4.1.根据补全查询名称，获取补全结果
        CompletionSuggestion suggestions = suggest.getSuggestion("suggestions");
        // 4.2.获取options
        List<CompletionSuggestion.Entry.Option> options = suggestions.getOptions();
        // 4.3.遍历
        List<String> list = new ArrayList<>(options.size());
        for (CompletionSuggestion.Entry.Option option : options) {
            String text = option.getText().toString();
            list.add(text);
        }
        return list;
    } catch (IOException e) {
        throw new RuntimeException(e);
    }
}
```

# 3.数据同步

elasticsearch中的酒店数据来自于mysql数据库，因此mysql数据发生改变时，elasticsearch也必须跟着改变，这个就是elasticsearch与mysql之间的**数据同步**。

![image-20210723214758392](/assets/image-20210723214758392.B2v_rk3C.png)

## 3.1.思路分析

常见的数据同步方案有三种：

* 同步调用
* 异步通知
* 监听binlog

### 3.1.1.同步调用

方案一：同步调用

![image-20210723214931869](/assets/image-20210723214931869.DybQE7Hk.png)

基本步骤如下：

* hotel-demo对外提供接口，用来修改elasticsearch中的数据
* 酒店管理服务在完成数据库操作后，直接调用hotel-demo提供的接口，

### 3.1.2.异步通知

方案二：异步通知

![image-20210723215140735](/assets/image-20210723215140735.B52CM_ow.png)

流程如下：

* hotel-admin对mysql数据库数据完成增、删、改后，发送MQ消息
* hotel-demo监听MQ，接收到消息后完成elasticsearch数据修改

### 3.1.3.监听binlog

方案三：监听binlog

![image-20210723215518541](/assets/image-20210723215518541.Co7YQCBc.png)

流程如下：

* 给mysql开启binlog功能
* mysql完成增、删、改操作都会记录在binlog中
* hotel-demo基于canal监听binlog变化，实时更新elasticsearch中的内容

### 3.1.4.选择

方式一：同步调用

* 优点：实现简单，粗暴
* 缺点：业务耦合度高

方式二：异步通知

* 优点：低耦合，实现难度一般
* 缺点：依赖mq的可靠性

方式三：监听binlog

* 优点：完全解除服务间耦合
* 缺点：开启binlog增加数据库负担、实现复杂度高

## 3.2.实现数据同步

### 3.2.1.思路

利用课前资料提供的hotel-admin项目作为酒店管理的微服务。当酒店数据发生增、删、改时，要求对elasticsearch中数据也要完成相同操作。

步骤：

* 导入课前资料提供的hotel-admin项目，启动并测试酒店数据的CRUD

* 声明exchange、queue、RoutingKey

* 在hotel-admin中的增、删、改业务中完成消息发送

* 在hotel-demo中完成消息监听，并更新elasticsearch中数据

* 启动并测试数据同步功能

### 3.2.2.导入demo

导入课前资料提供的hotel-admin项目：

![image-20210723220237930](/assets/image-20210723220237930.D_TO47cg.png)

运行后，访问 http://localhost:8099

![image-20210723220354464](/assets/image-20210723220354464.bqvmN-pQ.png)

其中包含了酒店的CRUD功能：

![image-20210723220511090](/assets/image-20210723220511090.TJwwKQ8k.png)

### 3.2.3.声明交换机、队列

MQ结构如图：

![image-20210723215850307](/assets/image-20210723215850307.WujRKf7h.png)

#### 1）引入依赖

在hotel-admin、hotel-demo中引入rabbitmq的依赖：

```xml
<!--amqp-->
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-amqp</artifactId>
</dependency>
```

#### 2）声明队列交换机名称

在hotel-admin和hotel-demo中的`cn.itcast.hotel.constatnts`包下新建一个类`MqConstants`：

```java
package cn.itcast.hotel.constatnts;

    public class MqConstants {
    /**
     * 交换机
     */
    public final static String HOTEL_EXCHANGE = "hotel.topic";
    /**
     * 监听新增和修改的队列
     */
    public final static String HOTEL_INSERT_QUEUE = "hotel.insert.queue";
    /**
     * 监听删除的队列
     */
    public final static String HOTEL_DELETE_QUEUE = "hotel.delete.queue";
    /**
     * 新增或修改的RoutingKey
     */
    public final static String HOTEL_INSERT_KEY = "hotel.insert";
    /**
     * 删除的RoutingKey
     */
    public final static String HOTEL_DELETE_KEY = "hotel.delete";
}
```

#### 3）声明队列交换机

在hotel-demo中，定义配置类，声明队列、交换机：

```java
package cn.itcast.hotel.config;

import cn.itcast.hotel.constants.MqConstants;
import org.springframework.amqp.core.Binding;
import org.springframework.amqp.core.BindingBuilder;
import org.springframework.amqp.core.Queue;
import org.springframework.amqp.core.TopicExchange;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

@Configuration
public class MqConfig {
    @Bean
    public TopicExchange topicExchange(){
        return new TopicExchange(MqConstants.HOTEL_EXCHANGE, true, false);
    }

    @Bean
    public Queue insertQueue(){
        return new Queue(MqConstants.HOTEL_INSERT_QUEUE, true);
    }

    @Bean
    public Queue deleteQueue(){
        return new Queue(MqConstants.HOTEL_DELETE_QUEUE, true);
    }

    @Bean
    public Binding insertQueueBinding(){
        return BindingBuilder.bind(insertQueue()).to(topicExchange()).with(MqConstants.HOTEL_INSERT_KEY);
    }

    @Bean
    public Binding deleteQueueBinding(){
        return BindingBuilder.bind(deleteQueue()).to(topicExchange()).with(MqConstants.HOTEL_DELETE_KEY);
    }
}
```

### 3.2.4.发送MQ消息

在hotel-admin中的增、删、改业务中分别发送MQ消息：

![image-20210723221843816](/assets/image-20210723221843816.8PArv-TZ.png)

### 3.2.5.接收MQ消息

hotel-demo接收到MQ消息要做的事情包括：

* 新增消息：根据传递的hotel的id查询hotel信息，然后新增一条数据到索引库
* 删除消息：根据传递的hotel的id删除索引库中的一条数据

1）首先在hotel-demo的`cn.itcast.hotel.service`包下的`IHotelService`中新增新增、删除业务

```java
void deleteById(Long id);

void insertById(Long id);
```

2）给hotel-demo中的`cn.itcast.hotel.service.impl`包下的HotelService中实现业务：

```java
@Override
public void deleteById(Long id) {
    try {
        // 1.准备Request
        DeleteRequest request = new DeleteRequest("hotel", id.toString());
        // 2.发送请求
        client.delete(request, RequestOptions.DEFAULT);
    } catch (IOException e) {
        throw new RuntimeException(e);
    }
}

@Override
public void insertById(Long id) {
    try {
        // 0.根据id查询酒店数据
        Hotel hotel = getById(id);
        // 转换为文档类型
        HotelDoc hotelDoc = new HotelDoc(hotel);

        // 1.准备Request对象
        IndexRequest request = new IndexRequest("hotel").id(hotel.getId().toString());
        // 2.准备Json文档
        request.source(JSON.toJSONString(hotelDoc), XContentType.JSON);
        // 3.发送请求
        client.index(request, RequestOptions.DEFAULT);
    } catch (IOException e) {
        throw new RuntimeException(e);
    }
}
```

3）编写监听器

在hotel-demo中的`cn.itcast.hotel.mq`包新增一个类：

```java
package cn.itcast.hotel.mq;

import cn.itcast.hotel.constants.MqConstants;
import cn.itcast.hotel.service.IHotelService;
import org.springframework.amqp.rabbit.annotation.RabbitListener;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Component;

@Component
public class HotelListener {

    @Autowired
    private IHotelService hotelService;

    /**
     * 监听酒店新增或修改的业务
     * @param id 酒店id
     */
    @RabbitListener(queues = MqConstants.HOTEL_INSERT_QUEUE)
    public void listenHotelInsertOrUpdate(Long id){
        hotelService.insertById(id);
    }

    /**
     * 监听酒店删除的业务
     * @param id 酒店id
     */
    @RabbitListener(queues = MqConstants.HOTEL_DELETE_QUEUE)
    public void listenHotelDelete(Long id){
        hotelService.deleteById(id);
    }
}
```

# 4.集群

单机的elasticsearch做数据存储，必然面临两个问题：海量数据存储问题、单点故障问题。

* 海量数据存储问题：将索引库从逻辑上拆分为N个分片（shard），存储到多个节点
* 单点故障问题：将分片数据在不同节点备份（replica ）

**ES集群相关概念**:

* 集群（cluster）：一组拥有共同的 cluster name 的 节点。

* 节点（node)   ：集群中的一个 Elasticearch 实例

* 分片（shard）：索引可以被拆分为不同的部分进行存储，称为分片。在集群环境下，一个索引的不同分片可以拆分到不同的节点中

  解决问题：数据量太大，单点存储量有限的问题。

  ![image-20200104124440086](/assets/image-20200104124440086-5602723.BSVh6Me5.png)

  > 此处，我们把数据分成3片：shard0、shard1、shard2

* 主分片（Primary shard）：相对于副本分片的定义。

* 副本分片（Replica shard）每个主分片可以有一个或者多个副本，数据和主分片一样。

  ​

数据备份可以保证高可用，但是每个分片备份一份，所需要的节点数量就会翻一倍，成本实在是太高了！

为了在高可用和成本间寻求平衡，我们可以这样做：

* 首先对数据分片，存储到不同节点
* 然后对每个分片进行备份，放到对方节点，完成互相备份

这样可以大大减少所需要的服务节点数量，如图，我们以3分片，每个分片备份一份为例：

![image-20200104124551912](/assets/image-20200104124551912.t3a8Jkbp.png)

现在，每个分片都有1个备份，存储在3个节点：

* node0：保存了分片0和1
* node1：保存了分片0和2
* node2：保存了分片1和2

## 4.1.搭建ES集群

参考课前资料的文档：

![image-20210723222732427](/assets/image-20210723222732427.BjUimuJ-.png)

其中的第四章节：

![image-20210723222812619](/assets/image-20210723222812619.DWdE7AVA.png)

## 4.2.集群脑裂问题

### 4.2.1.集群职责划分

elasticsearch中集群节点有不同的职责划分：

![image-20210723223008967](/assets/image-20210723223008967.-YMHL2EK.png)

默认情况下，集群中的任何一个节点都同时具备上述四种角色。

但是真实的集群一定要将集群职责分离：

* master节点：对CPU要求高，但是内存要求第
* data节点：对CPU和内存要求都高
* coordinating节点：对网络带宽、CPU要求高

职责分离可以让我们根据不同节点的需求分配不同的硬件去部署。而且避免业务之间的互相干扰。

一个典型的es集群职责划分如图：

![image-20210723223629142](/assets/image-20210723223629142.Dx2Q1WoG.png)

### 4.2.2.脑裂问题

脑裂是因为集群中的节点失联导致的。

例如一个集群中，主节点与其它节点失联：

![image-20210723223804995](/assets/image-20210723223804995.CcIP14aZ.png)

此时，node2和node3认为node1宕机，就会重新选主：

![image-20210723223845754](/assets/image-20210723223845754.I-8-qTg2.png)

当node3当选后，集群继续对外提供服务，node2和node3自成集群，node1自成集群，两个集群数据不同步，出现数据差异。

当网络恢复后，因为集群中有两个master节点，集群状态的不一致，出现脑裂的情况：

![image-20210723224000555](/assets/image-20210723224000555.ceKJG9mi.png)

解决脑裂的方案是，要求选票超过 ( eligible节点数量 + 1 ）/ 2 才能当选为主，因此eligible节点数量最好是奇数。对应配置项是discovery.zen.minimum\_master\_nodes，在es7.0以后，已经成为默认配置，因此一般不会发生脑裂问题

例如：3个节点形成的集群，选票必须超过 （3 + 1） / 2 ，也就是2票。node3得到node2和node3的选票，当选为主。node1只有自己1票，没有当选。集群中依然只有1个主节点，没有出现脑裂。

### 4.2.3.小结

master eligible节点的作用是什么？

* 参与集群选主
* 主节点可以管理集群状态、管理分片信息、处理创建和删除索引库的请求

data节点的作用是什么？

* 数据的CRUD

coordinator节点的作用是什么？

* 路由请求到其它节点

* 合并查询到的结果，返回给用户

## 4.3.集群分布式存储

当新增文档时，应该保存到不同分片，保证数据均衡，那么coordinating node如何确定数据该存储到哪个分片呢？

### 4.3.1.分片存储测试

插入三条数据：

![image-20210723225006058](/assets/image-20210723225006058.C106keaw.png)

![image-20210723225034637](/assets/image-20210723225034637.CsdDEPHx.png)

![image-20210723225112029](/assets/image-20210723225112029.fxAALhf8.png)

测试可以看到，三条数据分别在不同分片：

![image-20210723225227928](/assets/image-20210723225227928.B8orcI9t.png)

结果：

![image-20210723225342120](/assets/image-20210723225342120.CqnxzPKQ.png)

### 4.3.2.分片存储原理

elasticsearch会通过hash算法来计算文档应该存储到哪个分片：

![image-20210723224354904](/assets/image-20210723224354904.NC7B5rJg.png)

说明：

* \_routing默认是文档的id
* 算法与分片数量有关，因此索引库一旦创建，分片数量不能修改！

新增文档的流程如下：

![image-20210723225436084](/assets/image-20210723225436084.CXilft-Y.png)

解读：

* 1）新增一个id=1的文档
* 2）对id做hash运算，假如得到的是2，则应该存储到shard-2
* 3）shard-2的主分片在node3节点，将数据路由到node3
* 4）保存文档
* 5）同步给shard-2的副本replica-2，在node2节点
* 6）返回结果给coordinating-node节点

## 4.4.集群分布式查询

elasticsearch的查询分成两个阶段：

* scatter phase：分散阶段，coordinating node会把请求分发到每一个分片

* gather phase：聚集阶段，coordinating node汇总data node的搜索结果，并处理为最终结果集返回给用户

![image-20210723225809848](/assets/image-20210723225809848.Dci6zVq8.png)

## 4.5.集群故障转移

集群的master节点会监控集群中的节点状态，如果发现有节点宕机，会立即将宕机节点的分片数据迁移到其它节点，确保数据安全，这个叫做故障转移。

1）例如一个集群结构如图：

![image-20210723225945963](/assets/image-20210723225945963.Fz2zmwkn.png)

现在，node1是主节点，其它两个节点是从节点。

2）突然，node1发生了故障：

![image-20210723230020574](/assets/image-20210723230020574.Cj-BDpej.png)

宕机后的第一件事，需要重新选主，例如选中了node2：

![image-20210723230055974](/assets/image-20210723230055974.CC_lXrJn.png)

node2成为主节点后，会检测集群监控状态，发现：shard-1、shard-0没有副本节点。因此需要将node1上的数据迁移到node2、node3：

![image-20210723230216642](/assets/image-20210723230216642.BNAG3QQe.png)

---

---
url: /Java/架构设计/分布式/02.分布式搜索/4_ElasticSearch进阶功能.md
---

# Elasticsearch进阶功能

https://www.bilibili.com/video/BV1hh411D7sb

[Elasticsearch学习笔记\_elasticsearch笔记-CSDN博客](https://blog.csdn.net/u011863024/article/details/115721328)

---

---
url: /Java/架构设计/分布式/02.分布式搜索/2_搜索引擎Elasticsearch02.md
---

# Elasticsearch数据搜索

在之前的文章中，导入了大量数据到elasticsearch中，实现了elasticsearch的数据存储功能。但elasticsearch最擅长的还是搜索和数据分析。

所以，继续研究elasticsearch的数据搜索功能。分别使用**DSL**和**RestClient**实现搜索。

# 1.DSL查询文档

elasticsearch的查询依然是基于JSON风格的DSL来实现的。

## 1.1.DSL查询分类

Elasticsearch提供了基于JSON的DSL（[Domain Specific Language](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl.html)）来定义查询。常见的查询类型包括：

* **查询所有**：查询出所有数据，一般测试用。例如：match\_all

* **全文检索（full text）查询** ：利用分词器对用户输入内容分词，然后去倒排索引库中匹配。例如：
  * match\_query
  * multi\_match\_query

* **精确查询**：根据精确词条值查找数据，一般是查找keyword、数值、日期、boolean等类型字段。例如：
  * ids
  * range
  * term

* **地理（geo）查询**：根据经纬度查询。例如：
  * geo\_distance
  * geo\_bounding\_box

* **复合（compound）查询**：复合查询可以将上述各种查询条件组合起来，合并查询条件。例如：
  * bool
  * function\_score

查询的语法基本一致：

```json
GET /indexName/_search
{
  "query": {
    "查询类型": {
      "查询条件": "条件值"
    }
  }
}
```

我们以查询所有为例，其中：

* 查询类型为match\_all
* 没有查询条件

```json
// 查询所有
GET /indexName/_search
{
  "query": {
    "match_all": {
    }
  }
}
```

其它查询无非就是**查询类型**、**查询条件**的变化。

## 1.2.全文检索查询

### 1.2.1.使用场景

全文检索查询的基本流程如下：

* 对用户搜索的内容做分词，得到词条
* 根据词条去倒排索引库中匹配，得到文档id
* 根据文档id找到文档，返回给用户

比较常用的场景包括：

* 商城的输入框搜索
* 百度输入框搜索

例如京东：

![image-20210721165326938](/assets/image-20210721165326938.CiivL5Xl.png)

因为是拿着词条去匹配，因此参与搜索的字段也必须是可分词的text类型的字段。

### 1.2.2.基本语法

常见的全文检索查询包括：

* match查询：单字段查询
* multi\_match查询：多字段查询，任意一个字段符合条件就算符合查询条件

match查询语法如下：

```json
GET /indexName/_search
{
  "query": {
    "match": {
      "FIELD": "TEXT"
    }
  }
}
```

mulit\_match语法如下：

```json
GET /indexName/_search
{
  "query": {
    "multi_match": {
      "query": "TEXT",
      "fields": ["FIELD1", " FIELD12"]
    }
  }
}
```

### 1.2.3.示例

match查询示例：

![image-20210721170455419](/assets/image-20210721170455419.NLLjIH8o.png)

multi\_match查询示例：

![image-20210721170720691](/assets/image-20210721170720691.BxbNB4Od.png)

可以看到，两种查询结果是一样的，为什么？

因为我们将brand、name、business值都利用copy\_to复制到了all字段中。因此你根据三个字段搜索，和根据all字段搜索效果当然一样了。

但是，搜索字段越多，对查询性能影响越大，因此建议采用copy\_to，然后单字段查询的方式。

### 1.2.4.总结

match和multi\_match的区别是什么？

* match：根据一个字段查询
* multi\_match：根据多个字段查询，参与查询字段越多，查询性能越差

## 1.3.精准查询

精确查询一般是查找keyword、数值、日期、boolean等类型字段。所以**不会**对搜索条件分词。常见的有：

* term：根据词条精确值查询
* range：根据值的范围查询

### 1.3.1.term查询

因为精确查询的字段搜是不分词的字段，因此查询的条件也必须是**不分词**的词条。查询时，用户输入的内容跟自动值完全匹配时才认为符合条件。如果用户输入的内容过多，反而搜索不到数据。

语法说明：

```json
// term查询
GET /indexName/_search
{
  "query": {
    "term": {
      "FIELD": {
        "value": "VALUE"
      }
    }
  }
}
```

示例：

当我搜索的是精确词条时，能正确查询出结果：

![image-20210721171655308](/assets/image-20210721171655308.C12HGuTG.png)

但是，当我搜索的内容不是词条，而是多个词语形成的短语时，反而搜索不到：

![image-20210721171838378](/assets/image-20210721171838378.B9nU9zMA.png)

### 1.3.2.range查询

范围查询，一般应用在对数值类型做范围过滤的时候。比如做价格范围过滤。

基本语法：

```json
// range查询
GET /indexName/_search
{
  "query": {
    "range": {
      "FIELD": {
        "gte": 10, // 这里的gte代表大于等于，gt则代表大于
        "lte": 20 // lte代表小于等于，lt则代表小于
      }
    }
  }
}
```

示例：

![image-20210721172307172](/assets/image-20210721172307172.DAAwlRA3.png)

### 1.3.3.总结

精确查询常见的有哪些？

* term查询：根据词条精确匹配，一般搜索keyword类型、数值类型、布尔类型、日期类型字段
* range查询：根据数值范围查询，可以是数值、日期的范围

## 1.4.地理坐标查询

所谓的地理坐标查询，其实就是根据经纬度查询，官方文档：https://www.elastic.co/guide/en/elasticsearch/reference/current/geo-queries.html

常见的使用场景包括：

* 携程：搜索我附近的酒店
* 滴滴：搜索我附近的出租车
* 微信：搜索我附近的人

附近的酒店：

![image-20210721172645103](/assets/image-20210721172645103.Cm_2Uzb4.png)

附近的车：

![image-20210721172654880](assets/image-20210721172654880.png)

### 1.4.1.矩形范围查询

矩形范围查询，也就是geo\_bounding\_box查询，查询坐标落在某个矩形范围的所有文档：

![DKV9HZbVS6](/assets/DKV9HZbVS6.JHd_KYSr.gif)

查询时，需要指定矩形的**左上**、**右下**两个点的坐标，然后画出一个矩形，落在该矩形内的都是符合条件的点。

语法如下：

```json
// geo_bounding_box查询
GET /indexName/_search
{
  "query": {
    "geo_bounding_box": {
      "FIELD": {
        "top_left": { // 左上点
          "lat": 31.1,
          "lon": 121.5
        },
        "bottom_right": { // 右下点
          "lat": 30.9,
          "lon": 121.7
        }
      }
    }
  }
}
```

这种并不符合“附近的人”这样的需求，所以我们就不做了。

### 1.4.2.附近查询

附近查询，也叫做距离查询（geo\_distance）：查询到指定中心点小于某个距离值的所有文档。

换句话来说，在地图上找一个点作为圆心，以指定距离为半径，画一个圆，落在圆内的坐标都算符合条件：

![vZrdKAh19C](/assets/vZrdKAh19C.JH_L-6e1.gif)

语法说明：

```json
// geo_distance 查询
GET /indexName/_search
{
  "query": {
    "geo_distance": {
      "distance": "15km", // 半径
      "FIELD": "31.21,121.5" // 圆心
    }
  }
}
```

示例：

我们先搜索陆家嘴附近15km的酒店：

![image-20210721175443234](/assets/image-20210721175443234.Bu53QqZn.png)

发现共有47家酒店。

然后把半径缩短到3公里：

![image-20210721182031475](/assets/image-20210721182031475.QLBUPaqf.png)

可以发现，搜索到的酒店数量减少到了5家。

## 1.5.复合查询

复合（compound）查询：复合查询可以将其它简单查询组合起来，实现更复杂的搜索逻辑。常见的有两种：

* fuction score：算分函数查询，可以控制文档相关性算分，控制文档排名
* bool query：布尔查询，利用逻辑关系组合多个其它的查询，实现复杂搜索

### 1.5.1.相关性算分

当我们利用match查询时，文档结果会根据与搜索词条的关联度打分（\_score），返回结果时按照分值降序排列。

例如，我们搜索 "虹桥如家"，结果如下：

```json
[
  {
    "_score" : 17.850193,
    "_source" : {
      "name" : "虹桥如家酒店真不错",
    }
  },
  {
    "_score" : 12.259849,
    "_source" : {
      "name" : "外滩如家酒店真不错",
    }
  },
  {
    "_score" : 11.91091,
    "_source" : {
      "name" : "迪士尼如家酒店真不错",
    }
  }
]
```

在elasticsearch中，早期使用的打分算法是TF-IDF算法，公式如下：

![image-20210721190152134](/assets/image-20210721190152134.DdO-uFq_.png)

在后来的5.1版本升级中，elasticsearch将算法改进为BM25算法，公式如下：

![image-20210721190416214](/assets/image-20210721190416214.Z7t7-uyh.png)

TF-IDF算法有一各缺陷，就是词条频率越高，文档得分也会越高，单个词条对文档影响较大。而BM25则会让单个词条的算分有一个上限，曲线更加平滑：

![image-20210721190907320](/assets/image-20210721190907320.D0JnKqm6.png)

小结：elasticsearch会根据词条和文档的相关度做打分，算法由两种：

* TF-IDF算法
* BM25算法，elasticsearch5.1版本后采用的算法

### 1.5.2.算分函数查询

根据相关度打分是比较合理的需求，但**合理的不一定是产品经理需要**的。

以百度为例，你搜索的结果中，并不是相关度越高排名越靠前，而是谁掏的钱多排名就越靠前。如图：

![image-20210721191144560](/assets/image-20210721191144560.Bepv-AvF.png)

要想认为控制相关性算分，就需要利用elasticsearch中的function score 查询了。

#### 1）语法说明

![image-20210721191544750](/assets/image-20210721191544750.BQXiUG11.png)

function score 查询中包含四部分内容：

* **原始查询**条件：query部分，基于这个条件搜索文档，并且基于BM25算法给文档打分，**原始算分**（query score)
* **过滤条件**：filter部分，符合该条件的文档才会重新算分
* **算分函数**：符合filter条件的文档要根据这个函数做运算，得到的**函数算分**（function score），有四种函数
  * weight：函数结果是常量
  * field\_value\_factor：以文档中的某个字段值作为函数结果
  * random\_score：以随机数作为函数结果
  * script\_score：自定义算分函数算法
* **运算模式**：算分函数的结果、原始查询的相关性算分，两者之间的运算方式，包括：
  * multiply：相乘
  * replace：用function score替换query score
  * 其它，例如：sum、avg、max、min

function score的运行流程如下：

* 1）根据**原始条件**查询搜索文档，并且计算相关性算分，称为**原始算分**（query score）
* 2）根据**过滤条件**，过滤文档
* 3）符合**过滤条件**的文档，基于**算分函数**运算，得到**函数算分**（function score）
* 4）将**原始算分**（query score）和**函数算分**（function score）基于**运算模式**做运算，得到最终结果，作为相关性算分。

因此，其中的关键点是：

* 过滤条件：决定哪些文档的算分被修改
* 算分函数：决定函数算分的算法
* 运算模式：决定最终算分结果

#### 2）示例

需求：给“如家”这个品牌的酒店排名靠前一些

翻译一下这个需求，转换为之前说的四个要点：

* 原始条件：不确定，可以任意变化
* 过滤条件：brand = "如家"
* 算分函数：可以简单粗暴，直接给固定的算分结果，weight
* 运算模式：比如求和

因此最终的DSL语句如下：

```json
GET /hotel/_search
{
  "query": {
    "function_score": {
      "query": {  .... }, // 原始查询，可以是任意条件
      "functions": [ // 算分函数
        {
          "filter": { // 满足的条件，品牌必须是如家
            "term": {
              "brand": "如家"
            }
          },
          "weight": 2 // 算分权重为2
        }
      ],
      "boost_mode": "sum" // 加权模式，求和
    }
  }
}
```

测试，在未添加算分函数时，如家得分如下：

![image-20210721193152520](/assets/image-20210721193152520.CKUaJ0ka.png)

添加了算分函数后，如家得分就提升了：

![image-20210721193458182](/assets/image-20210721193458182.BJC_Ezby.png)

#### 3）小结

function score query定义的三要素是什么？

* 过滤条件：哪些文档要加分
* 算分函数：如何计算function score
* 加权方式：function score 与 query score如何运算

### 1.5.3.布尔查询

布尔查询是一个或多个查询子句的组合，每一个子句就是一个**子查询**。子查询的组合方式有：

* must：必须匹配每个子查询，类似“与”
* should：选择性匹配子查询，类似“或”
* must\_not：必须不匹配，**不参与算分**，类似“非”
* filter：必须匹配，**不参与算分**

比如在搜索酒店时，除了关键字搜索外，我们还可能根据品牌、价格、城市等字段做过滤：

![image-20210721193822848](/assets/image-20210721193822848.CCh3daVG.png)

每一个不同的字段，其查询的条件、方式都不一样，必须是多个不同的查询，而要组合这些查询，就必须用bool查询了。

需要注意的是，搜索时，参与**打分的字段越多，查询的性能也越差**。因此这种多条件查询时，建议这样做：

* 搜索框的关键字搜索，是全文检索查询，使用must查询，参与算分
* 其它过滤条件，采用filter查询。不参与算分

#### 1）语法示例：

```json
GET /hotel/_search
{
  "query": {
    "bool": {
      "must": [
        {"term": {"city": "上海" }}
      ],
      "should": [
        {"term": {"brand": "皇冠假日" }},
        {"term": {"brand": "华美达" }}
      ],
      "must_not": [
        { "range": { "price": { "lte": 500 } }}
      ],
      "filter": [
        { "range": {"score": { "gte": 45 } }}
      ]
    }
  }
}
```

#### 2）示例

需求：搜索名字包含“如家”，价格不高于400，在坐标31.21,121.5周围10km范围内的酒店。

分析：

* 名称搜索，属于全文检索查询，应该参与算分。放到must中
* 价格不高于400，用range查询，属于过滤条件，不参与算分。放到must\_not中
* 周围10km范围内，用geo\_distance查询，属于过滤条件，不参与算分。放到filter中

![image-20210721194744183](/assets/image-20210721194744183.BDuY4r3D.png)

#### 3）小结

bool查询有几种逻辑关系？

* must：必须匹配的条件，可以理解为“与”
* should：选择性匹配的条件，可以理解为“或”
* must\_not：必须不匹配的条件，不参与打分
* filter：必须匹配的条件，不参与打分

# 2.搜索结果处理

搜索的结果可以按照用户指定的方式去处理或展示。

## 2.1.排序

elasticsearch默认是根据相关度算分（\_score）来排序，但是也支持自定义方式对搜索[结果排序](https://www.elastic.co/guide/en/elasticsearch/reference/current/sort-search-results.html)。可以排序字段类型有：keyword类型、数值类型、地理坐标类型、日期类型等。

### 2.1.1.普通字段排序

keyword、数值、日期类型排序的语法基本一致。

**语法**：

```json
GET /indexName/_search
{
  "query": {
    "match_all": {}
  },
  "sort": [
    {
      "FIELD": "desc"  // 排序字段、排序方式ASC、DESC
    }
  ]
}
```

排序条件是一个数组，也就是可以写多个排序条件。按照声明的顺序，当第一个条件相等时，再按照第二个条件排序，以此类推

**示例**：

需求描述：酒店数据按照用户评价（score)降序排序，评价相同的按照价格(price)升序排序

![image-20210721195728306](/assets/image-20210721195728306.B158NSH-.png)

### 2.1.2.地理坐标排序

地理坐标排序略有不同。

**语法说明**：

```json
GET /indexName/_search
{
  "query": {
    "match_all": {}
  },
  "sort": [
    {
      "_geo_distance" : {
          "FIELD" : "纬度，经度", // 文档中geo_point类型的字段名、目标坐标点
          "order" : "asc", // 排序方式
          "unit" : "km" // 排序的距离单位
      }
    }
  ]
}
```

这个查询的含义是：

* 指定一个坐标，作为目标点
* 计算每一个文档中，指定字段（必须是geo\_point类型）的坐标 到目标点的距离是多少
* 根据距离排序

**示例：**

需求描述：实现对酒店数据按照到你的位置坐标的距离升序排序

提示：获取你的位置的经纬度的方式：https://lbs.amap.com/demo/jsapi-v2/example/map/click-to-get-lnglat/

假设我的位置是：31.034661，121.612282，寻找我周围距离最近的酒店。

![image-20210721200214690](/assets/image-20210721200214690.M8hFi1dy.png)

## 2.2.分页

elasticsearch 默认情况下只返回top10的数据。而如果要查询更多数据就需要修改分页参数了。elasticsearch中通过修改from、size参数来控制要返回的分页结果：

* from：从第几个文档开始
* size：总共查询几个文档

类似于mysql中的`limit ?, ?`

### 2.2.1.基本的分页

分页的基本语法如下：

```json
GET /hotel/_search
{
  "query": {
    "match_all": {}
  },
  "from": 0, // 分页开始的位置，默认为0
  "size": 10, // 期望获取的文档总数
  "sort": [
    {"price": "asc"}
  ]
}
```

### 2.2.2.深度分页问题

现在，我要查询990~1000的数据，查询逻辑要这么写：

```json
GET /hotel/_search
{
  "query": {
    "match_all": {}
  },
  "from": 990, // 分页开始的位置，默认为0
  "size": 10, // 期望获取的文档总数
  "sort": [
    {"price": "asc"}
  ]
}
```

这里是查询990开始的数据，也就是 第990~第1000条 数据。

不过，elasticsearch内部分页时，必须先查询 0~1000条，然后截取其中的990 ~ 1000的这10条：

![image-20210721200643029](/assets/image-20210721200643029.eotzKYp-.png)

查询TOP1000，如果es是单点模式，这并无太大影响。

但是elasticsearch将来一定是集群，例如我集群有5个节点，我要查询TOP1000的数据，并不是每个节点查询200条就可以了。

因为节点A的TOP200，在另一个节点可能排到10000名以外了。

因此要想获取整个集群的TOP1000，必须先查询出每个节点的TOP1000，汇总结果后，重新排名，重新截取TOP1000。

![image-20210721201003229](/assets/image-20210721201003229.Dg8DXAOt.png)

那如果我要查询9900~10000的数据呢？是不是要先查询TOP10000呢？那每个节点都要查询10000条？汇总到内存中？

当查询分页深度较大时，汇总数据过多，对内存和CPU会产生非常大的压力，因此elasticsearch会禁止from+ size 超过10000的请求。

针对深度分页，ES提供了两种解决方案，[官方文档](https://www.elastic.co/guide/en/elasticsearch/reference/current/paginate-search-results.html)：

* search after：分页时需要排序，原理是从上一次的排序值开始，查询下一页数据。官方推荐使用的方式。
* scroll：原理将排序后的文档id形成快照，保存在内存。官方已经不推荐使用。

### 2.2.3.小结

分页查询的常见实现方案以及优缺点：

* `from + size`：
  * 优点：支持随机翻页
  * 缺点：深度分页问题，默认查询上限（from + size）是10000
  * 场景：百度、京东、谷歌、淘宝这样的随机翻页搜索

* `after search`：
  * 优点：没有查询上限（单次查询的size不超过10000）
  * 缺点：只能向后逐页查询，不支持随机翻页
  * 场景：没有随机翻页需求的搜索，例如手机向下滚动翻页

* `scroll`：
  * 优点：没有查询上限（单次查询的size不超过10000）
  * 缺点：会有额外内存消耗，并且搜索结果是非实时的
  * 场景：海量数据的获取和迁移。从ES7.1开始不推荐，建议用 after search方案。

## 2.3.高亮

### 2.3.1.高亮原理

什么是高亮显示呢？

我们在百度，京东搜索时，关键字会变成红色，比较醒目，这叫高亮显示：

![image-20210721202705030](/assets/image-20210721202705030.qvua32QS.png)

高亮显示的实现分为两步：

* 1）给文档中的所有关键字都添加一个标签，例如`<em>`标签
* 2）页面给`<em>`标签编写CSS样式

### 2.3.2.实现高亮

**高亮的语法**：

```json
GET /hotel/_search
{
  "query": {
    "match": {
      "FIELD": "TEXT" // 查询条件，高亮一定要使用全文检索查询
    }
  },
  "highlight": {
    "fields": { // 指定要高亮的字段
      "FIELD": {
        "pre_tags": "<em>",  // 用来标记高亮字段的前置标签
        "post_tags": "</em>" // 用来标记高亮字段的后置标签
      }
    }
  }
}
```

**注意：**

* 高亮是对关键字高亮，因此**搜索条件必须带有关键字**，而不能是范围这样的查询。
* 默认情况下，**高亮的字段，必须与搜索指定的字段一致**，否则无法高亮
* 如果要对非搜索字段高亮，则需要添加一个属性：required\_field\_match=false

**示例**：

![image-20210721203349633](/assets/image-20210721203349633.BJPtKN2N.png)

## 2.4.总结

查询的DSL是一个大的JSON对象，包含下列属性：

* query：查询条件
* from和size：分页条件
* sort：排序条件
* highlight：高亮条件

示例：

![image-20210721203657850](/assets/image-20210721203657850.DAP4sV_G.png)

# 3.RestClient查询文档

文档的查询同样适用昨天学习的 RestHighLevelClient对象，基本步骤包括：

* 1）准备Request对象
* 2）准备请求参数
* 3）发起请求
* 4）解析响应

## 3.1.快速入门

我们以match\_all查询为例

### 3.1.1.发起查询请求

![image-20210721203950559](/assets/image-20210721203950559.T_azR6MF.png)

代码解读：

* 第一步，创建`SearchRequest`对象，指定索引库名

* 第二步，利用`request.source()`构建DSL，DSL中可以包含查询、分页、排序、高亮等
  * `query()`：代表查询条件，利用`QueryBuilders.matchAllQuery()`构建一个match\_all查询的DSL

* 第三步，利用client.search()发送请求，得到响应

这里关键的API有两个，一个是`request.source()`，其中包含了查询、排序、分页、高亮等所有功能：

![image-20210721215640790](/assets/image-20210721215640790.DyGkWdMr.png)

另一个是`QueryBuilders`，其中包含match、term、function\_score、bool等各种查询：

![image-20210721215729236](/assets/image-20210721215729236.BbTSQYoo.png)

### 3.1.2.解析响应

响应结果的解析：

![image-20210721214221057](/assets/image-20210721214221057.DDLPFgN5.png)

elasticsearch返回的结果是一个JSON字符串，结构包含：

* `hits`：命中的结果
  * `total`：总条数，其中的value是具体的总条数值
  * `max_score`：所有结果中得分最高的文档的相关性算分
  * `hits`：搜索结果的文档数组，其中的每个文档都是一个json对象
    * `_source`：文档中的原始数据，也是json对象

因此，我们解析响应结果，就是逐层解析JSON字符串，流程如下：

* `SearchHits`：通过response.getHits()获取，就是JSON中的最外层的hits，代表命中的结果
  * `SearchHits#getTotalHits().value`：获取总条数信息
  * `SearchHits#getHits()`：获取SearchHit数组，也就是文档数组
    * `SearchHit#getSourceAsString()`：获取文档结果中的\_source，也就是原始的json文档数据

### 3.1.3.完整代码

完整代码如下：

```java
@Test
void testMatchAll() throws IOException {
    // 1.准备Request
    SearchRequest request = new SearchRequest("hotel");
    // 2.准备DSL
    request.source()
        .query(QueryBuilders.matchAllQuery());
    // 3.发送请求
    SearchResponse response = client.search(request, RequestOptions.DEFAULT);

    // 4.解析响应
    handleResponse(response);
}

private void handleResponse(SearchResponse response) {
    // 4.解析响应
    SearchHits searchHits = response.getHits();
    // 4.1.获取总条数
    long total = searchHits.getTotalHits().value;
    System.out.println("共搜索到" + total + "条数据");
    // 4.2.文档数组
    SearchHit[] hits = searchHits.getHits();
    // 4.3.遍历
    for (SearchHit hit : hits) {
        // 获取文档source
        String json = hit.getSourceAsString();
        // 反序列化
        HotelDoc hotelDoc = JSON.parseObject(json, HotelDoc.class);
        System.out.println("hotelDoc = " + hotelDoc);
    }
}
```

### 3.1.4.小结

查询的基本步骤是：

1. 创建SearchRequest对象

2. 准备Request.source()，也就是DSL。

   ① QueryBuilders来构建查询条件

   ② 传入Request.source() 的 query() 方法

3. 发送请求，得到结果

4. 解析结果（参考JSON结果，从外到内，逐层解析）

## 3.2.match查询

全文检索的match和multi\_match查询与match\_all的API基本一致。差别是查询条件，也就是query的部分。

![image-20210721215923060](/assets/image-20210721215923060.CSAimNqx.png)

因此，Java代码上的差异主要是request.source().query()中的参数了。同样是利用QueryBuilders提供的方法：

![image-20210721215843099](/assets/image-20210721215843099.WKKw_YwK.png)

而结果解析代码则完全一致，可以抽取并共享。

完整代码如下：

```java
@Test
void testMatch() throws IOException {
    // 1.准备Request
    SearchRequest request = new SearchRequest("hotel");
    // 2.准备DSL
    request.source()
        .query(QueryBuilders.matchQuery("all", "如家"));
    // 3.发送请求
    SearchResponse response = client.search(request, RequestOptions.DEFAULT);
    // 4.解析响应
    handleResponse(response);

}
```

## 3.3.精确查询

精确查询主要是两者：

* term：词条精确匹配
* range：范围查询

与之前的查询相比，差异同样在查询条件，其它都一样。

查询条件构造的API如下：

![image-20210721220305140](/assets/image-20210721220305140.XQQII4cg.png)

## 3.4.布尔查询

布尔查询是用must、must\_not、filter等方式组合其它查询，代码示例如下：

![image-20210721220927286](/assets/image-20210721220927286.B0MeMAK7.png)

可以看到，API与其它查询的差别同样是在查询条件的构建，QueryBuilders，结果解析等其他代码完全不变。

完整代码如下：

```java
@Test
void testBool() throws IOException {
    // 1.准备Request
    SearchRequest request = new SearchRequest("hotel");
    // 2.准备DSL
    // 2.1.准备BooleanQuery
    BoolQueryBuilder boolQuery = QueryBuilders.boolQuery();
    // 2.2.添加term
    boolQuery.must(QueryBuilders.termQuery("city", "杭州"));
    // 2.3.添加range
    boolQuery.filter(QueryBuilders.rangeQuery("price").lte(250));

    request.source().query(boolQuery);
    // 3.发送请求
    SearchResponse response = client.search(request, RequestOptions.DEFAULT);
    // 4.解析响应
    handleResponse(response);

}
```

## 3.5.排序、分页

搜索结果的排序和分页是与query同级的参数，因此同样是使用request.source()来设置。

对应的API如下：

![image-20210721221121266](/assets/image-20210721221121266.DTk96no-.png)

完整代码示例：

```java
@Test
void testPageAndSort() throws IOException {
    // 页码，每页大小
    int page = 1, size = 5;

    // 1.准备Request
    SearchRequest request = new SearchRequest("hotel");
    // 2.准备DSL
    // 2.1.query
    request.source().query(QueryBuilders.matchAllQuery());
    // 2.2.排序 sort
    request.source().sort("price", SortOrder.ASC);
    // 2.3.分页 from、size
    request.source().from((page - 1) * size).size(5);
    // 3.发送请求
    SearchResponse response = client.search(request, RequestOptions.DEFAULT);
    // 4.解析响应
    handleResponse(response);

}
```

## 3.6.高亮

高亮的代码与之前代码差异较大，有两点：

* 查询的DSL：其中除了查询条件，还需要添加高亮条件，同样是与query同级。
* 结果解析：结果除了要解析\_source文档数据，还要解析高亮结果

### 3.6.1.高亮请求构建

高亮请求的构建API如下：

![image-20210721221744883](/assets/image-20210721221744883.DeKAojx_.png)

上述代码省略了查询条件部分，但是大家不要忘了：高亮查询必须使用全文检索查询，并且要有搜索关键字，将来才可以对关键字高亮。

完整代码如下：

```java
@Test
void testHighlight() throws IOException {
    // 1.准备Request
    SearchRequest request = new SearchRequest("hotel");
    // 2.准备DSL
    // 2.1.query
    request.source().query(QueryBuilders.matchQuery("all", "如家"));
    // 2.2.高亮
    request.source().highlighter(new HighlightBuilder().field("name").requireFieldMatch(false));
    // 3.发送请求
    SearchResponse response = client.search(request, RequestOptions.DEFAULT);
    // 4.解析响应
    handleResponse(response);

}
```

### 3.6.2.高亮结果解析

高亮的结果与查询的文档结果默认是分离的，并不在一起。

因此解析高亮的代码需要额外处理：

![image-20210721222057212](/assets/image-20210721222057212.3HbuB8lR.png)

代码解读：

* 第一步：从结果中获取source。hit.getSourceAsString()，这部分是非高亮结果，json字符串。还需要反序列为HotelDoc对象
* 第二步：获取高亮结果。hit.getHighlightFields()，返回值是一个Map，key是高亮字段名称，值是HighlightField对象，代表高亮值
* 第三步：从map中根据高亮字段名称，获取高亮字段值对象HighlightField
* 第四步：从HighlightField中获取Fragments，并且转为字符串。这部分就是真正的高亮字符串了
* 第五步：用高亮的结果替换HotelDoc中的非高亮结果

完整代码如下：

```java
private void handleResponse(SearchResponse response) {
    // 4.解析响应
    SearchHits searchHits = response.getHits();
    // 4.1.获取总条数
    long total = searchHits.getTotalHits().value;
    System.out.println("共搜索到" + total + "条数据");
    // 4.2.文档数组
    SearchHit[] hits = searchHits.getHits();
    // 4.3.遍历
    for (SearchHit hit : hits) {
        // 获取文档source
        String json = hit.getSourceAsString();
        // 反序列化
        HotelDoc hotelDoc = JSON.parseObject(json, HotelDoc.class);
        // 获取高亮结果
        Map<String, HighlightField> highlightFields = hit.getHighlightFields();
        if (!CollectionUtils.isEmpty(highlightFields)) {
            // 根据字段名获取高亮结果
            HighlightField highlightField = highlightFields.get("name");
            if (highlightField != null) {
                // 获取高亮值
                String name = highlightField.getFragments()[0].string();
                // 覆盖非高亮结果
                hotelDoc.setName(name);
            }
        }
        System.out.println("hotelDoc = " + hotelDoc);
    }
}
```

# 4.实战案例

下面，通过案例来实战演练下之前学习的知识。

实现四部分功能：

* 酒店搜索和分页
* 酒店结果过滤
* 我周边的酒店
* 酒店竞价排名

启动我们提供的hotel-demo项目，其默认端口是8089，访问http://localhost:8090，就能看到项目页面了：

![image-20210721223159598](/assets/image-20210721223159598.BDTRO2th.png)

## 4.1.酒店搜索和分页

案例需求：实现黑马旅游的酒店搜索功能，完成关键字搜索和分页

### 4.1.1.需求分析

在项目的首页，有一个大大的搜索框，还有分页按钮：

![image-20210721223859419](/assets/image-20210721223859419.DF-kD-g_.png)

点击搜索按钮，可以看到浏览器控制台发出了请求：

![image-20210721224033789](/assets/image-20210721224033789.BFYyUOc5.png)

请求参数如下：

![image-20210721224112708](/assets/image-20210721224112708.BZJM-5an.png)

由此可以知道，我们这个请求的信息如下：

* 请求方式：POST
* 请求路径：/hotel/list
* 请求参数：JSON对象，包含4个字段：
  * key：搜索关键字
  * page：页码
  * size：每页大小
  * sortBy：排序，目前暂不实现
* 返回值：分页查询，需要返回分页结果PageResult，包含两个属性：
  * `total`：总条数
  * `List<HotelDoc>`：当前页的数据

因此，我们实现业务的流程如下：

* 步骤一：定义实体类，接收请求参数的JSON对象
* 步骤二：编写controller，接收页面的请求
* 步骤三：编写业务实现，利用RestHighLevelClient实现搜索、分页

### 4.1.2.定义实体类

实体类有两个，一个是前端的请求参数实体，一个是服务端应该返回的响应结果实体。

1）请求参数

前端请求的json结构如下：

```json
{
    "key": "搜索关键字",
    "page": 1,
    "size": 3,
    "sortBy": "default"
}
```

因此，我们在`cn.itcast.hotel.pojo`包下定义一个实体类：

```java
package cn.itcast.hotel.pojo;

import lombok.Data;

@Data
public class RequestParams {
    private String key;
    private Integer page;
    private Integer size;
    private String sortBy;
}
```

2）返回值

分页查询，需要返回分页结果PageResult，包含两个属性：

* `total`：总条数
* `List<HotelDoc>`：当前页的数据

因此，我们在`cn.itcast.hotel.pojo`中定义返回结果：

```java
package cn.itcast.hotel.pojo;

import lombok.Data;

import java.util.List;

@Data
public class PageResult {
    private Long total;
    private List<HotelDoc> hotels;

    public PageResult() {
    }

    public PageResult(Long total, List<HotelDoc> hotels) {
        this.total = total;
        this.hotels = hotels;
    }
}
```

### 4.1.3.定义controller

定义一个HotelController，声明查询接口，满足下列要求：

* 请求方式：Post
* 请求路径：/hotel/list
* 请求参数：对象，类型为RequestParam
* 返回值：PageResult，包含两个属性
  * `Long total`：总条数
  * `List<HotelDoc> hotels`：酒店数据

因此，我们在`cn.itcast.hotel.web`中定义HotelController：

```java
@RestController
@RequestMapping("/hotel")
public class HotelController {

    @Autowired
    private IHotelService hotelService;
	// 搜索酒店数据
    @PostMapping("/list")
    public PageResult search(@RequestBody RequestParams params){
        return hotelService.search(params);
    }
}
```

### 4.1.4.实现搜索业务

我们在controller调用了IHotelService，并没有实现该方法，因此下面我们就在IHotelService中定义方法，并且去实现业务逻辑。

1）在`cn.itcast.hotel.service`中的`IHotelService`接口中定义一个方法：

```java
/**
 * 根据关键字搜索酒店信息
 * @param params 请求参数对象，包含用户输入的关键字 
 * @return 酒店文档列表
 */
PageResult search(RequestParams params);
```

2）实现搜索业务，肯定离不开RestHighLevelClient，我们需要把它注册到Spring中作为一个Bean。在`cn.itcast.hotel`中的`HotelDemoApplication`中声明这个Bean：

```java
@Bean
public RestHighLevelClient client(){
    return  new RestHighLevelClient(RestClient.builder(
        HttpHost.create("http://192.168.150.101:9200")
    ));
}
```

3）在`cn.itcast.hotel.service.impl`中的`HotelService`中实现search方法：

```java
@Override
public PageResult search(RequestParams params) {
    try {
        // 1.准备Request
        SearchRequest request = new SearchRequest("hotel");
        // 2.准备DSL
        // 2.1.query
        String key = params.getKey();
        if (key == null || "".equals(key)) {
            boolQuery.must(QueryBuilders.matchAllQuery());
        } else {
            boolQuery.must(QueryBuilders.matchQuery("all", key));
        }

        // 2.2.分页
        int page = params.getPage();
        int size = params.getSize();
        request.source().from((page - 1) * size).size(size);

        // 3.发送请求
        SearchResponse response = client.search(request, RequestOptions.DEFAULT);
        // 4.解析响应
        return handleResponse(response);
    } catch (IOException e) {
        throw new RuntimeException(e);
    }
}

// 结果解析
private PageResult handleResponse(SearchResponse response) {
    // 4.解析响应
    SearchHits searchHits = response.getHits();
    // 4.1.获取总条数
    long total = searchHits.getTotalHits().value;
    // 4.2.文档数组
    SearchHit[] hits = searchHits.getHits();
    // 4.3.遍历
    List<HotelDoc> hotels = new ArrayList<>();
    for (SearchHit hit : hits) {
        // 获取文档source
        String json = hit.getSourceAsString();
        // 反序列化
        HotelDoc hotelDoc = JSON.parseObject(json, HotelDoc.class);
		// 放入集合
        hotels.add(hotelDoc);
    }
    // 4.4.封装返回
    return new PageResult(total, hotels);
}
```

## 4.2.酒店结果过滤

需求：添加品牌、城市、星级、价格等过滤功能

### 4.2.1.需求分析

在页面搜索框下面，会有一些过滤项：

![image-20210722091940726](/assets/image-20210722091940726.De0-Alwg.png)

传递的参数如图：

![image-20210722092051994](/assets/image-20210722092051994.CfafyQU_.png)

包含的过滤条件有：

* brand：品牌值
* city：城市
* minPrice~maxPrice：价格范围
* starName：星级

我们需要做两件事情：

* 修改请求参数的对象RequestParams，接收上述参数
* 修改业务逻辑，在搜索条件之外，添加一些过滤条件

### 4.2.2.修改实体类

修改在`cn.itcast.hotel.pojo`包下的实体类RequestParams：

```java
@Data
public class RequestParams {
    private String key;
    private Integer page;
    private Integer size;
    private String sortBy;
    // 下面是新增的过滤条件参数
    private String city;
    private String brand;
    private String starName;
    private Integer minPrice;
    private Integer maxPrice;
}
```

### 4.2.3.修改搜索业务

在HotelService的search方法中，只有一个地方需要修改：requet.source().query( ... )其中的查询条件。

在之前的业务中，只有match查询，根据关键字搜索，现在要添加条件过滤，包括：

* 品牌过滤：是keyword类型，用term查询
* 星级过滤：是keyword类型，用term查询
* 价格过滤：是数值类型，用range查询
* 城市过滤：是keyword类型，用term查询

多个查询条件组合，肯定是boolean查询来组合：

* 关键字搜索放到must中，参与算分
* 其它过滤条件放到filter中，不参与算分

因为条件构建的逻辑比较复杂，这里先封装为一个函数：

![image-20210722092935453](/assets/image-20210722092935453.CQ0yHoFF.png)

buildBasicQuery的代码如下：

```java
private void buildBasicQuery(RequestParams params, SearchRequest request) {
    // 1.构建BooleanQuery
    BoolQueryBuilder boolQuery = QueryBuilders.boolQuery();
    // 2.关键字搜索
    String key = params.getKey();
    if (key == null || "".equals(key)) {
        boolQuery.must(QueryBuilders.matchAllQuery());
    } else {
        boolQuery.must(QueryBuilders.matchQuery("all", key));
    }
    // 3.城市条件
    if (params.getCity() != null && !params.getCity().equals("")) {
        boolQuery.filter(QueryBuilders.termQuery("city", params.getCity()));
    }
    // 4.品牌条件
    if (params.getBrand() != null && !params.getBrand().equals("")) {
        boolQuery.filter(QueryBuilders.termQuery("brand", params.getBrand()));
    }
    // 5.星级条件
    if (params.getStarName() != null && !params.getStarName().equals("")) {
        boolQuery.filter(QueryBuilders.termQuery("starName", params.getStarName()));
    }
	// 6.价格
    if (params.getMinPrice() != null && params.getMaxPrice() != null) {
        boolQuery.filter(QueryBuilders
                         .rangeQuery("price")
                         .gte(params.getMinPrice())
                         .lte(params.getMaxPrice())
                        );
    }
	// 7.放入source
    request.source().query(boolQuery);
}
```

## 4.3.我周边的酒店

需求：我附近的酒店

### 4.3.1.需求分析

在酒店列表页的右侧，有一个小地图，点击地图的定位按钮，地图会找到你所在的位置：

![image-20210722093414542](/assets/image-20210722093414542.-VMedQxc.png)

并且，在前端会发起查询请求，将你的坐标发送到服务端：

![image-20210722093642382](/assets/image-20210722093642382.D4CUhlHN.png)

我们要做的事情就是基于这个location坐标，然后按照距离对周围酒店排序。实现思路如下：

* 修改RequestParams参数，接收location字段
* 修改search方法业务逻辑，如果location有值，添加根据geo\_distance排序的功能

### 4.3.2.修改实体类

修改在`cn.itcast.hotel.pojo`包下的实体类RequestParams：

```java
package cn.itcast.hotel.pojo;

import lombok.Data;

@Data
public class RequestParams {
    private String key;
    private Integer page;
    private Integer size;
    private String sortBy;
    private String city;
    private String brand;
    private String starName;
    private Integer minPrice;
    private Integer maxPrice;
    // 我当前的地理坐标
    private String location;
}

```

### 4.3.3.距离排序API

我们以前学习过排序功能，包括两种：

* 普通字段排序
* 地理坐标排序

我们只讲了普通字段排序对应的java写法。地理坐标排序只学过DSL语法，如下：

```json
GET /indexName/_search
{
  "query": {
    "match_all": {}
  },
  "sort": [
    {
      "price": "asc"  
    },
    {
      "_geo_distance" : {
          "FIELD" : "纬度，经度",
          "order" : "asc",
          "unit" : "km"
      }
    }
  ]
}
```

对应的java代码示例：

![image-20210722095227059](/assets/image-20210722095227059.BjMVjuer.png)

### 4.3.4.添加距离排序

在`cn.itcast.hotel.service.impl`的`HotelService`的`search`方法中，添加一个排序功能：

![image-20210722095902314](/assets/image-20210722095902314.CY5vrwkt.png)

完整代码：

```java
@Override
public PageResult search(RequestParams params) {
    try {
        // 1.准备Request
        SearchRequest request = new SearchRequest("hotel");
        // 2.准备DSL
        // 2.1.query
        buildBasicQuery(params, request);

        // 2.2.分页
        int page = params.getPage();
        int size = params.getSize();
        request.source().from((page - 1) * size).size(size);

        // 2.3.排序
        String location = params.getLocation();
        if (location != null && !location.equals("")) {
            request.source().sort(SortBuilders
                                  .geoDistanceSort("location", new GeoPoint(location))
                                  .order(SortOrder.ASC)
                                  .unit(DistanceUnit.KILOMETERS)
                                 );
        }

        // 3.发送请求
        SearchResponse response = client.search(request, RequestOptions.DEFAULT);
        // 4.解析响应
        return handleResponse(response);
    } catch (IOException e) {
        throw new RuntimeException(e);
    }
}
```

### 4.3.5.排序距离显示

重启服务后，测试我的酒店功能：

![image-20210722100040674](/assets/image-20210722100040674.BbUJ8iD3.png)

发现确实可以实现对我附近酒店的排序，不过并没有看到酒店到底距离我多远，这该怎么办？

排序完成后，页面还要获取我附近每个酒店的具体**距离**值，这个值在响应结果中是独立的：

![image-20210722095648542](/assets/image-20210722095648542.DsVyO0Zr.png)

因此，我们在结果解析阶段，除了解析source部分以外，还要得到sort部分，也就是排序的距离，然后放到响应结果中。

我们要做两件事：

* 修改HotelDoc，添加排序距离字段，用于页面显示
* 修改HotelService类中的handleResponse方法，添加对sort值的获取

1）修改HotelDoc类，添加距离字段

```java
package cn.itcast.hotel.pojo;

import lombok.Data;
import lombok.NoArgsConstructor;


@Data
@NoArgsConstructor
public class HotelDoc {
    private Long id;
    private String name;
    private String address;
    private Integer price;
    private Integer score;
    private String brand;
    private String city;
    private String starName;
    private String business;
    private String location;
    private String pic;
    // 排序时的 距离值
    private Object distance;

    public HotelDoc(Hotel hotel) {
        this.id = hotel.getId();
        this.name = hotel.getName();
        this.address = hotel.getAddress();
        this.price = hotel.getPrice();
        this.score = hotel.getScore();
        this.brand = hotel.getBrand();
        this.city = hotel.getCity();
        this.starName = hotel.getStarName();
        this.business = hotel.getBusiness();
        this.location = hotel.getLatitude() + ", " + hotel.getLongitude();
        this.pic = hotel.getPic();
    }
}

```

2）修改HotelService中的handleResponse方法

![image-20210722100613966](/assets/image-20210722100613966.agMcUOpy.png)

重启后测试，发现页面能成功显示距离了：

![image-20210722100838604](/assets/image-20210722100838604.DwmR5dGW.png)

## 4.4.酒店竞价排名

需求：让指定的酒店在搜索结果中排名置顶

### 4.4.1.需求分析

要让指定酒店在搜索结果中排名置顶，效果如图：

![image-20210722100947292](/assets/image-20210722100947292.BqfGrOoT.png)

页面会给指定的酒店添加**广告**标记。

那怎样才能让指定的酒店排名置顶呢？

我们之前学习过的function\_score查询可以影响算分，算分高了，自然排名也就高了。而function\_score包含3个要素：

* 过滤条件：哪些文档要加分
* 算分函数：如何计算function score
* 加权方式：function score 与 query score如何运算

这里的需求是：让**指定酒店**排名靠前。因此我们需要给这些酒店添加一个标记，这样在过滤条件中就可以**根据这个标记来判断，是否要提高算分**。

比如，我们给酒店添加一个字段：isAD，Boolean类型：

* true：是广告
* false：不是广告

这样function\_score包含3个要素就很好确定了：

* 过滤条件：判断isAD 是否为true
* 算分函数：我们可以用最简单暴力的weight，固定加权值
* 加权方式：可以用默认的相乘，大大提高算分

因此，业务的实现步骤包括：

1. 给HotelDoc类添加isAD字段，Boolean类型

2. 挑选几个你喜欢的酒店，给它的文档数据添加isAD字段，值为true

3. 修改search方法，添加function score功能，给isAD值为true的酒店增加权重

### 4.4.2.修改HotelDoc实体

给`cn.itcast.hotel.pojo`包下的HotelDoc类添加isAD字段：

![image-20210722101908062](/assets/image-20210722101908062.DGCTPCi0.png)

### 4.4.3.添加广告标记

接下来，我们挑几个酒店，添加isAD字段，设置为true：

```json
POST /hotel/_update/1902197537
{
    "doc": {
        "isAD": true
    }
}
POST /hotel/_update/2056126831
{
    "doc": {
        "isAD": true
    }
}
POST /hotel/_update/1989806195
{
    "doc": {
        "isAD": true
    }
}
POST /hotel/_update/2056105938
{
    "doc": {
        "isAD": true
    }
}
```

### 4.4.4.添加算分函数查询

接下来我们就要修改查询条件了。之前是用的boolean 查询，现在要改成function\_socre查询。

function\_score查询结构如下：

![image-20210721191544750](/assets/image-20210721191544750.BQXiUG11.png)

对应的JavaAPI如下：

![image-20210722102850818](/assets/image-20210722102850818.ErbAXqmG.png)

我们可以将之前写的boolean查询作为**原始查询**条件放到query中，接下来就是添加**过滤条件**、**算分函数**、**加权模式**了。所以原来的代码依然可以沿用。

修改`cn.itcast.hotel.service.impl`包下的`HotelService`类中的`buildBasicQuery`方法，添加算分函数查询：

```java
private void buildBasicQuery(RequestParams params, SearchRequest request) {
    // 1.构建BooleanQuery
    BoolQueryBuilder boolQuery = QueryBuilders.boolQuery();
    // 关键字搜索
    String key = params.getKey();
    if (key == null || "".equals(key)) {
        boolQuery.must(QueryBuilders.matchAllQuery());
    } else {
        boolQuery.must(QueryBuilders.matchQuery("all", key));
    }
    // 城市条件
    if (params.getCity() != null && !params.getCity().equals("")) {
        boolQuery.filter(QueryBuilders.termQuery("city", params.getCity()));
    }
    // 品牌条件
    if (params.getBrand() != null && !params.getBrand().equals("")) {
        boolQuery.filter(QueryBuilders.termQuery("brand", params.getBrand()));
    }
    // 星级条件
    if (params.getStarName() != null && !params.getStarName().equals("")) {
        boolQuery.filter(QueryBuilders.termQuery("starName", params.getStarName()));
    }
    // 价格
    if (params.getMinPrice() != null && params.getMaxPrice() != null) {
        boolQuery.filter(QueryBuilders
                         .rangeQuery("price")
                         .gte(params.getMinPrice())
                         .lte(params.getMaxPrice())
                        );
    }

    // 2.算分控制
    FunctionScoreQueryBuilder functionScoreQuery =
        QueryBuilders.functionScoreQuery(
        // 原始查询，相关性算分的查询
        boolQuery,
        // function score的数组
        new FunctionScoreQueryBuilder.FilterFunctionBuilder[]{
            // 其中的一个function score 元素
            new FunctionScoreQueryBuilder.FilterFunctionBuilder(
                // 过滤条件
                QueryBuilders.termQuery("isAD", true),
                // 算分函数
                ScoreFunctionBuilders.weightFactorFunction(10)
            )
        });
    request.source().query(functionScoreQuery);
}
```

---

---
url: /Java/架构设计/分布式/07.分布式日志收集/0_ElasticStack概述.md
---

# ElasticStack概述

> 官网：https://www.elastic.co/cn

## 分布式日志的出现

随着现在各种软件系统的复杂度越来越高，特别是部署到云上之后，再想登录各个节点上查看各个模块的 log，基本是不可行了。因为不仅效率低下，而且有时由于安全性，不可能让工程师直接访问各个物理节点。而且现在大规模的软件系统基本都采用集群的部署方式，意味着对每个 service，会启动多个完全一样的 POD 对外提供服务，每个 container 都会产生自己的 log，仅从产生的 log 来看，你根本不知道是哪个 POD 产生的，这样对查看分布式的日志更加困难。

所以在云时代，需要一个收集并分析 log 的解决方案。首先需要将分布在各个角落的 log 收集到一个集中的地方，方便查看。收集了之后，还可以进行各种统计分析，甚至用流行的大数据或 maching learning 的方法进行分析。当然，对于传统的软件部署方式，也需要这样的 log 的解决方案。

## **Elastic Stack 技术栈**

包含了数据的整合 => 提取 => 存储 => 使用，一整套！

各组件介绍：

* beats 套件：从各种不同类型的文件 / 应用中采集数据。比如：a,b,c,d,e,aa,bb,cc
* Logstash：从多个采集器或数据源来抽取 / 转换数据，向 es 输送。比如：a,bb,cc
* elasticsearch：存储、查询数据
* kibana：可视化 es 的数据

## ELK概念

ELK 作为日志收集的一种方案，基本就是事实上的标准。ELK 是三个开源项目的首字母缩写，如下：

> E: Elasticsearch
>
> L: Logstash
>
> K: Kibana

Logstash 的主要作用是收集分布在各处的 log 并进行处理；

Elasticsearch 则是一个集中存储 log 的地方，更重要的是它是一个全文检索以及分析的引擎，它能让用户以近乎实时的方式来查看、分析海量的数据；

Kibana 则是为 Elasticsearch 开发的前端 GUI，让用户可以很方便的以图形化的接口查询 Elasticsearch 中存储的数据，同时也提供了各种分析的模块，比如构建 dashboard 的功能。

将 ELK 中的 L 理解成 Logging Agent 更合适。Elasticsearch 和 Kibana 基本就是存储、检索和分析 log 的标准方案，而 Logstash 则并不是唯一的收集 log 的方案，Fluentd 和 Filebeats 也能用于收集 log。所以现在网上有 ELK，EFK 之类的缩写。

通常一个小型的 cluster 有三个节点，在这三个节点上可能会运行几十个甚至上百个容器。而我们只需要在每个节点上启动一个 logging agent 的实例（在 kubernetes 中就是 DaemonSet 的概念）即可。

## **Elasticsearch 概念**

> Elasticsearch详细文档可参考分布式搜索中Elasticsearch学习笔记

把它当成 MySQL 一样的数据库去学习和理解。

入门学习：

* Index 索引 => MySQL 里的表（table）
* 建表、增删改查（查询需要花费的学习时间最多）
* 用客户端去调用 ElasticSearch（3 种）
* 语法：SQL、代码的方法（4 种语法）

ES 相比于 MySQL，能够自动帮我们做分词，能够非常高效、灵活地查询内容。

---

---
url: /Java/架构设计/分布式/07.分布式日志收集/1_Elasticsearch安装.md
---

# ELK篇之安装ElasticSearch和Kibana

传统方式安装ES是一件比较费劲的事情，使用Docker能够非常轻松的安装ElasticSearch。而Kibana是一个针对Elasticsearch的开源分析及可视化平台，使用Kibana可以查询、查看并与存储在ES索引的数据进行交互操作，使用Kibana能执行高级的数据分析，并能以图表、表格和地图的形式查看数据。

> 🐉注意：**只要是一套技术，所有版本必须一致！！！**

## 使用Docker部署单点ES

### 创建网络

需要部署kibana容器，因此需要让es和kibana容器互联。这里先创建一个网络

```sh
docker network create es-net
```

### 加载镜像

> Elasticsearch官网教程：https://www.elastic.co/guide/en/elasticsearch/reference/7.17/setup.html

```sh
# 方式一：从官网下载后，本地上传到虚拟机中，然后运行命令加载即可
docker load -i es.tar
# 方式二：获取docker镜像
docker pull elasticsearch:7.17.0
```

### 运行

运行docker命令，部署单点es：

```sh
docker run -d \
	--name es \
    -e "ES_JAVA_OPTS=-Xms512m -Xmx512m" \
    -e "discovery.type=single-node" \
    -v es-data:/usr/share/elasticsearch/data \
    -v es-plugins:/usr/share/elasticsearch/plugins \
    --privileged \
    --network es-net \
    -p 9200:9200 \
    -p 9300:9300 \
elasticsearch:7.17.0
```

命令解释：

* `-e "cluster.name=es-docker-cluster"`：设置集群名称
* `-e "http.host=0.0.0.0"`：监听的地址，可以外网访问
* `-e "ES_JAVA_OPTS=-Xms512m -Xmx512m"`：内存大小
* `-e "discovery.type=single-node"`：非集群模式
* `-v es-data:/usr/share/elasticsearch/data`：挂载逻辑卷，绑定es的数据目录
* `-v es-logs:/usr/share/elasticsearch/logs`：挂载逻辑卷，绑定es的日志目录
* `-v es-plugins:/usr/share/elasticsearch/plugins`：挂载逻辑卷，绑定es的插件目录
* `--privileged`：授予逻辑卷访问权
* `--network es-net` ：加入一个名为es-net的网络中
* `-p 9200:9200`：端口映射配置

在浏览器中输入：`http://服务器IP地址:9200` ，即可看到elasticsearch的响应结果：

```json
{
  "name" : "e2a76165fe3f",
  "cluster_name" : "docker-cluster",
  "cluster_uuid" : "V2ivzrgSTWCP4E_gjWrBww",
  "version" : {
    "number" : "7.17.0",
    "build_flavor" : "default",
    "build_type" : "docker",
    "build_hash" : "bee86328705acaa9a6daede7140defd4d9ec56bd",
    "build_date" : "2022-01-28T08:36:04.875279988Z",
    "build_snapshot" : false,
    "lucene_version" : "8.11.1",
    "minimum_wire_compatibility_version" : "6.8.0",
    "minimum_index_compatibility_version" : "6.0.0-beta1"
  },
  "tagline" : "You Know, for Search"
}
```

## 使用Docker部署Kibana

### 部署

> Kibana官网教程：https://www.elastic.co/guide/en/elasticsearch/reference/7.17/setup.html

```sh
# 方式一：从官网下载后，本地上传到虚拟机中，然后运行命令加载即可
docker load -i kibana.tar
# 方式二：获取docker镜像
docker pull kibana:7.17.0
```

### 运行

运行docker命令，部署kibana

```sh
docker run -d \
--name kibana \
-e ELASTICSEARCH_HOSTS=http://es:9200 \
--network=es-net \
-p 5601:5601  \
kibana:7.17.0
```

* `--network es-net` ：加入一个名为es-net的网络中，与elasticsearch在同一个网络中

* `-e ELASTICSEARCH_HOSTS=http://es:9200"`：设置elasticsearch的地址，因为kibana已经与elasticsearch在一个网络，因此可以用容器名直接访问elasticsearch

  ```sh
  # 注意：如果忘记设置该项，需要进入容器修改yml中的es地址
  # 1.进入容器
  docker exec -it kibana bash
  # 2.修改ElasticSearch地址
  vi /usr/share/kibana/config/kibana.yml
  # 3.测试：重启kibana容器,访问 http://ip地址:5601
  docker restart kibana
  ```

* `-p 5601:5601`：端口映射配置

kibana启动一般比较慢，需要多等待一会，可以通过命令：

```sh
docker logs -f kibana
```

查看运行日志，当查看到下面的日志，说明成功：

```sh
{"type":"log","@timestamp":"2023-11-14T14:29:23+00:00","tags":["info","plugins-service"],"pid":7,"message":"Plugin \"metricsEntities\" is disabled."}
{"type":"log","@timestamp":"2023-11-14T14:29:23+00:00","tags":["info","http","server","Preboot"],"pid":7,"message":"http server running at http://0.0.0.0:5601"}
```

此时，在浏览器输入地址访问：`http://服务器IP地址:5601`，即可看到结果。

### 扩展：基于数据卷加载配置文件方式运行

* a.从容器复制kibana配置文件出来
* b.修改配置文件为对应ES服务器地址
* c.通过数据卷加载配置文件方式启动

```shell
docker run -d -v /home/tools/kibana/kibana.yml:/usr/share/kibana/config/kibana.yml  --name kibana -p 5601:5601 kibana:7.17.0
```

### DevTools

kibana中提供了一个DevTools界面编写DSL来操作elasticsearch。并且对DSL语句有自动补全功能。

Home——Management——Dev Tools下

```json
GET _search
{
  "query": {
    "match_all": {}
  }
}
```

### 设置中文

```sh
# 查看Kibana容器id
docker ps 
# 进入容器
docker exec -it Kibana容器id bash
# 进入config 目录下
cd config/
# 编辑 kibana.yml 文件
vi kibana.yml 
# 添加一行配置即可
i18n.locale: "zh-CN"
# 退出容器
exit
# 重启Kibana
docker restart Kibana容器id/name
```

问题1：执行`vi kibana.yml`报错 bash: vi: command not found

```sh
# 在容器内更新
apt-get update
# 然后安装vim
apt-get install vim
```

问题2：使用`apt`命令报错 E: List directory /var/lib/apt/lists/partial is missing. - Acquire (13: Permission）

```sh
# 权限不够，使用root权限进入容器
docker exec -u 0 -it Kibana容器id /bin/bash # 0 表示root
# 然后就可以使用apt-get命令了
```

## 使用Docker安装IK分词器

### 在线安装ik插件（较慢）

```sh
# 进入容器内部
docker exec -it es /bin/bash

# 在线下载并安装
./bin/elasticsearch-plugin  install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.17.0/elasticsearch-analysis-ik-7.17.0.zip

#退出
exit
#重启容器
docker restart es
```

### 离线安装ik插件（推荐）

> 官方下载：https://github.com/medcl/elasticsearch-analysis-ik/releases/tag/v7.17.7

1、查看数据卷目录

安装插件需要知道elasticsearch的plugins目录位置，如果用了数据卷挂载方式，需要查看elasticsearch的数据卷目录，通过下面命令查看:

```sh
docker volume inspect es-plugins
```

显示结果：

```sh
[
    {
        "CreatedAt": "2023-11-14T22:04:34+08:00",
        "Driver": "local",
        "Labels": null,
        "Mountpoint": "/var/lib/docker/volumes/es-plugins/_data",
        "Name": "es-plugins",
        "Options": null,
        "Scope": "local"
    }
]
```

说明plugins目录被挂载到了：`/var/lib/docker/volumes/es-plugins/_data `这个目录中。

2、解压缩分词器安装包

从官网下载ik分词器压缩包，解压缩，重命名为ik

3、上传到es容器的插件数据卷中

上传到es容器的插件数据卷中，也就是`/var/lib/docker/volumes/es-plugins/_data `

重启容器

```sh
# 4、重启容器
docker restart es
# 查看es日志
docker logs -f es
```

4、测试

IK分词器包含两种模式：

* `ik_smart`：最少切分
* `ik_max_word`：最细切分

```json
GET /_analyze
{
  "analyzer": "ik_max_word",
  "text": "徐晓龙和小狐狸学elasticsearch"
}
```

结果：

```json
{
  "tokens" : [
    {
      "token" : "徐",
      "start_offset" : 0,
      "end_offset" : 1,
      "type" : "CN_CHAR",
      "position" : 0
    },
    {
      "token" : "晓",
      "start_offset" : 1,
      "end_offset" : 2,
      "type" : "CN_CHAR",
      "position" : 1
    },
    {
      "token" : "龙",
      "start_offset" : 2,
      "end_offset" : 3,
      "type" : "CN_CHAR",
      "position" : 2
    },
    {
      "token" : "和",
      "start_offset" : 3,
      "end_offset" : 4,
      "type" : "CN_CHAR",
      "position" : 3
    },
    {
      "token" : "小",
      "start_offset" : 4,
      "end_offset" : 5,
      "type" : "CN_CHAR",
      "position" : 4
    },
    {
      "token" : "狐狸",
      "start_offset" : 5,
      "end_offset" : 7,
      "type" : "CN_WORD",
      "position" : 5
    },
    {
      "token" : "学",
      "start_offset" : 7,
      "end_offset" : 8,
      "type" : "CN_CHAR",
      "position" : 6
    },
    {
      "token" : "elasticsearch",
      "start_offset" : 8,
      "end_offset" : 21,
      "type" : "ENGLISH",
      "position" : 7
    }
  ]
}
```

### 扩展词词典

随着互联网的发展，“造词运动”也越发的频繁。出现了很多新的词语，在原有的词汇列表中并不存在。比如：“奥力给”，“小黑子” 等。

所以词汇也需要不断的更新，IK分词器提供了扩展词汇的功能。

1）打开IK分词器config目录：

`/var/lib/docker/volumes/es-plugins/_data/ik/config `

在IKAnalyzer.cfg.xml配置文件内容添加：

```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE properties SYSTEM "http://java.sun.com/dtd/properties.dtd">
<properties>
        <comment>IK Analyzer 扩展配置</comment>
        <!--用户可以在这里配置自己的扩展字典 *** 添加扩展词典-->
        <entry key="ext_dict">ext.dic</entry>
</properties>
```

新建一个 ext.dic，可以参考config目录下复制一个配置文件进行修改

```properties
奥力给
小黑子
```

重启elasticsearch

```sh
docker restart es

# 查看 日志
docker logs -f es
```

日志中已经成功加载ext.dic配置文件

5）测试效果：

```json
GET /_analyze
{
  "analyzer": "ik_max_word",
  "text": "小黑子得到了乐趣，哥哥得到了热度，只有真爱粉破防了。"
}
```

> 注意当前文件的编码必须是 UTF-8 格式，严禁使用Windows记事本编辑

### 停用词词典

在互联网项目中，在网络间传输的速度很快，所以很多语言是不允许在网络上传递的，如：关于宗教、政治等敏感词语，那么在搜索时也应该忽略当前词汇。

IK分词器也提供了强大的停用词功能，让我们在索引时就直接忽略当前的停用词汇表中的内容。

1）IKAnalyzer.cfg.xml配置文件内容添加：

```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE properties SYSTEM "http://java.sun.com/dtd/properties.dtd">
<properties>
        <comment>IK Analyzer 扩展配置</comment>
        <!--用户可以在这里配置自己的扩展字典-->
        <entry key="ext_dict">ext.dic</entry>
         <!--用户可以在这里配置自己的扩展停止词字典  *** 添加停用词词典-->
        <entry key="ext_stopwords">stopword.dic</entry>
</properties>
```

3）在 stopword.dic 添加停用词

```properties
习大大
```

4）重启elasticsearch

```sh
# 重启服务
docker restart es
docker restart kibana

# 查看 日志
docker logs -f es
```

日志中已经成功加载stopword.dic配置文件

5）测试效果：

```json
GET /_analyze
{
  "analyzer": "ik_max_word",
  "text": "习大大都点赞,奥力给！"
}
```

> 注意当前文件的编码必须是 UTF-8 格式，严禁使用Windows记事本编辑

## 使用docker-compose部署ES集群

部署es集群可以直接使用docker-compose来完成，不过要求Linux虚拟机至少有**4G**的内存空间。

编写一个docker-compose文件：

```yml
version: '2.2'
services:
  es01:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.0
    container_name: es01
    environment:
      - node.name=es01
      - cluster.name=es-docker-cluster
      - discovery.seed_hosts=es02,es03
      - cluster.initial_master_nodes=es01,es02,es03
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - data01:/usr/share/elasticsearch/data
    ports:
      - 9200:9200
    networks:
      - elastic
  es02:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.0
    container_name: es02
    environment:
      - node.name=es02
      - cluster.name=es-docker-cluster
      - discovery.seed_hosts=es01,es03
      - cluster.initial_master_nodes=es01,es02,es03
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - data02:/usr/share/elasticsearch/data
    networks:
      - elastic
  es03:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.0
    container_name: es03
    environment:
      - node.name=es03
      - cluster.name=es-docker-cluster
      - discovery.seed_hosts=es01,es02
      - cluster.initial_master_nodes=es01,es02,es03
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - data03:/usr/share/elasticsearch/data
    networks:
      - elastic

volumes:
  data01:
    driver: local
  data02:
    driver: local
  data03:
    driver: local

networks:
  elastic:
    driver: bridge
```

Run `docker-compose` to bring up the cluster:

```sh
docker-compose up
```

## 使用Docker安装Logstach

> 下载地址：https://www.elastic.co/cn/downloads/logstash

1、下载镜像

```sh
docker pull docker.elastic.co/logstash/logstash:7.17.15
```

2、新建挂载文件

```
mkdir -p /home/tools/logstash/config
mkdir -p /home/tools/logstash/conf.d
mkdir -p /home/tools/logstash/logs
```

3、赋权

```sh
chmod -777 /home/tools/logstash
```

4、挂载配置文件

4.1、新建配置文件`logstash.yml`，放入`/home/tools/logstash/config/`中，在容器启动后，使用的就是该文件配置。

`logstash.yml`文件内容

```yml
http.host: "0.0.0.0"  # 不需要指定ip，填写"0.0.0.0"即可
path.config: /usr/share/logstash/config/conf.d/*.conf
path.logs: /usr/share/logstash/logs

xpack.monitoring.enabled: true
xpack.monitoring.elasticsearch.username: logstash_system  #es xpack账号密码
xpack.monitoring.elasticsearch.password: {密码}            #es xpack账号密码
xpack.monitoring.elasticsearch.hosts: ["http://{ip1}:9200", "http://{ip2}:9200"]  #es地址
```

4.2、挂载日志收集文件

新建自定义日志收集文件，将文件放入`/home/tools/logstash/conf.d/`，在收集日志时，使用的就是该配置。

以如下配置为例，文件名log\_to\_es.conf

```conf
input {
  tcp {
    mode => "server"
    port => 5044
    codec => "json"
  }
}
filter {}
output {
  elasticsearch {
    action => "index"
    hosts  => ["192.168.64.128:9200"]
    index  => "springboot-%{+YYYY.MM.dd}"
  }
}
```

5、部署容器，启动

```sh
docker run -dit --name=logstash \
  --restart=always --privileged=true\
  -e ES_JAVA_OPTS="-Xms512m -Xmx512m" \
  -v /home/tools/logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml \
  -v /home/tools/logstash/conf.d/:/usr/share/logstash/config/conf.d/ \
  -v /home/tools/logstash/logs/:/usr/share/logstash/logs/ \
  -p 5044:5044 \
  logstash:7.17.15
```

参数详解：

* -p 5044:5044：映射的端口号，与上文conf.d下配置中的input一定要相同！多个地址往后拼接即可`-p 5045:5045-p 5046:5046`
* \--name=logstash：容器名称
* \--restart=always --privileged=true：启动配置
* -e ES\_JAVA\_OPTS="-Xms512m -Xmx512m"：指定内存
* -v /home/tools/logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml：配置文件挂载
* -v /home/tools/logstash/conf.d/:/usr/share/logstash/config/conf.d/：日志收集配置挂载位置
* -v /home/tools/logstash/logs/:/usr/share/logstash/logs/：日志挂载位置
* -d logstash:7.17.15：指定镜像

---

---
url: /Java/架构设计/分布式/07.分布式日志收集/2_Logstash使用.md
---

# ELK篇之Logstash使用

传输 和 处理 数据的管道
https://www.elastic.co/guide/en/logstash/7.17/getting-started-with-logstash.html
https://artifacts.elastic.co/downloads/logstash/logstash-7.17.9-windows-x86\_64.zip

好处：用起来方便，插件多
缺点：成本更大、一般要配合其他组件使用（比如 kafka）

事件 Demo：

```bash
cd logstash-7.17.9
.\bin\logstash.bat -e "input { stdin { } } output { stdout {} }"
```

快速开始文档：https://www.elastic.co/guide/en/logstash/7.17/running-logstash-windows.html
监听 udp 并输出：

```nginx
# Sample Logstash configuration for receiving
# UDP syslog messages over port 514

input {
  udp {
    port => 514
    type => "syslog"
  }
}

output {
  stdout { codec => rubydebug }
}
```

要把 MySQL 同步给 Elasticsearch。

问题 1：找不到 mysql 的包
Error: unable to load mysql-connector-java-5.1.36-bin.jar from :jdbc\_driver\_library, file not readable (please check user and group permissions for the path)
Exception: LogStash::PluginLoadingError

解决：修改 Logstash 任务配置中的 jdbc\_driver\_library 为驱动包的绝对路径（驱动包可以从 maven 仓库中拷贝）

增量配置：是不是可以只查最新更新的？可以记录上次更新的数据时间，只查出来 > 该更新时间的数据

小知识：预编译 SQL 的优点？

1. 灵活
2. 模板好懂
3. 快（有缓存）
4. 部分防注入

sql\_last\_value 是取上次查到的数据的最后一行的指定的字段，如果要全量更新，只要删除掉 E:\software\ElasticStack\logstash-7.17.9\data\plugins\inputs\jdbc\logstash\_jdbc\_last\_run 文件即可（这个文件存储了上次同步到的数据）

```nginx
input {
  jdbc {
    jdbc_driver_library => "E:\software\ElasticStack\logstash-7.17.9\config\mysql-connector-java-8.0.29.jar"
    jdbc_driver_class => "com.mysql.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://localhost:3306/my_db"
    jdbc_user => "root"
    jdbc_password => "123456"
    statement => "SELECT * from post where updateTime > :sql_last_value"
    tracking_column => "updatetime"
    tracking_column_type => "timestamp"
    use_column_value => true
    parameters => { "favorite_artist" => "Beethoven" }
    schedule => "*/5 * * * * *"
    jdbc_default_timezone => "Asia/Shanghai"
  }
}

output {
  stdout { codec => rubydebug }
}
```

注意查询语句中要按 updateTime 排序，保证最后一条是最大的：

```nginx
input {
  jdbc {
    jdbc_driver_library => "E:\software\ElasticStack\logstash-7.17.9\config\mysql-connector-java-8.0.29.jar"
    jdbc_driver_class => "com.mysql.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://localhost:3306/my_db"
    jdbc_user => "root"
    jdbc_password => "123456"
    statement => "SELECT * from post where updateTime > :sql_last_value and updateTime < now() order by updateTime desc"
    tracking_column => "updatetime"
    tracking_column_type => "timestamp"
    use_column_value => true
    parameters => { "favorite_artist" => "Beethoven" }
    schedule => "*/5 * * * * *"
    jdbc_default_timezone => "Asia/Shanghai"
  }
}

output {
  stdout { codec => rubydebug }

  elasticsearch {
    hosts => "http://localhost:9200"
    index => "post_v1"
    document_id => "%{id}"
  }
}
```

两个问题：
1字段全变成小写了
2多了一些我们不想同步的字段

可以编写过滤：

```nginx
input {
  jdbc {
    jdbc_driver_library => "E:\software\ElasticStack\logstash-7.17.9\config\mysql-connector-java-8.0.29.jar"
    jdbc_driver_class => "com.mysql.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://localhost:3306/my_db"
    jdbc_user => "root"
    jdbc_password => "123456"
    statement => "SELECT * from post where updateTime > :sql_last_value and updateTime < now() order by updateTime desc"
    tracking_column => "updatetime"
    tracking_column_type => "timestamp"
    use_column_value => true
    parameters => { "favorite_artist" => "Beethoven" }
    schedule => "*/5 * * * * *"
    jdbc_default_timezone => "Asia/Shanghai"
  }
}

filter {
    mutate {
        rename => {
          "updatetime" => "updateTime"
          "userid" => "userId"
          "createtime" => "createTime"
          "isdelete" => "isDelete"
        }
        remove_field => ["thumbnum", "favournum"]
    }
}

output {
  stdout { codec => rubydebug }

  elasticsearch {
    hosts => "127.0.0.1:9200"
    index => "post_v1"
    document_id => "%{id}"
  }
}
```

参考资料：

https://www.bilibili.com/video/BV1Nt4y1m7qL

https://developer.aliyun.com/article/826944

https://www.cnblogs.com/hahaha111122222/p/14949786.html

https://www.cnblogs.com/likecoke/p/13646653.html

---

---
url: /Java/架构设计/分布式/07.分布式日志收集/3_SpringBoot集成ELK.md
---

# ELK篇之SpringBoot集成ELK

## 项目中集成Logstash

### 增加依赖

`pom.xml`增加`logback` 整合 `logstash` 的依赖

```xml
<dependency>
    <groupId>net.logstash.logback</groupId>
    <artifactId>logstash-logback-encoder</artifactId>
    <version>7.3</version>
</dependency>
```

### 增加配置

`application.yaml` 文件中添加`logstash`配置

```yml
server:
  port: 8081

spring:
  application:
    name: springboot_elk

log:
  # logstash 地址和端口，注意修改
  logstashhost: 192.168.64.128:5044
```

### 增加日志组件

`logback-spring.xm`l增加`logback`组件

配置方式一

```xml
<?xml version="1.0" encoding="UTF-8"?>
<configuration debug="false">
    <!--提取配置文件中的服务名-->
    <springProperty scope="context" name="springApplicationName" source="spring.application.name"/>
    <!-- 读取SpringBoot配置文件获取logstash的地址和端口 -->
    <springProperty scope="context" name="logstash-host" source="log.logstash-host"/>
    <property name="LOG_HOME" value="logs/demo.log"/>
    <appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">
        <encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder">
            <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{50} - %msg%n</pattern>
        </encoder>
    </appender>

    <appender name="logstash" class="net.logstash.logback.appender.LogstashTcpSocketAppender">
        <destination>${logstash-host}</destination>
        <encoder class="net.logstash.logback.encoder.LogstashEncoder">
            <!--定义appName的名字是服务名,多服务时,根据这个进行区分日志-->
            <customFields>{"appName": "${springApplicationName}"}</customFields>
        </encoder>
    </appender>

    <root level="INFO">
        <appender-ref ref="STDOUT"/>
        <appender-ref ref="logstash"/>
    </root>
</configuration>
```

配置方式二

```xml
<?xml version="1.0" encoding="UTF-8"?>
<configuration debug="false">
   <springProperty scope="context" name="springApplicationName" source="spring.application.name" />
   <property name="LOG_HOME" value="logs/demo.log" />
   <appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">
       <encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder">
           <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{50} - %msg%n</pattern>
       </encoder>
   </appender>

   <!--DEBUG日志输出到LogStash-->
   <appender name="LOG_STASH_DEBUG" class="net.logstash.logback.appender.LogstashTcpSocketAppender">
       <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
           <level>DEBUG</level>
       </filter>
       <destination>124.223.119.48:4560</destination>
       <encoder charset="UTF-8" class="net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder">
           <providers>
               <timestamp>
                   <timeZone>Asia/Shanghai</timeZone>
               </timestamp>
               <!--自定义日志输出格式-->
               <pattern>
                   <pattern>
                       {
                       "project": "elk",
                       "level": "%level",
                       "service": "${springApplicationName:-}",
                       "pid": "${PID:-}",
                       "thread": "%thread",
                       "class": "%logger",
                       "message": "%message",
                       "stack_trace": "%exception"
                       }
                   </pattern>
               </pattern>
           </providers>
       </encoder>
   </appender>

   <root >
       <appender-ref ref="STDOUT" />
       <appender-ref ref="LOG_STASH_DEBUG" />
   </root>
</configuration>
```

### 业务增加日志

业务代码增加日志输出进行测试

```java
@Slf4j
@RequestMapping("/api")
@RestController
public class ElkController {

    /**
     * 测试输出log的访问方法
     */
    @GetMapping("/testLog")
    public String testLog() {
        log.info("{日志},{}", "测试输出一个日志");
        log.error("{日志},{}", "测试输出一个错误日志");
        return "success";
    }

}
```

## Kibana看板配置

日志收集文件配置参考上一篇es安装文章

Kibana索引配置![image-20231118222611162](/assets/image-20231118222611162.lrhtgTin.png)

进入索引列表

![image-20231118222640606](/assets/image-20231118222640606.D6xKll1V.png)

创建索引

![image-20231118223003203](/assets/image-20231118223003203.B772NjYe.png)

进入Discover中，切换索引就可以看到输出的日志

![image-20231118223130547](/assets/image-20231118223130547.CHR-ApW0.png)

日志添加筛选条件

![image-20231118223235934](/assets/image-20231118223235934.DudU3Z6m.png)

## 参考资料

\[1]. https://blog.csdn.net/m0\_51510236/article/details/130413227

\[2]. https://www.zhihu.com/tardis/zm/art/347378314

\[3]. https://blog.csdn.net/captain\_zkk/article/details/124799087

\[4]. [自建elk+filebeat+grafana日志收集平台\_51CTO博客\_elk filebeat](https://blog.51cto.com/u_12970189/2391070)

\[5]. [Elasticsearch：如何在 Elastic Agents 中配置 Beats 来采集定制日志\_elastic\_agen\_Elastic](https://blog.csdn.net/UbuntuTouch/article/details/128213642)

\[6]. https://www.bilibili.com/video/BV1sP4y1U7eh

---

---
url: /Java/微服务专栏/01.SpringCloud/3_Eureka注册服务.md
---

# Eureka注册服务

## 一、Eureka简介

RestTemplate在进行微服务访问的时候，需要明确地通过微服务的地址进行调用。这样直接利用地址的调用，一旦出现服务端主机地址变更，则消费端就需要进行大量的修改。同时，微服务的主要目的是提高业务处理能力，因此往往会若干个相同业务的微服务一同参与运算。为了解决这样的问题，在微服务的使用中需要采用Eureka注册中心对所有微服务进行管理。所有的微服务在启动后需要全部向Eureka中进行服务注册，而后客户端直接利用Eureka进行服务信息的获得，以实现微服务调用。

![Image00323](/assets/Image00323.CJjpAj8Y.jpg)

## 二、定义Eureka服务端

在SpringCloud中大量使用了Netflix的开源项目，而其中Eureka就属于Netflix提供的发现服务组件，该应用程序需要由开发者自行定义。

### 1、引入依赖

创建新的子模块springcloud-eureka

修改pom.xml配置文件，除了引入SpringBoot相关依赖库之外，还需要引入Eureka相关依赖库。

```xml
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-eureka-server</artifactId>
</dependency>
```

### 2、修改配置文件

修改application.yml配置文件，进行Eureka服务器配置。

```yaml
eureka: 
  instance:                   # eureak实例定义
    hostname: eureka-7001.com # 定义Eureka实例所在的主机名称
```

### 3、修改启动类

定义程序启动主类，添加Eureka相关注解。

```java
@SpringBootApplication
@EnableEurekaServer                    // 启用Eureka服务
public class SpringbootEurekaApplication {

    public static void main(String[] args) {
        SpringApplication.run(SpringbootEurekaApplication.class, args);
    }
}
```

### 4、修改主机配置

修改hosts配置文件，追加主机配置。

在Windows系统中，Hosts文件的位置是：`C:\Windows\System32\drivers\etc\hosts`

修改hosts后需要刷新DNS缓存使之生效，在cmd命令行中执行命令：`ipconfig/flushdns`

```conf
127.0.0.1 eureka-7001.com
```

此时配置的主机名称eureka-7001.com与application.yml中配置的Eureka运行主机名称相同。

### 5、访问

启动Eureka服务端，随后输入访问地址http://eureka-7001.com:7001/，可以看见管理界面。

![image-20250227211209675](/assets/image-20250227211209675.Bvf3xjG0.png)

### 异常1

**提示：关于程序运行中的TransportException异常。**

虽然现在已经配置完了Eureka注册中心，但在运行中却会发现控制台上会输出如下错误信息：

```
com.netflix.discovery.shared.transport.TransportException: Cannot execute request on any known server
```

之所以会有这些错误信息，主要是因为Eureka在默认配置下自己也是一个微服务，并且该微服务应该向Eureka中注册，但却无法找到主机所导致的。要想解决这个问题，需要修改application.yml配置文件，追加配置项。

修改application.yml配置文件，追加如下配置。

```yaml
server:
  port: 7001
eureka:
  client:
    # EurekaServer的地址，现在是自己的地址，如果是集群，需要加上其它Server的地址
    service-url:
      defaultZone: http://127.0.0.1:${server.port}/eureka
    # 不把自己注册到eureka服务列表
    register-with-eureka: false
    # 拉取eureka服务信息
    fetch-registry: false # false表示自己就是注册中心，不需要从注册中心获取注册列表信息
  instance: # eureak实例定义
    hostname: eureka-7001.com # 定义Eureka实例所在的主机名称
```

此时的程序配置了微服务要注册的Eureka服务地址，但是服务信息注册没有意义，所以配置了register-with-eureka与fetch-registry选项，不再在Eureka注册中心中显示该微服务信息。

### 异常2

```
java.lang.TypeNotPresentException: Type javax.xml.bind.JAXBContext not present
```

原因：JAXBContext不存在，网上查找的原因说是因为JAXB-API是java ee的一部分，java9引入了模块化的概念，在jdk11中在jdk11中没有在默认的类路径中，使得JAXB默认没有加载。

解决

```xml
<!-- jaxb模块引用 - start -->
<dependency>
   <groupId>javax.xml.bind</groupId>
    <artifactId>jaxb-api</artifactId>
</dependency>
<dependency>
    <groupId>com.sun.xml.bind</groupId>
    <artifactId>jaxb-impl</artifactId>
    <version>2.3.0</version>
</dependency>
<dependency>
    <groupId>org.glassfish.jaxb</groupId>
    <artifactId>jaxb-runtime</artifactId>
    <version>2.3.0</version>
</dependency>
<dependency>
    <groupId>javax.activation</groupId>
    <artifactId>activation</artifactId>
    <version>1.1.1</version>
</dependency>
<!-- jaxb模块引用 - end -->
```

也可以使用java9模块命令方式引入jaxb-api：

```bash
--add-modles java.xml.bind
```

## 三、向Eureka中注册微服务

Eureka注册中心搭建成功后，所有的微服务都应该向Eureka中进行注册，此时应该进行微服务程序的配置，在微服务中引入Eureka客户端依赖，并且配置Eureka地址。

### 1、引入相关依赖

修改pom.xml配置文件，引入相关依赖库。

```xml
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-eureka</artifactId>
</dependency>
```

### 2、修改配置文件

修改application.yml配置文件，追加Eureka客户端配置。

```yaml
eureka:
  client:                                         # 客户端进行Eureka注册的配置
    service-url:                                  # 定义Eureka服务地址
      defaultZone: http://eureka-7001.com:7001/eureka
```

如果要向Eureka中进行微服务注册，还需要为当前微服务定义名称。

```yaml
spring:
  application:
    name: springcloud-dept-service                  # 定义微服务名称
```

### 3、修改启动类

修改程序启动类

```java
@SpringBootApplication
@EnableEurekaClient					// 启用Eureka客户端
public class SpringbootDeptApplication {

    public static void main(String[] args) {
        SpringApplication.run(SpringbootDeptApplication.class, args);
    }

}
```

这里定义了@EnableEurekaClient注解信息，微服务启动之后，该服务会自动注册到Eureka服务器之中。分别启动Eureka注册中心微服务与部门微服务之后，就可以通过Eureka注册中心观察到所注册的微服务信息。

![image-20250308205614239](/assets/image-20250308205614239.BlABIcKr.png)

## 四、Eureka服务信息

前面实现了微服务向Eureka中的注册处理，但是此时微服务的注册信息并不完整，开发者可以通过微服务的进一步配置，实现更加详细的信息显示。

### 1、修改配置，增加主机名称显示

修改application.yml配置文件，追加微服务所在主机名称的显示。

```yaml
server:
  port: 7002
spring:
  application:
    name: springcloud-dept-service                  # 定义微服务名称
eureka:
  client:                                           # 客户端进行Eureka注册的配置
    service-url:                                    # 定义Eureka服务地址
      defaultZone: http://eureka-7001.com:7001/eureka
  instance:
    instance-id: springcloud-dept.com                # 显示主机名称
```

![image-20250308215840860](/assets/image-20250308215840860._0Mp0Xju.png)

### 2、修改配置，增加主机IP地址显示

修改application.yml，修改服务信息的连接主机为IP地址。

```yaml
spring:
  application:
    name: springcloud-dept-service                  # 定义微服务名称
eureka:
  client:                                           # 客户端进行Eureka注册的配置
    service-url:                                    # 定义Eureka服务地址
      defaultZone: http://127.0.0.1:7001/eureka
  instance:
    instance-id: springcloud-dept.com                # 显示主机名称
    prefer-ip-address: true                          # 访问的路径变为IP地址
```

追加prefer-ip-address配置项之后，会在显示链接信息处显示IP地址。

用户打开微服务的信息之后，可以使用/info路径查看信息。由于默认状态下无法显示，此时可以使用Actuator显示微服务信息（此配置与SpringBoot中的Actuator相同）。

### 3、增加actuator依赖

要查看微服务详细信息，需要修改pom.xml文件，追加actuator依赖。

```xml
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-actuator</artifactId>
</dependency>
```

### 4、增加信息处理的插件

修改pom.xml文件，追加信息处理的插件。

```xml
<resources>
    <resource>
        <directory>src/main/resources</directory>
        <includes>
            <include>**/*.properties</include>
            <include>**/*.yml</include>
            <include>**/*.xml</include>
            <include>**/*.tld</include>
            <include>**/*.p12</include>
        </includes>
        <filtering>true</filtering>
    </resource>
</resources>
...
<plugin>
    <groupId>org.apache.maven.plugins</groupId>
    <artifactId>maven-resources-plugin</artifactId>
    <configuration>
        <delimiters>
            <delimiter>$</delimiter>
        </delimiters>
    </configuration>
</plugin>
```

### 5、修改配置，增加info的信息

修改application.yml配置文件，追加info的相关信息。

```
info:
  app.name: springcloud-dept-service
  company.name: http://luckilyxxl.xyz
  build.artifactId: $project.artifactId$
  build.version: $project.version$
```

此时，当用户通过Eureka打开微服务时就可以显示微服务的相应信息。

http://127.0.0.1:7002/actuator/info

![image-20250312231308806](/assets/image-20250312231308806.Bk-lnEwF.png)

```json
{
    "app": {
        "name": "springcloud-dept-service"
    },
    "company": {
        "name": "http://luckilyxxl.xyz"
    },
    "build": {
        "artifactId": "springcloud-eureka",
        "version": "0.0.1"
    }
}
```

## 五、Eureka发现管理

Eureka的主要作用是进行微服务注册。在整个微服务的运行过程中，Eureka也需要对微服务的状态进行监听，对无用的微服务可以进行清除处理，也可以通过发现管理查看Eureka信息。

### 1、设置Eureka清理时间

修改applicaiton.yml配置文件，设置微服务清理间隔。

```yaml
eureka:
  server:
    eviction-interval-timer-in-ms: 1000 # 设置清理的间隔时间，而后这个时间使用的是毫秒单位（默认是60秒）
```

一旦触发清理操作后，会在控制台显示如下信息：

```sh
2025-03-08 22:20:03.433  INFO 20268 --- [nio-7001-exec-8] c.n.e.registry.AbstractInstanceRegistry  : Registered instance SPRINGCLOUD-DEPT-SERVICE/springcloud-dept.com with status DOWN (replication=true)
2025-03-08 22:20:03.445  INFO 20268 --- [a-EvictionTimer] c.n.e.registry.AbstractInstanceRegistry  : Running the evict task with compensationTime 7ms
```

### 2、关闭Eureka保护

在Eureka使用过程中经常会看见如下所示的提示文字：

```sh
EMERGENCY! EUREKA MAY BE INCORRECTLY CLAIMING INSTANCES ARE UP WHEN THEY’RE NOT. RENEWALS ARE LESSER THAN THRESHOLD AND HENCE THE INSTANCES ARE NOT BEING EXPIRED JUST TO BE SAFE.
```

该提示的核心意义在于：当某一个微服务不可用时（可能出现了更名或者是宕机等因素），由于所有的微服务提供有保护模式，所以Eureka是不会对微服务信息进行清理的。如果希望关闭这种保护模式（一般不推荐），则可以通过修改application.yml来实现。在Eureka中增加如下配置：

```yaml
eureka: 
  server:
    enable-self-preservation: false     # 设置为false表示关闭保护模式
```

### 3、修改微服务默认心跳

所有注册到Eureka中的微服务如果要与Eureka之间保持联系，依靠的是心跳机制。用户可以根据网络环境自行进行心跳机制的配置，只需要修改微服务中的application.yml即可。

```yaml
eureka:
  client:                                         # 客户端进行Eureka注册的配置
    service-url:                                  # 定义Eureka服务地址
      defaultZone: http://eureka-7001.com:7001/eureka
  instance:
    lease-renewal-interval-in-seconds: 2          # 设置心跳的时间间隔（默认是30秒）
    lease-expiration-duration-in-seconds: 5       # 如果现在超过了5秒的间隔（默认是90秒）
    instance-id: springcloud-dept.com             # 显示主机名称
    prefer-ip-address: false                      # 访问的路径变为IP地址
```

### 4、Eureka发现信息

所有注册到Eureka中的微服务均可以定义Eureka发现信息，只需要在相应的微服务中获取DiscoveryClient对象即可。修改DeptRest程序类，追加新的方法。

```java
@Resource
private DiscoveryClient client ;	// 进行Eureka的发现服务

@RequestMapping("/dept/discover")
public Object discover() {	// 直接返回发现服务信息
    return this.client ;
}
```

### 5、启动类增加注解

在程序启动类中追加发现服务配置注解。

```java
@SpringBootApplication
@EnableEurekaClient                    // 启用Eureka客户端
@EnableDiscoveryClient
@EnableJpaRepositories(basePackages = "com.xxl.springboot.dept.dao")
public class SpringbootDeptApplication {

    public static void main(String[] args) {
        SpringApplication.run(SpringbootDeptApplication.class, args);
    }

}
```

程序启动之后通过访问地址`http://127.0.0.1:7002/dept/discover`，可以查询到微服务的相关信息。

![image-20250312231253821](/assets/image-20250312231253821.-zIOBhaA.png)

```json
{
    "discoveryClients": [
        {
            "services": [
                "springcloud-dept-service"
            ]
        },
        {
            "services": []
        }
    ],
    "services": [
        "springcloud-dept-service"
    ]
}
```

## 六、Eureka安全配置

在整个SpringCloud微服务架构中，Eureka是一个重要的注册中心，并且只能够注册自己所需要的微服务。为了保证Eureka安全，需要为Eureka引入SpringSecurity实现安全配置。

### 1、【Eureka注册中心】引入Security依赖

修改pom.xml配置文件，引入SpringSecurity的依赖包。

```xml
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-security</artifactId>
</dependency>
```

### 2、【Eureka注册中心】增加安全配置

修改application.yml配置文件，追加安全配置。

```yaml
server:
  port: 7001                  # 定义运行端口

security:
  basic:
    enabled: true         # 启用安全认证处理
  user:
    name: admin           # 用户名
    password: admin       # 密码

eureka: 
  client:                    # 客户端进行Eureka注册的配置
    service-url:
      defaultZone: http://127.0.0.1:7001/eureka
    register-with-eureka: false    # 当前的微服务不注册到eureka之中
    fetch-registry: false     # 不通过eureka获取注册信息
  instance:                   # eureak实例定义
    hostname: localhost       # 定义Eureka实例所在的主机名称
```

在本配置中追加了新的用户名admin/admin，而Eureka微服务本身也需要设置一个Eureka服务器的访问地址，所以要修改defaultZone的访问路径，追加认证信息。

### 3、【业务服务】修改配置文件，增加授权连接

修改application.yml配置文件，进行授权的注册连接。

```yaml
eureka:
  client:             # 客户端进行Eureka注册的配置
    service-url:      # 定义Eureka服务地址
      defaultZone: http://127.0.0.1:7001/eureka
```

此时，只有认证信息正确的微服务才可以在Eureka中进行注册。

## 七、Eureka-HA机制

Eureka是整个微服务架构中的核心组件，如果Eureka服务器出现问题，则所有的微服务都无法注册，这样整个项目就会彻底瘫痪。为了避免出现这样的问题，可采用Eureka集群的模式来处理，即使用多台主机共同实现Eureka注册服务，这样即使有一台主机出现问题，另外的主机也可以正常提供服务支持。

2台Eureka主机的集群搭建

![Image00353](/assets/Image00353.DvZqebC5.jpg)

3台Eureka主机的集群搭建

![Image00354](/assets/Image00354.B8DPE7JE.jpg)

通过上图可以发现，在实现Eureka集群时最重要的实现形式就是某一台Eureka主机（客户端）需要向其他的Eureka主机进行注册。

1．【操作系统】修改hosts主机文件，追加3个主机名称，与要创建的3个Eureka微服务对应。

2．【mldncloud-eureka-server-a项目】修改application.yml配置文件，追加集群环境配置。

3．【mldncloud-eureka-server-b项目】修改application.yml配置文件，追加集群环境配置。

4．【mldncloud-eureka-server-c项目】修改application.yml配置文件，追加集群环境配置。

5．【mldncloud-dept-service-8001项目】修改application.yml配置文件，向3台主机同时注册微服务。

6．【mldncloud-eureka-server-\*项目】启动所有的Eureka服务，登录其中任意一台Eureka控制台，就可以看见Eureka-HA集群主机，同时注册的微服务会在3台主机上同时存在，如图8-10所示。

## 八、Eureka服务发布

Eureka作为独立的微服务存在，也需要进行项目打包与部署。在实际项目环境中，由于Eureka需要HA机制的支撑，所以本节将利用profile实现多个环境的配置。

1．【mldncloud-eureka-profile项目】修改application.yml配置文件，设置多个profile配置。

2．【mldncloud-eureka-profile项目】修改pom.xml配置文件，追加打包插件。

3．【mldncloud-eureka-profile项目】通过Maven打包clean package，如图8-11所示。随后就可以在项目的目录中发现生成的eureka-server.jar文件。

4．【操作系统】使用默认配置启动Eureka服务java -jar eureka-server.jar。

5．【操作系统】使用其他profile启动Eureka。

运行product-7102：java -jar eureka-server.jar --spring.profiles.active=product-7102。

运行product-7103：java -jar eureka-server.jar --spring.profiles.active=product-7103。

## 参考资料

\[1]. https://blog.csdn.net/Pireley/article/details/133784749

\[2]. https://www.jianshu.com/p/cc61898291e3

---

---
url: /常用框架/EasyExcel/6_FastExcel.md
---

# FastExcel

https://github.com/fast-excel/fastexcel

---

---
url: /Java/微服务专栏/04.远程调用Feign/Feign远程调用.md
---

# Feign远程调用

先来看以前利用RestTemplate发起远程调用的代码：

![image-20210714174814204](/assets/image-20210714174814204.DzVhutg7.png)

存在下面的问题：

•代码可读性差，编程体验不统一

•参数复杂URL难以维护

Feign是一个声明式的http客户端，官方地址：https://github.com/OpenFeign/feign

其作用就是帮助我们优雅的实现http请求的发送，解决上面提到的问题。

![image-20210714174918088](/assets/image-20210714174918088.DgYRRi4A.png)

## 2.1.Feign替代RestTemplate

Fegin的使用步骤如下：

### 1）引入依赖

我们在order-service服务的pom文件中引入feign的依赖：

```xml
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-openfeign</artifactId>
</dependency>
```

### 2）添加注解

在order-service的启动类添加注解开启Feign的功能：

![image-20210714175102524](/assets/image-20210714175102524.BLeAN9aW.png)

### 3）编写Feign的客户端

在order-service中新建一个接口，内容如下：

```java
package cn.xxl.order.client;

import cn.xxl.order.pojo.User;
import org.springframework.cloud.openfeign.FeignClient;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.PathVariable;

@FeignClient("userservice")
public interface UserClient {
    @GetMapping("/user/{id}")
    User findById(@PathVariable("id") Long id);
}
```

这个客户端主要是基于SpringMVC的注解来声明远程调用的信息，比如：

* 服务名称：userservice
* 请求方式：GET
* 请求路径：/user/{id}
* 请求参数：Long id
* 返回值类型：User

这样，Feign就可以帮助我们发送http请求，无需自己使用RestTemplate来发送了。

### 4）测试

修改order-service中的OrderService类中的queryOrderById方法，使用Feign客户端代替RestTemplate：

![image-20210714175415087](/assets/image-20210714175415087.Im5by22U.png)

是不是看起来优雅多了。

### 5）总结

使用Feign的步骤：

① 引入依赖

② 添加@EnableFeignClients注解

③ 编写FeignClient接口

④ 使用FeignClient中定义的方法代替RestTemplate

## 2.2.自定义配置

Feign可以支持很多的自定义配置，如下表所示：

| 类型                   | 作用             | 说明                                                   |
| ---------------------- | ---------------- | ------------------------------------------------------ |
| **feign.Logger.Level** | 修改日志级别     | 包含四种不同的级别：NONE、BASIC、HEADERS、FULL         |
| feign.codec.Decoder    | 响应结果的解析器 | http远程调用的结果做解析，例如解析json字符串为java对象 |
| feign.codec.Encoder    | 请求参数编码     | 将请求参数编码，便于通过http请求发送                   |
| feign. Contract        | 支持的注解格式   | 默认是SpringMVC的注解                                  |
| feign. Retryer         | 失败重试机制     | 请求失败的重试机制，默认是没有，不过会使用Ribbon的重试 |

一般情况下，默认值就能满足我们使用，如果要自定义时，只需要创建自定义的@Bean覆盖默认Bean即可。

下面以日志为例来演示如何自定义配置。

### 2.2.1.配置文件方式

基于配置文件修改feign的日志级别可以针对单个服务：

```yaml
feign:  
  client:
    config: 
      userservice: # 针对某个微服务的配置
        loggerLevel: FULL #  日志级别 
```

也可以针对所有服务：

```yaml
feign:  
  client:
    config: 
      default: # 这里用default就是全局配置，如果是写服务名称，则是针对某个微服务的配置
        loggerLevel: FULL #  日志级别 
```

而日志的级别分为四种：

* NONE：不记录任何日志信息，这是默认值。
* BASIC：仅记录请求的方法，URL以及响应状态码和执行时间
* HEADERS：在BASIC的基础上，额外记录了请求和响应的头信息
* FULL：记录所有请求和响应的明细，包括头信息、请求体、元数据。

### 2.2.2.Java代码方式

也可以基于Java代码来修改日志级别，先声明一个类，然后声明一个Logger.Level的对象：

```java
public class DefaultFeignConfiguration  {
    @Bean
    public Logger.Level feignLogLevel(){
        return Logger.Level.BASIC; // 日志级别为BASIC
    }
}
```

如果要**全局生效**，将其放到启动类的@EnableFeignClients这个注解中：

```java
@EnableFeignClients(defaultConfiguration = DefaultFeignConfiguration .class) 
```

如果是**局部生效**，则把它放到对应的@FeignClient这个注解中：

```java
@FeignClient(value = "userservice", configuration = DefaultFeignConfiguration .class) 
```

## 2.3.Feign使用优化

Feign底层发起http请求，依赖于其它的框架。其底层客户端实现包括：

•URLConnection：默认实现，不支持连接池

•Apache HttpClient ：支持连接池

•OKHttp：支持连接池

因此提高Feign的性能主要手段就是使用**连接池**代替默认的URLConnection。

这里我们用Apache的HttpClient来演示。

1）引入依赖

在order-service的pom文件中引入Apache的HttpClient依赖：

```xml
<!--httpClient的依赖 -->
<dependency>
    <groupId>io.github.openfeign</groupId>
    <artifactId>feign-httpclient</artifactId>
</dependency>
```

2）配置连接池

在order-service的application.yml中添加配置：

```yaml
feign:
  client:
    config:
      default: # default全局的配置
        loggerLevel: BASIC # 日志级别，BASIC就是基本的请求和响应信息
  httpclient:
    enabled: true # 开启feign对HttpClient的支持
    max-connections: 200 # 最大的连接数
    max-connections-per-route: 50 # 每个路径的最大连接数
```

接下来，在FeignClientFactoryBean中的loadBalance方法中打断点：

![image-20210714185925910](/assets/image-20210714185925910.Ch54lu3G.png)

Debug方式启动order-service服务，可以看到这里的client，底层就是Apache HttpClient：

![image-20210714190041542](/assets/image-20210714190041542.CO0K4122.png)

总结，Feign的优化：

1.日志级别尽量用basic

2.使用HttpClient或OKHttp代替URLConnection

①  引入feign-httpClient依赖

②  配置文件开启httpClient功能，设置连接池参数

## 2.4.最佳实践

所谓最近实践，就是使用过程中总结的经验，最好的一种使用方式。

自习观察可以发现，Feign的客户端与服务提供者的controller代码非常相似：

feign客户端：

![image-20210714190542730](/assets/image-20210714190542730.COQaMLHu.png)

UserController：

![image-20210714190528450](/assets/image-20210714190528450.B8fJURxn.png)

有没有一种办法简化这种重复的代码编写呢？

### 2.4.1.继承方式

一样的代码可以通过继承来共享：

1）定义一个API接口，利用定义方法，并基于SpringMVC注解做声明。

2）Feign客户端和Controller都集成改接口

![image-20210714190640857](/assets/image-20210714190640857.Bukl5Dsw.png)

优点：

* 简单
* 实现了代码共享

缺点：

* 服务提供方、服务消费方紧耦合

* 参数列表中的注解映射并不会继承，因此Controller中必须再次声明方法、参数列表、注解

### 2.4.2.抽取方式

将Feign的Client抽取为独立模块，并且把接口有关的POJO、默认的Feign配置都放到这个模块中，提供给所有消费者使用。

例如，将UserClient、User、Feign的默认配置都抽取到一个feign-api包中，所有微服务引用该依赖包，即可直接使用。

![image-20210714214041796](/assets/image-20210714214041796.CuNJwiVe.png)

### 2.4.3.实现基于抽取的最佳实践

#### 1）抽取

首先创建一个module，命名为feign-api：

![image-20210714204557771](/assets/image-20210714204557771.B43DwDpN.png)

项目结构：

![image-20210714204656214](/assets/image-20210714204656214.BD0KntnA.png)

在feign-api中然后引入feign的starter依赖

```xml
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-openfeign</artifactId>
</dependency>
```

然后，order-service中编写的UserClient、User、DefaultFeignConfiguration都复制到feign-api项目中

![image-20210714205221970](/assets/image-20210714205221970.BeWeJJID.png)

#### 2）在order-service中使用feign-api

首先，删除order-service中的UserClient、User、DefaultFeignConfiguration等类或接口。

在order-service的pom文件中中引入feign-api的依赖：

```xml
<dependency>
    <groupId>cn.xxl.demo</groupId>
    <artifactId>feign-api</artifactId>
    <version>1.0</version>
</dependency>
```

修改order-service中的所有与上述三个组件有关的导包部分，改成导入feign-api中的包

#### 3）重启测试

重启后，发现服务报错了：

![image-20210714205623048](/assets/image-20210714205623048.YW7zXwZZ.png)

这是因为UserClient现在在cn.xxl.feign.clients包下，

而order-service的@EnableFeignClients注解是在cn.xxl.order包下，不在同一个包，无法扫描到UserClient。

#### 4）解决扫描包问题

方式一：

指定Feign应该扫描的包：

```java
@EnableFeignClients(basePackages = "cn.xxl.feign.clients")
```

方式二：

指定需要加载的Client接口：

```java
@EnableFeignClients(clients = {UserClient.class})
```

---

---
url: /Java/解决方案/数据同步/2_Flink-CDC.md
---

# Flink

Flink介绍：https://zhuanlan.zhihu.com/p/642671403

Flink CDC：https://developer.aliyun.com/article/1399426

Flink SQL：https://cloud.tencent.com/developer/article/1455860

---

---
url: /daily/开源项目/3_FlowLong飞龙工作流.md
---

# FlowLong飞龙工作流

> 项目地址：<https://gitee.com/aizuda/flowlong/tree/dev/>

功能模块介绍

1. flowlong-core：FlowLong的核心模块，提供了工作流引擎的核心功能，包括流程定义、流程实例、任务节点、任务列表等核心概念的抽象和实现。同时，它还提供了流程实例的生命周期管理、任务节点的执行、任务列表的展示等功能。

   ```
   assist：断言、日期、对象判断、流数据工具类
   cache：包含ConcurrentHashMap类型的localCache成员变量，用于存储缓存数据
   core：存放枚举信息，流程对象
   entity：实体类
   exception：异常类
   handler：处理器接口
   listener：流程引擎、流程实例、流程任务监听接口
   model：条件节点等数据模型
   scheduling：定时相关
   ```

2. flowlong-mybatis-plus：FlowLong与MyBatis Plus的集成模块，提供了基于MyBatis Plus的数据库操作功能，包括流程定义、流程实例、任务节点等数据的存储和查询。通过该模块，用户可以方便地将FlowLong工作流项目与现有的数据库系统进行集成。

3. flowlong-spring-boot-starter：FlowLong与Spring Boot的集成模块，提供了基于Spring Boot的启动和配置功能，方便用户快速搭建FlowLong工作流项目。通过该模块，用户可以轻松地将FlowLong工作流项目集成到现有的Spring Boot应用中。

4. flowlong-solon-plugin：FlowLong与Solon的集成模块，提供了基于Solon的插件功能，方便用户将FlowLong工作流项目与Solon平台进行集成。通过该模块，用户可以在Solon平台上方便地使用FlowLong工作流项目提供的功能。

---

---
url: /daily/博客文档/VitPress/3_Frontmatter.md
---

# Frontmatter

---

---
url: /10.配置/10.frontmatter 配置.md
---

# frontmatter 配置

`frontmatter` 支持基于页面的配置。在每个 Markdown 文件中，可以使用 `frontmatter` 配置来覆盖 [主题配置](/reference/config) 中的大部分选项。

## 首页配置

### description

Teek 提供了 `description` 选项，用于在首页 Banner 展示一些描述信息，您可以通过 `tk.description` 或者 `tk.banner.description` 来配置首页的 `description`。

::: tip
`description` 获取优先级：`tk.banner.description` > `banner.description` > `tk.description` >。
:::

```yaml
---
layout: home

tk:
  description:
    - 故事由我书写，旅程由你见证，传奇由她聆听 —— 来自 Young Kbt
    - 积跬步以至千里，致敬每个爱学习的你 —— 来自 Evan Xu
    - 这一生波澜壮阔或是不惊都没问题 —— 来自 Weibw
---
```

### features

Teek 提供了 `features` 选项，用于在首页展示一些功能介绍。

请通过 `frontmatter.tk.features` 配置 `features`，这样可以避免与 VitePress 的 `features` 冲突。

如果您是博客风格，你也可以通过 `frontmatter.tk.banner.features` 或 `frontmatter.banner.features` 配置 `features`。

::: tip `features` 获取优先级

* 文档风格：：`frontmatter.tk.features` > `teekConfig.features`

* 博客风格：`frontmatter.tk.banner.features` > `frontmatter.banner.features` > `teekConfig.banner.features`
  :::

* 文档风格的 `features` 配置内容请看 [Doc Feature](/reference/config/global-config#features)。

* 博客风格风格的 `features` 配置内容请看 [Banner Feature](/reference/banner-config.html#banner)。

::: code-group

```yaml [文档风格]
tk:
  teekHome: false
  features:
    - title: 快速开发
      details: 提供了完整版参考代码和精简版开发代码
      image: /feature/ui.svg
      highlights:
        - title: 从零安装：运行 <code>pnpm add vitepress-theme-teek vitepress</code> 以从 NPM 下载 Teek 主题。
        - title: 现有模板：运行 <code>git clone https://github.com/Kele-Bingtang/vitepress-theme-teek-docs-template.git</code> 以下载当前文档模板。

    - title: 拥有丰富的 Features，并持续更新
      details: 满足大部分开发场景。
      image: /feature/features.svg
      features:
        - title: 最新流行稳定技术栈
          icon: icon-github
          details: 基于 Vue3.2、TypeScript、Vite4、Pinia、Element-Plus 等最新技术栈开发
          link: /guide/intro

        - title: 简单上手 & 学习
          icon: <svg viewBox="0 0 24 24" width="1.2em" height="1.2em"><path fill="currentColor" d="m23 12l-7.071 7.071l-1.414-1.414L20.172 12l-5.657-5.657l1.414-1.414L23 12zM3.828 12l5.657 5.657l-1.414 1.414L1 12l7.071-7.071l1.414 1.414L3.828 12z"></path></svg>
          details: 项目结构清晰，代码简单、易读。

        - title: 规范工程化工作流
          icon: /teek-logo-mini.svg
          details: 配置 Eslint、Prettier、Husky、Commitlint、Lint-staged 规范前端工程代码规范。

        - title: 完善的打包优化方案
          icon: icon-github
          details: 内置规范的打包目录，提供打包压缩功能，减少打包体积。

        - title: 丰富的组件
          icon: /teek-logo-mini.svg
          details: 提供丰富的通用组件、业务组件。
          link: /ecosystem/components

        - title: 常用 Hook 函数
          icon: icon-gitee
          details: 提供丰富的组件、常用 Hooks 封装，实现复用思想，减少重复开发，提高效率。

        - title: 个性化主题配置
          icon: icon-xiangce
          details: 提供主题颜色配置，暗黑、灰色、色弱等模式切换。
          link: /guide/theme-enhance

        - title: 多种布局配置
          icon: /teek-logo-mini.svg
          details: 提供多种布局、标签栏切换，布局显隐，满足大部分场景。

        - title: 项目权限管控
          icon: /teek-logo-mini.svg
          details: 采用 RBAC 权限管控，提供菜单、路由及按钮粗细粒度权限管理方案

        - title: 国际化
          icon: /teek-logo-mini.svg
          details: 内置常用国际化转换函数，支持自定义国际化切换，

        - title: IFrame 嵌入
          icon: /teek-logo-mini.svg
          details: 提供 IFrame 嵌入、缓存功能，支持门户 Portal 布局。

        - title: 自定义指令
          icon: /teek-logo-mini.svg
          details: 内置多种 Vue 自定义指令，提供傻瓜式指令一键注册功能。

        - title: Axios 封装
          icon: /teek-logo-mini.svg
          details: 基于 Axios 封装常用请求模块，内置业务拦截器、异常拦截器。

        - title: 多种图标类型
          icon: /teek-logo-mini.svg
          details: 支持 IconFont、SVG、Iconify 等多种图标类型渲染。
          link: /guide/icon-use

    - title: 布局
      details: 多种布局、标签栏切换，布局组件显隐
      image: /feature/layout.svg
      highlights:
        - title: 六大布局
          icon: /teek-logo-mini.svg
          details: 内置纵向、经典、横向、分栏、混合、子系统六大布局切换

        - title: 深色模式
          icon: /teek-logo-mini.svg
          details: 可以自由切换浅色模式与深色模式

        - title: 主题色切换
          icon: /teek-logo-mini.svg
          details: 支持自定义主题色并允许用户在预设的主题颜色之间切换

        - title: 布局组件
          icon: /teek-logo-mini.svg
          details: 支持图标、面包屑、导航栏等组件显隐，内置缓存功能，记住用户的布局配置
```

```yaml [博客风格]
---
layout: home

tk:
  features:
    - title: 指南
      details: Hd Security 使用指南说明
      link: /01.指南/
      images: /img/web.png
    - title: 设计
      description: Hd Security 设计思路说明
      link: /design/
      images: /img/ui.png
    - title: API
      description: Hd Security 所有的 API 介绍
      link: /07.API/01.API - 登录/
      images: /img/other.png
---
```

:::

### 主题配置

在首页 `index.md` 的 `frontmatter`，可以覆盖 `config` 主题配置的首页相关选项：

* `banner` 横幅
* `topArticle` 精选文章
* `category` 分类
* `tag` 标签
* `friendLink` 友情链接
* `docAnalysis` 站点分析
* `post` 文章列表
* `article` 文章信息（作者、创建时间、分类、标签等）
* `page` 分页
* ...

在首页 `index.md` 的 `frontmatter` 中配置时，建议以 `tk` 开头，然后是主题配置的选项，举个示例：

```yaml
---
tk:
  banner:
    enabled: true
    bgStyle: fullImg
    imgSrc:
      - /img/bg1.jpg
      - /img/bg2.jpg
  category:
    enabled: true
    limit: 7
  article:
    showIcon: false
  page:
    pageSize: 20
---
```

这些配置将会覆盖 `config` 主题配置中的对应选项。

::: tip
不以 `tk` 开头也是可以的，Teek 支持的 frontmatter 既可以是 `tk.xx.xx`，也可以是 `xx.xx`，其中 `tk.xx.xx` 优先级更高。
:::

## 文章页配置

在文章页的 `frontmatter`，可以覆盖 `config` 主题配置的文章页相关选项：

* `author` 作者信息
* `article` 文章信息（作者、创建时间、分类、标签等）
* `breadcrumb` 面包屑
* `articleShare` 文章分享
* `appreciation` 赞赏
* ...

在文章页的 `frontmatter` 中配置时，直接填写 `config` 主题配置的对应选项，举个示例：

```yaml
---
author:
  name: TianKe
  link: https://github.com/Kele-Bingtang/vitepress-theme-teek
article:
  showCategory: true
breadcrumb:
  separator: -
---
```

## 文章配置

除了支持覆盖主题配置的选项，Teek 还提供了以下额外的选项：

```yaml
---
title: 标题
date: 2025-03-07 01:16:28
permalink: /pages/b1ad26
categories:
  - 分类 1
  - 分类 2
tags:
  - 标签 1
titleTag: 原创
top: true
sticky: 1
sidebar: true
article: true
comment: true
description: 文章摘要
coverImg: /img/web.png
docAnalysis: true
inCatalogue: true
autoTitle: true
articleUpdate: true
inHomePost: true
---
```

### title

* 类型：`string`

页面标题，将作为一级标题显示在页面上。

支持填写 HTML 标签 ：

::: code-group

```yaml [基础 HTML]
---
title: frontmatter 配置 <span style="color:#395AE3">原创</span>
---
```

```yaml [Badge 组件]
---
title: frontmatter 配置 <Badge type="tip" text="原创" />
---
```

```yaml [TkTitleTag 组件]
---
title: frontmatter 配置 <TkTitleTag type="tip" text="原创" position="right" />
---
```

```yaml [TkIcon 图标]
---
title: frontmatter 配置 <TkIcon icon="simple-icons:gitee" color="var(--tk-theme-color)" />
---
```

:::

除此之外，`frontmatter.title` 支持所有的基础 HTML 以及已经全局注册的 Vue 组件。

::: tip 如何注册全局 Vue 组件
在 `.vitepress/theme/index.ts` 中的 `enhanceApp` 函数中进行，通过 `app.component("your componentName", your component)` 方法进行注册。
:::

### date

* 类型：`string`

页面创建时间，将作为创建时间显示在首页的文章列表、文章页顶部。

### permalink

* 类型：`string`

页面永久链接，将作为页面访问的 URL 路径，该配置项由 `vitepress-plugin-permalink` 提供。

### categories

* 类型：`string[]`

分类，将显示在首页的文章列表、分类卡片、文章页顶部，并在分类页渲染所有分类的文章。

### tags

* 类型：`string[]`

标签，将显示在首页的文章列表、标签卡片、文章页顶部，并在标签页渲染所有标签的文章。

### titleTag&#x20;

用于给标题添加 `原创`、`转载`、`优质`、`推荐` 等自定义标记。

添加了标题标记的文章，在文章列表、文章页、归档页、目录页的文章标题都会显示此标记。

除此之外，也可以直接在 `frontmatter.title` 前后通过 `<TkTitleTag>` 组件的方式添加标题标记。

::: code-group

```yaml [标题后]
---
title: frontmatter 配置 <TkTitleTag type="tip" text="原创" position="right" />
---
```

```yaml [标题前]
---
title: <TkTitleTag type="tip" text="原创" position="right" /> frontmatter 配置
---
```

:::

### top

* 类型：`boolean`
* 默认值：`false`

标记为精选文章。如果为 `true`，则在首页的精选文章卡片中显示，如果多个文章都设置了 `top: true`，则按照 `date` 进行排序（最新时间在上面）。

### sticky

* 类型：`number`

文章置顶，设置了此项将在首页文章列表中处于置顶位置，如果同时设置了 `top: true`，则在精选文章卡片的序号添加高亮背景色，背景色请看 [主题配置 - tagColor](/config/theme#tagColor)。

### sidebar

* 类型：`boolean`
* 默认值：`true`

侧边栏，`true` 表示显示侧边栏。设置为 `false` 表示不显示侧边栏。

### article

* 类型：`boolean`
* 默认值：`true`

非文章页的标记，非文章页如目录页、关于、友情链接等自定义页面，需要设置此项。设置之后这个页面将被认定为非文章页，不显示面包屑和文章信息（作者、时间、分类、标签等），不显示在如下模块中：

* 首页的文章列表
* 归档页
* 文章最近更新栏

### comment

* 类型：`boolean`
* 默认值：`true`

评论功能，`true` 表示显示评论区。设置为 `false` 表示不显示评论区。

### description

* 类型：`string`
* 默认值：null

文章摘要，将显示在首页的文章列表。

### coverImg

* 类型：`string`
* 默认值：null

封面图片，将显示在首页的文章列表。

### inCatalogue

* 类型：`boolean`
* 默认值：`true`

目录页，`true` 表示允许 Markdown 文档纳入目录里。设置为 `false` 表示不允许，该配置项由 `vitepress-plugin-catalogue` 插件提供。

### docAnalysis

* 类型：`boolean`
* 默认值：`true`

站点分析，`true` 表示允许站点信息功能对 Markdown 文档进行数据分析和统计。设置为 `false` 表示不允许，该配置项由 `vitepress-plugin-doc-analysis` 插件提供。

### autoTitle

* 类型：`boolean`
* 默认值：`true`

如果 Markdown 不设置一级标题，在访问页面的时候自动添加一级标题。设置为 `false` 表示不允许自动添加一级标题，该配置项由 `vitepress-plugin-md-h1` 插件提供。

一级标题获取优先级：`frontmatter.title` > 文件名

### articleUpdate&#x20;

* 类型：`boolean`
* 默认值：`true`

是否在文章底部显示最近更新栏。

### inHomePost&#x20;

* 类型：`boolean`
* 默认值：`true`

Markdown 文档是否在首页的文章列表中显示。

### sidebarSort&#x20;

* 类型：`number`
* 默认值：`9999`

侧边栏排序，值越小越靠前，设置此项将覆盖文件名序号进行排序，由 `vitepress-plugin-sidebar-resolve` 插件提供。

### sidebarPrefix&#x20;

* 类型：`string`
* 默认值：`""`

侧边栏标题前缀，可以添加 HTML 如 iconfont 图标，由 `vitepress-plugin-sidebar-resolve` 插件提供。

### sidebarSuffix&#x20;

* 类型：`string`
* 默认值：`""`

侧边栏标题后缀，可以添加 HTML 如 iconfont 图标，由 `vitepress-plugin-sidebar-resolve` 插件提供。

## 功能页配置

上述的 `frontmatter` 配置为通用配置，适用于任何 Markdown 文档。

除此之外，还有一些针对功能页的 `frontmatter` 局部配置，详细请看 [目录页配置](/reference/catalogue-page) 和 [功能页配置](/reference/function-page)。

---

---
url: /Java/微服务专栏/02.服务网关Gateway/Gateway服务网关.md
---

# Gateway服务网关

Spring Cloud Gateway 是 Spring Cloud 的一个全新项目，该项目是基于 Spring 5.0，Spring Boot 2.0 和 Project Reactor 等响应式编程和事件流技术开发的网关，它旨在为微服务架构提供一种简单有效的统一的 API 路由管理方式。

## 1.为什么需要网关

Gateway网关是服务的守门神，所有微服务的统一入口。

网关的**核心功能特性**：

* 请求路由
* 权限控制
* 限流

架构图：

![image-20210714210131152](/assets/image-20210714210131152.DFjpa_hc.png)

**权限控制**：网关作为微服务入口，需要校验用户是是否有请求资格，如果没有则进行拦截。

**路由和负载均衡**：一切请求都必须先经过gateway，但网关不处理业务，而是根据某种规则，把请求转发到某个微服务，这个过程叫做路由。当然路由的目标服务有多个时，还需要做负载均衡。

**限流**：当请求流量过高时，在网关中按照下流的微服务能够接受的速度来放行请求，避免服务压力过大。

在SpringCloud中网关的实现包括两种：

* gateway
* zuul

Zuul是基于Servlet的实现，属于阻塞式编程。而SpringCloudGateway则是基于Spring5中提供的WebFlux，属于响应式编程的实现，具备更好的性能。

## 2.gateway快速入门

下面，我们就演示下网关的基本路由功能。基本步骤如下：

1. 创建SpringBoot工程gateway，引入网关依赖
2. 编写启动类
3. 编写基础配置和路由规则
4. 启动网关服务进行测试

### 1）创建gateway服务，引入依赖

创建服务：

![image-20210714210919458](/assets/image-20210714210919458.BMI2xO5-.png)

引入依赖：

```xml
<!--网关-->
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-gateway</artifactId>
</dependency>
<!--nacos服务发现依赖-->
<dependency>
    <groupId>com.alibaba.cloud</groupId>
    <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>
</dependency>
```

### 2）编写启动类

```java
package cn.itcast.gateway;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

@SpringBootApplication
public class GatewayApplication {

	public static void main(String[] args) {
		SpringApplication.run(GatewayApplication.class, args);
	}
}
```

### 3）编写基础配置和路由规则

创建application.yml文件，内容如下：

```yaml
server:
  port: 10010 # 网关端口
spring:
  application:
    name: gateway # 服务名称
  cloud:
    nacos:
      server-addr: localhost:8848 # nacos地址
    gateway:
      routes: # 网关路由配置
        - id: user-service # 路由id，自定义，只要唯一即可
          # uri: http://127.0.0.1:8081 # 路由的目标地址 http就是固定地址
          uri: lb://userservice # 路由的目标地址 lb就是负载均衡，后面跟服务名称
          predicates: # 路由断言，也就是判断请求是否符合路由规则的条件
            - Path=/user/** # 这个是按照路径匹配，只要以/user/开头就符合要求
```

我们将符合`Path` 规则的一切请求，都代理到 `uri`参数指定的地址。

本例中，我们将 `/user/**`开头的请求，代理到`lb://userservice`，lb是负载均衡，根据服务名拉取服务列表，实现负载均衡。

### 4）重启测试

重启网关，访问http://localhost:10010/user/1时，符合`/user/**`规则，请求转发到uri：http://userservice/user/1，得到了结果：

![image-20210714211908341](/assets/image-20210714211908341.CLIaN6ol.png)

### 5）网关路由的流程图

整个访问的流程如下：

![image-20210714211742956](/assets/image-20210714211742956.C09-oPHz.png)

总结：

网关搭建步骤：

1. 创建项目，引入nacos服务发现和gateway依赖

2. 配置application.yml，包括服务基本信息、nacos地址、路由

路由配置包括：

1. 路由id：路由的唯一标示

2. 路由目标（uri）：路由的目标地址，http代表固定地址，lb代表根据服务名负载均衡

3. 路由断言（predicates）：判断路由的规则，

4. 路由过滤器（filters）：对请求或响应做处理

接下来，就重点来学习路由断言和路由过滤器的详细知识

## 3.断言工厂

我们在配置文件中写的断言规则只是字符串，这些字符串会被Predicate Factory读取并处理，转变为路由判断的条件

例如Path=/user/\*\*是按照路径匹配，这个规则是由

`org.springframework.cloud.gateway.handler.predicate.PathRoutePredicateFactory`类来

处理的，像这样的断言工厂在SpringCloudGateway还有十几个:

| **名称**   | **说明**                       | **示例**                                                     |
| ---------- | ------------------------------ | ------------------------------------------------------------ |
| After      | 是某个时间点后的请求           | -  After=2037-01-20T17:42:47.789-07:00\[America/Denver]       |
| Before     | 是某个时间点之前的请求         | -  Before=2031-04-13T15:14:47.433+08:00\[Asia/Shanghai]       |
| Between    | 是某两个时间点之前的请求       | -  Between=2037-01-20T17:42:47.789-07:00\[America/Denver],  2037-01-21T17:42:47.789-07:00\[America/Denver] |
| Cookie     | 请求必须包含某些cookie         | - Cookie=chocolate, ch.p                                     |
| Header     | 请求必须包含某些header         | - Header=X-Request-Id, \d+                                   |
| Host       | 请求必须是访问某个host（域名） | -  Host=**.somehost.org,**.anotherhost.org                   |
| Method     | 请求方式必须是指定方式         | - Method=GET,POST                                            |
| Path       | 请求路径必须符合指定规则       | - Path=/red/{segment},/blue/\*\*                               |
| Query      | 请求参数必须包含指定参数       | - Query=name, Jack或者-  Query=name                          |
| RemoteAddr | 请求者的ip必须是指定范围       | - RemoteAddr=192.168.1.1/24                                  |
| Weight     | 权重处理                       |                                                              |

我们只需要掌握Path这种路由工程就可以了。

## 4.过滤器工厂

GatewayFilter是网关中提供的一种过滤器，可以对进入网关的请求和微服务返回的响应做处理：

![image-20210714212312871](/assets/image-20210714212312871.DVTQez-V.png)

### 3.4.1.路由过滤器的种类

Spring提供了31种不同的路由过滤器工厂。例如：

| **名称**             | **说明**                     |
| -------------------- | ---------------------------- |
| AddRequestHeader     | 给当前请求添加一个请求头     |
| RemoveRequestHeader  | 移除请求中的一个请求头       |
| AddResponseHeader    | 给响应结果中添加一个响应头   |
| RemoveResponseHeader | 从响应结果中移除有一个响应头 |
| RequestRateLimiter   | 限制请求的流量               |

### 3.4.2.请求头过滤器

下面我们以AddRequestHeader 为例来讲解。

> **需求**：给所有进入userservice的请求添加一个请求头：Truth=itcast is freaking awesome!

只需要修改gateway服务的application.yml文件，添加路由过滤即可：

```yaml
spring:
  cloud:
    gateway:
      routes:
      - id: user-service 
        uri: lb://userservice 
        predicates: 
        - Path=/user/** 
        filters: # 过滤器
        - AddRequestHeader=Truth, Itcast is freaking awesome! # 添加请求头
```

当前过滤器写在userservice路由下，因此仅仅对访问userservice的请求有效。

### 3.4.3.默认过滤器

如果要对所有的路由都生效，则可以将过滤器工厂写到default下。格式如下：

```yaml
spring:
  cloud:
    gateway:
      routes:
      - id: user-service 
        uri: lb://userservice 
        predicates: 
        - Path=/user/**
      default-filters: # 默认过滤项
      - AddRequestHeader=Truth, Itcast is freaking awesome! 
```

### 3.4.4.总结

过滤器的作用是什么？

① 对路由的请求或响应做加工处理，比如添加请求头

② 配置在路由下的过滤器只对当前路由的请求生效

defaultFilters的作用是什么？

① 对所有路由都生效的过滤器

## 5.全局过滤器

上一节学习的过滤器，网关提供了31种，但每一种过滤器的作用都是固定的。如果我们希望拦截请求，做自己的业务逻辑则没办法实现。

### 3.5.1.全局过滤器作用

全局过滤器的作用也是处理一切进入网关的请求和微服务响应，与GatewayFilter的作用一样。区别在于GatewayFilter通过配置定义，处理逻辑是固定的；而GlobalFilter的逻辑需要自己写代码实现。

定义方式是实现GlobalFilter接口。

```java
public interface GlobalFilter {
    /**
     *  处理当前请求，有必要的话通过{@link GatewayFilterChain}将请求交给下一个过滤器处理
     *
     * @param exchange 请求上下文，里面可以获取Request、Response等信息
     * @param chain 用来把请求委托给下一个过滤器 
     * @return {@code Mono<Void>} 返回标示当前过滤器业务结束
     */
    Mono<Void> filter(ServerWebExchange exchange, GatewayFilterChain chain);
}
```

在filter中编写自定义逻辑，可以实现下列功能：

* 登录状态判断
* 权限校验
* 请求限流等

### 3.5.2.自定义全局过滤器

需求：定义全局过滤器，拦截请求，判断请求的参数是否满足下面条件：

* 参数中是否有authorization，

* authorization参数值是否为admin

如果同时满足则放行，否则拦截

实现：

在gateway中定义一个过滤器：

```java
import org.springframework.cloud.gateway.filter.GatewayFilterChain;
import org.springframework.cloud.gateway.filter.GlobalFilter;
import org.springframework.core.annotation.Order;
import org.springframework.http.HttpStatus;
import org.springframework.stereotype.Component;
import org.springframework.web.server.ServerWebExchange;
import reactor.core.publisher.Mono;

@Order(-1)
@Component
public class AuthorizeFilter implements GlobalFilter {
    @Override
    public Mono<Void> filter(ServerWebExchange exchange, GatewayFilterChain chain) {
        // 1.获取请求参数
        MultiValueMap<String, String> params = exchange.getRequest().getQueryParams();
        // 2.获取authorization参数
        String auth = params.getFirst("authorization");
        // 3.校验
        if ("admin".equals(auth)) {
            // 放行
            return chain.filter(exchange);
        }
        // 4.拦截
        // 4.1.禁止访问，设置状态码
        exchange.getResponse().setStatusCode(HttpStatus.FORBIDDEN);
        // 4.2.结束处理
        return exchange.getResponse().setComplete();
    }
}
```

### 3.5.3.过滤器执行顺序

请求进入网关会碰到三类过滤器：当前路由的过滤器、DefaultFilter、GlobalFilter

请求路由后，会将当前路由过滤器和DefaultFilter、GlobalFilter，合并到一个过滤器链（集合）中，排序后依次执行每个过滤器：

![image-20210714214228409](/assets/image-20210714214228409.Du2fRKpZ.png)

排序的规则是什么呢？

* 每一个过滤器都必须指定一个int类型的order值，**order值越小，优先级越高，执行顺序越靠前**。
* GlobalFilter通过实现Ordered接口，或者添加@Order注解来指定order值，由我们自己指定
* 路由过滤器和defaultFilter的order由Spring指定，默认是按照声明顺序从1递增。
* 当过滤器的order值一样时，会按照 defaultFilter > 路由过滤器 > GlobalFilter的顺序执行。

详细内容，可以查看源码：

`org.springframework.cloud.gateway.route.RouteDefinitionRouteLocator#getFilters()`方法是先加载defaultFilters，然后再加载某个route的filters，然后合并。

`org.springframework.cloud.gateway.handler.FilteringWebHandler#handle()`方法会加载全局过滤器，与前面的过滤器合并后根据order排序，组织过滤器链

## 6.跨域问题

### 6.1.什么是跨域问题

跨域：域名不一致就是跨域，主要包括：

* 域名不同： www.taobao.com 和 www.taobao.org 和 www.jd.com 和 miaosha.jd.com

* 域名相同，端口不同：localhost:8080和localhost8081

跨域问题：浏览器禁止请求的发起者与服务端发生跨域ajax请求，请求被浏览器拦截的问题

解决方案：CORS，这个以前应该学习过，这里不再赘述了。不知道的小伙伴可以查看https://www.ruanyifeng.com/blog/2016/04/cors.html

### 6.2.模拟跨域问题

新建html页面文件：

```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Document</title>
</head>
<body>
<pre>
spring:
  cloud:
    gateway:
      globalcors: # 全局的跨域处理
        add-to-simple-url-handler-mapping: true # 解决options请求被拦截问题
        corsConfigurations:
          '[/**]':
            allowedOrigins: # 允许哪些网站的跨域请求
              - "http://localhost:8090"
              - "http://www.leyou.com"
            allowedMethods: # 允许的跨域ajax的请求方式
              - "GET"
              - "POST"
              - "DELETE"
              - "PUT"
              - "OPTIONS"
            allowedHeaders: "*" # 允许在请求中携带的头信息
            allowCredentials: true # 是否允许携带cookie
            maxAge: 360000 # 这次跨域检测的有效期
</pre>
</body>
<script src="https://unpkg.com/axios/dist/axios.min.js"></script>
<script>
  axios.get("http://localhost:10010/user/1?authorization=admin")
  .then(resp => console.log(resp.data))
  .catch(err => console.log(err))
</script>
</html>
```

放入tomcat或者nginx这样的web服务器中，启动并访问。

可以在浏览器控制台看到下面的错误：

![image-20210714215832675](/assets/image-20210714215832675.DMN-4j5B.png)

从localhost:8090访问localhost:10010，端口不同，显然是跨域的请求。

### 6.3.解决跨域问题

在gateway服务的application.yml文件中，添加下面的配置：

```yaml
spring:
  cloud:
    gateway:
      # 。。。
      globalcors: # 全局的跨域处理
        add-to-simple-url-handler-mapping: true # 解决options请求被拦截问题
        corsConfigurations:
          '[/**]':
            allowedOrigins: # 允许哪些网站的跨域请求 
              - "http://localhost:8090"
            allowedMethods: # 允许的跨域ajax的请求方式
              - "GET"
              - "POST"
              - "DELETE"
              - "PUT"
              - "OPTIONS"
            allowedHeaders: "*" # 允许在请求中携带的头信息
            allowCredentials: true # 是否允许携带cookie
            maxAge: 360000 # 这次跨域检测的有效期
```

---

---
url: /Java/JVM性能调优/01.JVM概念/5_GC垃圾回收.md
---

# GC垃圾回收

## 垃圾回收的区域

![img](/assets/kuangstudyea734fba-0acf-411a-9145-b09bf0f8c0fa.D3v2Enlg.jpg)

## GC之引用计数法

![img](/assets/kuangstudyba7e21f0-b594-4d03-a045-f0bc7999d1b9.TXZtGFyU.jpg)

## GC之复制算法

![img](/assets/kuangstudyfcf0c24d-0c57-444f-a3e7-75bf360afaa6.Bt345Vew.jpg)
![img](/assets/kuangstudye7037e48-3068-4698-beea-ab5bd6c93f89.r2G0lF_p.jpg)

* 好处：没有内存的碎片。
* 坏处：浪费了内存空间（多了一半空间to永远是空）。假设对象100%存活（极端情况），不适合使用复制算法。

### 使用场景

复制算法最佳使用场景：对象存活度较低的时候（新生区）

## GC之标记清除压缩算法

### 标记清除

![img](/assets/kuangstudy917c317f-d10a-4a63-adcc-1d9f569ca63e.C-X93vOr.jpg)

* 优点：不需要额外的空间。
* 缺点：两次扫描，严重浪费时间，会产生内存碎片。

### 标记清除压缩

![img](/assets/kuangstudy47e8d6fc-3f99-4296-b75e-608518e1403f.DPYPruTy.jpg)

### 标记清除压缩（改进）

可以进行多次标记清除，再进行一次压缩。

## GC算法总结

内存效率：复制算法>标记清除算法>标记压缩算法（时间复杂度）
内存整齐度：复制算法=标记压缩算法>标记清除算法
内存利用率：标记压缩算法=标记清除算法>复制算法

思考一个问题：难道没有最优算法吗？
答案：没有，没有最好的算法，只有最合适的算法——》GC：分代收集算法

年轻代：

* 存活率低
* 复制算法

老年代：

* 区域大：存活率高
* 标记清除（内存碎片不是太多）+标记压缩混合实现

---

---
url: /daily/博客文档/其他博客文档/Github_Pages加速.md
---

# Github Pages加速

## 一、前言

在很多时候，我发现我在访问我的博客时，非常缓慢。这将很影响我的浏览体验，GitHub pages是GitHub提供的一项静态站点托管网页，由于国内问题，访问延迟200-300ms，当然这还是在网络较好的时候，在网络差的情况下会很难加载完全网页，异常烦躁。于是，我开始搜索相关教程，决定解决这个问题。

## 二、自定义域名

### 1、注册域名

以阿里云为例，注册地址：https://wanwang.aliyun.com

### 2、域名解析

购买域名后，需要进行域名解析映射到Github Pages的域名上。

![image-20241201141128561](/assets/image-20241201141128561.D3bgV_nb.png)

添加记录，确定保存。

![image-20241201141214530](/assets/image-20241201141214530.jc-tIAHk.png)

### 3、GitHub Pages绑定自定义域名

到GitHub去绑定自己的域名

![image-20241201141013087](/assets/image-20241201141013087.CBrDlXDH.png)

保存之后 github 会自动的在仓库根目录里生成一个`CNAME`文件，里面存储着域名配置信息。

完成以上的步骤后，就**已经可以通过自己的域名去访问**的Github静态网站了，像现在就可以通过<http://luckilyxxl.xyz/>去访问我的个人博客了，不过此时**还只能使用http进行访问，还不能够通过https进行访问**，不过不急，在完成Cloudflare的CDN加速的步骤中就可以完成https的设置。

## 三、Cloudflare进行CDN加速

大部分针对国内加速的cdn服务都需要网站备案才能使用，但是由于我的Github Pages是托管在Github上的，在国外，所以无法备案，并且我暂时也没有将博客移植到国内的云服务器主机上的想法，因此最终选择了Cloudflare平台的cdn服务，没有选择国内其它的主流平台。最后，需要说明的一点是，由于使用cdn服务需要实名认证，因此考虑到个人信息安全，建议尽量选择主流的平台，防止个人信息泄露。

Cloudflare地址：<https://dash.cloudflare.com/>

### 1、注册账号

通过<https://dash.cloudflare.com/sign-up>链接进行注册，注册过程非常简单，这里不再说明。

### 2、添加网站

账号注册好后，进入控制台，点击“添加站点”加入需要使用cdn加速的站点。

![image-20241201142026997](/assets/image-20241201142026997.D1Pyl5lW.png)

注意填入的站点域名不需要加入`www.`前缀，如下图。

![image-20241201141935379](/assets/image-20241201141935379.D9lJvIOI.png)

完成以上步骤后，下一步选择计划，个人完全免费版本即可。

![image-20241201145149001](/assets/image-20241201145149001.CXf4QHSp.png)

### 3、添加DNS记录

提交之后会自动扫描域名对应的解析记录，如果没有可以自己添加

Github Pages的IP为以下四个

```
185.199.108.153
185.199.109.153
185.199.110.153
185.199.111.153
```

![image-20241201150408842](/assets/image-20241201150408842.3EAG4ymn.png)

### 4、更改DNS服务器至Cloudflare

通过域名的运营商修改对应的 DNS 记录

![image-20241201145542728](/assets/image-20241201145542728.BBFCFU9n.png)

![image-20241201145526010](/assets/image-20241201145526010.Dca7lPpE.png)

设置完毕了，等一段时间再用命令行验证一下。

## 参考资料

方案确定：https://blog.csdn.net/craig\_cc/article/details/105560504

主流CDN平台：https://xiaoshen.blog.csdn.net/article/details/129872749

⭐⭐⭐使用cloudflare免费加速github page：https://monkeywie.cn/2020/08/20/fast-github-page-with-cloudflare

Github Pages自定义域名：https://developer.aliyun.com/article/1335710

GitHub Pages自定义域名：https://www.cnblogs.com/yuelblog/p/15829774.html

Github Pages 绑定个人域名：https://segmentfault.com/a/1190000011203711

---

---
url: /daily/博客文档/其他博客文档/Github个人主页改造.md
---

# Github个人主页改造

## 一、前言

`GitHub` 有一项特色功能 - `GitHub profile`，以及一些列开源工具、项目来帮助打造自己特色的 `GitHub profile`。

`GitHub profile` 也是最近两年 `GitHub` 才新加的功能，开发者可以通过编写 `README` 打造属于自己的个人 `GitHub` 首页。

**一句话总结：你可以通过README.md来自定义你Github首页**

## 二、使用

官方说明文档：https://docs.github.com/zh/account-and-profile/setting-up-and-managing-your-github-profile/customizing-your-profile/managing-your-profile-readme

整体使用很简单，分以下几步

1. 创建一个同名仓库
2. 引用模板
3. 为内容添加有趣模块

### 2.1、创建一个同名仓库

1. 在任何页面的右上角，选择 ，然后单击“新建存储库”。
2. 在“Repository name（仓库名称）”下，输入与您的 GitHub 用户名匹配的仓库名称。 例如，如果您的用户名是 "octocat"，则仓库名称必须为 "octocat"。
3. 选择“Public”。
4. 选择“使用 README 初始化此存储库”。

profile 属于彩蛋类功能，创建时在下方将会出现提示。如果勾选自动创建 README，将会创建一个特殊的 README 模版，长这样：

```xml
### Hi there 👋

<!--
**GULU-H/GULU-H** is a ✨ _special_ ✨ repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- 🔭 I’m currently working on ...
- 🌱 I’m currently learning ...
- 👯 I’m looking to collaborate on ...
- 🤔 I’m looking for help with ...
- 💬 Ask me about ...
- 📫 How to reach me: ...
- 😄 Pronouns: ...
- ⚡ Fun fact: ...
-->
```

默认创建的内容肯定不满足我们需求，接下来看看大佬们都怎么玩的~

### 2.2、引用模板

Github上有很多大神改造的模板

**项目1：awesome-github-profile**

地址：https://zzetao.github.io/awesome-github-profile

该项目提供了丰富的模版，可以从中选择喜爱的模版进行二次开发。

**项目2：Awesome-Profile-README-templates**

地址：https://github.com/kautukkundan/Awesome-Profile-README-templates

该仓库没有概览图，但是可以进入项目目录点击各 markdown 文件进行查看。

**项目3：gh-profile-readme-generator**

地址：https://rahuldkjain.github.io/gh-profile-readme-generator/

使用该网站可通过填写表单为你生成 profile，如果懒得二次定制可以使用该网站进行生成。

## 三、为内容添加有趣模块

如果上面选择了一个有趣的模板后，则可以再为内容添加一些小部件

### 3.1、徽章badge

徽章我们见过很多，其实就是一个Markdown图片链接，借助shields.io来生成即可

地址：https://shields.io/

图标：https://simpleicons.org/

[md\_notes/使用shields给仓库生成技术栈标签.md at master · caolib/md\_notes (github.com)](https://github.com/caolib/md_notes/blob/master/使用shields给仓库生成技术栈标签.md)

### 3.2、waka 时间展示

地址：https://github.com/marketplace/actions/waka-readme

该项目可以生成一个代码提交图等内容，详情可看官网介绍

### 3.3、展示 GitHub stars 等信息

地址：https://github.com/anuraghazra/github-readme-stats

### 3.4、GitHub contributions贪吃蛇游戏

地址：https://github.com/Platane/snk

该项目可以根据你的贡献量生成贪吃蛇动画

https://juejin.cn/post/7119378607629140005

https://blog.csdn.net/weixin\_43332715/article/details/133864425

## 参考资料

https://www.cnblogs.com/mq0036/p/18089397

---

---
url: /Java/容器/Jenkins/Gitlab安装.md
---

# Gitlab代码托管服务器安装

## Gitlab简介

> 官网： https://about.gitlab.com/
>
> 极狐（GitLab中国发行版）：https://gitlab.cn/install/

GitLab 是一个用于仓库管理系统的开源项目，使用Git作为代码管理工具，并在此基础上搭建起来的
web服务。

GitLab和GitHub一样属于第三方基于Git开发的作品，免费且开源（基于MIT协议），与Github类似，
可以注册用户，任意提交你的代码，添加SSHKey等等。不同的是，GitLab是可以部署到自己的服务器
上，数据库等一切信息都掌握在自己手上，适合团队内部协作开发，你总不可能把团队内部的智慧总放
在别人的服务器上吧？简单来说可把GitLab看作个人版的GitHub。

为什么使用？

企业中为了保护代码的安全性，是不可能将代码放到github等第三方平台的，因此搭建私有git仓库就成了必备技能。目前来说，GitLab是最常用的DevOps工具，是唯一一个以单个应用程序交付的DevOps平台，在全球各类分析报告中赢得关注，广泛覆盖。而极狐GitLab是GitLab中国发行版，一站式交付的开放一体化DevOps平台。极狐GitLab基于GitLab EE版本每月持续同步更新，并针对中国用户需求添加独立研发的功能特性。

## Gitlab安装

### Ubuntu下安装Gitlab

进入官网下载

https://packages.gitlab.com/gitlab/gitlab-ce

选择需要的版本

![image-20231112122705238](/assets/image-20231112122705238.vNuGmmuD.png)

复制下载链接及安装命令安装

![image-20231112122729109](/assets/image-20231112122729109.BPCNKVyw.png)

命令如下

```bash
# 如下载过程如果很慢，可以浏览器输入链接地址下载，然后通过scp命令传到服务器上，再继续执行下一个命令
curl -s https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.deb.sh | sudo bash

# Install
sudo apt-get install gitlab-ce=16.3.6-ce.0
```

安装成功后的页面

![image-20231112122950660](/assets/image-20231112122950660.DVoMh03P.png)

安装完成后，修改配置文件

```bash
sudo vim /etc/gitlab/gitlab.rb
// ubuntu图形化界面下可以使用 gedit 命令直接打开文件修改
sudo gedit /etc/gitlab/gitlab.rb
```

gitlab.rb配置，33行左右修改端口号

```bash
##! Note: During installation/upgrades, the value of the environment variable
##! EXTERNAL_URL will be used to populate/replace this value.
##! On AWS EC2 instances, we also attempt to fetch the public hostname/IP
##! address from AWS. For more details, see:
##! https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
#external_url 'http://gitlab.example.com'
external_url 'http://127.0.0.1:8081'

## Roles for multi-instance GitLab
```

启动

```bash
sudo gitlab-ctl reconfigure //启动服务，加载配置
```

补充命令

```bash
sudo gitlab-ctl start # 启动所有 gitlab 组件；
sudo gitlab-ctl stop # 停止所有 gitlab 组件；
sudo gitlab-ctl restart # 重启所有 gitlab 组件；
sudo gitlab-ctl status # 查看服务状态；
sudo gitlab-ctl reconfigure # 启动服务；
sudo vim /etc/gitlab/gitlab.rb # 修改默认的配置文件；
gitlab-rake gitlab:check SANITIZE=true --trace # 检查gitlab；
sudo gitlab-ctl tail # 查看日志；
```

### Centos下安装Gitlab

1. 安装相关依赖

```bash
yum -y install policycoreutils openssh-server openssh-clients postfix
```

2. 启动ssh服务&设置为开机启动

```bash
systemctl enable sshd && sudo systemctl start sshd
```

3. 设置postfix开机自启，并启动，postfix支持gitlab发信功能

```bash
systemctl enable postfix && systemctl start postfix
```

4. 开放ssh以及http服务，然后重新加载防火墙列表

```bash
firewall-cmd --add-service=ssh --permanent
firewall-cmd --add-service=http --permanent
firewall-cmd --reload
```

如果关闭防火墙就不需要做以上配置

5. 下载gitlab包，并且安装

> 在线下载安装包：
>
> wget https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el6/gitlab-ce-12.4.2-ce.0.el6.x86\_64.rpm
>
> 安装：
>
> rpm -i gitlab-ce-12.4.2-ce.0.el6.x86\_64.rpm

6. 修改gitlab配置

> vi /etc/gitlab/gitlab.rb
>
> 修改gitlab访问地址和端口，默认为80，改为自定义
>
> external\_url 'http://192.168.64.128:8081'
>
> nginx\['listen\_port'] = 82

7. 重载配置及启动gitlab

```bash
gitlab-ctl reconfigure
gitlab-ctl restart
```

8. 把端口添加到防火墙

```bash
firewall-cmd --zone=public --add-port=82/tcp --permanent
firewall-cmd --reload
```

启动成功后，看到修改管理员root密码的页面，修改密码后，然后登录即可。

### Docker下安装Gitlab

docker下安装Gitlab，docker的安装参考本网站——容器相关——Docker分类下的《Linux安装Docker》

```bash
mkdir /etc/gitlab
mkdir /var/log/gitlab
mkdir /var/opt/gitlab

docker run --detach \
  --hostname 服务器公网IP或虚拟机IP地址 \
  --publish 443:443 --publish 80:80 \
  --name gitlab \
  --restart always \
  --volume $GITLAB_HOME/config:/etc/gitlab:Z \
  --volume $GITLAB_HOME/logs:/var/log/gitlab:Z \
  --volume $GITLAB_HOME/data:/var/opt/gitlab:Z \
  --shm-size 256m \
  registry.gitlab.cn/omnibus/gitlab-jh:latest
```

进入容器并查看密码

```shell
docker exec -it  gitlab /bin/bash #进入容器
 cat /etc/gitlab/initial_root_password #查看密码
exit #退出容器
```

访问Gitlab：浏览器输入您服务器的**IP地址**，如果出现502错误首先虚拟机/云服务器内存至少需要4g，如果满足条件需要多等一会即可，10分钟内一般能好。账号是root，密码是刚才进入容器查看的密码。

## 参考资料

\[1]. [ubuntu安装GitLab笔记]()

\[2]. https://www.cnblogs.com/chenjian688/p/16423193.html

---

---
url: /daily/日常笔记/Go语言学习.md
---

# Go语言学习

> 学习资料
>
> https://www.bilibili.com/video/BV1ae41157o9
>
> https://www.kuangstudy.com/bbs/1579340844195336194
>
> https://blog.csdn.net/qq\_56517253/article/details/131988859

### Go语言环境安装

下载地址：https://studygolang.com/dl

![image-20231113232701421](/assets/image-20231113232701421.BSqO9WcG.png)安装完成后，命令行输入 `go version` 是否安装成功查看，使用 `go env` 查看环境

```powershell
PS C:\Windows\system32> go version
go version go1.21.4 windows/amd64
```

新版本不再需要配置GOROOT、GOPATH环境变量，有需要可自己更改GOPATH（存放下载的资源和代码路径）。GOPATH目录下建三个文件夹：**src、pkg、bin**。

---

---
url: /常用框架/重试框架/2_Guava-Retry.md
---

# Guava-Retry

Guava retryer工具与spring-retry类似，都是通过定义重试者角色来包装正常逻辑重试，但是Guava retryer有更优的策略定义，在支持重试次数和重试频度控制基础上，能够兼容支持多个异常或者自定义实体对象的重试源定义，让重试功能有更多的灵活性。

Guava Retryer也是线程安全的，入口调用逻辑采用的是Java.util.concurrent.Callable的call方法。

### 1、引入依赖

```xml
<!-- https://mvnrepository.com/artifact/com.github.rholder/guava-retrying -->
<dependency>
    <groupId>com.github.rholder</groupId>
    <artifactId>guava-retrying</artifactId>
    <version>2.0.0</version>
</dependency>
```

### 2、重试方法

```java
/**
 * Guava-Retry 重试方法
 *
 * @author xxl
 * @date 2022/12/26 23:20
 */
@Slf4j
public class RetryDemoTask {

    /**
     * 重试方法
     *
     * @return
     */
    public static boolean retryTask(String param) {
        log.info("收到请求参数:{}", param);

        int i = RandomUtils.nextInt(0, 11);
        log.info("随机生成的数:{}", i);
        if (i < 2) {
            log.info("为0,抛出参数异常.");
            throw new IllegalArgumentException("参数异常");
        } else if (i < 5) {
            log.info("为1,返回true.");
            return true;
        } else if (i < 7) {
            log.info("为2,返回false.");
            return false;
        } else {
            //为其他
            log.info("大于2,抛出自定义异常.");
            throw new RemoteAccessException("大于2,抛出自定义异常");
        }
    }

}
```

### 3、重试测试类

```java
/**
 * Guava-Retry 重试测试类
 *
 * @author xxl
 * @date 2022/12/26 23:23
 */
public class GuavaRetryTest {

    @Test
    public void fun01() {
        // RetryerBuilder 构建重试实例 retryer,可以设置重试源且可以支持多个重试源，可以配置重试次数或重试超时时间，以及可以配置等待时间间隔
        Retryer<Boolean> retryer = RetryerBuilder.<Boolean>newBuilder()
                .retryIfExceptionOfType(RemoteAccessException.class)//设置异常重试源
                .retryIfResult(res -> res == false)  //设置根据结果重试
                .withWaitStrategy(WaitStrategies.fixedWait(3, TimeUnit.SECONDS)) //设置等待间隔时间
                .withStopStrategy(StopStrategies.stopAfterAttempt(3)) //设置最大重试次数
                .build();

        try {
            retryer.call(() -> RetryDemoTask.retryTask("abc"));
        } catch (Exception e) {
            e.printStackTrace();
        }
    }

}
```

### 4、运行测试

```
17:24:19.994 [main] INFO com.xxl.retry.guava_retry.RetryDemoTask - 收到请求参数:abc
17:24:20.006 [main] INFO com.xxl.retry.guava_retry.RetryDemoTask - 随机生成的数:6
17:24:20.006 [main] INFO com.xxl.retry.guava_retry.RetryDemoTask - 为2,返回false.
17:24:23.010 [main] INFO com.xxl.retry.guava_retry.RetryDemoTask - 收到请求参数:abc
17:24:23.010 [main] INFO com.xxl.retry.guava_retry.RetryDemoTask - 随机生成的数:10
17:24:23.011 [main] INFO com.xxl.retry.guava_retry.RetryDemoTask - 大于2,抛出自定义异常.
17:24:26.016 [main] INFO com.xxl.retry.guava_retry.RetryDemoTask - 收到请求参数:abc
17:24:26.016 [main] INFO com.xxl.retry.guava_retry.RetryDemoTask - 随机生成的数:1
17:24:26.016 [main] INFO com.xxl.retry.guava_retry.RetryDemoTask - 为0,抛出参数异常.
java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: 参数异常
	at com.github.rholder.retry.Retryer$ExceptionAttempt.<init>(Retryer.java:254)
	at com.github.rholder.retry.Retryer.call(Retryer.java:163)
	at com.xxl.retry.guava_retry.GuavaRetryTest.fun01(GuavaRetryTest.java:31)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	...
Caused by: java.lang.IllegalArgumentException: 参数异常
	at com.xxl.retry.guava_retry.RetryDemoTask.retryTask(RetryDemoTask.java:28)
	at com.xxl.retry.guava_retry.GuavaRetryTest.lambda$fun01$1(GuavaRetryTest.java:31)
```

### 参考资料

guava-retrying参数详解：https://cloud.tencent.com/developer/article/1752086

https://www.cnblogs.com/leeego-123/p/12741108.html

https://blog.csdn.net/ch98000/article/details/126554577

---

---
url: /Java/Java开发技巧/03.高效编程/4_Guava工具包常用API.md
---

# Guava工具包常用API

Guava 是一套来自 Google 的核心 Java 库，其中包括新的集合类型（如 multimap 和 multiset）、不可变的集合、图库，以及并发、I/O、散列、缓存、基元、字符串等实用工具！它被广泛用于 Google 内部的大多数 Java 项目，也被许多其他公司广泛使用。它被广泛用于 Google 内部的大多数 Java 项目，也被许多其他公司广泛使用。 最为重要的是它能帮助我们拯救垃圾代码，guava内部使用的API都是经过处理优化的，背靠Google公司，因此我们可以放心大胆的使用它。

## 引用依赖

```xml
<dependency>
    <groupId>com.google.guava</groupId>
    <artifactId>guava</artifactId>
    <version>30.1.1-jre</version>
</dependency>
```

## 不可变集合

创建对象的不可变拷贝是一项很好的防御性编程技巧。优点如下：

* 当对象被不可信的库调用时，不可变形式是安全的
* 不可变对象被多个线程调用时，不存在竞态条件问题
* 不可变集合不需要考虑变化，因此可以节省时间和空间。
* 不可变对象因为有固定不变，可以作为常量来安全使用。

三种创建方式

```java
public void immutable() {
    List<Integer> list = new ArrayList<>();
    list.add(1);
    list.add(2);
    list.add(3);

    // 方式一：通过已经存在的方式创建
    ImmutableSet<Integer> immutableSet1 = ImmutableSet.copyOf(list);

    // 方式二：通过初始值直接创建集合
    ImmutableSet<Integer> immutableSet2 = ImmutableSet.of(1,2,3);

    // 方式三：通过追加的形式闯将
    ImmutableSet<Object> immutableSet3 = ImmutableSet.builder().add(1).addAll(Sets.newHashSet(2, 3)).add(4).build();

    // 使用方式和正常集合一样，add和remove等改变数据的方式不可用。

}
```

## Multiset可重复无序集合

`Set`：不重复的无序集合

`List`：可重复的有序集合

`Multiset`：`Guava` 提供的可重复无序集合，可以将它看成，没有元素顺序限制的`ArrayList`

### 使用方式

> 为了方便理解，我们对 `API` 进行不同角度的拆解辅助理解

从 `ArrayList` 角度理解

| 方法       | 说明                                                   |
| :--------- | :----------------------------------------------------- |
| add(T)     | 添加单个给定元素                                       |
| iterator() | 返回一个迭代器，包含 `Multiset` 所有元素，包括重复元素 |
| size()     | 元素总个数，包括重复元素                               |

从 `Map` 角度理解

| 方法          | 说明                                                   |
| :------------ | :----------------------------------------------------- |
| count(Object) | 返回指定元素的个数，例如传入“A”，则返回集合中“A”的个数 |
| entrySet()    | 返回 `Set>`，和 `Map`的\`\`entrySet`类似                |
| elementSet()  | 返回所有不重复元素的`Set`，和 `Map`的`KeySet\` 类似  |

### 实战演练

实战:使用 `Multiset` 集合类，实现统计篇文章中文字出现次数功能。

java

```java
private static final String text =
    "玉楼春·己卯岁元日" +
    "毛滂 〔宋代〕" +
    "一年滴尽莲花漏。碧井酴酥沉冻酒。晓寒料峭尚欺人，春态苗条先到柳。" +
    "佳人重劝千长寿。柏叶椒花芬翠袖。醉乡深处少相知，只与东君偏故旧。";

@Test
public void test01() {
    Multiset<Object> multiset = HashMultiset.create();
    // 将string转换成char数组
    char[] chars = text.toCharArray();
    Chars.asList(chars).stream().forEach(multiset::add);

    // 总共多少字符
    System.out.println("size:" + multiset.size());
    // 查询“人”出现了几次
    System.out.println("count:" + multiset.count('人'));
}
```

## 集合工具类

> `setls` 工具类的常用方法：并集 / 交集 / 差集 / 分解集合中的所有子集 / 求两个集合的笛卡尔积 `Lists` 工具类的常用方式：反转 / 拆分

`Set` 集合相关方法

```java
private static final Set set1 = Sets.newHashSet(1,2,3);
private static final Set set2 = Sets.newHashSet(3,4,5);

@Test
public void text02(){
    // 并集 [1, 2, 3, 4, 5]
    Set<Integer> set = Sets.union(set1, set2);

    // 交集 [3]
    Set<Integer>  intersection = Sets.intersection(set1, set2);

    // 差集：元素属于A不属于B [1, 2]
    Set<Integer> difference = Sets.difference(set1, set2);

    // 将set拆成所有可能性的子集合
    Set<Set<Integer>> powerSet = Sets.powerSet(set1);
    // [[],[1],[2],[1,2],[3],[1,3],[2,3],[1,2,3]]
    System.out.println(JSON.toJSONString(powerSet));

    // 计算两个集合的笛卡尔积
    Set<List<Integer>> product = Sets.cartesianProduct(set1,set2);
    // [[1,3],[1,4],[1,5],[2,3],[2,4],[2,5],[3,3],[3,4],[3,5]]
    System.out.println(JSON.toJSONString(product));
}
```

`List` 集合

```java
@Test
public void text03() {
    ArrayList<Integer> list = Lists.newArrayList(1, 2, 3, 4, 5, 6, 7);

    // 拆分：每3个一组 [[1,2,3],[4,5,6],[7]]
    List<List<Integer>> partition = Lists.partition(list, 3);

    // 反转
    List<Integer> reverse = Lists.reverse(list);
}
```

---

---
url: /Java/并发编程/2_HashMap.md
---

# HashMap

HashMap 作为 Java 中最常用的键值对存储容器，在 JDK8 中迎来了重大优化，核心围绕存储结构、插入规则、哈希算法和扩容机制展开，相比 JDK7 大幅提升了高并发场景下的性能和稳定性。

## 一、核心差异总览

| 对比维度     | JDK7 实现                  | JDK8 实现                             |
| ------------ | -------------------------- | ------------------------------------- |
| 存储结构     | 数组 + 链表                | 数组 + 链表 + 红黑树                  |
| 链表插入方式 | 头插法（新节点放链表头部） | 尾插法（新节点放链表尾部）            |
| HASH 算法    | 复杂右移（4 次异或运算）   | 简化右移（仅 16 位高位异或）          |
| 扩容时机     | 插入节点前触发             | 插入节点后触发                        |
| 扩容转移方式 | 逐个元素重新计算下标转移   | 按哈希高位批量分组转移（低位 + 高位） |
| 潜在问题     | 多线程扩容可能导致死锁     | 解决死锁问题，性能更优                |

## 二、存储结构差异

### 1. JDK7：数组 + 链表

JDK7 的 HashMap 仅由「数组（哈希桶）+ 链表」组成：

* 数组：作为哈希表的主体，每个数组元素对应一个链表的头节点，用于快速定位哈希桶位置。
* 链表：用于解决哈希碰撞问题，当多个 key 计算出相同的数组下标时，会以链表形式存储在该哈希桶下。
* 缺陷：当链表过长时，查询效率会退化至 O (n)，大量哈希碰撞场景下性能较差。

### 2. JDK8：数组 + 链表 + 红黑树

JDK8 引入红黑树优化链表过长的问题，形成「数组 + 链表 + 红黑树」的混合结构：

* 红黑树特性（平衡二叉树的一种）：
  1. 调整规则比 AVL 树（完全平衡二叉树）宽松，插入 / 删除效率更高。
  2. 插入效率比链表低，但查询效率比链表高。查询效率为 O (logn)，介于链表（O (n)）和 AVL 树（O (logn)，调整成本更高）之间，是性能与效率的折中。
  3. 仅在链表长度达到阈值时触发转换，不影响少量数据的存储性能。
* 核心阈值（JDK8 内置常量）：
  1. `TREEIFY_THRESHOLD = 8`：当链表元素个数超过 8 时，链表自动转为红黑树。
  2. `UNTREEIFY_THRESHOLD = 6`：当红黑树节点个数小于 6 时，红黑树自动转回链表（避免少量节点时红黑树的调整成本高于查询收益）。

## 三、核心操作实现差异

在HashMap中，我们最常用的操作大概就是`put`与`get`了，来看看核心代码：

### 1. Put 操作（插入键值对）

#### JDK7 Put 核心逻辑

JDK7 的 Put 操作流程简洁，核心分为 3 步，采用头插法插入节点：

```java
// 核心步骤伪代码
int hash = hash(key); // 1. 计算 key 的 hash 值
int i = indexFor(hash, table.length); // 2. 通过 indexFor 方法计算数组下标
table[i] = newNode; // 3. 头插法：将 newNode 加入链表，新节点直接覆盖数组下标，原链表挂在新节点后
```

* 头插法原因：无需遍历链表，插入效率高。
* 缺陷：多线程扩容时，头插法会导致链表反转，可能引发死锁。
* 特殊处理：key 为 null 的元素固定存在数组第 0 个位置，且仅允许一个 null key。

#### JDK8 Put 核心逻辑

jdk8中引入了红黑树，但并不是说链表并不存在，查阅源码，我们可以发现两个非常关键的值：

```java
static final int TREEIFT_THRESHOLD=8;
static final int UNTREEIFT_THRESHOLD=6;
```

当链表的元素超过8时，会自动转成红黑树；当红黑树的节点数小于6时，变回链表。

```java
final V putVal(int hash, K key, V value, boolean onlyIfAbsent,
               boolean evict) {
    Node<K,V>[] tab; Node<K,V> p; int n, i;
    if ((tab = table) == null || (n = tab.length) == 0)
        n = (tab = resize()).length;
    //当前插入的数组位置为空，可以直接插入
    if ((p = tab[i = (n - 1) & hash]) == null)
        tab[i] = newNode(hash, key, value, null);
    else {
        Node<K,V> e; K k;
        //key相等情况，e在最后处理
        if (p.hash == hash &&
            ((k = p.key) == key || (key != null && key.equals(k))))
            e = p;
        //判断是红黑树的树节点
        else if (p instanceof TreeNode)
            e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);
        //是链表结构 
       else {
            for (int binCount = 0; ; ++binCount) { //binCount是遍历链表过程中计数
                //遍历链表，循环到尾结点，把新元素加在尾部，break
                if ((e = p.next) == null) {
                    p.next = newNode(hash, key, value, null);
                    //判断是否大于变成树的阈值-1,7会变成8，变成红黑树
                    if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st
                        treeifyBin(tab, hash);
                    break;
                }
                //找到相等元素也break
                if (e.hash == hash &&
                    ((k = e.key) == key || (key != null && key.equals(k))))
                    break;
                p = e;
            }
        }
        //重复key
        if (e != null) { // existing mapping for key
            V oldValue = e.value;
            if (!onlyIfAbsent || oldValue == null)
                e.value = value;
            afterNodeAccess(e);
            return oldValue;
        }
    }
    ++modCount;
    //扩容
    if (++size > threshold)
        resize();
    afterNodeInsertion(evict);
    return null;
}
```

看完这段代码，我们就明白了为什么jdk8中使用的是尾插法。因为在判断是使用链表或红黑树的过程中，要判断是否超过8个元素，至少需要遍历一遍，所以使用尾插法，新元素可以直接插入在尾结点。

JDK8 的 Put 操作（`putVal` 方法）更严谨，采用尾插法插入节点，并支持红黑树插入：

1. 校验数组是否为空，为空则先扩容（`resize()`）。
2. 根据 hash 值计算数组下标，若该位置为空，直接插入新节点。
3. 若该位置已有节点，分 3 种情况处理：
   * 节点 key 与插入 key 相同：记录该节点，后续更新 value。
   * 节点是红黑树节点（`TreeNode`）：调用红黑树插入方法（`putTreeVal`）。
   * 节点是链表节点：遍历链表至尾部，用尾插法插入新节点；插入后判断链表长度是否超过 7（`TREEIFY_THRESHOLD - 1`），若是则转为红黑树（`treeifyBin`）。
4. 若存在重复 key，更新对应节点的 value 并返回旧值。
5. 插入成功后，判断元素数量是否超过扩容阈值，若是则触发扩容。

* 尾插法原因：遍历链表计数（判断是否转红黑树）时，已遍历至尾部，尾插法无需额外开销，同时避免了多线程扩容的死锁问题。

### 2. Get 操作（查询键值对）

#### JDK7 Get 核心逻辑

`get`操作就比较简单了，先找到数组的下标，再比较key是否和给定的key相同，不同则顺着链表找下一个，直到找到或为空。具体通过`getEntry()`方法，遍历比较hash值是否相等，比较key是否相等。

```java
//key不为null,获取value
final Entry<K,V> getEntry(Object key) {
        if (size == 0) {//判断链表中是否有值
         //链表中没值,也就是没有value
            return null;
        }
       //链表中有值,获取key的hash值
        int hash = (key == null) ? 0 : hash(key);
        // 在“该hash值对应的链表”上查找“键值等于key”的元素
        for (Entry<K,V> e = table[indexFor(hash, table.length)];
             e != null;
             e = e.next) {
            Object k;
            //判断key是否相同
            if (e.hash == hash &&
                ((k = e.key) == key || (key != null && key.equals(k))))
                return e;//key相等,返回相应的value
             }
        return null;//链表中没有相应的key
}
```

JDK7 的 Get 操作（`getEntry` 方法）流程简单，仅支持链表查询：

1. 校验数组是否为空，为空则直接返回 null。
2. 计算 key 的 hash 值，通过 `indexFor` 方法获取数组下标。
3. 遍历该下标对应的链表，依次比较节点 hash 值和 key（== 或 `equals`），匹配成功则返回节点，否则返回 null。

* 查询效率：链表长度越短，效率越高，最坏情况 O (n)。

#### JDK8 Get 核心逻辑

`get`方法大体思路不变，计算下标，然后遍历，只不过是比jdk7中多加上一个判断是否用链表存储还是红黑树存储的步骤：

1. 计算 key 的 hash 值和数组下标，定位到对应哈希桶。
2. 若该位置节点直接匹配 key，返回节点。
3. 若不匹配，判断节点类型：
   * 链表节点：遍历链表查询，O (n)。
   * 红黑树节点：红黑树查找，O (logn)。
4. 未匹配到则返回 null。

* 优化点：红黑树大幅提升长链表场景下的查询效率。

### 3. Hash 算法差异

#### JDK7 Hash 算法（复杂）

JDK7 的 Hash 算法包含多次右移和异或运算，目的是让高位参与哈希计算，减少碰撞：

```java
final int hash(Object k){
    int h = hashSeed;
    // 对 String 类型 key 做特殊哈希处理
    if(0 != h && k instanceof String) {
        return sun.misc.Hashing.stringHash32((String)k);
    }
    h ^= k.hashCode();
    h ^= (h >>> 20) ^ (h >>> 12);
    return h ^ (h >>> 7) ^ (h >>> 4);
}
```

* 关键：通过 4 次右移（20、12、7、4）和异或运算，将高位特征融入低位，减少哈希碰撞。

在这当中，暂且不看hash种子，可以看到计算中存在大量的右移操作，那么为什么要进行右移呢，这是考虑到了碰撞性问题。之前提到使用`indexFor`来计算数组下标：

```java
static int indexFor(int h , int length){    
    return h & (length-1);
}
```

这里提一点，HashMap的长度一定是一个二的次方数，这点是在它的初始化和扩容中被限定的。这里在计算下表时，一个二的次方数减去1，能够保证它的二进制数的后几位数字全部是1，便于计算下标。

举个例子，HashMap长度为16，这样计算出的hash值与0000 1111做与运算，只需要取后四位，就实现了数组下标的计算。而与操作的计算速度比取余操作是要快上一些的。

回到上面，继续讲为什么要进行大量的右移操作，还是以长度为16来看，如果几个key计算出的hash值为：

```bash
1010 0110
0010 0110
0000 0110
```

我们发现，只要后四位一样，hash值都一样，碰撞性很高，所以这时要引入右移操作，让高位也能参与到与运算。让链表分散，减少链表长度。

#### JDK8 Hash 算法（简化）

JDK8 精简了 Hash 算法，仅保留 16 位高位右移异或，配合红黑树弥补性能损耗：

```java
static final int hash(Object key) {
    int h;
    // key 为 null 时 hash 值为 0，否则将 hashCode 与高位 16 位异或
    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}
```

* 简化原因：数组下标计算仅使用 hash 值的低位，将高位 16 位与低位异或，既保留高位特征，又简化计算。
* 配合优化：红黑树提升了哈希碰撞场景下的查询效率，无需依赖复杂哈希算法减少碰撞。

### 4. 数组下标计算

JDK7 和 JDK8 均使用 `h & (length - 1)` 计算数组下标（替代取余运算，效率更高），前提是 HashMap 数组长度为 2 的幂次方（初始化和扩容时强制保证）：

* 原理：2 的幂次方减 1 的二进制形式为「末尾全 1」（如 16-1=15，二进制 1111），与 hash 值做与运算时，仅保留 hash 值的低位，快速定位数组下标。
* 示例：hash 值为 1010 0110，数组长度 16（length-1=15，二进制 0000 1111），与运算结果为 0000 0110，对应数组下标 6。

## 四、扩容机制差异

扩容是 HashMap 的核心耗时操作，目的是增大数组容量，减少链表长度，提升查询效率。默认扩容因子（负载因子）为 0.75，即当元素数量达到数组容量的 75% 时，触发扩容，容量翻倍。

扩容是对数组进行扩容，而不是链表或红黑树。在初始化时数组的默认长度是16，前面提到当存储的元素很多时会发生hash碰撞。我们扩容的目的是将长链表的长度减短，提高查询效率。

由于扩容的源码比较长，就不贴在这里，只列出核心思想。

### 1. JDK7 扩容机制

只有当数量大于阈值，且当前插入位置不为空时才会进行扩容，并且容量为原先2倍。

在将老的table转移到新的table时，需要重新计算数组的下标。

扩容后，重新计算下标。以从16位扩容到32位为例：

```bash
h      1010 0110
31     0001 1111
结果    0000 0110    （与之前相同）
```

```bash
h     1011 0110
31    0001 1111
结果  0001 0110    （与之前不同，相当于比之前加了16）
```

扩容后，数组下标可能改变，也可能不变。这时要看扩容的那一位的哈希值是1还是0，如果是1则不同，0则相同。但是在这个过程中，有可能造成死锁问题。

* 扩容时机：元素插入前，当元素数量 > 扩容阈值，且当前插入位置不为空时触发。
* 转移方式：逐个遍历旧数组元素，重新计算 hash 值和数组下标，将元素转移到新数组中（头插法）。
* 潜在问题：多线程下，头插法会导致链表反转，引发死锁；逐个转移效率较低。
* 下标变化：扩容后元素下标可能不变（hash 高位为 0）或变为「原下标 + 旧数组容量」（hash 高位为 1）。

### 2. JDK8 扩容机制

JDK8扩容中，为了避免之前提到的死锁问题，改进了扩容方法。通过判断这1位是0还是1，是0则不变。如果是1 ，加上原先数组大小。

```java
newTab[j + oldCap] = hiHead;
//oldCap是原先的数组长度
```

* 扩容时机：元素插入后，当元素数量 > 扩容阈值时触发（无需判断插入位置是否为空）。
* 转移方式：批量分组转移，无需重新计算 hash 值，仅通过 hash 高位判断元素归属：
  1. 遍历旧数组链表 / 红黑树，按 hash 高位是否为 1，将元素分为「低位组」（下标不变）和「高位组」（下标 = 原下标 + 旧数组容量）。
  2. 分别将低位组和高位组批量转移到新数组的对应位置。
* 优化点：
  1. 批量转移效率高于逐个转移，无需重新计算 hash 值。
  2. 尾插法避免了多线程扩容的死锁问题。
  3. 红黑树转移时，直接按高位分组，无需重新构建红黑树（必要时拆分红黑树）。

### 总结：

* 扩容这一操作非常耗时，默认达到75%按照2倍进行扩容，这个75%也就是factor扩容因子。
* JDK7中扩容是在节点还没有加到HashMap前发生的；JDK8中扩容是在节点加到HashMap后发生的。
* JDK7扩容是一个一个元素计算然后转移，JDK8是先遍历，判断哪些是放到新数组的低位，哪些是高位，然后将low的元素和high的元素分别组合起来，一次性转移到新的数组中。

## 五、核心总结

1. 存储结构：JDK8 引入红黑树，解决 JDK7 链表过长导致的查询性能退化问题，查询效率从 O (n) 优化至 O (logn)。
2. 插入规则：JDK8 采用尾插法，解决 JDK7 头插法在多线程扩容时的死锁问题，同时配合链表计数逻辑，无额外性能损耗。
3. 哈希算法：JDK8 简化哈希计算，通过红黑树弥补性能损耗，兼顾效率和简洁性。
4. 扩容机制：JDK8 批量分组转移，无需重新计算 hash 值，效率更高，且解决了死锁问题。
5. 性能优化：JDK8 HashMap 在高哈希碰撞、多线程（非并发安全，并发推荐 ConcurrentHashMap）场景下，性能和稳定性均优于 JDK7。
6. 注意事项：扩容操作耗时较高，初始化 HashMap 时，建议根据预估元素数量指定初始容量，减少扩容次数。

## 参考资料

https://trunks2008.github.io/concurrent/HashMap.html

https://www.cnblogs.com/javawxid/p/15644349.html

---

---
url: /daily/博客文档/其他博客文档/Hexo博客搭建.md
---

# Hexo博客搭建

官网（本文档来自官网整理） https://hexo.io/zh-cn/docs/

npm安装教程：https://www.cnblogs.com/goldlong/p/8027997.html

https://www.hojun.cn/2018/06/08/%E6%89%8B%E6%8A%8A%E6%89%8B%E6%95%99%E4%BD%A0%E6%90%AD%E5%BB%BAHEXO%E5%8D%9A%E5%AE%A2-%E4%B8%89/

next 主题：http://theme-next.iissnan.com/getting-started.html

码云+Hexo搭建个人博客+评论功能接入(本文参考)：http://zwd596257180.gitee.io/blog/2019/04/15/hexo\_manong\_bog/

根据自己安装环境有所修改

基于Gitee+Hexo搭建个人博客：https://segmentfault.com/a/1190000018662692

https://xu\_xiaolong.gitee.io

# 1、准备环境

Gitee账号（或Github）

Git：https://git-scm.com/download/

NodeJS：http://nodejs.cn/

SublimeText（推荐使用）：http://www.sublimetext.cn/

# 2、开始搭建

## 安装Hexo

```yml
npm install hexo-cli -g  //安装 或(npm install -g hexo )
# -g 指定全局安装，可以使用hexo命令
```

新建个文件夹，例入 Hexo\_Blog ,在该目录下运行cmd，开始执行命令安装

或直接使用命令创建安装

```
hexo init Hexo_Blog      //博客名
cd Hexo_Blog             //进入目录
npm install              //安装依赖
hexo server -p 5555      //本地运行测试（localhost:5555）或使用(hexo s -p 5555)命令或（hexo s）默认4000端口
```

## 部署发布博客到Gitee上

1、创建Gitee账号 https://gitee.com/

2、创建仓库

3、在Hexo\_Blog—themes—\_config.yml下配置

```yml
# Deployment
## Docs: https://hexo.io/docs/deployment.html
deploy:
  type: git
  #repo: https://github.com/Daneliya/Daneliya.github.io.git
  repo: https://gitee.com/xu_xiaolong/Hexo_Blog.git
  branch: master
```

注意：冒号后面一定要有空格，否则不能正确识别。

4、安装hexo插件

```
npm install hexo-deployer-git --save
```

5、部署

```
hexo g  //编译项目
hexo d  //发布项目
```

6、Gitee Pages设置

在项目的服务中选择Pages选项，选择 master 分支，点击 部署/更新（第一次是 启动）。

# 3、主题功能美化改造

其他主题：https://hexo.io/themes/

| 主题名称   | 预览                                                         | 代码                                                         |
| ---------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| skapp      | [预览](http://blog.minfive.com/archives/)                    | [代码](https://github.com/Mrminfive/hexo-theme-skapp)        |
| Amazing    | [预览](https://removeif.github.io/removeif-demo/)            | [代码](https://github.com/removeif/hexo-theme-amazing)       |
| volantis   | [文档及预览](https://volantis.js.org/v2/getting-started/)https://www.heson10.com/posts/53612.htmlhttps://hasaik.com/ | [代码](https://github.com/xaoxuu/hexo-theme-volantis)      [问题1](https://blog.csdn.net/Mint6/article/details/79830063)   [问题2](https://segmentfault.com/q/1010000003734223) |
| gal        | [预览](https://myau.moe/)                                    | [代码](https://github.com/ZEROKISEKI/hexo-theme-gal)         |
| Asnippet   | [预览](https://www.91h5.cc/)                                 | [代码](https://github.com/shenliyang/hexo-theme-snippet)     |
| Tree       | [预览](https://wujun234.github.io/)                          | [代码](https://github.com/wujun234/hexo-theme-tree)          |
| Yilia-plus | https://github.com/JoeyBling/hexo-theme-yilia-plus           | https://joeybling.github.io/                                 |
| Mustom     | https://ma-jinyao.cn/                                        | https://github.com/jinyaoMa/hexo-theme-mustom                |

代理：https://www.91h5.cc/archives/61443.html

## 3.1、Next主题设置

在项目根目录下的 **themes** 文件中，打开 **Git Bash** ，用命令行克隆下新的主题。

这里用的 Next 主题。

```
git clone https://github.com/theme-next/next.git
```

启用（\_config.yml大约77行）

```
# Extensions
## Plugins: https://hexo.io/plugins/
## Themes: https://hexo.io/themes/
theme: next
```

## 3.2、发布文章

\*\*方法一：\*\*在项目根目录下，打开 **Git Bash** ，执行新建命令，然后 hexo 会自动在指定目录下生成对应文件，如下图所示。然后找到新建好的文件，打开即可进行编辑。

```
hexo new "此处输入文章名字"
```

\*\*方法二：\*\*可以直接把已经准备的 md 格式的文章复制到 项目名称 /source/\_posts 目录下，然后打开文件，在文件头加入 front-matter 部分，**title** 表示文章标题，**date** 表示发布时间。

front-matte 书写的时候要注意，冒号后面要跟一个空格号。

## 3.3、主题风格设置

打开主题文件夹下的 \_config.yml 配置文件（注意：这里要区别，不是項目根目录，主题文件夹的路径为：新建空白文件夹名称/themes/主题文件夹名称）。通过查找功能找到 **Schemes** 模块，修改为 **Pisces** 风格。如果喜欢其他风格可以自己修改。

```
# --------------------------------------------------------
# Scheme Settings
# --------------------------------------------------------

# Schemes
#scheme: Muse
#scheme: Mist
scheme: Pisces
#scheme: Gemini
```

刷新页面可以看到新风格的界面

## 3.4、博客左侧栏语言设置

在上面的网站界面，可以发现网站的文字是英文，只要修改一下语言模式即可。打开根目录文件夹下的 \_config.yml 配置文件。找到 **language**，设置为 **zh-CN**。标题等其他参数的设置如下。可以对照效果图的具体位置，根据自己的实际需求进行修改。（注意：修改了项目根目录下的 \_config.yml配置文件，需要重启部署项目后才能生效）

## 3.5、分类设置

### 3.5.1、添加分类

* 在项目根目录下，执行下面的命令行，新建分类页面，然后会在项目根目录下的 **source** 文件夹中新建一个 **categories** 文件夹。

  ```
  hexo new page categories
  ```

* 打开 **categories** 文件夹中的 **index.md** 文件，添加 **type** 字段，设置为 **“categories”**。

* 接着到主题文件夹下的 \_config.yml 配置文件下，找到 **menu** 模块，把 **categories** 的注释给去掉。

* 刷新页面（如果刷新没效果，可以重启服务），可以在页面左侧栏上看到多了一个“分类”列表。

### 3.5.2、将文章添加到对应分类

* 文章发布前，在 front-matter 部分，多写一个 **categories** 字段，然后参数写上类别的名称，保存后重启服务，在网页上点击“分类”，可以看到分类下已经生成了刚刚设置的类别，并把刚刚发布的文章归类在此类别下。

## 3.6、标签设置

* 方法跟分类设置一样，所以不再赘述介绍。

* 但是需要补充一点， front-matter 中字段有多个参数的时候，可以使用如下图的写法。

  ```
  tags:
  	-Java
  	-Jsp
  ```

## 3.7、Hexo 博客添加站内搜索

* NexT主题支持集成 **Swiftype**、 微搜索、**Local Search** 和 **Algolia**。下面介绍 **Local Search** 的安装吧。注意：安装的时候要是项目根目录下安装。
* 安装 hexo-generator-search

```
npm install hexo-generator-search --save
```

* 安装 hexo-generator-searchdb

```
npm install hexo-generator-searchdb --save
```

* 在项目根目录下的 \_config.yml 配置文件的文末添加下面这段代码。

```
search:
  path: search.xml
  field: post
  format: html
  limit: 10000
```

* 编辑主题文件夹的 \_config.yml 配置文件，设置 Local searchenable 为 **ture**。

## 3.8、博客头像设置

themes 下的  next 中的 \_config.yml 中，大约154行

```yml
# Sidebar Avatar
# in theme directory(source/images): /images/avatar.gif
# in site  directory(source/uploads): /uploads/avatar.gif
avatar: /images/avatar.gif
```

## 3.9、右上角 fork me 设置

* 在 [GitHub Corners](http://tholman.com/github-corners/) 上选择你喜欢的挂饰，复制方框内的代码。
* 打开主题文件夹下的 **layout** 文件夹，用记事本的方式打&#x5F00;**\_layout.swig**，把刚刚复制的代码放到下面，并把 **href** 的参数，修改为自己的 github 链接（放自己要跳转的网址即可）。
* 重启服务器，查看效果图。

## 3.10、网页背景设置

### 3.10.1、动态背景设置

## 3.11、点击出现桃心效果设置

## 3.18、设置网站图片 Favicon

## 3.19、网页顶部进度加载条设置

* 在主题文件夹的 \_config.yml 配置文件中，搜索到 **pace** 后，把其值改为 **true** 即可，然后选择一款你喜欢的样式。

## 动态背景

themes 下的  next 中的 \_config.yml 中，大约695行

```yml
pace_theme: pace-theme-minimal

# Canvas-nest
canvas_nest: true

# three_waves
three_waves: false

# canvas_lines
canvas_lines: false

# canvas_sphere
canvas_sphere: false
```

## 文章新建

```
hexo n 文件名（英文）    //新建文章
```

```
---
title: Java_Design_Pattern   //文章名
date: 2019-07-29 20:20:08
tags:              //标签
    -Java
categories: Java   //分类
---
# 六大设计原则     //标题
## **1.单一职责原则**
```

```
hexo s   //启动本地
进入localhost:4000
hexo g
hexo d
联网可看
```

##

## 开通微信打赏功能

themes 下的  next 中的 \_config.yml 中，大约238行

```yml
# Wechat Subscriber
wechat_subscriber:
  enabled: true
  qcode: /path/to/your/wechatqcode ex. /uploads/wechat-qcode.jpg
  description: ex. subscribe to my blog by scanning my public wechat account

```

## 3.27、网站底部加上访问量

* 在项目根目录下安装 hexo 插件。

```
npm install hexo-wordcount --save
```

* 打开主题文件夹下的 layout/\_partials/footer.swig 文件，在文末添加上下面这段代码。

```
<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共{{ totalcount(site) }}字</span>
</div>
```

* 刷新页面查看效果图

## 3.28、外链网易云音乐设置

## 3.31、文章显示阅读数量设置

在主题文件下的 \_config.yml 配置文件中，定位到 **busuanzi\_count**，把 **enable** 的值修改为 **true**。

```yml
# Show PV/UV of the website/page with busuanzi.
# Get more information on http://ibruce.info/2015/04/04/busuanzi/
busuanzi_count:
  # count values only if the other configs are false
  enable: true
  # custom uv span for the whole site
  site_uv: true
  site_uv_header: <i class="fa fa-user"></i>
  site_uv_footer:
  # custom pv span for the whole site
  site_pv: true
  site_pv_header: <i class="fa fa-eye"></i>
  site_pv_footer:
  # custom pv span for one page only
  page_pv: true
  page_pv_header: <i class="fa fa-file-o"></i>
  page_pv_footer:
```

qcode 使用云图床的地址

description 简介

# 遇到问题

## 问题1：

Hexo本地使用时出现 FATAL can not read a block mapping entry（无法读取块映射条目）

```
D:\Program Files (x86)\Hexo_Blog>hexo s -p 5555
FATAL can not read a block mapping entry; a multiline key may not be an implicit key at line 9, column 9:
    keywords:
            ^
YAMLException: can not read a block mapping entry; a multiline key may not be an implicit key at line 9, column 9:
    keywords:
            ^
    at generateError (D:\Program Files (x86)\Hexo_Blog\node_modules\js-yaml\lib\js-yaml\loader.js:167:10)
    at throwError (D:\Program Files (x86)\Hexo_Blog\node_modules\js-yaml\lib\js-yaml\loader.js:173:9)
    at readBlockMapping (D:\Program Files (x86)\Hexo_Blog\node_modules\js-yaml\lib\js-yaml\loader.js:1073:9)
    at composeNode (D:\Program Files (x86)\Hexo_Blog\node_modules\js-yaml\lib\js-yaml\loader.js:1359:12)
    at readDocument (D:\Program Files (x86)\Hexo_Blog\node_modules\js-yaml\lib\js-yaml\loader.js:1519:3)
    at loadDocuments (D:\Program Files (x86)\Hexo_Blog\node_modules\js-yaml\lib\js-yaml\loader.js:1575:5)
    at Object.load (D:\Program Files (x86)\Hexo_Blog\node_modules\js-yaml\lib\js-yaml\loader.js:1596:19)
    at Hexo.yamlHelper (D:\Program Files (x86)\Hexo_Blog\node_modules\hexo\lib\plugins\renderer\yaml.js:7:15)
    at Hexo.tryCatcher (D:\Program Files (x86)\Hexo_Blog\node_modules\bluebird\js\release\util.js:16:23)
    at Hexo.<anonymous> (D:\Program Files (x86)\Hexo_Blog\node_modules\bluebird\js\release\method.js:15:34)
    at Promise.then.text (D:\Program Files (x86)\Hexo_Blog\node_modules\hexo\lib\hexo\render.js:60:20)
    at tryCatcher (D:\Program Files (x86)\Hexo_Blog\node_modules\bluebird\js\release\util.js:16:23)
    at Promise._settlePromiseFromHandler (D:\Program Files (x86)\Hexo_Blog\node_modules\bluebird\js\release\promise.js:517:31)
    at Promise._settlePromise (D:\Program Files (x86)\Hexo_Blog\node_modules\bluebird\js\release\promise.js:574:18)
    at Promise._settlePromise0 (D:\Program Files (x86)\Hexo_Blog\node_modules\bluebird\js\release\promise.js:619:10)
    at Promise._settlePromises (D:\Program Files (x86)\Hexo_Blog\node_modules\bluebird\js\release\promise.js:699:18)
    at _drainQueueStep (D:\Program Files (x86)\Hexo_Blog\node_modules\bluebird\js\release\async.js:138:12)
    at _drainQueue (D:\Program Files (x86)\Hexo_Blog\node_modules\bluebird\js\release\async.js:131:9)
    at Async._drainQueues (D:\Program Files (x86)\Hexo_Blog\node_modules\bluebird\js\release\async.js:147:5)
    at Immediate.Async.drainQueues [as _onImmediate] (D:\Program Files (x86)\Hexo_Blog\node_modules\bluebird\js\release\async.js:17:14)
    at runCallback (timers.js:705:18)
    at tryOnImmediate (timers.js:676:5)
```

可能原因：

1、安装server模块 hexo g后先安装 在你的blog根目录下npm install 然后你就发现hexo s可以正常使用了

2、配置中要有空格（参照：https://www.jianshu.com/p/dec7cfe1fe30）

我的原因：

\_config.yml 中有个配置后没加空格

## 问题2：

项目名是用户名

https://blog.csdn.net/qq32933432/article/details/87955133

## 问题3：

next 主题配置更改后不会立即生效，有个缓冲时间。

https://github.com/hexojs/hexo/issues/67

问题4：npm问题

npm install

```shell
npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@2.1.3 (node_modules\fsevents):
npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@2.1.3: wanted {"os":"darwin","arch":"any"} (current: {"os":"win32","arch":"x64"})

audited 254 packages in 2.178s

5 packages are looking for funding
  run `npm fund` for details

found 1 low severity vulnerability
  run `npm audit fix` to fix them, or `npm audit` for details
```

npm audit fix

```powershell
npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@2.1.3 (node_modules\fsevents):
npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@2.1.3: wanted {"os":"darwin","arch":"any"} (current: {"os":"win32","arch":"x64"})

up to date in 0.91s

5 packages are looking for funding
  run `npm fund` for details

fixed 0 of 1 vulnerability in 254 scanned packages
  1 vulnerability required manual review and could not be updated
```

npm audit --json

```powershell
"overview": "Affected versions of `minimist` are vulnerable to prototype pollution. Arguments are not properly sanitized, allowing an attacker to modify the prototype of `Object`, causing the addition or modification of an existing property that will exist on all objects.  \nParsing the argument `--__proto__.y=Polluted` adds a `y` property with value `Polluted` to all objects. The argument `--__proto__=Polluted` raises and uncaught error and crashes the application.  \nThis is exploitable if attackers have control over the arguments
being passed to `minimist`.\n",
      "recommendation": "Upgrade to versions 0.2.1, 1.2.3 or later.",
      "references": "- [GitHub commit 1](https://github.com/substack/minimist/commit/4cf1354839cb972e38496d35e12f806eea92c11f#diff-a1e0ee62c91705696ddb71aa30ad4f95)\n- [GitHub commit 2](https://github.com/substack/minimist/commit/63e7ed05aa4b1889ec2f3b196426db4500cbda94)",
```

npm audit

npm show minimist version

https://blog.csdn.net/weixin\_34067980/article/details/94043735

https://github.com/substack/minimist/commit/4cf1354839cb972e38496d35e12f806eea92c11f#diff-a1e0ee62c91705696ddb71aa30ad4f95)

---

---
url: /常用框架/SpringAIAlibaba/Agent Framework/2_Hooks和Interceptors.md
---

# Hooks 和 Interceptors

> 让开发者在每个步骤控制和自定义 Agent 执行

Hooks 和 Interceptors 提供了一种更精细控制 Agent 内部行为的方式。

核心 Agent 循环涉及调用模型、让其选择要执行的工具，直到不需要调用工具时完成。

![reactagent](/assets/reactagent.j2irOl0U.png)

Hooks 和 Interceptors 在这些步骤的前后暴露了钩子点，允许你：

![reactagent](/assets/reactagent-hook.BWVzaL1D.png)

* **监控**: 通过日志、分析和调试跟踪 Agent 行为
* **修改**: 转换提示、工具选择和输出格式
* **控制**: 添加重试、回退和提前终止逻辑
* **强制执行**: 应用速率限制、护栏和 PII 检测

通过将它们传递给 `ReactAgent.builder()` 来添加 Hooks 和 Interceptors：

```java
import com.alibaba.cloud.ai.graph.agent.ReactAgent;
import com.alibaba.cloud.ai.graph.agent.hook.*;
import com.alibaba.cloud.ai.graph.agent.interceptor.*;

ReactAgent agent = ReactAgent.builder()
    .name("my_agent")
    .model(chatModel)
    .tools(tools)
    .hooks(loggingHook, messageTrimmingHook)
    .interceptors(guardrailInterceptor, retryInterceptor)
    .build();
```

## 一、Hooks 和 Interceptors 能做什么？

* 监控。使用日志、分析和调试跟踪 Agent 行为。
* 修改。转换提示、工具选择和输出格式。
* 控制。添加重试、回退和提前终止逻辑。
* 强制执行。应用速率限制、护栏和 PII 检测。

## 二、内置实现

Spring AI Alibaba 为常见用例提供了预构建的 Hooks 和 Interceptors 实现：

### 2.1、消息压缩（Summarization）

当接近 token 限制时自动压缩对话历史。

**适用场景**：

* 超出上下文窗口的长期对话
* 具有大量历史记录的多轮对话
* 需要保留完整对话上下文的应用程序

```java
/**
 * SummarizationHook 消息压缩示例
 */
@SneakyThrows
public static void summarizationHook() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();
    // 模型配置
    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();
    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();
    // 创建消息压缩 Hook // [!code ++:6]
    SummarizationHook summarizationHook = SummarizationHook.builder()
        .model(chatModel)
        .maxTokensBeforeSummary(4000)
        .messagesToKeep(20)
        .build();
    // 创建 Agent
    ReactAgent agent = ReactAgent.builder()
        .name("my_agent")
        .model(chatModel)
        .hooks(summarizationHook) // [!code ++]
        .build();
}
```

**配置选项**：

* `model`: 用于生成摘要的 ChatModel
* `maxTokensBeforeSummary`: 触发摘要之前的最大 token 数
* `messagesToKeep`: 摘要后保留的最新消息数

### 2.2、Human-in-the-Loop（人机协同）

暂停 Agent 执行以获得人工批准、编辑或拒绝工具调用。

**适用场景**：

* 需要人工批准的高风险操作（数据库写入、金融交易）
* 人工监督是强制性的合规工作流程
* 长期对话，使用人工反馈引导 Agent

```java
/**
 * HumanInTheLoopHook 人机协同示例
 */
@SneakyThrows
public static void humanReviewHook() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();
    // 模型配置
    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();
    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();
    // 创建工具（示例） // [!code ++:8]
    ToolCallback sendEmailTool = createSendEmailTool();
    ToolCallback deleteDataTool = createDeleteDataTool();
    // 创建 Human-in-the-Loop Hook
    HumanInTheLoopHook humanReviewHook = HumanInTheLoopHook.builder()
        .approvalOn("sendEmailTool", ToolConfig.builder().description("Please confirm sending the email.").build())
        .approvalOn("deleteDataTool", ToolConfig.builder().description("Please confirm deleting the data.").build())
        .build();
    // 创建 Agent
    ReactAgent agent = ReactAgent.builder()
        .name("supervised_agent")
        .model(chatModel)
        .tools(sendEmailTool, deleteDataTool) // [!code ++:2]
        .hooks(humanReviewHook)
        .saver(new MemorySaver())
        .build();
}
```

**重要提示**：Human-in-the-loop Hook 需要 checkpointer 来维护跨中断的状态。示例中我们演示用了 `RedisSaver`。

### 2.3、模型调用限制（Model Call Limit）

限制模型调用次数以防止无限循环或过度成本。

**适用场景**：

* 防止失控的 Agent 进行太多 API 调用
* 在生产部署中强制执行成本控制
* 在特定调用预算内测试 Agent 行为

```java
/**
 * ModelCallLimitHook 模型调用限制示例
 */
public static void modelCallLimit() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();
    // 模型配置
    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();
    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();
    // 创建 Agent
    ReactAgent agent = ReactAgent.builder()
        .name("my_agent")
        .model(chatModel)
        .hooks(ModelCallLimitHook.builder().runLimit(5).build())  // 限制模型调用次数为5次 // [!code ++]
        .saver(new MemorySaver())
        .build();
}
```

### 2.4、PII 检测（Personally Identifiable Information）

检测和处理对话中的个人身份信息。

**适用场景**：

* 具有合规要求的医疗保健和金融应用
* 需要清理日志的客户服务 Agent
* 任何处理敏感用户数据的应用程序

```java
/**
 * PIIDetectionHook PII 检测示例
 */
@SneakyThrows
public static void piiDetectionHook() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();
    // 模型配置
    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();
    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();
    // 创建 PIIDetection Hook // [!code ++:6]
    PIIDetectionHook pii = PIIDetectionHook.builder()
        .piiType(PIIType.EMAIL)
        .strategy(RedactionStrategy.REDACT)
        .applyToInput(true)
        .build();
    // 创建 Agent
    ReactAgent agent = ReactAgent.builder()
        .name("secure_agent")
        .model(chatModel)
        .hooks(pii) // [!code ++]
        .build();
}
```

### 2.5、工具重试（Tool Retry）

自动重试失败的工具调用，具有可配置的指数退避。

**适用场景**：

* 处理外部 API 调用中的瞬态故障
* 提高依赖网络的工具的可靠性
* 构建优雅处理临时错误的弹性 Agent

```java
/**
 * ToolRetryInterceptor 工具重试示例
 */
@SneakyThrows
public static void toolRetryInterceptor() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();
    // 模型配置
    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();
    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();
    // 创建工具（示例）
    ToolCallback searchTool = createSearchTool();
    ToolCallback databaseTool = createDatabaseTool();
    // 创建 Agent
    ReactAgent agent = ReactAgent.builder()
        .name("resilient_agent")
        .model(chatModel)
        .tools(searchTool, databaseTool)
        .interceptors(ToolRetryInterceptor.builder() // [!code ++:4]
                      .maxRetries(2)
                      .onFailure(ToolRetryInterceptor.OnFailureBehavior.RETURN_MESSAGE)
                      .build())
        .build();
}
```

### 2.6、Planning（规划）

在执行工具之前强制执行一个规划步骤，以概述 Agent 将要采取的步骤。

**适用场景**：

* 需要执行复杂、多步骤任务的 Agent
* 通过在执行前显示 Agent 的计划来提高透明度
* 通过检查建议的计划来调试错误

```java
/**
 * TodoListInterceptor 规划示例
 */
@SneakyThrows
public static void todoListInterceptor() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();
    // 模型配置
    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();
    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();
    // 创建工具（示例）
    ToolCallback myTool = createSampleTool();
    // 创建 Agent
    ReactAgent agent = ReactAgent.builder()
        .name("planning_agent")
        .model(chatModel)
        .tools(myTool)
        .interceptors(TodoListInterceptor.builder().build()) // [!code ++]
        .build();
}
```

### 2.7、LLM Tool Selector（LLM 工具选择器）

使用一个 LLM 来决定在多个可用工具之间选择哪个工具。

**适用场景**：

* 当多个工具可以实现相似目标时
* 需要根据细微的上下文差异进行工具选择
* 动态选择最适合特定输入的工具

```java
/**
 * ToolSelectionInterceptor LLM 工具选择器示例
 */
@SneakyThrows
public static void toolSelectionInterceptor() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();
    // 模型配置
    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();
    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();
    // 创建工具（示例）
    ToolCallback tool1 = createSampleTool();
    ToolCallback tool2 = createSampleTool();
    // 创建 Agent
    ReactAgent agent = ReactAgent.builder()
        .name("smart_selector_agent")
        .model(chatModel)
        .tools(tool1, tool2)
        .interceptors(ToolSelectionInterceptor.builder().build()) // [!code ++]
        .build();
}
```

### 2.8、LLM Tool Emulator（LLM 工具模拟器）

在没有实际执行工具的情况下，使用 LLM 模拟工具的输出。

**适用场景**：

* 在演示或测试期间模拟 API
* 在开发过程中为工具提供占位符行为
* 在不产生实际成本或副作用的情况下测试 Agent 逻辑

```java
/**
 * ToolEmulatorInterceptor LLM 工具模拟器示例
 */
@SneakyThrows
public static void toolEmulatorInterceptor() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();
    // 模型配置
    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();
    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();
    // 创建工具（示例）
    ToolCallback simulatedTool = createSampleTool();
    // 创建 Agent
    ReactAgent agent = ReactAgent.builder()
        .name("emulator_agent")
        .model(chatModel)
        .tools(simulatedTool)
        .interceptors(ToolEmulatorInterceptor.builder().model(chatModel).build()) // [!code ++]
        .build();
}
```

### 2.9、Context Editing（上下文编辑）

在将上下文发送给 LLM 之前对其进行修改，以注入、删除或修改信息。

**适用场景**：

* 向 LLM 提供额外的上下文或指令
* 从对话历史中删除不相关或冗余的信息
* 动态修改上下文以引导 Agent 的行为

```java
/**
 * ContextEditingInterceptor 上下文编辑示例
 */
@SneakyThrows
public static void contextEditingInterceptor() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();
    // 模型配置
    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();
    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();
    // 创建 Agent
    ReactAgent agent = ReactAgent.builder()
        .name("context_aware_agent")
        .model(chatModel)
        .interceptors(ContextEditingInterceptor.builder().trigger(120000).clearAtLeast(60000).build()) // [!code ++]
        .build();
}
```

## 三、自定义 Hooks 和 Interceptors

通过实现在 Agent 执行流程中特定点运行的钩子来构建自定义功能。

可以通过以下方式创建自定义功能：

1. **ModelHook** - 在模型调用前后执行
2. **AgentHook** - 在 Agent 开始和结束时执行
3. **ModelInterceptor** - 拦截和修改模型请求/响应
4. **ToolInterceptor** - 拦截和修改工具调用

### 3.1、ModelHook

在模型调用前后执行自定义逻辑：

```java
package com.xxl.ai.framework.hook;

import com.alibaba.cloud.ai.graph.OverAllState;
import com.alibaba.cloud.ai.graph.RunnableConfig;
import com.alibaba.cloud.ai.graph.agent.hook.HookPosition;
import com.alibaba.cloud.ai.graph.agent.hook.HookPositions;
import com.alibaba.cloud.ai.graph.agent.hook.ModelHook;

import java.util.Map;
import java.util.concurrent.CompletableFuture;

/**
 * @Classname CustomModelHook
 * @Description 自定义 ModelHook
 * @Date 2025/12/14 21:28
 * @Created by xxl
 */
@HookPositions({HookPosition.BEFORE_MODEL, HookPosition.AFTER_MODEL})
public class CustomModelHook extends ModelHook {

    @Override
    public String getName() {
        return "custom_model_hook";
    }

    @Override
    public CompletableFuture<Map<String, Object>> beforeModel(OverAllState state, RunnableConfig config) {
        // 在模型调用前执行
        System.out.println("准备调用模型...");

        // 可以修改状态
        // 例如：添加额外的上下文
        return CompletableFuture.completedFuture(Map.of("extra_context", "某些额外信息"));
    }

    @Override
    public CompletableFuture<Map<String, Object>> afterModel(OverAllState state, RunnableConfig config) {
        // 在模型调用后执行
        System.out.println("模型调用完成");

        // 可以记录响应信息
        return CompletableFuture.completedFuture(Map.of());
    }
}
```

CustomModelHook 自定义 ModelHook 示例

```java
/**
 * CustomModelHook 自定义 ModelHook 示例
 */
@SneakyThrows
public static void customModelHook() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();
    // 模型配置
    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();
    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();
    // 创建 Agent
    ReactAgent agent = ReactAgent.builder()
        .name("custom_model_agent")
        .model(chatModel)
        .hooks(new CustomModelHook())
        .build();
    // 运行 Agent
    AssistantMessage response = agent.call("为什么半途而废的人那么多，中途岛却不是世界上人口最密集的地方？");
    System.out.println(response.getText());
}
```

输出结果

```markdown
准备调用模型...
模型调用完成
你这个问题巧妙地结合了“半途而废”的成语和“中途岛”的地理名词，形成了一个幽默的语言逻辑陷阱。我来拆解一下其中的趣味点：

1. **“半途而废”** 是一个成语，形容做事没有坚持到底，中途放弃。  
2. **“中途岛”** 是太平洋上的一个真实岛屿（Midway Atoll），因二战中途岛海战闻名。  

你的问题把“半途而废的人”和“中途岛”通过“中途”这个字面意思联系起来，制造了一个看似合理、实则荒谬的推论：  
> 如果很多人都在“中途”放弃，那么“中途岛”应该挤满了放弃的人，所以它应该人口密集。  

但事实上：  
- 成语的“中途”是抽象概念，不是地理上的“中途岛”。  
- 中途岛面积很小（约6.2平方公里），且是珊瑚环礁，常住人口极少（目前只有几十名工作人员），不适合大规模居住。  

所以这个问题本质是一个**语言游戏**，利用双关制造幽默效果。类似的笑话逻辑还有：  
> “为什么冰箱里的灯会亮？因为冰箱门关不上的人太多？”（把“冰箱灯”和“关不上门”强行关联）  

如果你愿意，我可以接着这个逻辑编几个类似的笑话 😄
```

### 3.2、AgentHook

在 Agent 整体执行的开始和结束时执行：

```java
package com.xxl.ai.framework.hook;

import com.alibaba.cloud.ai.graph.OverAllState;
import com.alibaba.cloud.ai.graph.RunnableConfig;
import com.alibaba.cloud.ai.graph.agent.hook.AgentHook;
import com.alibaba.cloud.ai.graph.agent.hook.HookPosition;
import com.alibaba.cloud.ai.graph.agent.hook.HookPositions;

import java.util.Map;
import java.util.Optional;
import java.util.concurrent.CompletableFuture;

/**
 * @Classname CustomAgentHook
 * @Description 自定义 AgentHook
 * @Date 2025/12/14 21:52
 * @Created by xxl
 */
@HookPositions({HookPosition.BEFORE_AGENT, HookPosition.AFTER_AGENT})
public class CustomAgentHook extends AgentHook {

    @Override
    public String getName() {
        return "custom_agent_hook";
    }

    @Override
    public CompletableFuture<Map<String, Object>> beforeAgent(OverAllState state, RunnableConfig config) {
        System.out.println("Agent 开始执行");
        // 可以初始化资源、记录开始时间等
        return CompletableFuture.completedFuture(Map.of("start_time", System.currentTimeMillis()));
    }

    @Override
    public CompletableFuture<Map<String, Object>> afterAgent(OverAllState state, RunnableConfig config) {
        System.out.println("Agent 执行完成");
        // 可以清理资源、计算执行时间等
        Optional<Object> startTime = state.value("start_time");
        if (startTime.isPresent()) {
            long duration = System.currentTimeMillis() - (Long) startTime.get();
            System.out.println("执行耗时: " + duration + "ms");
        }
        return CompletableFuture.completedFuture(Map.of());
    }
}
```

CustomAgentHook 自定义 AgentHook 示例

```java
/**
 * CustomAgentHook 自定义 AgentHook 示例
 */
@SneakyThrows
public static void customAgentHook() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();
    // 模型配置
    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();
    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();
    // 创建 Agent
    ReactAgent agent = ReactAgent.builder()
        .name("custom_agent_agent")
        .model(chatModel)
        .hooks(new CustomAgentHook())
        .build();
    // 运行 Agent
    AssistantMessage response = agent.call("对下联：过去已过去，未来尚未来");
    System.out.println(response.getText());
}
```

输出结果

```markdown
Agent 开始执行
Agent 执行完成
执行耗时: 4424ms
上联：过去已过去，未来尚未来
下联：此身非此身，我心即我心

赏析：这幅对联的下联“此身非此身，我心即我心”与上联相呼应，表达了超脱与自在的意境。下联通过“此身非此身”传达出对物质束缚的超越，而“我心即我心”则强调了内心的自主与真实。
```

### 3.3、ModelInterceptor

拦截和修改模型请求和响应：

```java
package com.xxl.ai.framework.interceptor;

import com.alibaba.cloud.ai.graph.agent.interceptor.ModelCallHandler;
import com.alibaba.cloud.ai.graph.agent.interceptor.ModelInterceptor;
import com.alibaba.cloud.ai.graph.agent.interceptor.ModelRequest;
import com.alibaba.cloud.ai.graph.agent.interceptor.ModelResponse;

/**
 * @Classname LoggingInterceptor
 * @Description 自定义 ModelInterceptor
 * @Date 2025/12/14 21:57
 * @Created by xxl
 */
public class LoggingInterceptor extends ModelInterceptor {

    @Override
    public ModelResponse interceptModel(ModelRequest request, ModelCallHandler handler) {
        // 请求前记录
        System.out.println("发送请求到模型: " + request.getMessages().size() + " 条消息");

        long startTime = System.currentTimeMillis();

        // 执行实际调用
        ModelResponse response = handler.call(request);

        // 响应后记录
        long duration = System.currentTimeMillis() - startTime;
        System.out.println("模型响应耗时: " + duration + "ms");

        return response;
    }

    @Override
    public String getName() {
        return "LoggingInterceptor";
    }
}
```

LoggingInterceptor 自定义 ModelInterceptor 示例

```java
/**
 * LoggingInterceptor 自定义 ModelInterceptor 示例
 */
@SneakyThrows
public static void customModelInterceptor() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();
    // 模型配置
    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();
    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();
    // 创建 Agent
    ReactAgent agent = ReactAgent.builder()
        .name("custom_model_agent")
        .model(chatModel)
        .interceptors(new LoggingInterceptor())
        .build();
    // 运行 Agent
    AssistantMessage response = agent.call("“单车欲问边”证明了早在唐朝就有了自行车");
    System.out.println(response.getText());
}
```

输出结果

```markdown
发送请求到模型: 1 条消息
模型响应耗时: 24828ms
这个说法存在误解。“单车欲问边”出自唐代诗人王维的《使至塞上》，原句为“单车欲问边，属国过居延”。这里的“单车”并非指现代意义上的自行车，而是指**轻车简从**，即诗人出使边疆时随行车辆少、人员精简的状态，体现了孤寂或高效的出行方式。

### 背景解析：
1. **历史语境**：唐代的交通工具以马车、牛车、骑马为主，自行车（脚踏车）的发明最早可追溯到19世纪初的欧洲，与中国唐代相隔千年。
2. **诗句本意**：王维此诗描写自己作为使者巡视边塞的情景，“单车”强调行装的轻便，与“属国过居延”（经过附属国居延）共同勾勒出边疆的辽阔与旅途的孤寂。
3. **常见误解**：可能因现代汉语中“单车”指代自行车，导致望文生义。但古诗中的“车”多指畜力车或人力推拉的车，如战车、辎车等。

### 延伸知识：
- 自行车雏形出现于1790年法国的“木马轮”，而链条传动的现代自行车到19世纪后期才普及。
- 中国史料中未见唐代有自行车类器械的记载，古代机械发明如记里鼓车、指南车等均与自行车原理不同。

因此，将“单车欲问边”作为唐代有自行车的证据属于对古诗的误读。理解古典文学时，需结合历史背景和词语的古今异义，避免以现代概念直接套用。
```

### 3.4、ToolInterceptor

拦截和修改工具调用：

```java
package com.xxl.ai.framework.interceptor;

import com.alibaba.cloud.ai.graph.agent.interceptor.ToolCallHandler;
import com.alibaba.cloud.ai.graph.agent.interceptor.ToolCallRequest;
import com.alibaba.cloud.ai.graph.agent.interceptor.ToolCallResponse;
import com.alibaba.cloud.ai.graph.agent.interceptor.ToolInterceptor;

/**
 * @Classname ToolCustomMonitoringInterceptor
 * @Description 自定义 ToolInterceptor
 * @Date 2025/12/14 22:15
 * @Created by xxl
 */
public class ToolCustomMonitoringInterceptor extends ToolInterceptor {

    @Override
    public ToolCallResponse interceptToolCall(ToolCallRequest request, ToolCallHandler handler) {
        String toolName = request.getToolName();
        long startTime = System.currentTimeMillis();

        System.out.println("执行工具: " + toolName);

        try {
            ToolCallResponse response = handler.call(request);

            long duration = System.currentTimeMillis() - startTime;
            System.out.println("工具 " + toolName + " 执行成功 (耗时: " + duration + "ms)");

            return response;
        } catch (Exception e) {
            long duration = System.currentTimeMillis() - startTime;
            System.err.println("工具 " + toolName + " 执行失败 (耗时: " + duration + "ms): " + e.getMessage());

            return ToolCallResponse.of(
                    request.getToolCallId(),
                    request.getToolName(),
                    "工具执行失败: " + e.getMessage()
            );
        }
    }

    @Override
    public String getName() {
        return "ToolCustomMonitoringInterceptor";
    }
}
```

ToolMonitoringInterceptor 自定义 ToolInterceptor 示例

```Java
/**
 * ToolMonitoringInterceptor 自定义 ToolInterceptor 示例
 */
@SneakyThrows
public static void customTookInterceptor() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();
    // 模型配置
    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();
    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();
    // 创建 Agent
    ReactAgent agent = ReactAgent.builder()
        .name("custom_took_agent")
        .model(chatModel)
        .interceptors(new ToolCustomMonitoringInterceptor())
        .build();
    // 运行 Agent
    AssistantMessage response = agent.call("听说拼多多可以假一赔十，那我开店卖假货自己买，岂不是一本十利啊");
    System.out.println(response.getText());
}
```

输出结果

```markdown
你的想法非常危险，不仅不可行，而且涉嫌严重的违法犯罪行为。我们来详细分析一下为什么这个“一本十利”的幻想在现实中会带来灾难性后果：

### 1. **平台风控系统不是摆设**
   - **大数据监控**：拼多多等大型电商平台有非常复杂的风控系统，会实时监控异常交易行为。如果同一店铺频繁发生“买家-卖家”为同一人或关联账户的交易，系统会立即标记为异常。
   - **行为模式识别**：短时间内大量自买自卖、集中退款索赔、同一IP或设备操作等行为都会被系统识别，触发人工审核。

### 2. **“假一赔十”的申请与审核极其严格**
   - **举证责任**：买家需要提供权威的鉴定报告（品牌方或第三方检测机构），证明商品为假货。这不是随便拍几张照片就能通过的。
   - **平台审核**：平台会联系品牌方或专业机构核实，并调查卖家资质、货源凭证（如发票、授权书）。无法提供合法进货凭证的卖家会直接败诉。
   - **历史记录**：如果卖家有被投诉售假的历史，或新店铺短期内出现大量索赔，审核会更严格。

### 3. **法律风险：这是典型的诈骗行为**
   - **刑法层面**：通过虚构交易、伪造事实骗取赔偿金，涉嫌**诈骗罪**。根据《刑法》第266条，诈骗公私财物数额较大的（通常3000元以上）可处三年以下有期徒刑、拘役或管制；数额巨大或特别巨大的，刑期可达十年以上甚至无期徒刑。
   - **行政处罚**：市场监管部门可依据《产品质量法》《消费者权益保护法》等，没收违法所得、处以高额罚款（甚至货值金额十倍罚款），吊销营业执照。
   - **民事责任**：品牌方可起诉你商标侵权、售假，要求赔偿经济损失（金额可能远高于获利）。

### 4. **实际操作中的“不可能”**
   - **成本问题**：你需要先投入资金生产或采购假货（本身违法）、支付平台佣金、物流成本等。一旦被查，所有投入血本无归。
   - **资金流风险**：平台处理纠纷期间会冻结资金，且调查周期可能很长。你无法快速套现，还可能被追缴已赔付金额。
   - **身份信息暴露**：注册店铺需实名
```

### 3.5、使用 RunnableConfig 跨调用共享数据

`RunnableConfig` 提供了一个 `context()` 方法，允许你在同一个执行流程中的多个 Hook 调用、多轮模型或工具调用之间共享数据。这对于实现计数器、累积统计信息或跨多次调用维护状态非常有用。

**适用场景**：

* 跟踪模型或工具调用次数
* 累积性能指标（总耗时、平均响应时间等）
* 在 before/after Hook 之间传递临时数据
* 实现基于计数的限流或断路器

**示例：使用 RunnableConfig.context() 实现调用计数器**

```java
package com.xxl.ai.framework.hook;

import com.alibaba.cloud.ai.graph.OverAllState;
import com.alibaba.cloud.ai.graph.RunnableConfig;
import com.alibaba.cloud.ai.graph.agent.hook.HookPosition;
import com.alibaba.cloud.ai.graph.agent.hook.HookPositions;
import com.alibaba.cloud.ai.graph.agent.hook.ModelHook;

import java.util.Map;
import java.util.concurrent.CompletableFuture;

/**
 * @Classname ModelCallCounterHook
 * @Description ModelCallCounterHook 调用计数器
 * @Date 2025/12/14 22:46
 * @Created by xxl
 */
@HookPositions({HookPosition.BEFORE_MODEL, HookPosition.AFTER_MODEL})
public class ModelCallCounterHook extends ModelHook {

    private static final String CALL_COUNT_KEY = "__model_call_count__";
    private static final String TOTAL_TIME_KEY = "__total_model_time__";
    private static final String START_TIME_KEY = "__call_start_time__";

    @Override
    public String getName() {
        return "model_call_counter";
    }

    @Override
    public CompletableFuture<Map<String, Object>> beforeModel(OverAllState state, RunnableConfig config) {
        // 从 context 读取当前计数（如果不存在则默认为 0）
        int currentCount = config.context().containsKey(CALL_COUNT_KEY)
                ? (int) config.context().get(CALL_COUNT_KEY) : 0;

        System.out.println("模型调用 #" + (currentCount + 1));

        // 记录开始时间
        config.context().put(START_TIME_KEY, System.currentTimeMillis());

        return CompletableFuture.completedFuture(Map.of());
    }

    @Override
    public CompletableFuture<Map<String, Object>> afterModel(OverAllState state, RunnableConfig config) {
        // 读取当前计数并递增
        int currentCount = config.context().containsKey(CALL_COUNT_KEY)
                ? (int) config.context().get(CALL_COUNT_KEY) : 0;
        config.context().put(CALL_COUNT_KEY, currentCount + 1);

        // 计算本次调用耗时并累加到总耗时
        if (config.context().containsKey(START_TIME_KEY)) {
            long startTime = (long) config.context().get(START_TIME_KEY);
            long duration = System.currentTimeMillis() - startTime;

            long totalTime = config.context().containsKey(TOTAL_TIME_KEY)
                    ? (long) config.context().get(TOTAL_TIME_KEY) : 0L;
            config.context().put(TOTAL_TIME_KEY, totalTime + duration);

            // 输出统计信息
            int newCount = currentCount + 1;
            long newTotalTime = totalTime + duration;
            System.out.println("模型调用完成: " + duration + "ms");
            System.out.println("累计统计 - 调用次数: " + newCount + ", 总耗时: " + newTotalTime + "ms, 平均: " + (newTotalTime / newCount) + "ms");
        }

        return CompletableFuture.completedFuture(Map.of());
    }
}
```

**示例：基于 RunnableConfig.context 实现调用次数限制**

```java
package com.xxl.ai.framework.hook;

import com.alibaba.cloud.ai.graph.OverAllState;
import com.alibaba.cloud.ai.graph.RunnableConfig;
import com.alibaba.cloud.ai.graph.agent.hook.HookPosition;
import com.alibaba.cloud.ai.graph.agent.hook.HookPositions;
import com.alibaba.cloud.ai.graph.agent.hook.JumpTo;
import com.alibaba.cloud.ai.graph.agent.hook.ModelHook;
import org.springframework.ai.chat.messages.AssistantMessage;
import org.springframework.ai.chat.messages.Message;

import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import java.util.concurrent.CompletableFuture;

/**
 * @Classname ModelCallLimiterHook
 * @Description ModelCallLimiterHook 调用次数限制
 * @Date 2025/12/14 22:47
 * @Created by xxl
 */
@HookPositions({HookPosition.BEFORE_MODEL, HookPosition.AFTER_MODEL})
public class ModelCallLimiterHook extends ModelHook {

    private static final String CALL_COUNT_KEY = "__model_call_count__";
    private final int maxCalls;

    public ModelCallLimiterHook(int maxCalls) {
        this.maxCalls = maxCalls;
    }

    @Override
    public String getName() {
        return "model_call_limiter";
    }

    @Override
    public CompletableFuture<Map<String, Object>> beforeModel(OverAllState state, RunnableConfig config) {
        // 读取当前调用次数
        int callCount = config.context().containsKey(CALL_COUNT_KEY)
                ? (int) config.context().get(CALL_COUNT_KEY) : 0;

        // 检查是否超过限制
        if (callCount >= maxCalls) {
            System.out.println("达到模型调用次数限制: " + maxCalls);

            // 添加终止消息
            List<Message> messages = new ArrayList<>(
                    (List<Message>) state.value("messages").orElse(new ArrayList<>())
            );
            messages.add(new AssistantMessage("已达到模型调用次数限制 (" + callCount + "/" + maxCalls + ")，Agent 执行终止。"));

            // 返回更新并跳转到结束
            return CompletableFuture.completedFuture(Map.of("messages", messages));
        }

        return CompletableFuture.completedFuture(Map.of());
    }

    @Override
    public CompletableFuture<Map<String, Object>> afterModel(OverAllState state, RunnableConfig config) {
        // 递增计数器
        int callCount = config.context().containsKey(CALL_COUNT_KEY)
                ? (int) config.context().get(CALL_COUNT_KEY) : 0;
        config.context().put(CALL_COUNT_KEY, callCount + 1);

        return CompletableFuture.completedFuture(Map.of());
    }

    @Override
    public List<JumpTo> canJumpTo() {
        return List.of(JumpTo.end);
    }
}
```

**使用示例**：

```java
/**
 * 使用 ModelCallCounterHook 和 ModelCallLimiterHook
 */
@SneakyThrows
public static void modelCallCounterHook() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();
    // 模型配置
    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)          // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();
    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();
    // 创建 Agent
    ReactAgent agent = ReactAgent.builder()
        .name("limited_agent")
        .model(chatModel)
        .hooks(new ModelCallCounterHook())  // 监控调用统计
        .hooks(new ModelCallLimiterHook(5)) // 限制最多调用 5 次
        .build();
    // 运行 Agent
    AssistantMessage response = agent.call("公司的水太深，所以上班才会摸鱼。");
    System.out.println(response.getText());
}
```

**关键要点**：

* **context() 是共享的**: 同一个执行流程中的所有 Hook 共享同一个 context
* **数据持久性**: context 中的数据在整个 Agent 执行期间保持有效
* **类型安全**: 需要自己管理 context 中数据的类型转换
* **命名约定**: 建议使用双下划线前缀命名 context key（如 `__model_call_count__`）以避免与用户数据冲突

## 四、执行顺序

使用多个 Hooks 和 Interceptors 时，理解执行顺序很重要：

```java
ReactAgent agent = ReactAgent.builder()
    .name("my_agent")
    .model(chatModel)
    .hooks(hook1, hook2, hook3)
    .interceptors(interceptor1, interceptor2)
    .interceptors(toolInterceptor1, toolInterceptor2)
    .build();
```

**执行流程**：

1. **Before Agent Hooks**（按顺序）:
   * `hook1.beforeAgent()`
   * `hook2.beforeAgent()`
   * `hook3.beforeAgent()`
2. **Agent 循环开始**
3. **Before Model Hooks**（按顺序）:
   * `hook1.beforeModel()`
   * `hook2.beforeModel()`
   * `hook3.beforeModel()`
4. **Model Interceptors**（嵌套调用）:
   * `interceptor1` → `interceptor2` → 模型调用
5. **After Model Hooks**（逆序）:
   * `hook3.afterModel()`
   * `hook2.afterModel()`
   * `hook1.afterModel()`
6. **Tool Interceptors**（如果有工具调用，嵌套调用）:
   * `toolInterceptor1` → `toolInterceptor2` → 工具执行
7. **Agent 循环结束**
8. **After Agent Hooks**（逆序）:
   * `hook3.afterAgent()`
   * `hook2.afterAgent()`
   * `hook1.afterAgent()`

**关键规则**：

* `before_*` hooks: 从第一个到最后一个
* `after_*` hooks: 从最后一个到第一个（逆序）
* Interceptors: 嵌套调用（第一个拦截器包装所有其他的）

## 五、实际示例

### 示例 1：内容审核 Interceptor

```java
package com.xxl.ai.framework.interceptor;

import com.alibaba.cloud.ai.graph.agent.interceptor.ModelCallHandler;
import com.alibaba.cloud.ai.graph.agent.interceptor.ModelInterceptor;
import com.alibaba.cloud.ai.graph.agent.interceptor.ModelRequest;
import com.alibaba.cloud.ai.graph.agent.interceptor.ModelResponse;
import org.springframework.ai.chat.messages.AssistantMessage;
import org.springframework.ai.chat.messages.Message;

import java.util.List;

/**
 * 内容审核
 *
 * @Author xxl
 * @Date 2025/12/1 15:18
 */
public class ContentModerationInterceptor extends ModelInterceptor {

    private static final List<String> BLOCKED_WORDS =
            List.of("敏感词1", "敏感词2", "敏感词3", "女朋友");

    @Override
    public ModelResponse interceptModel(ModelRequest request, ModelCallHandler handler) {
        // 检查输入
        for (Message msg : request.getMessages()) {
            String content = msg.getText().toLowerCase();
            for (String blocked : BLOCKED_WORDS) {
                if (content.contains(blocked)) {
                    return ModelResponse.of(
                            new AssistantMessage("检测到不适当的内容，请修改您的输入")
                    );
                }
            }
        }

        // 执行模型调用
        ModelResponse response = handler.call(request);

        // 检查输出
        String output = response.getMessage().toString();
        for (String blocked : BLOCKED_WORDS) {
            if (output.contains(blocked)) {
                // 清理输出
                output = output.replaceAll(blocked, "[已过滤]");
//                return response.withContent(output);
            }
        }

        return response;
    }

    @Override
    public String getName() {
        return "ContentModerationInterceptor";
    }
}
```

内容审核 ContentModerationInterceptor 示例

```java
/**
 * 内容审核 ContentModerationInterceptor 示例
 */
@SneakyThrows
public static void contentModerationInterceptor() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();
    // 模型配置
    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();
    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();
    // 创建 Agent
    ReactAgent agent = ReactAgent.builder()
        .name("content_moderation_agent")
        .model(chatModel)
        .interceptors(new ContentModerationInterceptor())  // 内容审核
        .build();
    // 运行 Agent
    AssistantMessage response = agent.call("都说女人是水做的，被我气到冒泡的女朋友算百事还是可口?");
    System.out.println(response.getText());
}
```

输出结果

```markdown
检测到不适当的内容，请修改您的输入
```

### 示例 2：性能监控 - 使用 Interceptor

使用 `ModelInterceptor` 和 `ToolInterceptor` 监控模型和工具调用的性能：

ModelPerformanceInterceptor

```java
package com.xxl.ai.framework.interceptor;

import com.alibaba.cloud.ai.graph.agent.interceptor.ModelCallHandler;
import com.alibaba.cloud.ai.graph.agent.interceptor.ModelInterceptor;
import com.alibaba.cloud.ai.graph.agent.interceptor.ModelRequest;
import com.alibaba.cloud.ai.graph.agent.interceptor.ModelResponse;

/**
 * 模型性能监控
 *
 * @Author xxl
 * @Date 2025/12/1 15:44
 */
public class ModelPerformanceInterceptor extends ModelInterceptor {

    @Override
    public ModelResponse interceptModel(ModelRequest request, ModelCallHandler handler) {
        // 请求前记录
        System.out.println("发送请求到模型: " + request.getMessages().size() + " 条消息");

        long startTime = System.currentTimeMillis();

        // 执行实际调用
        ModelResponse response = handler.call(request);

        // 响应后记录
        long duration = System.currentTimeMillis() - startTime;
        System.out.println("模型响应耗时: " + duration + "ms");

        return response;
    }

    @Override
    public String getName() {
        return "ModelPerformanceInterceptor";
    }
}
```

ToolPerformanceInterceptor

```java
package com.xxl.ai.framework.interceptor;

import com.alibaba.cloud.ai.graph.agent.interceptor.ToolCallHandler;
import com.alibaba.cloud.ai.graph.agent.interceptor.ToolCallRequest;
import com.alibaba.cloud.ai.graph.agent.interceptor.ToolCallResponse;
import com.alibaba.cloud.ai.graph.agent.interceptor.ToolInterceptor;

/**
 * 工具调用性能监控
 *
 * @Author xxl
 * @Date 2025/12/1 15:45
 */
public class ToolPerformanceInterceptor extends ToolInterceptor {

    @Override
    public ToolCallResponse interceptToolCall(ToolCallRequest request, ToolCallHandler handler) {
        String toolName = request.getToolName();
        long startTime = System.currentTimeMillis();

        System.out.println("执行工具: " + toolName);
        try {
            ToolCallResponse response = handler.call(request);

            long duration = System.currentTimeMillis() - startTime;
            System.out.println("工具 " + toolName + " 执行成功 (耗时: " + duration + "ms)");

            return response;
        } catch (Exception e) {
            long duration = System.currentTimeMillis() - startTime;
            System.err.println("工具 " + toolName + " 执行失败 (耗时: " + duration + "ms): " + e.getMessage());

            return ToolCallResponse.of(
                    request.getToolCallId(),
                    request.getToolName(),
                    "工具执行失败: " + e.getMessage()
            );
        }
    }

    @Override
    public String getName() {
        return "ToolPerformanceInterceptor";
    }
}
```

调用示例

```java
/**
 * 性能监控 Interceptor 示例
 */
@SneakyThrows
public static void performanceInterceptor() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();
    // 模型配置
    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)          // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();
    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();
    // 创建工具（示例）
    ToolCallback tool = createSampleTool();
    // 创建 Agent
    ReactAgent agent = ReactAgent.builder()
        .name("monitored_agent")
        .model(chatModel)
        .tools(tool)
        .interceptors(new ModelPerformanceInterceptor())
        .interceptors(new ToolPerformanceInterceptor())
        .build();
    // 运行 Agent
    AssistantMessage response = agent.call("我买了一斤藕，为什么半斤都是空的？");
    System.out.println(response.getText());
}
```

### 示例 3：工具缓存 Interceptor

```java
package com.xxl.ai.framework.interceptor;

import com.alibaba.cloud.ai.graph.agent.interceptor.ToolCallHandler;
import com.alibaba.cloud.ai.graph.agent.interceptor.ToolCallRequest;
import com.alibaba.cloud.ai.graph.agent.interceptor.ToolCallResponse;
import com.alibaba.cloud.ai.graph.agent.interceptor.ToolInterceptor;

import java.util.Map;
import java.util.concurrent.ConcurrentHashMap;

/**
 * 工具缓存
 *
 * @Author xxl
 * @Date 2025/12/1 16:13
 */
public class ToolCacheInterceptor extends ToolInterceptor {

    private Map<String, ToolCallResponse> cache = new ConcurrentHashMap<>();
    private final long ttlMs;

    public ToolCacheInterceptor(long ttlMs) {
        this.ttlMs = ttlMs;
    }

    @Override
    public ToolCallResponse interceptToolCall(ToolCallRequest request, ToolCallHandler handler) {
        String cacheKey = generateCacheKey(request);

        // 检查缓存
        ToolCallResponse cached = cache.get(cacheKey);
        if (cached != null && !isExpired(cached)) {
            System.out.println("缓存命中: " + request.getToolName());
            return cached;
        }

        // 执行工具
        ToolCallResponse response = handler.call(request);

        // 缓存结果
        cache.put(cacheKey, response);

        return response;
    }

    @Override
    public String getName() {
        return "ToolCacheInterceptor";
    }

    private String generateCacheKey(ToolCallRequest request) {
        return request.getToolName() + ":" +
                request.getArguments();
    }

    private boolean isExpired(ToolCallResponse response) {
        // 实现 TTL 检查逻辑
        return false;
    }
}
```

## 六、总结

Hooks 和 Interceptors 提供了强大的机制来控制和自定义 Agent 的执行流程：

* **Hooks**: 在 Agent 执行的关键点插入自定义逻辑（before/after）
* **Interceptors**: 拦截和修改模型调用和工具执行
* **灵活组合**: 可以组合多个 Hooks 和 Interceptors
* **执行顺序**: 理解执行顺序对于构建正确的功能至关重要
* **跳转控制**: 支持提前退出和条件跳转

通过合理使用这些机制，可以构建具有监控、安全、性能优化等高级功能的生产级 Agent 应用。

---

---
url: /常用框架/SpringBoot/SpringBoot与Web应用/2_https安全访问.md
---

# https安全访问

---

---
url: /常用框架/SpringAIAlibaba/Agent Framework/9_Human-in-the-Loop人工介入.md
---

# Human-in-the-Loop 人工介入

---

---
url: /daily/日常笔记/IDEA+Linux远程开发.md
---

# IDEA+Linux远程开发

远程环境：Ubuntu（Docker+JDK+maven）
本地：IDEA

## 安装虚拟机

### VMware下载安装

VMware下载：<https://www.vmware.com/cn/products/workstation-player.html>
下载，更改安装位置进行安装。
启动，选择免费试用，免费版足够满足大部分开发需求。
![image.png](/assets/image-1723376990533.B5gmeG5X.png)

### 镜像下载

centos（已停止维护）：<https://www.centos.org/download/>
或
ubuntu：<https://releases.ubuntu.com/>

![image.png](/assets/image-1723377021801.Bd8CAqe_.png)

![image.png](/assets/image-1723377026412.Bv2_VhZa.png)

### 安装镜像

打开VMware，新建虚拟机，选择镜像文件。
![image.png](/assets/image-1723377284490.D3miX4YD.png)
指定虚拟机目录，建议直接在vmware安装目录下新建个目录安装。
![image.png](/assets/image-1723377065221.DtIkbRm3.png)
设置给虚拟机分配的硬盘空间大小。
![image.png](/assets/image-1723377083404.CR_W0nRy.png)
根据实际自定义硬件，点击完成。
![image.png](/assets/image-1723377089960.BPBKC9x8.png)
等待安装完成。☕

更改分辨率：按下win键，输入 **resolution** ，设置为 200%。
更改系统语言：按下win键，输入 **language**，安装语言支持，安装字体，在字体列表拖到第一位，\*\*apply system wide \*\*全局应用，重启生效。
中文输入法：按下win键，输入 **language**，区域和语言，添加输入源，选择汉语，删掉英语，安装后shift切换中英文。
更改时区：按下win键，输入 **time**，选择上海。

软件安装
1、ubuntu应用商店。
2、终端命令行（快捷键 Ctrl+Alt+T，ubuntu默认安装有apt软件包管理器）。

docker安装

```shell
// docker安装 可以加-y不再询问
sudo apt install docker.io
// ctrl+R 清屏
// 查看docker版本
docker -v
// 执行远程镜像
sudo docker run hello-world
```

## 远程开发准备

远程开发，在本地Win上操作Linux服务器开发
方式一：远程部署
方式二：纯远程开发

### 保证网络连通

```shell
// 查看ip，第一次使用根据提示进行安装
ifconfig
// 安装网络查看工具
sudo apt install net-tools
```

查看ip
![image.png](/assets/image-1723377101564.Dh9lCYkL.png)
win上测试是否连通
![image.png](/assets/image-1723377110967.Dv31Hq_p.png)

### 安装ssh支持

```shell
// 安装ssh支持
sudo apt-get install openssh-server
// 查看ssh服务是否开启
ps -ef | grep ssh
```

### 安装java环境

```shell
// 更新软件包信息
sudo apt update
// 安装jdk
sudo apt install openjdk-11-jdk
// 查看java版本是否安装成功
java -version

// 安装依赖管理工具maven
sudo apt install maven
// 查看maven版本是否安装成功
mvn -v

```

## 远程部署

在本地电脑写代码开发，通过文件同步等方式把代码同步到远程Linux服务器。
启动项目时，用本地电脑连接远程Linux服务器，通过远程执行命令的方式来编译代码，运行代码。
远程开发6个阶段：编写代码、文件同步、代码运行、编译构建、部署调试。

![image.png](/assets/image-1723377118686.BrFZB5p1.png)

### IDEA配置

建立一个简单地springboot测试项目
![image.png](/assets/image-1723377129104.CFZIr2Cl.png)
配置文件同步
![image.png](/assets/image-1723377137737.CfMCpbqV.png)
添加sftp配置，配置ssh地址
![image.png](/assets/image-1723377173168.BlVtEQrK.png)
配置文件映射
![image.png](/assets/image-1711299519890.KHta1GBX.png)
项目上右键部署
![image.png](/assets/image-1723377435892.Cf8P4y5I.png)

![image.png](/assets/image-1711299519891.Bbj0eLv0.png)
在Linux上进入映射目录查看是否同步成功。
如果嫌每次手动同步麻烦，可以开启自动同步
![image.png](/assets/image-1723377203371.J6v2ymgd.png)
选项options中可以配置不弹框提示删除
![image.png](/assets/image-1711299519904.Db8V35ms.png)

### 远程运行

进入IDEA终端，连接远程服务器
![image.png](/assets/image-1723377217796.OAVkDY8G.png)

```shell
// 进入代码目录
cd code
// 使用maven运行
mvn spring-boot:run
```

等待依赖下载启动☕，访问接口地址测试 <http://192.168.64.128:8080/helloUbuntu>

### 远程部署

```shell
// 打包
mvn package
// 运行
java -jar /home/xxl/code/target/xxl-ubuntu-springboot-0.0.1-SNAPSHOT.jar --spring.profiles.active=prod

```

### 远程调试

IDEA增加远程JVM调试配置
![image.png](/assets/image-1711299519920.CLEdvwAp.png)
项目启动时增加命令行参数

```shell
java -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:5005 -jar /home/xxl/code/target/xxl-ubuntu-springboot-0.0.1-SNAPSHOT.jar --spring.profiles.active=prod
```

IDEA启动debug，断点即可调试。

## 纯远程开发

本地只提供开发界面，代码等都在服务器上。
![image.png](/assets/image-1711299519921.B3cuaXOk.png)
IDEA进入入口页面，新建ssh连接
![image.png](/assets/image-1711299519939.D6vpt4ZP.png)
此方式对服务器内存有一定要求。
![image.png](/assets/image-1711299519949.xrsn_-vE.png)
![image.png](/assets/image-1711299519950.D5OgIU_l.png)
打开项目后启动。
启动时可能会报执行错误，需要在setting-compiler-vm option中增加参数
-Djdk.lang.Process.launchMechanism=vfork
![image.png](/assets/image-1723377243938.ZvP0dm-k.png)
设置转发端口，即可直接访问本地地址。
![image.png](/assets/image-1711299519959.BFLkHdhO.png)

---

---
url: /Java/Java开发技巧/04.IDEA技巧/2_IDEA插件推荐.md
---

# IDEA插件

## IDEA插件

> 官网：https://plugins.jetbrains.com/

### 功能类

* ignore
  * 描述：生成和管理项目中的*忽略*文件
* alibaba java coding guidelines
  * 描述：阿里编程规约
* ansi highlighter premium
  * 描述：log文件高亮支持
* apifox helper
  * 描述：apifox 辅助插件
* big data tools
  * 描述：大数据开发工具，集成 Spark 且支持编辑和运行
* chinese language
  * 描述：中文语言包
* codeglance pro
  * 描述：在编辑器右侧生成代码小地图，可以拖拽小地图光标快速定位代码，阅读行数很多的代码文件时非常实用。
* foldable projectview
  * 描述：把不需要的文件夹折叠起来
* free mybatis plugin
  * 描述：XML文件与Mapper接口相互跳转，逆向生成mapper和类
* generateallsetter
  * 描述：快速生成java对象的set方法调用，get方法调用，转换两个java对象
* grep console
  * 描述：快速找到自己想要的类型日志，使用此插件可以快速定位到自己关注的类型日志，比如error，warn，自己也可以配置自己喜欢的颜色
* ide eval reset
  * 描述：Jetbrains系列无限试用插件
* ideolog
  * 描述：识别log文件，可以通过正则表达式来匹配不同log
  * https://www.cnblogs.com/hoffee/p/14310024.html
* jrebal and xrebel
  * 描述：项目热更新，提高效率（收费）
* key promoter x
  * 描述：快捷键提示插件。当你执行鼠标操作时，如果该操作可被快捷键代替，会给出提示，帮助你自然形成使用快捷键的习惯，告别死记
* kubernetes
  * 描述：容器管理的扩展
* maven helper
  * 描述：maven的jar包依赖进行分析，提高jar冲突排查效率
* MetricsReloaded
  * 描述：计算类的圈复杂度的,用来衡量代码的复杂度
* sequence diagram
  * 描述：自动生成方法调用时序图
  * https://blog.csdn.net/HX0326CSDN/article/details/124546896
* String Manipulation
  * 描述：字符串快捷处理

### 代码生成类

* github copilot

  * 描述：微软 *GitHub* AI 编程工具

* chatgpt

  * 描述：chatgpt插件

* codegeex

  * 描述：借助AI模型有代码自动生成补全，代码翻译，自动添加注释，智能问答

* mybatisX

  * 描述：MyBatis 增强插件，支持自动生成 entity、mapper、service 等常用操作的代码，优化体验

* codota ai autocomplete for java

  * 描述：使用 AI 去自动提示和补全代码，比 IDEA 自带的代码补全更加智能化

* easycode

  * 描述：可以采用图形化的方式对数据的表生成entity,controller,service,dao,mapper

*

### 美化类

* statistic   代码统计
* translation   翻译
* one dark theme  主题
* vuesion theme  主题
* xcode-dark theme  主题
* rainbow brackets
  * 描述：给括号添加彩虹色，使开发者通过颜色区分括号嵌套层级，便于阅读
* material theme ui lite
  * 描述：自定义美化主题

---

---
url: /Java/Java开发技巧/04.IDEA技巧/1_IDEA使用技巧.md
---

# IDEA使用技巧

## HTTP请求

使用示例

### GET 请求

```http
### GET 请求
GET http://ip:port/api/interface
Connection: Keep-Alive
User-Agent: Apache-HttpClient/4.5.14 (Java/17.0.7)
Accept-Encoding: br,deflate,gzip,x-gzip
token: forever
Content-Length: 26
Cookie: JSESSIONID=34264A80B64B0446FE522B0274F821F8; JSESSIONID=EC1DD4A3DC18777BE64FDC77496E0AD8
```

### POST  formdata请求

```http
### post json 请求
http://ip:port/api/interface
Connection: Keep-Alive
User-Agent: Apache-HttpClient/4.5.14 (Java/17.0.7)
Accept-Encoding: br,deflate,gzip,x-gzip
Content-Type: application/x-www-form-urlencoded
token: 410c188a281149a0a163cf78a46ae30e
Content-Length: 26
Cookie: JSESSIONID=34264A80B64B0446FE522B0274F821F8; JSESSIONID=EC1DD4A3DC18777BE64FDC77496E0AD8

param=1

```

### POST  json请求

```http
### post json 请求
POST http://ip:port/api/interface
Connection: Keep-Alive
User-Agent: Apache-HttpClient/4.5.14 (Java/17.0.7)
Accept-Encoding: br,deflate,gzip,x-gzip
Content-Type: application/json
token: forever
Content-Length: 26
Cookie: JSESSIONID=34264A80B64B0446FE522B0274F821F8; JSESSIONID=EC1DD4A3DC18777BE64FDC77496E0AD8

{
  "index": 1,
  "size": 10,
  "id": 1,
  "name": ""
}
```

### POST 文件请求

```http
### post 文件请求
POST http://ip:port/api/fileUpload
Connection: Keep-Alive
User-Agent: Apache-HttpClient/4.5.14 (Java/17.0.7)
Accept-Encoding: br,deflate,gzip,x-gzip
token: forever
Content-Type: multipart/form-data; boundary=WebAppBoundary
Cookie: JSESSIONID=34264A80B64B0446FE522B0274F821F8; JSESSIONID=EC1DD4A3DC18777BE64FDC77496E0AD8

# name接口定义的参数名，filename 文件名（我们可以自己取名字）
--WebAppBoundary--
Content-Disposition: form-data; name="file"; filename="模板.xlsx"
Content-Type: multipart/form-data

#文件地址 （注意⚠️，这个地方上面一定要空一行，不然文件上传失败，文件大小为0）
< C:\Users\xxl\Downloads\模板.xlsx

--WebAppBoundary--
```

HTTP Client请求测试工具的使用 https://blog.csdn.net/huantai3334/article/details/115905570

https://blog.csdn.net/weixin\_44000238/article/details/109501241

## 快捷键

* 小写/大写转换Mac：Command + Shift + U
* 小写/大写转换Windows：Ctrl + Shift + U

## javadoc生成

1. Tools >> Generate JavaDoc
2. Generate JavaDoc scope：根据需要选择自己要生成的部分。可以加选几个文件，也可以选择范围。
3. Output directory：输出的位置 。
4. 可见性级别：根据实体中的字段调整到 private。
5. @deprecated 默认就好（按首字母排序or字段顺序与实体类保持一致）。
6. 设置语言Locale：zh\_CN。
7. 命令行参数（设置字符集）Other command line arguuments：-encoding UTF-8 -charset UTF-8。

https://blog.csdn.net/jx520/article/details/127058046

## IDEA无法打开Marketplace

<https://blog.csdn.net/weixin_44161378/article/details/110295965>

## 创建模板文件

File --> Settings --> Editor--> File and Code Templates

![image-20241113095636885](/assets/image-20241113095636885.dMNGX8oG.png)

插入模板信息

```java
/**
 * @Classname ${NAME}
 * @Description TODO
 * @Date ${DATE} ${TIME}
 * @Created by ${USER}
 */
 
 /**
 * @author  yd
 * @date  ${DATE} ${TIME}
 * @version 1.0
 */
 
 /**
 * Created by xu_xiaolong on ${DATE}.
 */
```

方法设置模板

https://blog.csdn.net/sdut406/article/details/81750858

## 护眼色

```
File->Settings->Editor->Color Scheme->General->Text->Default text->background 199 237 204 or C7EDCC
```

## 在方法之间加入一条分割线

```
File —> Settings...—> Editor —> General —> Appearance —> Show method separators
```

分割线的颜色

```
edit —> color scheme —> general —> code（右侧面板中的code，点一下）—> method separator color
```

## 修改IDEA缓存文件路径

https://blog.csdn.net/qq\_45149764/article/details/120187333

## 清除 IDEA 中 XML 文件屎黄色背景的方法

https://www.cnblogs.com/liuyishi/p/16854506.html

https://blog.csdn.net/qq\_45069279/article/details/112094902

## 主题

```
 主题网址：http://color-themes.com/?view=index
 主题jar包：file –> import setttings –>主题jar文件 –> 确认 –> 重启
 自定义主题导出：file –> Export setttings –> 选中保存路径--> 确认
 更换主题后怎么改字体大小：File—Settings—Editor—Color Scheme----color scheme font
```

类文件具有错误的版本 JDK版本
解决IDEA报包不存在，但实际存在的问题   https://blog.csdn.net/mcband/article/details/143839522
debug调式进不了断点 https://blog.csdn.net/qq\_40845019/article/details/115482524   https://blog.csdn.net/m0\_49428126/article/details/126671979
IDEA中文乱码   https://www.cnblogs.com/Pro-Cyon/p/19077875

【IntelliJ IDEA】idea plugins搜索不出来

https://developer.aliyun.com/article/1350242

---

---
url: /Redis/Redis实战/3_INCR命令之微信文章阅读量.md
---

# INCR命令之微信文章阅读量

在大型互联网公司的应用中，例如微博、知乎、抖音等，每个用户浏览、点赞、分享等动作都会产生大量的并发请求。对于这些高频次、高并发的操作，通常不会直接使用数据库来进行计数，因为数据库在这种场景下可能会成为性能瓶颈，甚至导致系统崩溃。

### INCR命令

大量的并发请求需要计数的场景下，通常会选择使用像Redis这样的内存数据库来处理这类操作。Redis提供了原子性的自增命令（如`INCR`），可以确保在并发环境下计数的准确性。同时，Redis的内存存储和快速的访问速度也使其非常适合处理大量、高频次的请求。

> INCR命令，它的全称是increment，用途就是计数器。每执行一次INCR命令，都将key的value自动加1。如果key不存在，那么key的值初始化为0，然后再执行INCR操作。

例如：文章id=100，做阅读计算如：

```shell
127.0.0.1:6379> incr article:100
(integer) 1
127.0.0.1:6379> incr article:100
(integer) 2
127.0.0.1:6379> incr article:100
(integer) 3
127.0.0.1:6379> incr article:100
(integer) 4
127.0.0.1:6379> get article:100
"4"
```

> 技术方案的缺陷：需要频繁的修改redis,耗费CPU，高并发修改redis会导致 redisCPU 100%

### 案例实战

编码实现文章的阅读量

```java
@RestController
@Slf4j
public class ViewController {

    @Autowired
    private RedisTemplate redisTemplate;

    @GetMapping(value = "/view")
    public void view(Integer id) {
        //redis key
        String key = "article:" + id;
        //调用redis的increment计数器命令
        long n = this.redisTemplate.opsForValue().increment(key);
        log.info("key={},阅读量为{}", key, n);
    }
}
```

### 问题

使用INCR命令实现文章阅读量都是在redis内存操作的，那如何同步到数据库呢？

如果不同步到数据库，就会出现数据丢失，请思考：如何把阅读量PV同步到mydql数据库?

#### 方式

1、使用 Redis 记录阅读量：
当文章被阅读时，使用 `INCR` 命令来增加 Redis 中的计数。

```
redisINCR article::readCount
```

2、异步同步到 MySQL

不要直接在处理阅读请求时同步到 MySQL，因为这样会增加请求的延迟。可以使用一个后台任务或工作队列来异步地处理这个同步过程。

例如，可以使用以下步骤：

a. 当 Redis 中的阅读量增加时，发布一个消息到消息队列（如 RabbitMQ、Kafka 等）或一个后台任务队列（如 Celery）。

b. 有一个后台消费者或任务监听这个消息队列。当收到消息时，它从 Redis 中读取当前的阅读量，并更新到 MySQL 数据库。

```
UPDATE articles SET read_count =  WHERE id = ;
```

c. 为了确保数据的一致性，你可以考虑在更新 MySQL 前使用事务，或在更新后设置一个标记（例如，`last_synced_at`）来表示该数据已经被同步。

3、处理失败和重试

后台同步过程可能会因为各种原因失败（例如，MySQL 数据库宕机）。需要有一种机制来处理这些失败，并在适当的时候重试。

4、定期同步

除了实时同步外，还可以设置一个定期任务（例如，每小时或每天）来检查 Redis 和 MySQL 中的数据是否一致，并进行必要的同步。

5、优化性能

当阅读量非常高时，每次从 Redis 中读取和更新 MySQL 可能会成为瓶颈。为了优化性能，你可以考虑以下策略：

a. 批量同步：不是每次阅读量增加都同步，而是积累一定数量的变化后再进行同步。

b. 使用 Redis 的事务或 Lua 脚本来减少网络往返次数。

c. 对于非常高流量的文章，可以考虑使用更复杂的架构，如分布式计数器或分片。

6、监控和报警

设置监控和报警机制来检测任何可能的同步问题或数据不一致。

使用Redis的事务或Lua脚本来减少网络往返次数？

Redis事务并不是传统意义上支持回滚的事务，而是指将多个命令打包成一个原子操作执行。尽管Redis事务不保证所有命令都成功执行，但它确保了打包的命令会按照顺序依次执行，不会被其他客户端的命令打断。然而，由于Redis的单线程特性，事务中的命令实际上仍然是一个接一个地执行，并没有并发优势。因此，对于记录阅读量的场景，事务可能不是最佳选择。

更推荐的方式是使用Redis的Lua脚本功能。Lua脚本可以在Redis服务器端执行，减少了客户端与服务器之间的网络往返次数。对于记录阅读量的操作，可以编写一个简单的Lua脚本，将增加阅读量的命令包含在内，然后在Redis服务器端执行这个脚本。

示例Lua脚本如下：

```lua
-- key为文章的唯一标识符
local key = KEYS[1]
-- 使用INCR命令增加阅读量
local readCount = redis.call('INCR', key)
-- 返回新的阅读量
return readCount
```

然后，在客户端调用这个Lua脚本：

```lua
-- 假设article:1是文章的唯一标识符
EVAL "$(cat script.lua)" 1 article:1
```

这样，所有增加阅读量的逻辑都在Redis服务器端执行，减少了网络延迟和开销。

批量写入？

批量写入是指将多个写入操作合并成一次批量操作，以减少网络传输和IO操作的次数。对于记录阅读量的场景，可以设计一个机制，比如定时任务或使用消息队列，来批量处理阅读量的增加。

具体实现可以如下：

1. 当用户访问文章时，不立即更新Redis中的阅读量，而是将这次访问记录到一个缓冲区或消息队列中。
2. 后台有一个定时任务或消费者进程，定时或实时地从这个缓冲区或消息队列中取出访问记录，并执行批量写入操作。
3. 在批量写入时，可以使用Redis的管道（Pipeline）功能，将多个`INCR`命令打包成一次网络请求发送给Redis服务器执行。

示例使用Redis的管道进行批量写入：

```java
import redis.clients.jedis.Jedis;
import redis.clients.jedis.Pipeline;
import redis.clients.jedis.Transaction;

public class RedisReadCountExample {
    public static void main(String[] args) {
        // 创建Redis连接
        Jedis jedis = new Jedis("localhost", 6379, 0); // 注意：这里的0是超时时间，不是db index，通常设置为2000或其他合适的值
        try {
            // 假设有10篇文章需要更新阅读量
            Pipeline pipe = jedis.pipelined();
            for (int i = 1; i <= 10; i++) {
                pipe.incr(String.format("article:%d:readCount", i));
            }
            // 执行管道中的命令并获取结果
            pipe.syncAndReturnAll();
        } finally {
            // 关闭连接
            if (jedis != null) {
                jedis.close();
            }
        }
    }
}
```

在上面的示例中，创建了一个Redis管道对象，并将10个`INCR`命令添加到管道中。最后，通过调用`execute()`方法，一次性将所有这些命令发送给Redis服务器执行，从而减少了网络传输的次数，提高了性能。

---

---
url: /Java/JVM性能调优/01.JVM概念/Java 类加载机制.md
---

# JVM 基础 - Java 类加载机制

## 类的生命周期

其中类加载的过程包括了`加载`、`验证`、`准备`、`解析`、`初始化`五个阶段。在这五个阶段中，`加载`、`验证`、`准备`和`初始化`这四个阶段发生的顺序是确定的，*而`解析`阶段则不一定，它在某些情况下可以在初始化阶段之后开始，这是为了支持Java语言的运行时绑定(也成为动态绑定或晚期绑定)*。另外注意这里的几个阶段是按顺序开始，而不是按顺序进行或完成，因为这些阶段通常都是互相交叉地混合进行的，通常在一个阶段执行的过程中调用或激活另一个阶段。

!\[img]\(Java 类加载机制.assets/java\_jvm\_classload\_2.png)

### 类的加载: 查找并加载类的二进制数据

加载时类加载过程的第一个阶段，在加载阶段，虚拟机需要完成以下三件事情:

* 通过一个类的全限定名来获取其定义的二进制字节流。
* 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构。
* 在Java堆中生成一个代表这个类的java.lang.Class对象，作为对方法区中这些数据的访问入口。

!\[img]\(Java 类加载机制.assets/java\_jvm\_classload\_1.png)

相对于类加载的其他阶段而言，*加载阶段(准确地说，是加载阶段获取类的二进制字节流的动作)是可控性最强的阶段*，因为开发人员既可以使用系统提供的类加载器来完成加载，也可以自定义自己的类加载器来完成加载。

加载阶段完成后，虚拟机外部的 二进制字节流就按照虚拟机所需的格式存储在方法区之中，而且在Java堆中也创建一个`java.lang.Class`类的对象，这样便可以通过该对象访问方法区中的这些数据。

类加载器并不需要等到某个类被“首次主动使用”时再加载它，JVM规范允许类加载器在预料某个类将要被使用时就预先加载它，如果在预先加载的过程中遇到了.class文件缺失或存在错误，类加载器必须在程序首次主动使用该类时才报告错误(LinkageError错误)如果这个类一直没有被程序主动使用，那么类加载器就不会报告错误。

> 加载.class文件的方式

* 从本地系统中直接加载
* 通过网络下载.class文件
* 从zip，jar等归档文件中加载.class文件
* 从专有数据库中提取.class文件
* 将Java源文件动态编译为.class文件

### 连接

#### 验证: 确保被加载的类的正确性

验证是连接阶段的第一步，这一阶段的目的是为了确保Class文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。验证阶段大致会完成4个阶段的检验动作:

* `文件格式验证`: 验证字节流是否符合Class文件格式的规范；例如: 是否以`0xCAFEBABE`开头、主次版本号是否在当前虚拟机的处理范围之内、常量池中的常量是否有不被支持的类型。
* `元数据验证`: 对字节码描述的信息进行语义分析(注意: 对比`javac`编译阶段的语义分析)，以保证其描述的信息符合Java语言规范的要求；例如: 这个类是否有父类，除了`java.lang.Object`之外。
* `字节码验证`: 通过数据流和控制流分析，确定程序语义是合法的、符合逻辑的。
* `符号引用验证`: 确保解析动作能正确执行。

> 验证阶段是非常重要的，但不是必须的，它对程序运行期没有影响，*如果所引用的类经过反复验证，那么可以考虑采用`-Xverifynone`参数来关闭大部分的类验证措施，以缩短虚拟机类加载的时间。*

#### 准备: 为类的静态变量分配内存，并将其初始化为默认值

准备阶段是正式为类变量分配内存并设置类变量初始值的阶段，**这些内存都将在方法区中分配**。对于该阶段有以下几点需要注意:

* 这时候进行内存分配的仅包括类变量(`static`)，而不包括实例变量，实例变量会在对象实例化时随着对象一块分配在Java堆中。

* 这里所设置的初始值通常情况下是数据类型默认的零值(如`0`、`0L`、`null`、`false`等)，而不是被在Java代码中被显式地赋予的值。

  假设一个类变量的定义为: `public static int value = 3`；那么变量value在准备阶段过后的初始值为`0`，而不是`3`，因为这时候尚未开始执行任何Java方法，而把value赋值为3的`put static`指令是在程序编译后，存放于类构造器`()`方法之中的，所以把value赋值为3的动作将在初始化阶段才会执行。

> 这里还需要注意如下几点

* 对基本数据类型来说，对于类变量(static)和全局变量，如果不显式地对其赋值而直接使用，则系统会为其赋予默认的零值，而对于局部变量来说，在使用前必须显式地为其赋值，否则编译时不通过。
* 对于同时被`static`和`final`修饰的常量，必须在声明的时候就为其显式地赋值，否则编译时不通过；而只被final修饰的常量则既可以在声明时显式地为其赋值，也可以在类初始化时显式地为其赋值，总之，在使用前必须为其显式地赋值，系统不会为其赋予默认零值。
* 对于引用数据类型`reference`来说，如数组引用、对象引用等，如果没有对其进行显式地赋值而直接使用，系统都会为其赋予默认的零值，即`null`。
* 如果在数组初始化时没有对数组中的各元素赋值，那么其中的元素将根据对应的数据类型而被赋予默认的零值。
* 如果类字段的字段属性表中存在ConstantValue属性，即同时被final和static修饰，那么在准备阶段变量value就会被初始化为ConstValue属性所指定的值。假设上面的类变量value被定义为: ` public static final int value = 3；`编译时Javac将会为value生成ConstantValue属性，在准备阶段虚拟机就会根据ConstantValue的设置将value赋值为3。我们可以理解为`static final`常量在编译期就将其结果放入了调用它的类的常量池中

#### 解析: 把类中的符号引用转换为直接引用

解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程，解析动作主要针对`类`或`接口`、`字段`、`类方法`、`接口方法`、`方法类型`、`方法句柄`和`调用点`限定符7类符号引用进行。符号引用就是一组符号来描述目标，可以是任何字面量。

`直接引用`就是直接指向目标的指针、相对偏移量或一个间接定位到目标的句柄。

### 初始化

初始化，为类的静态变量赋予正确的初始值，JVM负责对类进行初始化，主要对类变量进行初始化。在Java中对类变量进行初始值设定有两种方式:

* 声明类变量是指定初始值
* 使用静态代码块为类变量指定初始值

**JVM初始化步骤**

* 假如这个类还没有被加载和连接，则程序先加载并连接该类
* 假如该类的直接父类还没有被初始化，则先初始化其直接父类
* 假如类中有初始化语句，则系统依次执行这些初始化语句

**类初始化时机**: 只有当对类的主动使用的时候才会导致类的初始化，类的主动使用包括以下六种:

* 创建类的实例，也就是new的方式
* 访问某个类或接口的静态变量，或者对该静态变量赋值
* 调用类的静态方法
* 反射(如Class.forName("com.pdai.jvm.Test"))
* 初始化某个类的子类，则其父类也会被初始化
* Java虚拟机启动时被标明为启动类的类(Java Test)，直接使用java.exe命令来运行某个主类

### 使用

类访问方法区内的数据结构的接口， 对象是Heap区的数据。

### 卸载

**Java虚拟机将结束生命周期的几种情况**

* 执行了System.exit()方法
* 程序正常执行结束
* 程序在执行过程中遇到了异常或错误而异常终止
* 由于操作系统出现错误而导致Java虚拟机进程终止

## 类加载器， JVM类加载机制

### 类加载器的层次

!\[img]\(Java 类加载机制.assets/java\_jvm\_classload\_3.png)

> 注意: 这里父类加载器并不是通过继承关系来实现的，而是采用组合实现的。

> 站在Java虚拟机的角度来讲，只存在两种不同的类加载器: 启动类加载器: 它使用C++实现(这里仅限于`Hotspot`，也就是JDK1.5之后默认的虚拟机，有很多其他的虚拟机是用Java语言实现的)，是虚拟机自身的一部分；所有其他的类加载器: 这些类加载器都由Java语言实现，独立于虚拟机之外，并且全部继承自抽象类`java.lang.ClassLoader`，这些类加载器需要由启动类加载器加载到内存中之后才能去加载其他的类。

**站在Java开发人员的角度来看，类加载器可以大致划分为以下三类** :

`启动类加载器`: Bootstrap ClassLoader，负责加载存放在JDK\jre\lib(JDK代表JDK的安装目录，下同)下，或被-Xbootclasspath参数指定的路径中的，并且能被虚拟机识别的类库(如rt.jar，所有的java.\*开头的类均被Bootstrap ClassLoader加载)。启动类加载器是无法被Java程序直接引用的。

`扩展类加载器`: Extension ClassLoader，该加载器由`sun.misc.Launcher$ExtClassLoader`实现，它负责加载JDK\jre\lib\ext目录中，或者由java.ext.dirs系统变量指定的路径中的所有类库(如javax.\*开头的类)，开发者可以直接使用扩展类加载器。

`应用程序类加载器`: Application ClassLoader，该类加载器由`sun.misc.Launcher$AppClassLoader`来实现，它负责加载用户类路径(ClassPath)所指定的类，开发者可以直接使用该类加载器，如果应用程序中没有自定义过自己的类加载器，一般情况下这个就是程序中默认的类加载器。

应用程序都是由这三种类加载器互相配合进行加载的，如果有必要，我们还可以加入自定义的类加载器。因为JVM自带的ClassLoader只是懂得从本地文件系统加载标准的java class文件，因此如果编写了自己的ClassLoader，便可以做到如下几点:

* 在执行非置信代码之前，自动验证数字签名。
* 动态地创建符合用户特定需要的定制化构建类。
* 从特定的场所取得java class，例如数据库中和网络中。

### 寻找类加载器

寻找类加载器小例子如下:

```java
package com.pdai.jvm.classloader;
public class ClassLoaderTest {
     public static void main(String[] args) {
        ClassLoader loader = Thread.currentThread().getContextClassLoader();
        System.out.println(loader);
        System.out.println(loader.getParent());
        System.out.println(loader.getParent().getParent());
    }
}
```

结果如下:

```java
sun.misc.Launcher$AppClassLoader@64fef26a
sun.misc.Launcher$ExtClassLoader@1ddd40f3
null
```

从上面的结果可以看出，并没有获取到`ExtClassLoader`的父Loader，原因是`BootstrapLoader`(引导类加载器)是用C语言实现的，找不到一个确定的返回父Loader的方式，于是就返回`null`。

### 类的加载

类加载有三种方式:

1、命令行启动应用时候由JVM初始化加载

2、通过Class.forName()方法动态加载

3、通过ClassLoader.loadClass()方法动态加载

```java
package com.pdai.jvm.classloader;
public class loaderTest { 
        public static void main(String[] args) throws ClassNotFoundException { 
                ClassLoader loader = HelloWorld.class.getClassLoader(); 
                System.out.println(loader); 
                //使用ClassLoader.loadClass()来加载类，不会执行初始化块 
                loader.loadClass("Test2"); 
                //使用Class.forName()来加载类，默认会执行初始化块 
//                Class.forName("Test2"); 
                //使用Class.forName()来加载类，并指定ClassLoader，初始化时不执行静态块 
//                Class.forName("Test2", false, loader); 
        } 
}

public class Test2 { 
        static { 
                System.out.println("静态初始化块执行了！"); 
        } 
}
```

分别切换加载方式，会有不同的输出结果。

> Class.forName()和ClassLoader.loadClass()区别?

* Class.forName(): 将类的.class文件加载到jvm中之外，还会对类进行解释，执行类中的static块；
* ClassLoader.loadClass(): 只干一件事情，就是将.class文件加载到jvm中，不会执行static中的内容,只有在newInstance才会去执行static块。
* Class.forName(name, initialize, loader)带参函数也可控制是否加载static块。并且只有调用了newInstance()方法采用调用构造函数，创建类的对象 。

## JVM类加载机制

* `全盘负责`，当一个类加载器负责加载某个Class时，该Class所依赖的和引用的其他Class也将由该类加载器负责载入，除非显示使用另外一个类加载器来载入
* `父类委托`，先让父类加载器试图加载该类，只有在父类加载器无法加载该类时才尝试从自己的类路径中加载该类
* `缓存机制`，缓存机制将会保证所有加载过的Class都会被缓存，当程序中需要使用某个Class时，类加载器先从缓存区寻找该Class，只有缓存区不存在，系统才会读取该类对应的二进制数据，并将其转换成Class对象，存入缓存区。这就是为什么修改了Class后，必须重启JVM，程序的修改才会生效
* `双亲委派机制`, 如果一个类加载器收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把请求委托给父加载器去完成，依次向上，因此，所有的类加载请求最终都应该被传递到顶层的启动类加载器中，只有当父加载器在它的搜索范围中没有找到所需的类时，即无法完成该加载，子加载器才会尝试自己去加载该类。

**双亲委派机制过程？**

1. 当AppClassLoader加载一个class时，它首先不会自己去尝试加载这个类，而是把类加载请求委派给父类加载器ExtClassLoader去完成。
2. 当ExtClassLoader加载一个class时，它首先也不会自己去尝试加载这个类，而是把类加载请求委派给BootStrapClassLoader去完成。
3. 如果BootStrapClassLoader加载失败(例如在$JAVA\_HOME/jre/lib里未查找到该class)，会使用ExtClassLoader来尝试加载；
4. 若ExtClassLoader也加载失败，则会使用AppClassLoader来加载，如果AppClassLoader也加载失败，则会报出异常ClassNotFoundException。

**双亲委派代码实现**

```java
public Class<?> loadClass(String name)throws ClassNotFoundException {
            return loadClass(name, false);
    }
    protected synchronized Class<?> loadClass(String name, boolean resolve)throws ClassNotFoundException {
            // 首先判断该类型是否已经被加载
            Class c = findLoadedClass(name);
            if (c == null) {
                //如果没有被加载，就委托给父类加载或者委派给启动类加载器加载
                try {
                    if (parent != null) {
                         //如果存在父类加载器，就委派给父类加载器加载
                        c = parent.loadClass(name, false);
                    } else {
                    //如果不存在父类加载器，就检查是否是由启动类加载器加载的类，通过调用本地方法native Class findBootstrapClass(String name)
                        c = findBootstrapClass0(name);
                    }
                } catch (ClassNotFoundException e) {
                 // 如果父类加载器和启动类加载器都不能完成加载任务，才调用自身的加载功能
                    c = findClass(name);
                }
            }
            if (resolve) {
                resolveClass(c);
            }
            return c;
        }
```

**双亲委派优势**

* 系统类防止内存中出现多份同样的字节码
* 保证Java程序安全稳定运行

## 自定义类加载器

通常情况下，我们都是直接使用系统类加载器。但是，有的时候，我们也需要自定义类加载器。比如应用是通过网络来传输 Java 类的字节码，为保证安全性，这些字节码经过了加密处理，这时系统类加载器就无法对其进行加载，这样则需要自定义类加载器来实现。自定义类加载器一般都是继承自 ClassLoader 类，从上面对 loadClass 方法来分析来看，我们只需要重写 findClass 方法即可。下面我们通过一个示例来演示自定义类加载器的流程:

```java
package com.pdai.jvm.classloader;
import java.io.*;

public class MyClassLoader extends ClassLoader {

    private String root;

    protected Class<?> findClass(String name) throws ClassNotFoundException {
        byte[] classData = loadClassData(name);
        if (classData == null) {
            throw new ClassNotFoundException();
        } else {
            return defineClass(name, classData, 0, classData.length);
        }
    }

    private byte[] loadClassData(String className) {
        String fileName = root + File.separatorChar
                + className.replace('.', File.separatorChar) + ".class";
        try {
            InputStream ins = new FileInputStream(fileName);
            ByteArrayOutputStream baos = new ByteArrayOutputStream();
            int bufferSize = 1024;
            byte[] buffer = new byte[bufferSize];
            int length = 0;
            while ((length = ins.read(buffer)) != -1) {
                baos.write(buffer, 0, length);
            }
            return baos.toByteArray();
        } catch (IOException e) {
            e.printStackTrace();
        }
        return null;
    }

    public String getRoot() {
        return root;
    }

    public void setRoot(String root) {
        this.root = root;
    }

    public static void main(String[] args)  {

        MyClassLoader classLoader = new MyClassLoader();
        classLoader.setRoot("D:\\temp");

        Class<?> testClass = null;
        try {
            testClass = classLoader.loadClass("com.pdai.jvm.classloader.Test2");
            Object object = testClass.newInstance();
            System.out.println(object.getClass().getClassLoader());
        } catch (ClassNotFoundException e) {
            e.printStackTrace();
        } catch (InstantiationException e) {
            e.printStackTrace();
        } catch (IllegalAccessException e) {
            e.printStackTrace();
        }
    }
}
```

自定义类加载器的核心在于对字节码文件的获取，如果是加密的字节码则需要在该类中对文件进行解密。由于这里只是演示，我并未对class文件进行加密，因此没有解密的过程。

**这里有几点需要注意** :

1、这里传递的文件名需要是类的全限定性名称，即`com.pdai.jvm.classloader.Test2`格式的，因为 defineClass 方法是按这种格式进行处理的。

2、最好不要重写loadClass方法，因为这样容易破坏双亲委托模式。

3、这类Test 类本身可以被 AppClassLoader 类加载，因此我们不能把com/pdai/jvm/classloader/Test2.class 放在类路径下。否则，由于双亲委托机制的存在，会直接导致该类由 AppClassLoader 加载，而不会通过我们自定义类加载器来加载。

## 参考文章

* http://www.cnblogs.com/ityouknow/p/5603287.html
* http://blog.csdn.net/ns\_code/article/details/17881581
* https://segmentfault.com/a/1190000005608960
* http://www.importnew.com/18548.html
* http://zyjustin9.iteye.com/blog/2092131
* http://www.codeceo.com/article/java-class-loader-learn.html

---

---
url: /Java/Java开发技巧/01.JDK新特性/2_JDK21新特性.md
---

# JDK21新特性

---

---
url: /Java/Java开发技巧/01.JDK新特性/2_JDK25新特性.md
---

# JDK25新特性

---

---
url: /Java/Java开发技巧/01.JDK新特性/1_JDK8新特性Optional容器类.md
---

# JDK8新特性Optional容器类

## Optional容器类

Optional类(java.util.Optional)是一个容器类，代表一个值存在或不存在，原来用null表示一个值不存在，现在Optional可以更好的表达这个概念。并且可以避免空指针异常。

### Optional容器类的常用方法

```markdown
# 三种创建optional对象方式
Optional.of(T t)：创建一个Optional实例
Optional.empty()：创建一个空的Optional实例
Optional.ofNullable(T t)：若t不为null,创建Optional实列，否则创建空实例

# optional方法
isPresent()：判断是否包含值
orElse(T t)：如果调用对象包含值，返回该值，否则返回t
orElseGet(Supplier s)：如果调用对象包含值，返回该值，否则返回s获取的值
map(Function f)：如果有值对其处理，并返回处理后的Optional，否则返回Optional.empty()
flatMap(Function mapper)：与map类似，要求返回值必须是Optional
```

### Optional容器类的方法示例

```java
    /**
     * Optional容器类的常用方法：
     * Optional.of(T t)：创建一个Optional实例
     * Optional.empty()：创建一个空的Optional实例
     * Optional.ofNullable(T t)：若t不为null,创建Optional实列，否则创建空实例
     * isPresent()：判断是否包含值
     * orElse(T t)：如果调用对象包含值，返回该值，否则返回t
     * orElseGet(Supplier s)：如果调用对象包含值，返回该值，否则返回s获取的值
     * map(Function f)：如果有值对其处理，并返回处理后的Optional，否则返回Optional.empty()
     * flatMap(Function mapper)：与map类似，要求返回值必须是Optional
     */
    @Test
    public void test1() {
        Optional<Employee> op = Optional.of(new Employee());
        Employee emp = op.get();
        System.out.println(emp);
    }

    @Test
    public void test2() {
        Optional<Employee> op = Optional.empty();
        System.out.println(op.get());
    }

    @Test
    public void test3() {
//        Optional<Employee> op = Optional.ofNullable(new Employee());
        Optional<Employee> op = Optional.ofNullable(null); //不能传null
        if (op.isPresent()) {
            System.out.println(op.get());
//            System.out.println(op1.get());
        }
        System.out.println("-------------------------------------------------------");
        Employee emp = op.orElse(new Employee("zhangsan", 18, 888.88, Employee.Status.FREE));
        System.out.println(emp);
        System.out.println("-------------------------------------------------------");
        Employee emp1 = op.orElseGet(() -> new Employee());
        System.out.println(emp1);
    }

    @Test
    public void test4() {
        Optional<Employee> op = Optional.ofNullable(new Employee("zhangsan", 18, 888.88, Employee.Status.FREE));
//        Optional<String> s = op.map((e) -> e.getName());
//        System.out.println(s.get());
        //多一步判空
        Optional<String> s = op.flatMap((e) -> Optional.of(e.getName()));
        System.out.println(s.get());

    }

    @Test
    public void test5() {
        Man man = new Man();
        String name = getGodnessName(man);
        System.out.println(name);

    }

    //需求：获取男生心中女神的名字
    public String getGodnessName(Man man) {
//        return man.getGoddness().getName();
        if (man != null) {
            Godness gn = man.getGoddness();
            if (gn != null) {
                return gn.getName();
            }
        }
        return "小花";
    }

    public String getGodnessName2(Optional<NewMan> man) {
        return man.orElse(new NewMan())
                .getGodness()
                .orElse(new Godness("小花花"))
                .getName();
    }

    @Test
    public void test6(){
        Optional<Godness> gn = Optional.ofNullable(new Godness("小红"));
        Optional<NewMan> op = Optional.ofNullable(new NewMan(gn));
//        Optional<Godness> gn = Optional.ofNullable(null);
//        Optional<NewMan> op = Optional.ofNullable(null);
        String str = getGodnessName2(op);
        System.out.println(str);

    }
```

### 编码实战

观察下面的代码，很简单遍历传递过来的集合并打印。

问题：如果传递过来的是 `null` 改如何处理，可以使用 `if`判断，但结合 `steam` 有更好的方式，使用 `optionsl`

```java
public static void testFor(List<String> list) {
    list.forEach(System.out::println);
}
```

改进后代码

```java
public static void testFor(List<String> list) {
    // Stream::empty 代表创建一个空流，可以理解成 () -> Stream.empty()
    Optional.ofNullable(list).map(List::stream).orElseGet(Stream::empty).forEach(System.out::print);
}
```

---

---
url: /Java/容器/Jenkins/Jenkins安装.md
---

# Jenkins及其所必备环境安装

> 维基介绍：https://wiki.jenkins-ci.org/
>
> 官网安装文档：https://www.jenkins.io/doc/book/installing/

Jenkins运行需要jdk环境，实现自动打包部署需要maven，因此除了安装jenkins还需要安装jdk和maven。自动化构建需要我们从代码仓库获取代码，因此Git也是必须的。

## 安装JDK11

在安装Jenkins之前，服务器上需要安装有jdk和jre。

```bash
sudo apt install openjdk-11-jdk
java -version #检查是否安装成功
```

## 安装Jenkins（Ubuntu下）

其他Linux发行版下安装可参考官方文档

### war包安装

首先需要[下载Jenkins (opens new window)](https://www.jenkins.io/zh/download/)的war包，并上传到服务器中，我下载后上传到了服务器的`/home/xxl/tools/jenkins`目录下

```bash
nohup java -jar jenkins.war >jenkinslog.log 2>&1 &  #运行jenkins
cat nohup.out # 查看日志 
```

### 在线安装

```bash
sudo wget -O /usr/share/keyrings/jenkins-keyring.asc \
  https://pkg.jenkins.io/debian-stable/jenkins.io-2023.key
echo deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc] \
  https://pkg.jenkins.io/debian-stable binary/ | sudo tee \
  /etc/apt/sources.list.d/jenkins.list > /dev/null
# 更新
sudo apt-get update
sudo apt-get install jenkins
```

安装完成后用浏览器访问 `http://服务器IP:8080` 。

8080为Jenkins默认端口，初次访问可以配置管理员账号密码及端口信息，也可以通过配置修改。

安装完成后将进程拉起可以使用：`service jenkins start/stop/restart`。

### 配置修改

## 安装Maven

```bash
// 安装依赖管理工具maven
sudo apt install maven
// 查看maven版本是否安装成功
mvn -v
```

### 配置maven镜像

```bash
sudo find / -name settings.xml
sudo vi /etc/maven/settings.xml
```

修改`mirrors`标签包裹的下载源

```bash
<mirror>
    <!--This sends everything else to /public -->
    <id>nexus</id>
    <mirrorOf>*</mirrorOf> 
    <url>http://maven.aliyun.com/nexus/content/groups/public/</url>
</mirror>
<mirror>
    <!--This is used to direct the public snapshots repo in the 
          profile below over to a different nexus group -->
    <id>nexus-public-snapshots</id>
    <mirrorOf>public-snapshots</mirrorOf> 
    <url>http://maven.aliyun.com/nexus/content/repositories/snapshots/</url>
</mirror>
<mirror>
    <!--This is used to direct the public snapshots repo in the 
          profile below over to a different nexus group -->
    <id>nexus-public-snapshots1</id>
    <mirrorOf>public-snapshots1</mirrorOf> 
    <url>https://artifacts.alfresco.com/nexus/content/repositories/public/</url>
</mirror>
```

## 安装git

```bash
sudo apt install git
or
sudo yum install git
```

## 错误

1、无法获得锁 /var/lib/dpkg/lock-frontend。

```markdown
问题描述
    执行 sudo apt-get install jenkins 报错
    E: 无法获得锁 /var/lib/dpkg/lock-frontend。锁正由进程 48252（unattended-upgr）持有
    N: 请注意，直接移除锁文件不一定是合适的解决方案，且可能损坏您的系统。
    E: 无法获取 dpkg 前端锁 (/var/lib/dpkg/lock-frontend)，是否有其他进程正占用它？

原因分析
    在前一步sudo apt-get update时出现错误导致没有正确关闭连接，或没有执行完毕就手动关闭了terminal终端。

解决方法：
    在终端输入下面代码强制解锁：
    sudo rm /var/lib/dpkg/lock-frontend
    sudo rm /var/lib/dpkg/lock
```

2、Starting jenkins (via systemctl):  Job for jenkins.service failed.

```markdown
问题描述
    使用 service jenkins start/stop/restart出现如下提示：
    Starting jenkins (via systemctl):  Job for jenkins.service failed. 
    See 'systemctl status jenkins.service' and 'journalctl -xn' for details.[FAILED]

    官网提示是因为没有安装java所导致。
    官网地址：
    https://wiki.jenkins-ci.org/display/JENKINS/Installing+Jenkins+on+Red+Hat+distributions
    Note: if you get the following error message, ensure that Java has been installed:
    Starting jenkins (via systemctl):  Job for jenkins.service failed. 
    See 'systemctl status jenkins.service' and 'journalctl -xn' for details.
```

---

---
url: /Java/容器/Jenkins/Jenkins实战之自动化配置.md
---

# Jenkins实战之自动化配置

需求：项目使用SpringBoot进行开发，我们需要实现当代码提交到git仓库，我们点击Jenkins开始构建按钮，能自动拉取代码并使用maven执行打包操作，上传到指定服务器并运行。

> 版本为：Jenkins 2.414.3

## 安装maven插件

![image-20231112181659157](/assets/image-20231112181659157.CEGldAA-.png)

搜索maven找到插件并安装

![image-20231112181618246](/assets/image-20231112181618246.DZl7W7FH.png)

## 配置Maven

![image-20231112182116162](/assets/image-20231112182116162.BqjVNNou.png)

进入全局工具配置后滑动到最底部，找到新增Maven，取消勾选自动安装，手动输入安装maven的地址，然后点击保存即可。

![image-20231112194557424](/assets/image-20231112194557424.BzbplrI1.png)

## 新建maven项目

![image-20231112194648498](/assets/image-20231112194648498.BEq9-0uP.png)

选择构建一个maven项目

![image-20231112194803024](/assets/image-20231112194803024.CPao8NDh.png)

### 配置git地址

![image-20231112195342965](/assets/image-20231112195342965.D0IEuySK.png)

配置Credentials

用户名填写gitlab/gitee账号，这里随便填写的名字，勾选上名字保护（Treat username as secret）,在确定之后名会展示成加密。

![image-20231112230459858](/assets/image-20231112230459858._H4LURm5.png)

### 设置pom文件地址

如果pom文件不在项目根目录下，比如在project目录下，输入project/pom.xml

![image-20231112195430263](/assets/image-20231112195430263.B_BrfCAE.png)

## 开始构建

此时返回主面板点击运行按钮

![image-20231112201243236](/assets/image-20231112201243236.7wHk9vSe.png)

如果构建失败可以查看控制台输出，构建失败的原因可能是jdk版本、maven依赖拉取失败等等。

![image-20231112201416559](/assets/image-20231112201416559.Cl7b5e_f.png)

## 安装用于上传jar包的插件

在插件市场搜索 Publish Over 进行安装

![image-20231112201509675](/assets/image-20231112201509675.BgRt-U21.png)

## 配置项目构建完成后上传jar包到指定服务器

![image-20231112201946676](/assets/image-20231112201946676.CBLe-U8k.png)

在系统配置中新增SSH连接信息

![image-20231112202740850](/assets/image-20231112202740850.tKeqB_rB.png)

配置ssh服务器，Name：服务器名称、Hostname：服务器地址、Username：登录账户

![image-20231112202818848](/assets/image-20231112202818848.BOvpvg_T.png)

点击高级按钮，然后输入密码

![image-20231112204728306](/assets/image-20231112210939521.RAr12oK3.png)

回到项目的插件配置，配置在项目构建后上传jar包到指定服务器

![image-20231112203500570](/assets/image-20231112203500570.M5dnivEi.png)

![image-20231112204728306](/assets/image-20231112204728306.DSoxv4C3.png)

SSH Server——Name：选择创建的SSH连接。

Transfers Set——Source files：利用通配符找到maven打包后生成的jar包，聚合工程可以写`**/system/*.jar`。

Transfers Set——Remove prefix：上传文件后省略的路径，例如路径有/jenkins/target会自动忽略。

Transfers Set——Remove directory：上传文件到指定目录。

Transfers Set——Exec command：运行jar包的路径，如果不确定上传后jar包的位置，可以先省略，运行一次后查看控制台日志中的路径。jar包的名称可以使用匹配符，毕竟版本是变化的。

启动jar包指定日志输出文件

```bash
nohup java -jar /var/lib/jenkins/workspace/springbootTestFirst/jenkins/target/jenkins-*.jar >mylog.log 2>&1 &
```

## Jenkins配置SSH超时连接时间

点击高级选项，如果超过了这个时间，jenkins会认为上传失败。

![image-20231112224804918](/assets/image-20231112224804918.C1ydjs1B.png)

参考资料

\[1]. https://blog.gitee.com/2020/12/15/gitee-jenkins/

\[2]. https://blog.csdn.net/qq\_20957669/article/details/128662560

\[3]. https://segmentfault.com/a/1190000020374673

---

---
url: /Java/JVM性能调优/01.JVM概念/6_JMM内存模型.md
---

# JMM内存模型

## JMM是什么

JMM（Java Memory Model），Java的内存模型。

## JMM的作用

缓存一致性的协议，用来定义数据读写的规则。

JMM定义了线程工作内存和主内存的抽象关系：线程的共享变量存储在主内存中，每个线程都有一个私有的本地工作内存。

使用volatile关键字来解决共享变量的可见性的问题。

Java内存模型是围绕着并发编程中**原子性、可见性、有序性**这三个特征来建立的。

## JMM的操作

![img](/assets/kuangstudy14fa390b-435b-4b9f-8dc6-29e685e26172.BUyteG2n.jpg)

### JMM定义了8种操作来完成（每一种操作都是原子的、不可再拆分的）。

* lock（锁定）：作用于主内存的变量，它把一个变量标识为一条线程独占的状态。
* unlock（解锁）：作用于主内存的变量，它把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定。
* read（读取）：作用于主内存的变量，它把一个变量的值从主内存传输到线程的工作内存中，以便随后的load动作使用。
* load（载入）：作用于工作内存的变量，它把read操作从主内存中得到的变量值放入工作内存的变量副本中。
* use（使用）：作用于工作内存的变量，它把工作内存中一个变量的值传递给执行引擎（每当虚拟机遇到一个需要使用到该变量的值的字节码指令时将会执行这个操作）。
* assign（赋值）：作用于工作内存的变量，它把一个从执行引擎接收到的值赋给工作内存的变量（每当虚拟机遇到一个给该变量赋值的字节码指令时执行这个操作）。
* store（存储）：作用于工作内存的变量，它把工作内存中一个变量的值传送到主内存中，以便随后的write操作使用。
* write（写入）：作用于主内存的变量，它把store操作从工作内存中得到的变量的值放入主内存的变量中。

## JMM定义的规则

### 8种操作必须满足的规则：

* 不允许read和load、store和write操作之一单独出现。（不允许一个变量从主内存读取了但工作内存不接受；或者从工作内存发起回写了但主内存不接受的情况出现）
* 不允许一个线程丢弃它的最近的assign操作。（变量在工作内存中改变了值之后，必须把该变化同步回主内存）
* 不允许一个线程无原因地（没有发生过任何assign操作）把数据从线程的工作内存同步回主内存。
* 一个新的变量只能在主内存中“诞生”，不允许在工作内存中直接使用一个未被初始化（load或assign）的变量。（就是对一个变量实施use、store操作之前，必须先执行过了load和assign操作）
* 一个变量在同一时刻只允许一条线程对其进行lock操作，但lock操作可以被同一条线程重复执行多次，多次执行lock后，只有执行相同次数的unlock操作，变量才会被解锁。
* 如果对一个变量执行lock操作，那将会清空工作内存中此变量的值，在执行引擎使用这个变量前，需要重新执行load或assign操作初始化变量的值。
* 如果一个变量事先没有被lock操作锁定，那就不允许对它执行unlock操作，也不允许去unlock一个被其他线程锁定住的变量。
* 对一个变量执行unlock操作之前，必须先把此变量同步回主内存中（执行store、write操作）。

## 并发编程的三大特性

### 原子性

一个或多个程序指令，要么全部正确执行完毕不能被打断，或者全部不执行

### 可见性

当一个线程修改了某个共享变量的值，其它线程应当能够立即看到修改后的值。

### 有序性

程序执行代码指令的顺序应当保证按照程序指定的顺序执行，即便是编译优化，也应当保证程序源语一致。

---

---
url: /Java/并发编程/1_JUC.md
---

# JUC概念

## 一、JUC简介

* 自JDK1.5出现的，JUC（Java并发包）就是java.util .concurrent工具包的简称。
* 所属位置：jre1.8.0/lib/rj.jar，在jvm中rj.jar由👉bootsrap.classloader加载器通过双亲委派机制加载到内存。

## 二、volatile关键字

* Java中各线程变量都是私有的，为 保证多线程共享数据，常用volatile 关键字修饰数据以保证共享数据在内存中的可见性。

* 各线程数据都是从主内存中copy过来的，某一线程修改完数据之后再回写到主内存中去，加volatile就相当于一旦有一个线程修改完数据的**同时**主内存数据也修改**同时**通知其它线程这个数据已被修改其它线程停止对这个数据的所有操作（加volatile可以理解为直接操作主内存）

  ```java
  public class TestVolatile {
      public static void main(String[] args){ //这个线程是用来读取flag的值的
          ThreadDemo threadDemo = new ThreadDemo();
          Thread thread = new Thread(threadDemo);
          thread.start();
          while (true){
              if (threadDemo.isFlag()){
                  System.out.println("主线程读取到的flag = " + threadDemo.isFlag());
                  break;
              }
          }
      }
  }

  @Data
  class ThreadDemo implements Runnable{ //这个线程是用来修改flag的值的
      public  boolean flag = false;
      
      //加volatile保证flag在主线程的可见性
      //public  volatile boolean flag = false;
      
      @Override
      public void run() {
          try {
              Thread.sleep(200);
          } catch (InterruptedException e) {
              e.printStackTrace();
          }
          flag = true;
          System.out.println("ThreadDemo线程修改后的flag = " + isFlag());
      }
  }
  ```

* volatile和synchronized的区别：

  volatile禁止指令重排序。 volatile不具备互斥性(当一个线程持有锁时，其他线程进不来，这就是互斥性)。 volatile保证可见性但是不具备原子性(保证原子性：1.加锁、2.使用volatile保证可见，使用CAS原子类保证原子性)。

## 三、CAS原子类

* CAS：CompareAndSet，CAS涉及3个元素:内存地址、期盼值和目标值，只有内存地址对应的值和期望的值相同时，才把内存地址对应的值修改为目标值。
* CAS的缺点：
  * 效率低：底层采用遍历比较的方式，如果期望值和当前值比较不成功则会一直循环，时间一长导致CPU开销过大
  * 可能导致ABA问题：假设2个线程读取了主内存中的共享变量。如果一个线程对主内存中的值进行了修改后，又把新值改回了原来的值，而此时另一个线程进行CAS操作，发现原值和期盼的值是一样的，就顺利的进行了CAS操作。这就是CAS引发的ABA问题

### ABA问题的解决

* juc的atomic包下提供了**AtomicStampedReference**类，它相较于普通的Atomic原子类多增加了一个**版本号**的字段（相当于svn，git的版本号机制）
* ABA问题的演示与解决

```java
/**
 * @Description : TODO 测试ABA问题的产生与解决
 */
public class ABA {

    public static void main(String[] args) throws Exception
    {
        System.out.println("ABA问题的演示---------------------------------");
        AtomicReference<Integer> atomicReference = new AtomicReference<>(10);

        new Thread(()->{
            //先将数据修改成其他值,再修改回原值
            System.out.println(atomicReference.compareAndSet(10 , 11));
            //修改回原值 :10
            System.out.println(atomicReference.compareAndSet(11, 10));
        },"A").start();

        new Thread(()->{
            try
            {
                //让当前线程停止3秒中,让 A 线程先完成ABA问题的修改,然后此线程再执行
                TimeUnit.SECONDS.sleep(3);

                System.out.println("经过ABA操作后,数据修改: "+
                        atomicReference.compareAndSet(10,11)+" 为: " + atomicReference.get());
            }
            catch(Exception e)
            {
                e.printStackTrace();
            }
        },"B").start();

        TimeUnit.SECONDS.sleep(4);
        System.out.println("\n\nABA问题的解决办法---------------------------------------------------");

        AtomicStampedReference<Integer> atomicStampedReference = new AtomicStampedReference<>(10,1);

        new Thread(()->{
			//当前值，期望值，当前版本号，期望版本号
            atomicStampedReference.compareAndSet(10,11,atomicStampedReference.getStamp(),atomicStampedReference.getStamp()+1);
                //更改值的时候也更改版本号
                System.out.println("C线程第一次修改后的版号为 ：　" +atomicStampedReference.getStamp());
                //改回原值也要更新版本号
                atomicStampedReference.compareAndSet(11,10,
                        atomicStampedReference.getStamp(),atomicStampedReference.getStamp()+1);
                System.out.println("C线程第二次修改后的版号为 ：　" +atomicStampedReference.getStamp());
        },"C").start();

        new Thread(()->{
            try
            {
                TimeUnit.SECONDS.sleep(3);
                System.out.println(
                        "D线程修改: "
                                +
                         atomicStampedReference.compareAndSet(10,11,1,atomicStampedReference.getStamp()+1)
                );

            }
            catch(Exception e)
            {
                e.printStackTrace();
            }
        },"D").start();
    }
}
```

## 四、JUC下的常见类

* JUC的atomic包下运用了CAS的AtomicBoolean、AtomicInteger、AtomicReference等原子变量类

* JUC的locks包下的AbstractQueuedSynchronizer（AQS）以及使用AQS的ReentantLock（显式锁）、ReentrantReadWriteLock

  附：运用了AQS的类还有：Semaphore、CountDownLatch、ReentantLock（显式锁）、ReentrantReadWriteLock

* JUC下的一些同步工具类：CountDownLatch（闭锁）、Semaphore（信号量）、CyclicBarrier（栅栏）、FutureTask

### JUC下的一些并发容器类：

1. 使用写时复制类 ***CopyOnWriteArrayList***，此类适合读多写少的场合,它的性能比Vector好的多。

   * 它的读取方法没有使用加锁操作，而是在使用add，set等修改操作的时候将原内容和要修改的内容复制到新的副本中，写完后，再将副本赋予原数据。

   ```java
   /**
     * Appends the specified element to the end of this list.
     *
     * @param e element to be appended to this list
     * @return {@code true} (as specified by {@link Collection#add})
     */
   public boolean add(E e) {
       final ReentrantLock lock = this.lock;
       lock.lock();
       try {
           Object[] elements = getArray();
           int len = elements.length;
           Object[] newElements = Arrays.copyOf(elements, len + 1);
           newElements[len] = e;
           setArray(newElements);
           return true;
       } finally {
           lock.unlock();
       }
   }

   /**
     * 实现CopyOnWriteArraySet需要用到的方法
     */
   public boolean addIfAbsent(E e) {
       Object[] snapshot = getArray();
       return indexOf(e, snapshot, 0, snapshot.length) >= 0 ? false :
       addIfAbsent(e, snapshot);
   }
   private static int indexOf(Object o, Object[] elements,
                                  int index, int fence) {
       if (o == null) {
           for (int i = index; i < fence; i++)
               if (elements[i] == null)
                   return i;
       } else {
           for (int i = index; i < fence; i++)
               if (o.equals(elements[i]))
                   return i;
       }
       return -1;
   }
   ```

2. ***CopyOnWriteArraySet***： 值得一提的是：CopyOnWriteArraySet使用CopyOnWriteArrayList实现。

   ```java
   private final CopyOnWriteArrayList<E> al;

   public boolean add(E e) {
       //还是遍历list，看是否有这个元素
       return al.addIfAbsent(e);
   }
   ```

3. ***ConcurrentHashMap***: 并发map，很好的支持高性能和高并发（分段锁）。

   * jdk1.7之前使用分段数组+链表实现。jdk1.8后使用 数组+链表/红黑树 实现

   * jdk1.7之前给每段数据加锁，当一个线程访问其中一段数据时，其他数据也能被其他线程访问，也是非常的高效

   * jdk1.8后使用数组+链表/红黑树实现，其扩容等机制与HashMap一样，但是控制并发的方法改为了CAS+synchronized

     synchronized锁的只是链表的首节点或红黑树的首节点，这样一来，只要节点不冲突(hash不冲突)，synchronized也不会触发，更加高效

---

---
url: /Java/JVM性能调优/01.JVM概念/JVM.md
---

# JVM及GC一些概念

## JVM

* JVM是运行在操作系统之上的，它与硬件没有直接的交互

![img](/assets/jvm001.DY7qs7Y0.png)

### 类装载器

1. 启动类加载器（Bootstrap）C++编写：Java程序入口，加载Java基础包
2. 扩展类加载器（Extension）Java编写：加载 javax 包
3. 应用类加载器（AppClassLoader）Java编写：加载我们自己写的类
4. 用户自定义加载器：对自带的三种加载器不满足的话，继承ClassLoader，自定义一个我们自己需求的类加载器

![img](/assets/jvm002.HvuNAUPQ.png)

#### ClassLoader的双亲委派机制

* 比如我们自己也写了一个名为Java.Lang.String的类，但启动会报错，因为Java类加载是从顶部的启动类加载器：Bootstrap开始加载的

> 当一个类收到了类加载请求，它首先不会尝试自己去加载这个类，而是把这个请求委派给父类去完成。
>
> 每一个层次的类加载器都是如此，因此所有的加载请求都应该传送到BootStrap中，只有当父类加载器反馈无法完成这个请求的时候（在它的加载路径下没有找到所需的Class文件），子类加载器才会尝试自己去加载。

* 正因为这样，所以Java是沙箱安全的：防止我们自己写的代码污染Java源代码

### Execution Engine执行引擎

* 相当于解释执行器：负责将.Class二进制代码文件翻译为操作系统能读懂的机器码，提交操作系统执行

### Native Interface本地接口

* 本地接口的作用是融合不同的编程语言为 Java 所用，它的初衷是融合 C/C++程序，Java 诞生的时候是 C/C++横行的时候，要想立足，必须调用 C/C++程序，于是就在内存中专门开辟了一块区域处理标记为native的代码，它的具体做法是 Native Method Stack中登记 native方法，在Execution Engine 执行时加载native libraies。
* 目前该方法使用的越来越少了，除非是与硬件有关的应用，比如通过Java程序驱动打印机或者Java系统管理生产设备，在企业级应用中已经比较少见。因为现在的架构领域间的通信很发达，比如可以使用 Socket通信，也可以使用Web Service等等，不多做介绍。

### 永久代（元空间）

* 永久存储区是一个常驻内存区域，用于存放JDK自身所携带的 Class,Interface 的元数据，也就是说它存储的是运行环境必须的类信息，被装载进此区域的数据是不会被垃圾回收器回收掉的，关闭 JVM 才会释放此区域所占用的内存。

### 栈 stack

* 先进后出，负责Java程序的运行，随着线程的创建而开始，随着线程的消亡而结束
* 在程序从第一个方法（main）开始运行时，每个方法执行的同时都会创建一个栈帧，用于存储局部变量表、操作数栈、动态链接、方法出口等信息
* 栈帧中主要保存3 类数据：
  1. 本地变量（Local Variables）：输入参数和输出参数以及方法内的变量
  2. 栈操作（Operand Stack）：记录出栈、入栈的操作
  3. 栈帧数据（Frame Data）：包括类文件、方法等等

![img](/assets/jvm003.z2tr2xGw.png)

### 堆 heap

![img](/assets/jvm004.Ct8BIsh4.png)

1. 新生代
   * 伊甸区：占新生代的8/10 （98%的GC回收都在这）
   * from区： 占新生代的 1/10 （ GC杀完伊甸区没杀干净的 ）
   * to区： 占新生代的 1/10 （ GC杀伊甸区和from区没杀干净的 ）
2. 老年代
   * GC回收完新生代后残余的（新生代活过15次的）占堆内存的 2/3
   * GC回收一般只在新生代，只有Full GC的时候才会回收老年代
3. 元空间
   * 跟堆内存没有关系

* 堆调优

  | 参数               | 含义                                    |
  | ------------------ | --------------------------------------- |
  | -Xms               | 设置初始分配大小，默认为物理内存的 1/64 |
  | -Xmx               | 最大分配内存，默认为物理内存的 1/4      |
  | -XX:PrintGCDetails | 输出详细的GC处理日志                    |

  一般jvm调优指的就是堆调优

### jvm 调优

* 如何调优：一般初始内存-Xms和最大内存-Xmx调成一样大小，避免GC和应用程序争抢内存，导致内存值忽高忽低
* 配置：在IDEA中 -> 菜单栏Run -> Eidt Configurations -> VM optioins 中输入调优参数
* OOM: java.lang.OutOfMemory：

> 比方说配置的堆内存是4M，但是我们new了一个5M的数组，就会报堆内存溢出异常

> 如果出现java.lang.OutOfMemoryError: Java heap space异常，说明Java虚拟机的堆内存不够。原因有二： （1）Java虚拟机的堆内存设置不够，可以通过参数-Xms、-Xmx来调整。 （2）代码中创建了大量大对象，并且长时间不能被垃圾收集器收集（存在被引用）。

## GC

![img](/assets/gc001.CAUAwcTH.png)

### MinorGC的过程（复制->清空->互换）

1. eden、SurvivorFrom 复制到 SurvivorTo，年龄+1 首先，当Eden区满的时候会触发第一次GC,把还活着的对象拷贝到SurvivorFrom区，当Eden区再次触发GC的时候会扫描Eden区和From区域,对这两个区域进行垃圾回收，经过这次回收后还存活的对象,则直接复制到To区域（如果有对象的年龄已经达到了老年的标准，则赋值到老年代区），同时把这些对象的年龄+1
2. 清空 eden、SurvivorFrom 然后，清空Eden和SurvivorFrom中的对象，也即复制之后有交换，谁空谁是to
3. SurvivorTo和 SurvivorFrom 互换 最后，SurvivorTo和SurvivorFrom互换，原SurvivorTo成为下一次GC时的SurvivorFrom区。部分对象会在From和To区域中复制来复制去,如此交换15次(由JVM参数MaxTenuringThreshold决定,这个参数默认是15),最终如果还是存活,就存入到老年代

### GC算法

1. 引用计数法

   what：记录的是一个对象被引用的次数（有几个箭头指向我这个地址），如果有人用我就+1，没人用我就-1，到0的时候就被回收

   缺点：1）每次对象赋值的时候都要维护计数器，且计数器本身也有一定的消耗。2）较难处理循环引用

   谁用了：微软的COM，python

2. 复制算法

   > **年轻代**中使用的Minor GC，这种GC算法用的就是复制算法

   ![img](/assets/gc002.BaDrl0At.png)

   缺点： 消耗空间（每次都从from区复制到to区，也正是from区和to区所占空间为1:1的原因）

   ​			（对象存活率非常低才适合用）

   优点：不会产生内存碎片，效率高

3. 标记清除法

   > **老年代**一般是由标记清除或者是标记清除与标记整理的混合实现

   ![img](/assets/gc003.CmTCNNLD.png)

   ![img](/assets/gc004.BbcBlZWg.png)

   what：算法分为“标记”和“清除”两个阶段，首先标记出所需要回收的对象，在标记完成后统一回收掉所有被标记的对象。

   缺点：1）效率问题：标记和清除的效率都不高(遍历)。2）空间问题：标记清除之后会产生大量不连续的内存碎片，空间碎片太多会导致大对象无法分配到足够的连续内存，从而不得不提前触发GC，甚至程序中断。

4. 标记整理法

   ![img](/assets/gc005.DxDOyquS.png)

   在标记清除后，再对存活对象内存和可用内存进行一遍有序整理（相当于解决内存碎片问题），相当于标记清除的升级版，但效率肯定更慢一点。

5. 分代收集算法

   次数上频繁收集young区，次数上较少收集old区，基本不动元空间

## Minor GC 和 Full GC

> 都发生在堆中

* Minor GC：是新生代GC，指的是发生在新生代的垃圾收集动作。由于Java对象大都是朝生夕死，所以Minor GC的发生非常频繁，一般回收速度也比较快。

* Full GC/Major GC：是老年代的GC，出现Major GC一般都伴有Minor GC，Major GC肯定比Minor GC慢很多。

* 何时发生？

  Minor GC：当jvm无法为新对象分配空间的时候就会发生Minor GC，所以new对象的频率越高，越容易触发。

  Full GC：① 当老年代空间不足的时候会触发Full GC，Full GC 会同时将老年代和新生代的垃圾进行回收。 ②当发生Minor GC的时候可能触发Full GC，由于老年代要对年轻代进行担保，由于进行一次垃圾回收之前是无法确定有多少对象存活，因此老年代并不知道自己要担保多少空间，因此老年代采用动态估值的方法：也就是上一次回收发送时晋升到老年代的对象容量的平均值作为经验值，这样就会有一个问题，当发生一次Minor GC之后，存活的对象剧增（假设），但此时老年代并没有满，但是此时平均值增加了，就会发生Full GC。

## 栈溢出和堆溢出

* 栈溢出：栈溢出是指不断的调用方法，不断的压栈，最终超出了栈允许的栈深度，就会发生栈溢出，比如递归操作没有终止，死循环。
* 栈内存溢出：对于一台服务器而言，每一个用户请求，都会产生一个线程来处理这个请求，每一个线程对应着一个栈，栈会分配内存，此时如果请求过多，这时候内存不够了，就会发生栈内存溢出。
* 堆溢出：不断的new 一个对象，一直创建新的对象， 或者直接创建的对象太大了超过了堆内存（夸张的说）。

---

---
url: /Java/JVM性能调优/02.JVM性能监控与调优/1_JVM监控与诊断工具之命令行.md
---

# JVM监控与诊断工具之命令行

## JVM监控与诊断工具之命令行

进入到安装jdk的bin目录，有一系列辅助工具。这些辅助工具用来获取目标 JVM 不同方面、不同层次的信息，帮助开发人员很好地解决Java应用程序的一些疑难杂症。

![image-20240607224417249](/assets/image-20240607224417249.ybwZA_jT.png)

官方源码地址：http://hg.openjdk.java.net/jdk/jdk11/file/1ddf9a99e4ad/src/jdk.jcmd/share/classes/sun/tools

### :mushroom:1. jps：查看正在运行的Java进程

jps(Java Process Status)：显示指定系统内所有的HotSpot虚拟机进程（查看虚拟机进程信息），可用于查询正在运行的虚拟机进程。

说明：对于本地虚拟机进程来说，进程的本地虚拟机ID与操作系统的进程ID是一致的，是唯一的。

基本使用语法为：

```sh
jps [options] [hostid]
```

直接使用jps查看进程，也可以通过追加参数，来打印额外的信息。

```sh
C:\Users\xxl>jps
16528
29216 Jps
10632 RemoteMavenServer
14168
15592 Launcher
24600 SpringbootRedisActionApplication
```

#### **options参数**

* -q：仅仅显示LVMID（local virtual machine id），即本地虚拟机唯一id。不显示主类的名称等。
* -l：输出应用程序主类的全类名 或 如果进程执行的是jar包，则输出jar完整路径。
* -m：输出虚拟机进程启动时传递给主类main()的参数。
* -v：列出虚拟机进程启动时的JVM参数。比如：-Xms20m -Xmx50m是启动程序指定的jvm参数。

说明：以上参数可以综合使用。

补充：如果某 Java 进程关闭了默认开启的UsePerfData参数（即使用参数-XX：-UsePerfData），那么jps命令（以及下面介绍的jstat）将无法探知该Java 进程。

#### **hostid参数**

RMI注册表中注册的主机名。如果想要远程监控主机上的 java 程序，需要安装 jstatd。

对于具有更严格的安全实践的网络场所而言，可能使用一个自定义的策略文件来显示对特定的可信主机或网络的访问，尽管这种技术容易受到IP地址欺诈攻击。

如果安全问题无法使用一个定制的策略文件来处理，那么最安全的操作是不运行jstatd服务器，而是在本地使用jstat和jps工具。

### :mushroom:2. jstat：查看JVM统计信息

jstat（JVM Statistics Monitoring Tool）：用于监视虚拟机各种运行状态信息的命令行工具。它可以显示本地或者远程虚拟机进程中的类装载、内存、垃圾收集、JIT编译等运行数据。在没有GUI图形界面，只提供了纯文本控制台环境的服务器上，它将是运行期定位虚拟机性能问题的首选工具。常用于检测垃圾回收问题以及内存泄漏问题。

官方文档：https://docs.oracle.com/javase/8/docs/technotes/tools/unix/jstat.html

基本使用语法为：

```sh
jstat -<option> [-t] [-h<lines>] <vmid> [<interval> [<count>]]
```

查看命令相关参数：jstat -h 或 jstat -help

其中vmid是进程id号，也就是jps之后看到的前面的号码。

#### option参数

选项option可以由以下值构成。

类装载相关的：

* -class：显示ClassLoader的相关信息：类的装载、卸载数量、总空间、类装载所消耗的时间等

垃圾回收相关的：

* -gc：显示与GC相关的堆信息。包括Eden区、两个Survivor区、老年代、永久代等的容量、已用空间、GC时间合计等信息。
* -gccapacity：显示内容与-gc基本相同，但输出主要关注Java堆各个区域使用到的最大、最小空间。
* -gcutil：显示内容与-gc基本相同，但输出主要关注已使用空间占总空间的百分比。
* -gccause：与-gcutil功能一样，但是会额外输出导致最后一次或当前正在发生的GC产生的原因。
* -gcnew：显示新生代GC状况
* -gcnewcapacity：显示内容与-gcnew基本相同，输出主要关注使用到的最大、最小空间
* -geold：显示老年代GC状况
* -gcoldcapacity：显示内容与-gcold基本相同，输出主要关注使用到的最大、最小空间
* -gcpermcapacity：显示永久代使用到的最大、最小空间。

JIT相关的：

* -compiler：显示JIT编译器编译过的方法、耗时等信息
* -printcompilation：输出已经被JIT编译的方法

interval参数： 用于指定输出统计数据的周期，单位为毫秒。即：查询间隔。

count参数： 用于指定查询的总次数。

-t参数： 可以在输出信息前加上一个Timestamp列，显示程序的运行时间。（单位：秒）

-h参数： 可以在周期性数据输出时，输出多少行数据后输出一个表头信息。

jstat还可以用来判断是否出现内存泄漏。

1. 第1步：在长时间运行的 Java 程序中，我们可以运行jstat命令连续获取多行性能数据，并取这几行数据中 OU 列（即已占用的老年代内存）的最小值。
2. 第2步：然后，我们每隔一段较长的时间重复一次上述操作，来获得多组 OU 最小值。如果这些值呈上涨趋势，则说明该 Java 程序的老年代内存已使用量在不断上涨，这意味着无法回收的对象在不断增加，因此很有可能存在内存泄漏。

#### 示例

jstat -class：显示ClassLoader的相关信息

```sh
C:\Users\xxl>jstat -class 24600
Loaded  Bytes  Unloaded  Bytes     Time
  7704 14369.9        0     0.0       7.25
```

jstat -compiler

```sh
C:\Users\xxl>jstat -compiler 24600
Compiled Failed Invalid   Time   FailedType FailedMethod
    3951      0       0     1.14          0
```

jstat -printcompilation

```sh
C:\Users\xxl>jstat -printcompilation 24600
Compiled  Size  Type Method
    3953      5    1 org/apache/catalina/core/ContainerBase getBackgroundProcessorDelay
```

jstat -gc：显示与GC相关的堆信息

```sh
C:\Users\xxl>jstat -gc 24600 1000 10
 S0C    S1C    S0U    S1U      EC       EU        OC         OU       MC     MU    CCSC   CCSU   YGC     YGCT    FGC    FGCT    CGC    CGCT     GCT
 0.0   14336.0  0.0   14336.0 130048.0 112640.0  117760.0    3820.6   36176.0 35009.9 4608.0 4194.0      6    0.035   0      0.000   4      0.005    0.040
 0.0   14336.0  0.0   14336.0 130048.0 112640.0  117760.0    3820.6   36176.0 35009.9 4608.0 4194.0      6    0.035   0      0.000   4      0.005    0.040
 0.0   14336.0  0.0   14336.0 130048.0 112640.0  117760.0    3820.6   36176.0 35009.9 4608.0 4194.0      6    0.035   0      0.000   4      0.005    0.040
```

jstat -t：增加-t参数，在输出信息前加上一个Timestamp列，显示程序的运行时间

```sh
C:\Users\xxl>jstat -gc -t 24600 1000 10
Timestamp        S0C    S1C    S0U    S1U      EC       EU        OC         OU       MC     MU    CCSC   CCSU   YGC     YGCT    FGC    FGCT    CGC    CGCT     GCT
          726.1  0.0   14336.0  0.0   14336.0 130048.0 112640.0  117760.0    3820.6   36176.0 35009.9 4608.0 4194.0      6    0.035   0      0.000   4      0.005    0.040
          727.2  0.0   14336.0  0.0   14336.0 130048.0 112640.0  117760.0    3820.6   36176.0 35009.9 4608.0 4194.0      6    0.035   0      0.000   4      0.005    0.040
          728.1  0.0   14336.0  0.0   14336.0 130048.0 112640.0  117760.0    3820.6   36176.0 35009.9 4608.0 4194.0      6    0.035   0      0.000   4      0.005    0.040
          729.2  0.0   14336.0  0.0   14336.0 130048.0 112640.0  117760.0    3820.6   36176.0 35009.9 4608.0 4194.0      6    0.035   0      0.000   4      0.005    0.040
```

jstat -t -h：增加-h参数，输出多少行数据后输出一个表头信息

```sh
C:\Users\xxl>jstat -gc -t -h3 24600 1000 10
Timestamp        S0C    S1C    S0U    S1U      EC       EU        OC         OU       MC     MU    CCSC   CCSU   YGC     YGCT    FGC    FGCT    CGC    CGCT     GCT
          780.2  0.0   14336.0  0.0   14336.0 130048.0 112640.0  117760.0    3820.6   36176.0 35009.9 4608.0 4194.0      6    0.035   0      0.000   4      0.005    0.040
          781.3  0.0   14336.0  0.0   14336.0 130048.0 112640.0  117760.0    3820.6   36176.0 35009.9 4608.0 4194.0      6    0.035   0      0.000   4      0.005    0.040
          782.3  0.0   14336.0  0.0   14336.0 130048.0 112640.0  117760.0    3820.6   36176.0 35009.9 4608.0 4194.0      6    0.035   0      0.000   4      0.005    0.040
Timestamp        S0C    S1C    S0U    S1U      EC       EU        OC         OU       MC     MU    CCSC   CCSU   YGC     YGCT    FGC    FGCT    CGC    CGCT     GCT
          783.3  0.0   14336.0  0.0   14336.0 130048.0 112640.0  117760.0    3820.6   36176.0 35009.9 4608.0 4194.0      6    0.035   0      0.000   4      0.005    0.040
          784.3  0.0   14336.0  0.0   14336.0 130048.0 112640.0  117760.0    3820.6   36176.0 35009.9 4608.0 4194.0      6    0.035   0      0.000   4      0.005    0.040
          785.3  0.0   14336.0  0.0   14336.0 130048.0 112640.0  117760.0    3820.6   36176.0 35009.9 4608.0 4194.0      6    0.035   0      0.000   4      0.005    0.040
```

gc参数表头释义

| 表头 | 含义（字节）                                    |
| ---- | ----------------------------------------------- |
| EC   | Eden区的大小                                    |
| EU   | Eden区已使用的大小                              |
| S0C  | 幸存者0区的大小                                 |
| S1C  | 幸存者1区的大小                                 |
| S0U  | 幸存者0区已使用的大小                           |
| S1U  | 幸存者1区已使用的大小                           |
| MC   | 元空间的大小                                    |
| MU   | 元空间已使用的大小                              |
| OC   | 老年代的大小                                    |
| OU   | 老年代已使用的大小                              |
| CCSC | 压缩类空间的大小                                |
| CCSU | 压缩类空间已使用的大小                          |
| YGC  | 从应用程序启动到采样时young gc的次数            |
| YGCT | 从应用程序启动到采样时young gc消耗时间（秒）    |
| FGC  | 从应用程序启动到采样时full gc的次数             |
| FGCT | 从应用程序启动到采样时的full gc的消耗时间（秒） |
| GCT  | 从应用程序启动到采样时gc的总时间                |

### :mushroom:3. jinfo：实时查看和修改JVM配置参数

jinfo(Configuration Info for Java)：查看虚拟机配置参数信息，也可用于调整虚拟机的配置参数。在很多情况卡，Java应用程序不会指定所有的Java虚拟机参数。而此时，开发人员可能不知道某一个具体的Java虚拟机参数的默认值。在这种情况下，可能需要通过查找文档获取某个参数的默认值。这个查找过程可能是非常艰难的。但有了jinfo工具，开发人员可以很方便地找到Java虚拟机参数的当前值。

基本使用语法为：

```sh
jinfo [options] pid
```

说明：java 进程ID必须要加上

#### option参数

| 选项             | 选项说明                                                     |
| ---------------- | ------------------------------------------------------------ |
| no option        | 输出全部的参数和系统属性                                     |
| -flag name       | 输出对应名称的参数                                           |
| -flag \[+-]name   | 开启或者关闭对应名称的参数 只有被标记为manageable的参数才可以被动态修改 |
| -flag name=value | 设定对应名称的参数                                           |
| -flags           | 输出全部的参数                                               |
| -sysprops        | 输出系统属性                                                 |

#### 示例

jinfo -sysprops

```sh
C:\Users\xxl>jinfo -sysprops 24600
Java System Properties:
#Sat Jun 08 00:26:36 CST 2024
sun.desktop=windows
awt.toolkit=sun.awt.windows.WToolkit
java.specification.version=11
sun.cpu.isalist=amd64
sun.jnu.encoding=GBK
...
```

jinfo -flag name

```sh
C:\Users\xxl>jinfo -flag UseParallelGC 24600
-XX:-UseParallelGC

C:\Users\xxl>jinfo -flag UseG1GC 24600
-XX:+UseG1GC
```

jinfo -flag \[+-]name

```sh
C:\Users\xxl>jinfo -flag +PrintGCDetails 24600
C:\Users\xxl>jinfo -flag PrintGCDetails 24600
-XX:+PrintGCDetails

C:\Users\xxl>jinfo -flag -PrintGCDetails 24600
C:\Users\xxl>jinfo -flag PrintGCDetails 24600
-XX:-PrintGCDetails
```

jinfo动态修改VM参数，但并非所有参数都支持动态修改，如果操作了不支持的修改的参数，将会报类似如下的异常：

```sh
Exception in thread "main" com.sun.tools.attach.AttachOperationFailedException: flag 'PrintGCDetails' cannot be changed
```

使用如下命令显示出来的参数，基本上都是支持动态修改的：

```sh
java -XX:+PrintFlagsInitial
```

拓展：

```sh
java -XX:+PrintFlagsInitial # 查看所有JVM参数启动的初始值

java -XX:+PrintFlagsFinal # 查看所有JVM参数的最终值

java -XX:+PrintCommandLineFlags # 查看哪些已经被用户或者JVM设置过的详细的XX参数的名称和值
```

### :mushroom:4. jmap：导出内存映像文件&内存使用情况

jmap（JVM Memory Map）：作用一方面是获取dump文件（堆转储快照文件，二进制文件），它还可以获取目标Java进程的内存相关信息，包括Java堆各区域的使用情况、堆中对象的统计信息、类加载信息等。开发人员可以在控制台中输入命令“jmap -help”查阅jmap工具的具体使用方式和一些标准选项配置。

官方帮助文档：https://docs.oracle.com/en/java/javase/11/tools/jmap.html

基本使用语法为：

```sh
jmap [option] <pid>
jmap [option] <executable <core>
jmap [option] [server_id@] <remote server IP or hostname>
```

#### option参数

| 选项           | 作用                                                         |
| -------------- | ------------------------------------------------------------ |
| -dump          | 生成dump文件（Java堆转储快照），-dump:live只保存堆中的存活对象 |
| -heap          | 输出整个堆空间的详细信息，包括GC的使用、堆配置信息，以及内存的使用信息等 |
| -histo         | 输出堆空间中对象的统计信息，包括类、实例数量和合计容量，-histo:live只统计堆中的存活对象 |
| -J \<flag>     | 传递参数给jmap启动的jvm                                      |
| -finalizerinfo | 显示在F-Queue中等待Finalizer线程执行finalize方法的对象，仅linux/solaris平台有效 |
| -permstat      | 以ClassLoader为统计口径输出永久代的内存状态信息，仅linux/solaris平台有效 |
| -F             | 当虚拟机进程对-dump选项没有任何响应时，强制执行生成dump文件，仅linux/solaris平台有效 |

#### 示例

-dump

dump 堆到文件，format 指定输出格式，live 指明是活着的对象，file 指定文件名

```sh
C:\Users\xxl>jmap -dump:live,format=b,file=java.hprof 12946
```

-heap

```sh
C:\Users\xxl>jmap -heap 12946 # JDK8之后已不能使用
Error: -heap option used
Cannot connect to core dump or remote debug server. Use jhsdb jmap instead
C:\Users\xxl>jhsdb jmap --heap --pid 12946
```

-histo

```sh
# 查看实例信息，输出到本地log.txt文件
C:\Users\xxl>jmap -histo 12964 > ./log.txt
```

由于jmap将访问堆中的所有对象，为了保证在此过程中不被应用线程干扰，jmap需要借助安全点机制，让所有线程停留在不改变堆中数据的状态。也就是说，由jmap导出的堆快照必定是安全点位置的。这可能导致基于该堆快照的分析结果存在偏差。

举个例子，假设在编译生成的机器码中，某些对象的生命周期在两个安全点之间，那么:live选项将无法探知到这些对象。

另外，如果某个线程长时间无法跑到安全点，jmap将一直等下去。与前面讲的jstat则不同，垃圾回收器会主动将jstat所需要的摘要数据保存至固定位置之中，而jstat只需直接读取即可。

### :mushroom:5. jhat：JDK自带堆分析工具

jhat(JVM Heap Analysis Tool)：Sun JDK提供的jhat命令与jmap命令搭配使用，用于分析jmap生成的heap dump文件（堆转储快照）。jhat内置了一个微型的HTTP/HTML服务器，生成dump文件的分析结果后，用户可以在浏览器中查看分析结果（分析虚拟机转储快照信息）。

使用了jhat命令，就启动了一个http服务，端口是7000，即http://localhost:7000/，就可以在浏览器里分析。

说明：jhat命令在JDK9、JDK10中已经被删除，官方建议用VisualVM代替。

基本适用语法：

```
jhat <option> <dumpfile>
```

#### option参数

| option参数             | 作用                                   |
| ---------------------- | -------------------------------------- |
| -stack false｜true     | 关闭｜打开对象分配调用栈跟踪           |
| -refs false｜true      | 关闭｜打开对象引用跟踪                 |
| -port port-number      | 设置jhat HTTP Server的端口号，默认7000 |
| -exclude exclude-file  | 执行对象查询时需要排除的数据成员       |
| -baseline exclude-file | 指定一个基准堆转储                     |
| -debug int             | 设置debug级别                          |
| -version               | 启动后显示版本信息就退出               |
| -J \<flag>             | 传入启动参数，比如-J-Xmx512m           |

### :mushroom:6. jstack：打印JVM中线程快照

jstack（JVM Stack Trace）：用于生成虚拟机指定进程当前时刻的线程快照（虚拟机堆栈跟踪）。线程快照就是当前虚拟机内指定进程的每一条线程正在执行的方法堆栈的集合。

生成线程快照的作用：可用于定位线程出现长时间停顿的原因，如线程间死锁、死循环、请求外部资源导致的长时间等待等问题。这些都是导致线程长时间停顿的常见原因。当线程出现停顿时，就可以用jstack显示各个线程调用的堆栈情况。

官方帮助文档：https://docs.oracle.com/en/java/javase/11/tools/jstack.html

在thread dump中，要留意下面几种状态

* 死锁，Deadlock（重点关注）
* 等待资源，Waiting on condition（重点关注）
* 等待获取监视器，Waiting on monitor entry（重点关注）
* 阻塞，Blocked（重点关注）
* 执行中，Runnable
* 暂停，Suspended
* 对象等待中，Object.wait() 或 TIMED＿WAITING
* 停止，Parked

#### option参数

| option参数 | 作用                                         |
| ---------- | -------------------------------------------- |
| -F         | 当正常输出的请求不被响应时，强制输出线程堆栈 |
| -l         | 除堆栈外，显示关于锁的附加信息               |
| -m         | 如果调用本地方法的话，可以显示C/C++的堆栈    |

### :mushroom:7. jcmd：多功能命令行

在JDK 1.7以后，新增了一个命令行工具jcmd。它是一个多功能的工具，可以用来实现前面除了jstat之外所有命令的功能。比如：用它来导出堆、内存使用、查看Java进程、导出线程信息、执行GC、JVM运行时间等。

官方帮助文档：https://docs.oracle.com/en/java/javase/11/tools/jcmd.html

jcmd拥有jmap的大部分功能，并且在Oracle的官方网站上也推荐使用jcmd命令代jmap命令

jcmd -l：列出所有的JVM进程

jcmd 进程号 help：针对指定的进程，列出支持的所有具体命令

jcmd 进程号 具体命令：显示指定进程的指令命令的数据

* Thread.print 可以替换 jstack指令
* GC.class\_histogram 可以替换 jmap中的-histo操作
* GC.heap\_dump 可以替换 jmap中的-dump操作
* GC.run 可以查看GC的执行情况
* VM.uptime 可以查看程序的总执行时间，可以替换jstat指令中的-t操作
* VM.system\_properties 可以替换 jinfo -sysprops 进程id
* VM.flags 可以获取JVM的配置参数信息

### :mushroom:8. jstatd：远程主机信息收集

之前的指令只涉及到监控本机的Java应用程序，而在这些工具中，一些监控工具也支持对远程计算机的监控（如jps、jstat）。为了启用远程监控，则需要配合使用jstatd 工具。命令jstatd是一个RMI服务端程序，它的作用相当于代理服务器，建立本地计算机与远程监控工具的通信。jstatd服务器将本机的Java应用程序信息传递到远程计算机。

---

---
url: /Java/JVM性能调优/02.JVM性能监控与调优/2_JVM监控与诊断工具之GUI.md
---

# JVM监控与诊断工具之GUI

## JVM监控及诊断工具之GUI

### :cactus:工具概述

使用命令行工具或组合能帮助获取目标Java应用性能相关的基础信息，但它们存在下列局限：

1. 无法获取方法级别的分析数据，如方法间的调用关系、各方法的调用次数和调用时间等（这对定位应用性能瓶颈至关重要）。
2. 要求用户登录到目标 Java 应用所在的宿主机上，使用起来不是很方便。
3. 分析数据通过终端输出，结果展示不够直观。

为此，JDK提供了一些内存泄漏的分析工具，如jconsole，jvisualvm等，用于辅助开发人员定位问题，但是这些工具很多时候并不足以满足快速定位的需求。所以这里我们介绍的工具相对多一些、丰富一些。

### :cactus:工具目录

#### JDK自带的工具

1. **jconsole**：Java Monitoring and Management Console是从java5开始，在JDK中自带的java监控和管理控制台，用于对JVM中内存，线程和类等的监控，包括Java应用程序的运行概况、监控堆信息、永久区（或元空间）使用情况、类加载情况等。
2. **Visual VM**：JDK自带全能工具，JDK9后成为独立项目，它提供了一个可视界面，用于查看Java虚拟机上运行的基于Java技术的应用程序的详细信息，可以分析内存快照、线程快照；监控内存变化、GC变化等。
3. **JMC**：Java Mission Control，内置Java Flight Recorder。能够以极低的性能开销收集Java虚拟机的性能数据。

#### 第三方工具

1. **MAT**：MAT（Memory Analyzer Tool）是基于Eclipse的内存分析工具，是一个快速、功能丰富的Java heap分析工具，它可以帮助我们查找内存泄漏和减少内存消耗。
2. **JProfiler**：商业软件，需要付费。功能强大。
3. **GChisto**：一款专业分析gc日志的工具。
4. **IBM（IBM Thread and Monitor Dump Analyzer for Java）**
5. **Arthas**：阿里开源的线上监控诊断产品，通过全局视角实时查看应用 load、内存、gc、线程的状态信息，并能在不修改应用代码的情况下，对业务问题进行诊断，包括查看方法调用的出入参、异常，监测方法执行耗时，类加载信息等。

#### 在线工具

1. **gceasy.io**：需要传入JVM相关的数据，可以通过jstat命令生成导出文件，上传文件分析。

   ```text
   jps
   jstack 16064 > yootk_stack.log
   ```

#### IDEA插件

1. JDK VisualGC

   ```text
   # 自定义VM选项
   --add-exports=jdk.internal.jvmstat/sun.jvmstat.monitor=ALL-UNNAMED
   --add-exports=jdk.internal.jvmstat/sun.jvmstat.monitor.event=ALL-UNNAMED
   --add-exports=jdk.internal.jvmstat/sun.jvmstat.perfdata.monitor=ALL-UNNAMED
   ```

## JConsole

jconsole：从Java5开始，在JDK中自带的java监控和管理控制台。用于对JVM中内存、线程和类等的监控，是一个基于JMX（java management extensions）的GUI性能监控工具。

官方地址：https://docs.oracle.com/javase/7/docs/technotes/guides/management/jconsole.html

## Visual VM

Visual VM是一个功能强大的多合一故障诊断和性能监控的可视化工具。它集成了多个JDK命令行工具，使用Visual VM可用于显示虚拟机进程及进程的配置和环境信息（jps，jinfo），监视应用程序的CPU、GC、堆、方法区及线程的信息（jstat、jstack）等，甚至代替JConsole。在JDK 6 Update 7以后，Visual VM便作为JDK的一部分发布（VisualVM 在JDK／bin目录下）即：它完全免费。

主要功能：

1. 生成/读取堆内存/线程快照
2. 查看JVM参数和系统属性
3. 查看运行中的虚拟机进程
4. 程序资源的实时监控
5. JMX代理连接、远程环境监控、CPU分析和内存分析

官方地址：https://visualvm.github.io/index.html

## Eclipse MAT

MAT（Memory Analyzer Tool）工具是一款功能强大的Java堆内存分析器。可以用于查找内存泄漏以及查看内存消耗情况。MAT是基于Eclipse开发的，不仅可以单独使用，还可以作为插件的形式嵌入在Eclipse中使用。是一款免费的性能分析工具，使用起来非常方便。

MAT可以分析heap dump文件。在进行内存分析时，只要获得了反映当前设备内存映像的hprof文件，通过MAT打开就可以直观地看到当前的内存信息。一般说来，这些内存信息包含：

* 所有的对象信息，包括对象实例、成员变量、存储于栈中的基本类型值和存储于堆中的其他对象的引用值。
* 所有的类信息，包括classloader、类名称、父类、静态变量等
* GCRoot到所有的这些对象的引用路径
* 线程信息，包括线程的调用栈及此线程的线程局部变量（TLS）

MAT 不是一个万能工具，它并不能处理所有类型的堆存储文件。但是比较主流的厂家和格式，例如Sun，HP，SAP 所采用的 HPROF 二进制堆存储文件，以及 IBM的 PHD 堆存储文件等都能被很好的解析。

最吸引人的还是能够快速为开发人员生成内存泄漏报表，方便定位问题和分析问题。虽然MAT有如此强大的功能，但是内存分析也没有简单到一键完成的程度，很多内存问题还是需要我们从MAT展现给我们的信息当中通过经验和直觉来判断才能发现。

官方地址： https://www.eclipse.org/mat/downloads.php

## JProfiler

在运行Java的时候有时候想测试运行时占用内存情况，这时候就需要使用测试工具查看了。在eclipse里面有 Eclipse Memory Analyzer tool（MAT）插件可以测试，而在IDEA中也有这么一个插件，就是JProfiler。JProfiler 是由 ej-technologies 公司开发的一款 Java 应用性能诊断工具。功能强大，但是收费。

特点：

* 使用方便、界面操作友好（简单且强大）
* 对被分析的应用影响小（提供模板）
* CPU，Thread，Memory分析功能尤其强大
* 支持对jdbc，noSql，jsp，servlet，socket等进行分析
* 支持多种模式（离线，在线）的分析
* 支持监控本地、远程的JVM
* 跨平台，拥有多种操作系统的安装版本

主要功能：

1. 方法调用：对方法调用的分析可以帮助您了解应用程序正在做什么，并找到提高其性能的方法。
2. 内存分配：通过分析堆上对象、引用链和垃圾收集能帮您修复内存泄露问题，优化内存使用。
3. 线程和锁：JProfiler提供多种针对线程和锁的分析视图助您发现多线程问题。
4. 高级子系统：许多性能问题都发生在更高的语义级别上。例如，对于JDBC调用，您可能希望找出执行最慢的SQL语句。JProfiler支持对这些子系统进行集成分析。

官网地址：https://www.ej-technologies.com/products/jprofiler/overview.html

数据采集方式：

JProfier数据采集方式分为两种：Sampling（样本采集）和Instrumentation（重构模式）

Instrumentation：这是JProfiler全功能模式。在class加载之前，JProfier把相关功能代码写入到需要分析的class的bytecode中，对正在运行的jvm有一定影响。

* 优点：功能强大。在此设置中，调用堆栈信息是准确的。
* 缺点：若要分析的class较多，则对应用的性能影响较大，CPU开销可能很高（取决于Filter的控制）。因此使用此模式一般配合Filter使用，只对特定的类或包进行分析

Sampling：类似于样本统计，每隔一定时间（5ms）将每个线程栈中方法栈中的信息统计出来。

* 优点：对CPU的开销非常低，对应用影响小（即使你不配置任何Filter）
* 缺点：一些数据／特性不能提供（例如：方法的调用次数、执行时间）

注：JProfiler本身没有指出数据的采集类型，这里的采集类型是针对方法调用的采集类型。因为JProfiler的绝大多数核心功能都依赖方法调用采集的数据，所以可以直接认为是JProfiler的数据采集类型。

## Arthas

上述工具都必须在服务端项目进程中配置相关的监控参数，然后工具通过远程连接到项目进程，获取相关的数据。这样就会带来一些不便，比如线上环境的网络是隔离的，本地的监控工具根本连不上线上环境。并且类似于Jprofiler这样的商业工具，是需要付费的。

那么有没有一款工具不需要远程连接，也不需要配置监控参数，同时也提供了丰富的性能监控数据呢？

阿里巴巴开源的性能分析神器Arthas应运而生。

Arthas是Alibaba开源的Java诊断工具，深受开发者喜爱。在线排查问题，无需重启；动态跟踪Java代码；实时监控JVM状态。Arthas 支持JDK 6＋，支持Linux／Mac／Windows，采用命令行交互模式，同时提供丰富的 Tab 自动补全功能，进一步方便进行问题的定位和诊断。当你遇到以下类似问题而束手无策时，Arthas可以帮助你解决：

* 这个类从哪个 jar 包加载的？为什么会报各种类相关的 Exception？
* 我改的代码为什么没有执行到？难道是我没 commit？分支搞错了？
* 遇到问题无法在线上 debug，难道只能通过加日志再重新发布吗？
* 线上遇到某个用户的数据处理有问题，但线上同样无法 debug，线下无法重现！
* 是否有一个全局视角来查看系统的运行状况？
* 有什么办法可以监控到JVM的实时运行状态？
* 怎么快速定位应用的热点，生成火焰图？

> 官方地址：https://arthas.aliyun.com/doc/quick-start.html

安装方式：如果速度较慢，可以尝试国内的码云Gitee下载。

```sh
wget https://io/arthas/arthas-boot.jar
wget https://arthas/gitee/io/arthas-boot.jar
```

Arthas只是一个java程序，所以可以直接用java -jar运行。

除了在命令行查看外，Arthas目前还支持 Web Console。在成功启动连接进程之后就已经自动启动,可以直接访问 http://127.0.0.1:8563/ 访问，页面上的操作模式和控制台完全一样。

## Java Misssion Control

在Oracle收购Sun之前，Oracle的JRockit虚拟机提供了一款叫做 JRockit Mission Control 的虚拟机诊断工具。

在Oracle收购sun之后，Oracle公司同时拥有了Hotspot和 JRockit 两款虚拟机。根据Oracle对于Java的战略，在今后的发展中，会将JRokit的优秀特性移植到Hotspot上。其中一个重要的改进就是在Sun的JDK中加入了JRockit的支持。

在Oracle JDK 7u40之后，Mission Control这款工具己经绑定在Oracle JDK中发布。

自Java11开始，本节介绍的JFR己经开源。但在之前的Java版本，JFR属于Commercial Feature通过Java虚拟机参数-XX:+UnlockCommercialFeatures 开启。

Java Mission Control（简称JMC) ， Java官方提供的性能强劲的工具，是一个用于对 Java应用程序进行管理、监视、概要分析和故障排除的工具套件。它包含一个GUI客户端以及众多用来收集Java虚拟机性能数据的插件如 JMX Console（能够访问用来存放虚拟机齐个于系统运行数据的MXBeans）以及虚拟机内置的高效 profiling 工具 Java Flight Recorder（JFR）。

JMC的另一个优点就是：采用取样，而不是传统的代码植入技术，对应用性能的影响非常非常小，完全可以开着JMC来做压测（唯一影响可能是 full gc 多了）。

官方地址：https://github.com/JDKMissionControl/jmc

## 其他工具

### Flame Graphs（火焰图）

在追求极致性能的场景下，了解你的程序运行过程中cpu在干什么很重要，火焰图就是一种非常直观的展示CPU在程序整个生命周期过程中时间分配的工具。火焰图对于现代的程序员不应该陌生，这个工具可以非常直观的显示出调用找中的CPU消耗瓶颈。

网上的关于Java火焰图的讲解大部分来自于Brenden Gregg的博客 [http://brendangregg.com/flamegraphs.html ](http://brendangregg.com/flamegraphs.html)

火焰图，简单通过x轴横条宽度来度量时间指标，y轴代表线程栈的层次。

### Tprofiler

案例： 使用JDK自身提供的工具进行JVM调优可以将下 TPS 由2.5提升到20（提升了7倍），并准确 定位系统瓶颈。

系统瓶颈有：应用里释态对象不是太多、有大量的业务线程在频繁创建一些生命周期很长的临时对象，代码里有问题。

那么，如何在海量业务代码里边准确定位这些性能代码？这里使用阿里开源工具 Tprofiler 来定位 这些性能代码，成功解决掉了GC 过于频繁的性能瓶预，并最终在上次优化的基础上将 TPS 再提升了4倍，即提升到100。

* Tprofiler配置部署、远程操作、 日志阅谈都不太复杂，操作还是很简单的。但是其却是能够 起到一针见血、立竿见影的效果，帮我们解决了GC过于频繁的性能瓶预。
* Tprofiler最重要的特性就是能够统汁出你指定时间段内 JVM 的 top method 这些 top method 极有可能就是造成你 JVM 性能瓶颈的元凶。这是其他大多数 JVM 调优工具所不具备的，包括 JRockit Mission Control。JRokit 首席开发者 Marcus Hirt 在其私人博客《 Lom Overhead Method Profiling cith Java Mission Control》下的评论中曾明确指出  JRMC 井不支持 TOP 方法的统计。

官方地址：http://github.com/alibaba/Tprofiler

### Btrace

常见的动态追踪工具有BTrace、HouseHD（该项目己经停止开发）、Greys-Anatomy（国人开发 个人开发者）、Byteman（JBoss出品），注意Java运行时追踪工具井不限干这几种，但是这几个是相对比较常用的。

BTrace是SUN Kenai 云计算开发平台下的一个开源项目，旨在为java提供安全可靠的动态跟踪分析工具。官方定义是一个 Java 平台的安全的动态追踪工具，可以用来动态地追踪一个运行的 Java 程序。BTrace动态调整目标应用程序的类以注入跟踪代码（“字节码跟踪“）。

### YourKit

### JProbe

### Spring Insight

## 推荐资料

\[1]. https://www.bilibili.com/video/BV1oE411t7pZ

\[2]. https://www.bilibili.com/video/BV1BT411A738

\[3]. https://www.yuque.com/u21195183/jvm/lv1zot#6fd1f587

---

---
url: /Java/JVM性能调优/01.JVM概念/JVM快速入门篇.md
---

# JVM快速入门篇（狂神）

> 转载于：https://www.kuangstudy.com/bbs/1557549426359590914

## 前言

1. 请你谈谈你对jvm的理解？
2. Java8虚拟机和之前的变化更新？
3. 什么是OOM？什么是栈溢出StackOverFlowError？怎么分析？
4. jvm的常见调优参数有哪些？
5. 内存快照如何抓取？怎么分析Dump文件？
6. 谈谈jvm中，类加载器你的认识？

# 一、JVM

## 1. JVM位置

![img](/assets/kuangstudy1a93cf78-8acc-468f-a88d-f2392d794f28.DBzC4zDC.jpg)

## 2. JVM体系结构

### 1. jvm结构图

![img](/assets/kuangstudyf7a6bd23-1a71-4c56-bd4d-d59834f33cef.BWUjEuKC.jpg)

### 2. jvm垃圾回收

垃圾回收，指的的堆内存的垃圾回收
![img](/assets/kuangstudyc8672b81-4091-4096-a247-4aca3a5589ba.BATAr--G.jpg)

### 3. jvm调优

![img](/assets/kuangstudy2e1aa511-50f8-48c6-ac42-a7d3722cb4db.DCI9r1FF.jpg)

# 二、类加载器

## 1. 类加载的过程

![img](/assets/kuangstudy6e9779ad-7881-488f-92b5-c9a7696f7b1c.CVDg8Z-q.jpg)

## 1. 哪些类加载器

1. 引导类加载器（BootstrapClassloader）：用C++编写，是JVM自带的类加载器；负责加载Java的核心类库。（该加载器无法直接获取）
2. 扩展类加载器（ExtClassloader）：负责加载/jre/lib/ext目录下的jar包。
3. 应用程序类加载器（AppClassloader）：负责加载java -classpath或-D java.class.path所指的目录下的类与jar包。（最常用的加载器）

## 2. 双亲委派机制

1. 类加载器接收到一个加载请求时，他会委派给他的父加载器，实际上是去他父加载器的缓存中去查找是否有该类，如果有就加载返回，如果没有则继续委派给父类加载，直到顶层类加载器。
2. 如果顶层类加载器也没有加载该类，则会依次向下查找子加载器的加载路径，如果有就加载返回，如果都没有，则会抛出异常。

## 3. 沙箱安全机制

了解

# 三、native、方法区

## 1. native

凡是使用了native关键字的，说明Java的作用范围已经达不到了，它会去调用底层的C语言的库。

1. 进入本地方法栈。
2. 调用本地方法接口。JNI

JNI的作用：扩展Java的使用，融合不同的语言为Java所用。（最初是为了融合C、C++语言）

因为Java诞生的时候，C和C++非常火，想要立足，就有必要调用C、C++的程序。

所以Java在JVM内存区域专门开辟了一块标记区域Native Method Area Stack，用来登记native方法。
在最终执行（执行引擎执行）的时候，通过JNI来加载本地方法库中的方法。

1. 编写一个多线程启动方法。

   ```
    public class Test {     public static void main(String[] args) {         new Thread(()->{         },"MyThread").start();     } }
   ```

2. 点进去查看start方法。

   ```
    // Thread类中的start方法，底层是把线程加入到线程组，然后去调用本地方法start0 public class Thread implements Runnable {         public synchronized void start() {         if (threadStatus != 0)             throw new IllegalThreadStateException();         group.add(this);         boolean started = false;         try {             start0();             started = true;         } finally {             try {                 if (!started) {                     group.threadStartFailed(this);                 }             } catch (Throwable ignore) {                 /* do nothing. If start0 threw a Throwable then                   it will be passed up the call stack */             }         }     }     private native void start0(); }
   ```

## 2. 方法区

Method Area方法区（此区域属于共享区间，所有定义的方法的信息都保存在该区域）
方法区是被所有线程共享，所有字段、方法字节码、以及一些特殊方法（如构造函数，接口代码）也在此定义。

**静态变量、常量、类信息（构造方法、接口定义）、运行时的常量池存在方法区中，但是实例变量存在堆内存中，和方法区无关。**

https://www.kuangstudy.com/bbs/1356534259912916994#header17

## 3. PC寄存器

程序计数器：Program Counter Register
每个线程都有一个程序计数器，是线程私有的，就是一个指针，指向方法区中的方法字节码（用来存储指向像一条指令的地址，也即将要执行的指令代码），在执行引擎读取下一条指令，是一个非常小的内存空间，几乎可以忽略不计。

# 四、栈（后进先出）

## 1. 栈的作用

栈内存，主管程序的运行，生命周期和线程同步；
线程结束，栈内存也就释放了，对于栈来说，**不存在垃圾回收问题**。

## 2. 栈存储的东西

8大基本类型、对象引用，实例的方法。

## 3. 栈运行原理

### 3.1 简单结构图

![img](/assets/kuangstudy6dbadc75-e9f6-42a0-909e-e571bc37e230.jiTcNTdH.jpg)

### 3.2 详细结构图

![img](/assets/kuangstudya1f29d5c-99ea-46ff-954f-0de694823f69.DVBdUo_r.jpg)

## 4. 栈+堆+方法区的交互关系

![img](/assets/kuangstudy16832064-9e2c-4de9-8778-19c3d3b9a687.BwdylMI9.jpg)

# 五、堆

## 1. 三种JVM

1. Sun公司的HotSpot。（java -version查看）
2. BEA的JRockit
3. IBM的J9VM

## 2. 堆

Heap，一个JVM只有一个堆内存，堆内存的大小是可以调节的。

类加载器读取了类文件后，一般会把什么东西放到堆中？
类、方法、常量、变量、保存我们所有引用类型的真实对象。

堆内存中细分为三个区域：

* 新生区（伊甸园区）Young/New
* 养老区 old
* 永久区 Perm

![img](/assets/kuangstudycbc2c908-bf86-4263-9848-a63bfaa11fd7.9OFtfOnD.jpg)

### 2.1 新生区

新生区又叫做伊甸园区，包括：伊甸园区、幸存0区、幸存1区。

### 2.2 永久区

这个区域是**常驻内存**的。
用来存放JDK自身携带的Class对象、Interface元数据，存储的是Java运行时的一些环境或类信息~。
这个区域**不存在垃圾回收**。
关闭JVM虚拟机就会释放这个区域的内存。

什么情况下，在永久区就崩了？

* 一个启动类，加载了大量的第三方jar包。
* Tomcat部署了太多的应用。
* 大量动态生成的反射类；不断的被加载，直到内存满，就会出现OOM

### 2.3 永久代和元空间

什么是永久代和元空间？？
方法区是一种规范，不同的虚拟机厂商可以基于规范做出不同的实现，永久代和元空间就是出于不同jdk版本的实现。
方法区就像是一个接口，永久代与元空间分别是两个不同的实现类。
只不过永久代是这个接口最初的实现类，后来这个接口一直进行变更，直到最后彻底废弃这个实现类，由新实现类—元空间进行替代。

jdk1.8之前：
![img](/assets/kuangstudy590e2fb9-b6fe-465c-b9fe-c6281130c20d.hAx2L-qF.jpg)

jdk1.8以及之后：在堆内存中，逻辑上存在，物理上不存在（元空间使用的是本地内存）
![img](/assets/kuangstudy39072ea5-d640-4d2c-b80a-925e0780a0fc.DJKSU6F8.jpg)

### 2.4 常量池

1. 在jdk1.7之前，运行时常量池+字符串常量池是存放在方法区中，HotSpot VM对方法区的实现称为永久代。
   ![img](/assets/kuangstudyfa4dab7f-9a26-4298-b00c-a4b2d4afff7e.CuEhqo85.jpg)
2. 在jdk1.7中，字符串常量池从方法区移到堆中，运行时常量池保留在方法区中。
   ![img](/assets/kuangstudyfb7f3f21-0aec-4be9-b4ed-db5c61754645.CDMyqi2U.jpg)
3. jdk1.8之后，HotSpot移除永久代，使用元空间代替；此时字符串常量池保留在堆中，运行时常量池保留在方法区中，只是实现不一样了，JVM内存变成了直接内存。
   ![img](/assets/kuangstudydcc630f0-4406-4a14-9275-af78afa6ca73.BLwg8sts.jpg)

# 六、使用JProfiler工具分析OOM原因

https://www.bilibili.com/video/BV1iJ411d7jS?p=9\&vd\_source=345a382f2c86d3441cc342a80fc25545

```
public class Test {    public static void main(String[] args) {        // 获取虚拟机试图使用的最大内存        long max = Runtime.getRuntime().maxMemory();        // 获取jvm初始化总内存        long total = Runtime.getRuntime().totalMemory();        System.out.println("max="+max+"字节\t"+(max/(double)1021/1024)+"MB\t"+(max/(double)1021/1024/1024)+"GB");        System.out.println("total="+total+"字节\t"+(total/(double)1024/1024)+"MB\t"+(total/(double)1021/1024/1024)+"GB");        // 默认情况下：分配的最大内存是电脑内存的1/4；初始化的内存是电脑内存的1/64        // 分析OOM：        //          1.尝试扩大堆内存，看结果        //          2.分析内存，看一下哪个地方出现了问题（专业工具）JProfiler        // -Xms1024m -Xmx1024m -XX:+PrintGCDetails        // 305664K+699392K = 1005056K   981.5M    }}
```

# 七、GC垃圾回收

## 1. 垃圾回收的区域

![img](/assets/kuangstudyea734fba-0acf-411a-9145-b09bf0f8c0fa.D3v2Enlg.jpg)

## 2. GC之引用计数法

![img](/assets/kuangstudyba7e21f0-b594-4d03-a045-f0bc7999d1b9.TXZtGFyU.jpg)

## 3. GC之复制算法

![img](/assets/kuangstudyfcf0c24d-0c57-444f-a3e7-75bf360afaa6.Bt345Vew.jpg)
![img](/assets/kuangstudye7037e48-3068-4698-beea-ab5bd6c93f89.r2G0lF_p.jpg)

* 好处：没有内存的碎片。
* 坏处：浪费了内存空间（多了一半空间to永远是空）。假设对象100%存活（极端情况），不适合使用复制算法。

### 3.1 使用场景

复制算法最佳使用场景：对象存活度较低的时候（新生区）

## 4. GC之标记清除压缩算法

### 4.1 标记清除

![img](/assets/kuangstudy917c317f-d10a-4a63-adcc-1d9f569ca63e.C-X93vOr.jpg)

* 优点：不需要额外的空间。
* 缺点：两次扫描，严重浪费时间，会产生内存碎片。

### 4.2 标记清除压缩

![img](/assets/kuangstudy47e8d6fc-3f99-4296-b75e-608518e1403f.DPYPruTy.jpg)

### 4.2 标记清除压缩（改进）

可以进行多次标记清除，再进行一次压缩。

## 5. GC算法总结

内存效率：复制算法>标记清除算法>标记压缩算法（时间复杂度）
内存整齐度：复制算法=标记压缩算法>标记清除算法
内存利用率：标记压缩算法=标记清除算法>复制算法

思考一个问题：难道没有最优算法吗？
答案：没有，没有最好的算法，只有最合适的算法——》GC：分代收集算法

年轻代：

* 存活率低
* 复制算法

老年代：

* 区域大：存活率高
* 标记清除（内存碎片不是太多）+标记压缩混合实现

# 八、JMM内存模型

## 1. JMM是什么

JMM（Java Memory Model），Java的内存模型。

## 2. JMM的作用

缓存一致性的协议，用来定义数据读写的规则。

JMM定义了线程工作内存和主内存的抽象关系：线程的共享变量存储在主内存中，每个线程都有一个私有的本地工作内存。

使用volatile关键字来解决共享变量的可见性的问题。

Java内存模型是围绕着并发编程中**原子性、可见性、有序性**这三个特征来建立的。

## 3. JMM的操作

![img](/assets/kuangstudy14fa390b-435b-4b9f-8dc6-29e685e26172.BUyteG2n.jpg)

### JMM定义了8种操作来完成（每一种操作都是原子的、不可再拆分的）。

* lock（锁定）：作用于主内存的变量，它把一个变量标识为一条线程独占的状态。
* unlock（解锁）：作用于主内存的变量，它把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定。
* read（读取）：作用于主内存的变量，它把一个变量的值从主内存传输到线程的工作内存中，以便随后的load动作使用。
* load（载入）：作用于工作内存的变量，它把read操作从主内存中得到的变量值放入工作内存的变量副本中。
* use（使用）：作用于工作内存的变量，它把工作内存中一个变量的值传递给执行引擎（每当虚拟机遇到一个需要使用到该变量的值的字节码指令时将会执行这个操作）。
* assign（赋值）：作用于工作内存的变量，它把一个从执行引擎接收到的值赋给工作内存的变量（每当虚拟机遇到一个给该变量赋值的字节码指令时执行这个操作）。
* store（存储）：作用于工作内存的变量，它把工作内存中一个变量的值传送到主内存中，以便随后的write操作使用。
* write（写入）：作用于主内存的变量，它把store操作从工作内存中得到的变量的值放入主内存的变量中。

## 4. JMM定义的规则

### 8种操作必须满足的规则：

* 不允许read和load、store和write操作之一单独出现。（不允许一个变量从主内存读取了但工作内存不接受；或者从工作内存发起回写了但主内存不接受的情况出现）
* 不允许一个线程丢弃它的最近的assign操作。（变量在工作内存中改变了值之后，必须把该变化同步回主内存）
* 不允许一个线程无原因地（没有发生过任何assign操作）把数据从线程的工作内存同步回主内存。
* 一个新的变量只能在主内存中“诞生”，不允许在工作内存中直接使用一个未被初始化（load或assign）的变量。（就是对一个变量实施use、store操作之前，必须先执行过了load和assign操作）
* 一个变量在同一时刻只允许一条线程对其进行lock操作，但lock操作可以被同一条线程重复执行多次，多次执行lock后，只有执行相同次数的unlock操作，变量才会被解锁。
* 如果对一个变量执行lock操作，那将会清空工作内存中此变量的值，在执行引擎使用这个变量前，需要重新执行load或assign操作初始化变量的值。
* 如果一个变量事先没有被lock操作锁定，那就不允许对它执行unlock操作，也不允许去unlock一个被其他线程锁定住的变量。
* 对一个变量执行unlock操作之前，必须先把此变量同步回主内存中（执行store、write操作）。

## 5. 并发编程的三大特性

### 1. 原子性

一个或多个程序指令，要么全部正确执行完毕不能被打断，或者全部不执行

### 2. 可见性

当一个线程修改了某个共享变量的值，其它线程应当能够立即看到修改后的值。

### 3. 有序性

程序执行代码指令的顺序应当保证按照程序指定的顺序执行，即便是编译优化，也应当保证程序源语一致。

# 总结

1. 栈存哪些东西？具体怎么存？
2. 画出一个对象在内存中实例化的过程？
3. JVM的内存模型和分区~详细到每个区放什么？
4. 堆里面的分区有哪些？Eden，form，to，老年区，说说他们的特点！
5. GC的算法有哪些？标记清除法，标记压缩，复制算法，引用计数器，怎么用的？
6. 轻GC和重GC分别在什么时候发生？

---

---
url: /Java/JVM性能调优/01.JVM概念/0_JVM体系结构.md
---

# JVM体系结构

## JVM体系结构

![img](0_JVM体系结构.assets/jvmstructure001.png)

方法区和堆是所有线程共享的内存区域；

而java栈、本地方法栈和程序计数器是运行时数据区线程私有的内存区域。

垃圾回收，指的是堆内存的垃圾回收；方法区是特殊的堆，JVM调优99%是调堆（Heap）。

1. Java堆（Heap）：是Java虚拟机所管理的内存中最大的一块。Java堆是被所有线程共享的一块内
   存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实
   例都在这里分配内存。
2. 方法区（Method Area）：方法区（Method Area）与Java堆一样，是各个线程共享的内存区
   域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数
   据。
3. 程序计数器（Program Counter Register）：程序计数器（Program Counter Register）是一块
   较小的内存空间，它的作用可以看做是当前线程所执行的字节码的行号指示器。
4. JVM栈（JVM Stacks）：与程序计数器一样，Java虚拟机栈（Java Virtual Machine Stacks）也是
   线程私有的，它的生命周期与线程相同。虚拟机栈描述的是Java方法执行的内存模型：每个方
   法被执行的时候都会同时创建一个栈帧（Stack Frame）用于存储局部变量表、操作栈、动态
   链接、方法出口等信息。每一个方法被调用直至执行完成的过程，就对应着一个栈帧在虚拟机
   栈中从入栈到出栈的过程。
5. 本地方法栈（Native Method Stacks）：本地方法栈（Native Method Stacks）与虚拟机栈所发
   挥的作用是非常相似的，其区别不过是虚拟机栈为虚拟机执行Java方法（也就是字节码）服
   务，而本地方法栈则是为虚拟机使用到的Native方法服务。

## 类的生命周期

类从被加载到虚拟机内存中开始，到GC卸载出内存为止，它的整个生命周期包括：

`加载`、`连接`、`初始化`、`使用`和`卸载`，其中**前三部是类的加载的过程**，如下图；

![687474703a2f2f737466165787437676c6106e67](/assets/jvmstructure002.B8Vo8lUk.png)

1. 加载：查找并加载类的二进制数据，在Java堆中也创建一个java.lang.Class类的对象。
2. 连接：连接又包含三块内容：验证、准备、初始化。
   * 1）验证：文件格式、元数据、字节码、符号引用验证；
   * 2）准备：为类的静态变量分配内存，并将其初始化为默认值；
   * 3）解析：把类中的符号引用转换为直接引用。
3. 初始化：为类的静态变量赋予正确的初始值。
4. 使用：new出对象程序中使用。
5. 卸载：执行垃圾回收。

注意，加载、验证、准备、初始化、卸载这五个阶段发生的顺序是确定的，而解析阶段则不一定，它在某些情况下可以在初始化阶段之后开始。另外这7个阶段通常都是互相交叉的混合式进行的，通常会在一个阶段执行的过程中调用或激活另外一个阶段。

![745698ec-c029-42a2-9203-3966b3a69f45](/assets/745698ec-c029-42a2-9203-3966b3a69f45.By60YYos.png)

## JVM加载类的过程

JVM加载一个类的过程一般分为三个阶段：加载、连接和初始化。

JVM加载一个类时，首先通过类加载器找到类的字节码然后进行验证、准备和解析，最后执行类的静态代码块和静态变量的赋值操作。

![JVM如何加载一个类](/assets/jvmstructure003.DvZViR4U.png)

### 一个类什么时候进入JVM？

1. 虚拟机启动时，执行main()方法的时候；
2. new对象的时候；
3. 读取静态字段或静态方法的时候。

### 谁来负责将Class文件加载到内存？

1. **类装载器**负责加载Class文件，Class文件在文件开头特定的文件标识（CA FE BA BE）；
2. 类装载器只负责加载，是否可以允许由执行引擎（Execution Engine）决定。

### class文件存在内存哪个位置？

类加载器从class文件抽取类信息放在`方法区`；

类信息包括：方法代码，变量名，方法名，访问权限，返回值等等。

### Class对象存储在哪里？

**堆**。每当加载器从class文件加载一个类时都会加载类信息到方法区，同时生成class对象，Class对象new对象各对象实例。

## 参考资料

[JVM从入门到精通 (yuque.com)](https://www.yuque.com/u21195183/jvm)

[JVM 学习笔记（一）内存结构\_codeali csdn jvm内存结构-CSDN博客](https://blog.csdn.net/weixin_50280576/article/details/113742011)

---

---
url: /Java/JVM性能调优/02.JVM性能监控与调优/3_JVM运行时参数.md
---

# JVM运行时参数

## 一、JVM参数选项

> 官网地址：https://docs.oracle.com/javase/8/docs/technotes/tools/windows/java.html

### 标准参数选项

```sh
> java -help
用法: java [-options] class [args...]
           (执行类)
   或  java [-options] -jar jarfile [args...]
           (执行 jar 文件)
其中选项包括:
    -d32          使用 32 位数据模型 (如果可用)
    -d64          使用 64 位数据模型 (如果可用)
    -server       选择 "server" VM
                  默认 VM 是 server.

    -cp <目录和 zip/jar 文件的类搜索路径>
    -classpath <目录和 zip/jar 文件的类搜索路径>
                  用 ; 分隔的目录, JAR 档案
                  和 ZIP 档案列表, 用于搜索类文件。
    -D<名称>=<值>
                  设置系统属性
    -verbose:[class|gc|jni]
                  启用详细输出
    -version      输出产品版本并退出
    -version:<值>
                  警告: 此功能已过时, 将在
                  未来发行版中删除。
                  需要指定的版本才能运行
    -showversion  输出产品版本并继续
    -jre-restrict-search | -no-jre-restrict-search
                  警告: 此功能已过时, 将在
                  未来发行版中删除。
                  在版本搜索中包括/排除用户专用 JRE
    -? -help      输出此帮助消息
    -X            输出非标准选项的帮助
    -ea[:<packagename>...|:<classname>]
    -enableassertions[:<packagename>...|:<classname>]
                  按指定的粒度启用断言
    -da[:<packagename>...|:<classname>]
    -disableassertions[:<packagename>...|:<classname>]
                  禁用具有指定粒度的断言
    -esa | -enablesystemassertions
                  启用系统断言
    -dsa | -disablesystemassertions
                  禁用系统断言
    -agentlib:<libname>[=<选项>]
                  加载本机代理库 <libname>, 例如 -agentlib:hprof
                  另请参阅 -agentlib:jdwp=help 和 -agentlib:hprof=help
    -agentpath:<pathname>[=<选项>]
                  按完整路径名加载本机代理库
    -javaagent:<jarpath>[=<选项>]
                  加载 Java 编程语言代理, 请参阅 java.lang.instrument
    -splash:<imagepath>
                  使用指定的图像显示启动屏幕
有关详细信息, 请参阅 http://www.oracle.com/technetwork/java/javase/documentation/index.html。
```

**Server模式和Client模式**

Hotspot JVM有两种模式，分别是server和client，分别通过-server和-client模式设置

* 32位系统上，默认使用Client类型的JVM。要想使用Server模式，机器配置至少有2个以上的CPU和2G以上的物理内存。client模式适用于对内存要求较小的桌面应用程序，默认使用Serial串行垃圾收集器

* 64位系统上，只支持server模式的JVM，适用于需要大内存的应用程序，默认使用并行垃圾收集器

官网地址：https://docs.oracle.com/javase/8/docs/technotes/guides/vm/server-class.html

如何知道系统默认使用的是那种模式呢？

通过java -version命令：可以看到Server VM字样，代表当前系统使用是Server模式

```sh
> java -version
openjdk version "11.0.22" 2024-01-16
OpenJDK Runtime Environment (build 11.0.22+7-post-Ubuntu-0ubuntu222.04.1)
OpenJDK 64-Bit Server VM (build 11.0.22+7-post-Ubuntu-0ubuntu222.04.1, mixed mode, sharing)
```

### -X参数选项

```sh
> java -X
    -Xmixed           混合模式执行 (默认)
    -Xint             仅解释模式执行
    -Xbootclasspath:<用 ; 分隔的目录和 zip/jar 文件>
                      设置搜索路径以引导类和资源
    -Xbootclasspath/a:<用 ; 分隔的目录和 zip/jar 文件>
                      附加在引导类路径末尾
    -Xbootclasspath/p:<用 ; 分隔的目录和 zip/jar 文件>
                      置于引导类路径之前
    -Xdiag            显示附加诊断消息
    -Xnoclassgc       禁用类垃圾收集
    -Xincgc           启用增量垃圾收集
    -Xloggc:<file>    将 GC 状态记录在文件中 (带时间戳)
    -Xbatch           禁用后台编译
    -Xms<size>        设置初始 Java 堆大小
    -Xmx<size>        设置最大 Java 堆大小
    -Xss<size>        设置 Java 线程堆栈大小
    -Xprof            输出 cpu 配置文件数据
    -Xfuture          启用最严格的检查, 预期将来的默认值
    -Xrs              减少 Java/VM 对操作系统信号的使用 (请参阅文档)
    -Xcheck:jni       对 JNI 函数执行其他检查
    -Xshare:off       不尝试使用共享类数据
    -Xshare:auto      在可能的情况下使用共享类数据 (默认)
    -Xshare:on        要求使用共享类数据, 否则将失败。
    -XshowSettings    显示所有设置并继续
    -XshowSettings:all
                      显示所有设置并继续
    -XshowSettings:vm 显示所有与 vm 相关的设置并继续
    -XshowSettings:properties
                      显示所有属性设置并继续
    -XshowSettings:locale
                      显示所有与区域设置相关的设置并继续
```

如何知道JVM默认使用的是混合模式呢？

同样地，通过java -version命令：可以看到 mixed mode 字样，代表当前系统使用的是混合模式。

### -XX参数选项

**Boolean类型格式**

```sh
-XX:+<option>  启用option属性
-XX:-<option>  禁用option属性
```

非Boolean类型格式

```sh
-XX:<option>=<number>  设置option数值，可以带单位如k/K/m/M/g/G
-XX:<option>=<string>  设置option字符值
```

## 二、添加JVM参数选项

eclipse和idea中在Run Configurations中VM Options中配置即可。

**运行jar包**

```sh
java -Xms100m -Xmx100m -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -jar demo.jar
```

**Tomcat运行war包**

```sh
# linux下catalina.sh添加
JAVA_OPTS="-Xms512M -Xmx1024M"
# windows下catalina.bat添加
set "JAVA_OPTS=-Xms512M -Xmx1024M"
```

**程序运行中**

```sh
# 设置Boolean类型参数
jinfo -flag [+|-]<name> <pid>
# 设置非Boolean类型参数
jinfo -flag <name>=<value> <pid>
```

## 三、常用的JVM参数选项

### 打印设置的XX选项及值

```sh
-XX:+PrintCommandLineFlags 程序运行时JVM默认设置或用户手动设置的XX选项
-XX:+PrintFlagsInitial 打印所有XX选项的默认值
-XX:+PrintFlagsFinal 打印所有XX选项的实际值
-XX:+PrintVMOptions 打印JVM的参数
```

### 堆、栈、方法区等内存大小设置

```sh
# 栈
-Xss128k <==> -XX:ThreadStackSize=128k 设置线程栈的大小为128K

# 堆
-Xms2048m <==> -XX:InitialHeapSize=2048m 设置JVM初始堆内存为2048M
-Xmx2048m <==> -XX:MaxHeapSize=2048m 设置JVM最大堆内存为2048M
-Xmn2g <==> -XX:NewSize=2g -XX:MaxNewSize=2g 设置年轻代大小为2G
-XX:SurvivorRatio=8 设置Eden区与Survivor区的比值，默认为8
-XX:NewRatio=2 设置老年代与年轻代的比例，默认为2
-XX:+UseAdaptiveSizePolicy 设置大小比例自适应，默认开启
-XX:PretenureSizeThreadshold=1024 设置让大于此阈值的对象直接分配在老年代，只对Serial、ParNew收集器有效
-XX:MaxTenuringThreshold=15 设置新生代晋升老年代的年龄限制，默认为15
-XX:TargetSurvivorRatio 设置MinorGC结束后Survivor区占用空间的期望比例

# 方法区
-XX:MetaspaceSize / -XX:PermSize=256m 设置元空间/永久代初始值为256M
-XX:MaxMetaspaceSize / -XX:MaxPermSize=256m 设置元空间/永久代最大值为256M
-XX:+UseCompressedOops 使用压缩对象
-XX:+UseCompressedClassPointers 使用压缩类指针
-XX:CompressedClassSpaceSize 设置Klass Metaspace的大小，默认1G

# 直接内存
-XX:MaxDirectMemorySize 指定DirectMemory容量，默认等于Java堆最大值
```

### OutOfMemory相关的选项

```sh
-XX:+HeapDumpOnOutMemoryError 内存出现OOM时生成Heap转储文件，两者互斥
-XX:+HeapDumpBeforeFullGC 出现FullGC时生成Heap转储文件，两者互斥
-XX:HeapDumpPath=<path> 指定heap转储文件的存储路径，默认当前目录
-XX:OnOutOfMemoryError=<path> 指定可行性程序或脚本的路径，当发生OOM时执行脚本
```

### 垃圾收集器相关选项

垃圾收集器之间的搭配使用关系

![ec1538b4-783a-40ca-9f5a-1e22afb928b6](/assets/ec1538b4-783a-40ca-9f5a-1e22afb928b6.BJEaqKBE.png)

* 红色虚线表示在jdk8时被Deprecate，jdk9时被删除

* 绿色虚线表示在jdk14时被Deprecate

* 绿色虚框表示在jdk9时被Deprecate，jdk14时被删除

```sh
# Serial回收器
-XX:+UseSerialGC  年轻代使用Serial GC， 老年代使用Serial Old GC
# ParNew回收器
-XX:+UseParNewGC  年轻代使用ParNew GC
-XX:ParallelGCThreads  设置年轻代并行收集器的线程数。
	一般地，最好与CPU数量相等，以避免过多的线程数影响垃圾收集性能。
```

$$
\begin{equation}
{\boldsymbol{ParallelGCThreads}}
\=\left{
\begin{array}{ll}
\boldsymbol{CPU\_COUNT} & (\boldsymbol{CPU\_COUNT≤8}) \\
\boldsymbol{3+(5\*CPU\_COUNT/8)} & (\boldsymbol{CPU\_COUNT>8})
\end{array}\right.
\end{equation}
$$

```sh
# Parallel回收器
-XX:+UseParallelGC  年轻代使用 Parallel Scavenge GC，互相激活
-XX:+UseParallelOldGC  老年代使用 Parallel Old GC，互相激活
-XX:ParallelGCThreads
-XX:MaxGCPauseMillis  设置垃圾收集器最大停顿时间（即STW的时间），单位是毫秒。
	为了尽可能地把停顿时间控制在MaxGCPauseMills以内，收集器在工作时会调整Java堆大小或者其他一些参数。
	对于用户来讲，停顿时间越短体验越好；但是服务器端注重高并发，整体的吞吐量。
	所以服务器端适合Parallel，进行控制。该参数使用需谨慎。
-XX:GCTimeRatio  垃圾收集时间占总时间的比例（1 / (N＋1)），用于衡量吞吐量的大小
	取值范围（0,100），默认值99，也就是垃圾回收时间不超过1％。
	与前一个-XX：MaxGCPauseMillis参数有一定矛盾性。暂停时间越长，Radio参数就容易超过设定的比例。
-XX:+UseAdaptiveSizePolicy  设置Parallel Scavenge收集器具有自适应调节策略。
	在这种模式下，年轻代的大小、Eden和Survivor的比例、晋升老年代的对象年龄等参数会被自动调整，以达到在堆大小、吞吐量和停顿时间之间的平衡点。
	在手动调优比较困难的场合，可以直接使用这种自适应的方式，仅指定虚拟机的最大堆、目标的吞吐量（GCTimeRatio）和停顿时间（MaxGCPauseMills），让虚拟机自己完成调优工作。
```

```sh
# CMS回收器
-XX:+UseConcMarkSweepGC  年轻代使用CMS GC。
	开启该参数后会自动将-XX：＋UseParNewGC打开。即：ParNew（Young区）+ CMS（Old区）+ Serial Old的组合
-XX:CMSInitiatingOccupanyFraction  设置堆内存使用率的阈值，一旦达到该阈值，便开始进行回收。JDK5及以前版本的默认值为68，DK6及以上版本默认值为92％。
	如果内存增长缓慢，则可以设置一个稍大的值，大的阈值可以有效降低CMS的触发频率，减少老年代回收的次数可以较为明显地改善应用程序性能。
	反之，如果应用程序内存使用率增长很快，则应该降低这个阈值，以避免频繁触发老年代串行收集器。
	因此通过该选项便可以有效降低Fu1l GC的执行次数。
-XX:+UseCMSInitiatingOccupancyOnly  是否动态可调，使CMS一直按CMSInitiatingOccupancyFraction设定的值启动
-XX:+UseCMSCompactAtFullCollection  用于指定在执行完Full GC后对内存空间进行压缩整理
	以此避免内存碎片的产生。不过由于内存压缩整理过程无法并发执行，所带来的问题就是停顿时间变得更长了。
-XX:CMSFullGCsBeforeCompaction  设置在执行多少次Full GC后对内存空间进行压缩整理。
-XX:ParallelCMSThreads  设置CMS的线程数量。
	CMS 默认启动的线程数是(ParallelGCThreads＋3)/4，ParallelGCThreads 是年轻代并行收集器的线程数。
	当CPU 资源比较紧张时，受到CMS收集器线程的影响，应用程序的性能在垃圾回收阶段可能会非常糟糕。
-XX:ConcGCThreads  设置并发垃圾收集的线程数，默认该值是基于ParallelGCThreads计算出来的
-XX:+CMSScavengeBeforeRemark  强制hotspot在cms remark阶段之前做一次minor gc，用于提高remark阶段的速度
-XX:+CMSClassUnloadingEnable  如果有的话，启用回收Perm 区（JDK8之前）
-XX:+CMSParallelInitialEnabled  用于开启CMS initial-mark阶段采用多线程的方式进行标记
	用于提高标记速度，在Java8开始已经默认开启
-XX:+CMSParallelRemarkEnabled  用户开启CMS remark阶段采用多线程的方式进行重新标记，默认开启
-XX:+ExplicitGCInvokesConcurrent
-XX:+ExplicitGCInvokesConcurrentAndUnloadsClasses
	这两个参数用户指定hotspot虚拟在执行System.gc()时使用CMS周期
-XX:+CMSPrecleaningEnabled  指定CMS是否需要进行Pre cleaning阶段
```

```sh
# G1回收器
-XX:+UseG1GC 手动指定使用G1收集器执行内存回收任务。
-XX:G1HeapRegionSize 设置每个Region的大小。
	值是2的幂，范围是1MB到32MB之间，目标是根据最小的Java堆大小划分出约2048个区域。默认是堆内存的1/2000。
-XX:MaxGCPauseMillis  设置期望达到的最大GC停顿时间指标（JVM会尽力实现，但不保证达到）。默认值是200ms
-XX:ParallelGCThread  设置STW时GC线程数的值。最多设置为8
-XX:ConcGCThreads  设置并发标记的线程数。将n设置为并行垃圾回收线程数（ParallelGCThreads）的1/4左右。
-XX:InitiatingHeapOccupancyPercent 设置触发并发GC周期的Java堆占用率阈值。超过此值，就触发GC。默认值是45。
-XX:G1NewSizePercent  新生代占用整个堆内存的最小百分比（默认5％）
-XX:G1MaxNewSizePercent  新生代占用整个堆内存的最大百分比（默认60％）
-XX:G1ReservePercent=10  保留内存区域，防止 to space（Survivor中的to区）溢出
```

怎么选择垃圾回收器？

* 优先让JVM自适应，调整堆的大小

* 串行收集器：内存小于100M；单核、单机程序，并且没有停顿时间的要求

* 并行收集器：多CPU、高吞吐量、允许停顿时间超过1秒

* 并发收集器：多CPU、追求低停顿时间、快速响应（比如延迟不能超过1秒，如互联网应用）

* 官方推荐G1，性能高。现在互联网的项目，基本都是使用G1

特别说明：

* 没有最好的收集器，更没有万能的收集器

* 调优永远是针对特定场景、特定需求，不存在一劳永逸的收集器

### GC日志相关选项

```sh
-XX:+PrintGC <==> -verbose:gc  打印简要日志信息
-XX:+PrintGCDetails            打印详细日志信息
-XX:+PrintGCTimeStamps  打印程序启动到GC发生的时间，搭配-XX:+PrintGCDetails使用
-XX:+PrintGCDateStamps  打印GC发生时的时间戳，搭配-XX:+PrintGCDetails使用
-XX:+PrintHeapAtGC  打印GC前后的堆信息，如下图
-Xloggc:<file> 输出GC导指定路径下的文件中
```

```sh
-XX:+TraceClassLoading  监控类的加载
-XX:+PrintGCApplicationStoppedTime  打印GC时线程的停顿时间
-XX:+PrintGCApplicationConcurrentTime  打印垃圾收集之前应用未中断的执行时间
-XX:+PrintReferenceGC 打印回收了多少种不同引用类型的引用
-XX:+PrintTenuringDistribution  打印JVM在每次MinorGC后当前使用的Survivor中对象的年龄分布
-XX:+UseGCLogFileRotation 启用GC日志文件的自动转储
-XX:NumberOfGCLogFiles=1  设置GC日志文件的循环数目
-XX:GCLogFileSize=1M  设置GC日志文件的大小
```

### 其他参数

```sh
-XX:+DisableExplicitGC  禁用hotspot执行System.gc()，默认禁用
-XX:ReservedCodeCacheSize=<n>[g|m|k]、-XX:InitialCodeCacheSize=<n>[g|m|k]  指定代码缓存的大小
-XX:+UseCodeCacheFlushing  放弃一些被编译的代码，避免代码缓存被占满时JVM切换到interpreted-only的情况
-XX:+DoEscapeAnalysis  开启逃逸分析
-XX:+UseBiasedLocking  开启偏向锁
-XX:+UseLargePages  开启使用大页面
-XX:+PrintTLAB  打印TLAB的使用情况
-XX:TLABSize  设置TLAB大小
```

## 四、通过Java代码获取JVM参数

Java提供了java.lang.management包用于监视和管理Java虚拟机和Java运行时中的其他组件，它允许本地或远程监控和管理运行的Java虚拟机。其中ManagementFactory类较为常用，另外Runtime类可获取内存、CPU核数等相关的数据。通过使用这些api，可以监控应用服务器的堆内存使用情况，设置一些阈值进行报警等处理。

```java
@Test
public void MemoryMonitor() {
    MemoryMXBean memorymbean = ManagementFactory.getMemoryMXBean();
    MemoryUsage usage = memorymbean.getHeapMemoryUsage();
    System.out.println("INIT HEAP: " + usage.getInit() / 1024 / 1024 + "m");
    System.out.println("MAX HEAP: " + usage.getMax() / 1024 / 1024 + "m");
    System.out.println("USE HEAP: " + usage.getUsed() / 1024 / 1024 + "m");
    System.out.println("\nFull Information:");
    System.out.println("Heap Memory Usage: " + memorymbean.getHeapMemoryUsage());
    System.out.println("Non-Heap Memory Usage: " + memorymbean.getNonHeapMemoryUsage());

    System.out.println("=======================通过java来获取相关系统状态============================ ");
    System.out.println("当前堆内存大小totalMemory " + (int) Runtime.getRuntime().totalMemory() / 1024 / 1024 + "m");// 当前堆内存大小
    System.out.println("空闲堆内存大小freeMemory " + (int) Runtime.getRuntime().freeMemory() / 1024 / 1024 + "m");// 空闲堆内存大小
    System.out.println("最大可用总堆内存maxMemory " + Runtime.getRuntime().maxMemory() / 1024 / 1024 + "m");// 最大可用总堆内存大小
}
```

输出

```sh
INIT HEAP: 254m
MAX HEAP: 3605m
USE HEAP: 20m

Full Information:
Heap Memory Usage: init = 266338304(260096K) used = 21319824(20820K) committed = 255328256(249344K) max = 3780640768(3692032K)
Non-Heap Memory Usage: init = 2555904(2496K) used = 10066008(9830K) committed = 10813440(10560K) max = -1(-1K)
=======================通过java来获取相关系统状态============================ 
当前堆内存大小totalMemory 243m
空闲堆内存大小freeMemory 223m
最大可用总堆内存maxMemory 3605m
```

---

---
url: /Linux/Nginx/Kong.md
---

# Kong

[01、API 网关 Kong 简介及部署--IT疯狂面试题-全网最全Java面试题库 (sunycode.com)](http://www.sunycode.com/article/2760)

https://geekdaxue.co/read/kong-docs-cn/GETTING-STARTED-introduction.md

https://cloud.tencent.com/developer/article/1938873

https://zhuanlan.zhihu.com/p/109720608

---

---
url: /Java/容器/K8S/Kubernetes.md
---
# Kubernetes

> https://kubernetes.io/

## 参考资料

https://blog.csdn.net/su2231595742/article/details/124182312

https://www.bilibili.com/video/BV1Qe411v7Z9

---

---
url: /Java/Java开发技巧/02.函数式编程/1_Lambda表达式.md
---

# Lambda表达式

## 一、概述

Lambda是JDK8中一个语法糖。他可以对某些匿名内部类的写法进行简化。它是函数式编程思想的一个重要体现。让我们不用关注是什么对象。而是更关注我们对数据进行了什么操作。

## 二、核心原则

> 可推导可省略

## 三、省略规则

* 参数类型可以省略
* 方法体只有一句代码时大括号return和唯一一句代码的分号可以省略
* 方法只有一个参数时小括号可以省略
* 以上这些规则都记不住也可以省略不记

## 四、基本格式

```java
(参数列表)->{代码}
```

### 例一：创建线程

我们在创建线程并启动时可以使用匿名内部类的写法：

```java
new Thread(new Runnable() {
    @Override
    public void run() {
        System.out.println("你知道吗 我比你想象的 更想在你身边");
    }
}).start();
```

可以使用Lambda的格式对其进行修改。修改后如下：

```java
new Thread(()->{
    System.out.println("你知道吗 我比你想象的 更想在你身边");
}).start();
```

### 例二：调用Operator方法

现有方法定义如下，其中IntBinaryOperator是一个接口。先使用匿名内部类的写法调用该方法。

```java
public static int calculateNum(IntBinaryOperator operator){
    int a = 10;
    int b = 20;
    return operator.applyAsInt(a, b);
}

public static void main(String[] args) {
    int i = calculateNum(new IntBinaryOperator() {
        @Override
        public int applyAsInt(int left, int right) {
            return left + right;
        }
    });
    System.out.println(i);
}
```

Lambda写法：

```java
public static void main(String[] args) {
    int i = calculateNum((int left, int right)->{
        return left + right;
    });
    System.out.println(i);
}
```

### 例三：调用Predicate方法

现有方法定义如下，其中IntPredicate是一个接口。先使用匿名内部类的写法调用该方法。

```java
public static void printNum(IntPredicate predicate){
    int[] arr = {1,2,3,4,5,6,7,8,9,10};
    for (int i : arr) {
        if(predicate.test(i)){
            System.out.println(i);
        }
    }
}
public static void main(String[] args) {
    printNum(new IntPredicate() {
        @Override
        public boolean test(int value) {
            return value%2==0;
        }
    });
}
```

Lambda写法：

```java
public static void main(String[] args) {
    printNum((int value)-> {
        return value%2==0;
    });
}
public static void printNum(IntPredicate predicate){
    int[] arr = {1,2,3,4,5,6,7,8,9,10};
    for (int i : arr) {
        if(predicate.test(i)){
            System.out.println(i);
        }
    }
}
```

### 例四：调用Function方法

现有方法定义如下，其中Function是一个接口。先使用匿名内部类的写法调用该方法。

```java
public static <R> R typeConver(Function<String,R> function){
    String str = "1235";
    R result = function.apply(str);
    return result;
}
public static void main(String[] args) {
    Integer result = typeConver(new Function<String, Integer>() {
        @Override
        public Integer apply(String s) {
            return Integer.valueOf(s);
        }
    });
    System.out.println(result);
}
```

Lambda写法：

```java
Integer result = typeConver((String s)->{
    return Integer.valueOf(s);
});
System.out.println(result);
```

### 例五：调用Consumer方法

现有方法定义如下，其中IntConsumer是一个接口。先使用匿名内部类的写法调用该方法。

```java
public static void foreachArr(IntConsumer consumer){
    int[] arr = {1,2,3,4,5,6,7,8,9,10};
    for (int i : arr) {
        consumer.accept(i);
    }
}
public static void main(String[] args) {
    foreachArr(new IntConsumer() {
        @Override
        public void accept(int value) {
            System.out.println(value);
        }
    });
}
```

Lambda写法：

```java
public static void main(String[] args) {
    foreachArr((int value)->{
        System.out.println(value);
    });
}
```

---

---
url: /Python/AI大模型应用开发/5_LangChain介绍.md
---

# LangChain介绍

## 一、为什么我们需要LangChain？

### 1、从API调用到完整AI应用的跨越

原始API调用的局限性

* 缺乏记忆能力
  * 每次API请求独立，无法记住历史对话。
  * 示例：先问“爱因斯坦是谁？”，再问“他是哪国人？”时，AI因缺乏上下文而无法理解“他”指代谁。
* 上下文窗口限制
  * 大模型的上下文长度有限，无法一次性处理超长文档（如500页PDF）。
  * 超出部分会被截断，导致信息丢失。
* 不擅长精确计算
  * AI并非真正的计算器，而是基于概率预测下一个Token。
  * 在数学问题上容易出错，如错误计算订单总额，存在业务风险。

### 2、传统解决方案及其痛点

传统手动编码解决思路

* 实现记忆功能
  * 使用列表手动存储历史消息，并在每次请求时将完整对话历史传给AI。
* 处理长文档知识
  * 将文档转换为向量，存入向量数据库。
  * 用户提问时，通过相似性搜索提取相关段落，仅将相关内容作为上下文传给AI。
* 解决计算问题
  * 让AI生成执行计算的代码（如Python代码），再由程序执行代码得出准确结果。
* 开发痛点
  * 需重复编写相似的提示词和处理逻辑。
  * 开发效率低，维护成本高。

***

### 3、LangChain：专为AI应用开发而生的框架

LangChain的核心理念

* 超越简单的API调用
  * 主张AI应用应具备：
    * **上下文感知**：理解对话历史。
    * **外部数据连接**：访问知识库、数据库等。
    * **工具调用能力**：借助计算器、搜索引擎等外部工具完成任务。
* 目标
  * 简化复杂AI应用的开发流程，让开发者专注于应用逻辑而非底层细节。

***

### 4、LangChain的核心组件

* 常用组件
  * **Models**：支持多种大模型。
  * **Prompts & Templates**：提示词管理与模板化。
  * **Memory**：对话状态管理。
  * **Document Loaders**：加载各类文档（PDF、Word等）。
  * **Retrievers**：从数据库中检索相关信息。
  * **Chains**：将多个步骤或组件串联成工作流。
  * **Agents**：能自主决策并调用工具的智能体。

### 5、LangChain的优势

* 简化开发
  * 示例：实现记忆
    * 无需手动维护消息列表。
    * 创建 `ConversationBufferMemory` 实例，与模型一起传入对话链。
    * 框架自动处理历史消息的传递与更新，为AI“外接记忆”。
* 统一接口（抽象层）
  * 模型抽象：
    * 无论后端是 OpenAI、百度文心、阿里通义千问还是其他模型，都可视为 `ChatModel`。
    * 切换模型时，只需修改初始化代码，业务逻辑无需大改。
  * 数据库抽象：
    * 支持多种向量数据库（如Pinecone、Chroma、Faiss等）。
    * 切换数据库时，除初始化语句外，其余代码基本无需改动。
* 提升灵活性与可维护性
  * 极大降低开发复杂度。
  * 应用更易于维护、升级和扩展。

## 二、与Assistant API的对比

### 1、OpenAI Assistants API：开箱即用的智能助手

Assistants API 简介

* 定义与定位
  * Assistants API 是 OpenAI 推出的高级 API，旨在简化智能助手的开发。
  * 它不仅调用模型，还集成了**对话历史管理、文件访问和工具调用**等能力。
* 核心优势
  1. 自动维护对话历史
     * 无需开发者手动管理消息列表，API 自动保留上下文，解决“记忆”问题。
  2. 内置工具支持
     * 支持多种外部工具，如：
       * **文件检索器**：从上传的文档中检索信息。
       * **代码解释器**：执行代码进行精确计算或数据处理。
       * **自定义函数**：调用开发者定义的外部函数，扩展AI能力。

***

### 2、LangChain：通用、灵活的AI应用开发框架

LangChain 核心理念

* 框架定位
  * LangChain 是一个**开源的应用开发框架**，旨在构建强大的AI应用。
  * 强调AI应用应能**感知上下文、连接外部数据、调用工具**进行交互。
* 核心组件
  * 提供丰富的组件（Models, Prompts, Memory, Retrievers, Chains, Agents 等）来构建复杂应用。

***

### 3、Assistants API 与 LangChain 的关键区别

#### 3.1、本质区别

| 比较维度     | **Assistants API**                               | **LangChain**                            |
| :----------- | :----------------------------------------------- | :--------------------------------------- |
| **本质**     | **API**（OpenAI 提供的服务）                     | **应用框架**（开源开发工具）             |
| **使用方式** | 发送提示，接收响应，许多细节由 OpenAI 后台处理。 | 利用组件和工具**构建**应用，控制力更强。 |

#### 3.2、支持模型范围

* Assistants API
  * 仅支持 **OpenAI 自家的模型**（如 GPT-4）。
  * 无法直接集成其他厂商的模型（如百度文心、阿里通义千问）。
* LangChain
  * **通用框架**，不隶属于任何AI服务提供商。
  * 可集成**多种来源的AI模型**，提供极大的模型选择灵活性。

#### 3.3、简易性 vs. 灵活性

* Assistants API
  * **优点**：使用简单，上手快。OpenAI 隐藏了大量技术细节，适合快速开发。
  * **缺点**：定制化能力有限，开发者难以进行深度调整。
* LangChain
  * **优点**：功能极其灵活，作为**开源框架**，可查看、修改源码，满足复杂和高度定制化的需求。
  * **缺点**：学习曲线相对陡峭，需要更多配置。

#### 3.4、应用范围

* Assistants API
  * 主要用于构建**对话型应用**，如聊天机器人、虚拟助手。
* LangChain
  * 应用范围更广，可构建从**简单聊天机器人到复杂AI系统**的各类应用，因为它能轻松整合各种外部资源和接口。

***

### 4、选择建议

* **选择 Assistants API 如果：**
  * 你希望**快速上手**，开发基于 **OpenAI 模型**的应用。
  * 项目需求相对标准，不需要深度定制。
  * 优先考虑开发效率和简单性。
* **选择 LangChain 如果：**
  * 你需要构建**更广泛、更复杂或高度定制化**的应用。
  * 希望应用能**支持多种AI模型**（包括国产大模型），并具备灵活切换的能力。
  * 需要对应用的每个环节有**完全的控制权**。

## 三、安装LangChain及了解核心模块

### 1、LangChain的安装方法

LangChain的安装非常简单，与安装其他Python包的方式完全相同。

### 安装步骤：

1. **在 Jupyter Notebook 中安装：** 在代码单元格中输入以下命令并运行：

   ```
   !pip install langchain
   ```

2. **在终端或命令提示符 (CMD) 中安装：** 打开终端，输入以下命令：

   ```
   pip install langchain
   ```

   （对于Python 3环境，可能需要使用 `pip3 install langchain`）

> **提示**：LangChain的官方文档是学习的重要资源，建议收藏以备随时查阅。

***

### 2、LangChain的核心组件

LangChain提供了许多模块化组件，这些组件是构建复杂AI应用的基石。了解这些核心组件是学习LangChain的关键。

#### 2.1、Model (模型)

* **功能**：提供语言理解和生成的核心能力，是AI应用的“大脑”。
* **说明**：可以集成来自不同AI服务提供商的模型，如OpenAI、百度文心、阿里通义等。

#### 2.2、Memory (记忆)

* **功能**：用于存储和管理对话历史或相关的上下文信息。
* **重要性**：是构建**对话型AI应用**的关键，确保应用能保持对话的连贯性和上下文感知能力。

#### 2.3、Chain (链)

* **功能**：将不同的组件（如模型、提示词、工具等）**串联**起来，形成一个有序的执行流程。
* **优势**：允许创建复杂的、多步骤的应用流程，每个组件负责处理特定的任务。

#### 2.4、Retriever (检索器)

* **功能**：负责从外部信息源（如文档、数据库、网页）中检索相关信息。
* **重要性**：极大地扩展了模型的知识面，通过提供相关上下文，显著提高回答的准确性和相关性。

#### 2.5、Agent (智能体)

* **定义**：一个基于大模型的、能够执行一系列动作的智能体。
* 核心理念：
  * 利用大模型的**推理能力**。
  * 能够根据当前任务和环境，**动态评估**并**自主决定**下一步的行动路径（例如，是直接回答、查询数据库还是执行计算）。
  * 可以调用预定义的工具（如搜索、计算）来完成复杂任务。

---

---
url: /Java/容器/Docker/2_Linux安装Docker.md
---

# Linux安装Docker

Docker 分为 CE 和 EE 两大版本。CE 即社区版（免费，支持周期 7 个月），EE 即企业版，强调安全，付费使用，支持周期 24 个月。

Docker CE 分为 `stable` `test` 和 `nightly` 三个更新频道。

官方网站上有各种环境下的 [安装指南](https://docs.docker.com/install/)，这里主要介绍 Docker CE 在 CentOS上的安装。

## 一、CentOS安装Docker

Docker CE 支持 64 位版本 CentOS 7，并且要求内核版本不低于 3.10， CentOS 7 满足最低内核的要求，所以我们在CentOS 7安装Docker。

### 1.1.卸载（可选）

如果之前安装过旧版本的Docker，可以使用下面命令卸载：

```
yum remove docker \
                  docker-client \
                  docker-client-latest \
                  docker-common \
                  docker-latest \
                  docker-latest-logrotate \
                  docker-logrotate \
                  docker-selinux \
                  docker-engine-selinux \
                  docker-engine \
                  docker-ce
```

### 1.2.安装docker

首先需要虚拟机联网，安装yum工具

```sh
yum install -y yum-utils \
           device-mapper-persistent-data \
           lvm2 --skip-broken
```

然后更新本地镜像源：

```shell
# 设置docker镜像源
yum-config-manager \
    --add-repo \
    https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
    
sed -i 's/download.docker.com/mirrors.aliyun.com\/docker-ce/g' /etc/yum.repos.d/docker-ce.repo

yum makecache fast
```

然后输入命令：

```shell
yum install -y docker-ce
```

docker-ce为社区免费版本。稍等片刻，docker即可安装成功。

### 1.3.启动docker

Docker应用需要用到各种端口，逐一去修改防火墙设置。非常麻烦，因此建议大家直接关闭防火墙！

启动docker前，一定要关闭防火墙后！！

启动docker前，一定要关闭防火墙后！！

启动docker前，一定要关闭防火墙后！！

```sh
# 关闭
systemctl stop firewalld
# 禁止开机启动防火墙
systemctl disable firewalld
```

通过命令启动docker：

```sh
systemctl start docker  # 启动docker服务

systemctl stop docker  # 停止docker服务

systemctl restart docker  # 重启docker服务
```

然后输入命令，可以查看docker版本：

```sh
docker -v
```

### 1.4.配置镜像加速

docker官方镜像仓库网速较差，我们需要设置国内镜像服务：

参考阿里云的镜像加速文档：https://cr.console.aliyun.com/cn-hangzhou/instances/mirrors

```shell
# Centos/Ubuntu 通过修改daemon配置文件/etc/docker/daemon.json来使用加速器

sudo mkdir -p /etc/docker
sudo tee /etc/docker/daemon.json <<-'EOF'
{
  "registry-mirrors": ["https://2ktfn1p8.mirror.aliyuncs.com"]
}
EOF
sudo systemctl daemon-reload
sudo systemctl restart docker
```

## 二、CentOS7安装DockerCompose

### 2.1.下载

Linux下需要通过命令下载：

```sh
# 安装
curl -L https://github.com/docker/compose/releases/download/1.23.1/docker-compose-`uname -s`-`uname -m` > /usr/local/bin/docker-compose
# 提示 command not found
apt-get install curl -y # ubuntu/debian
yum install curl -y # centos
```

本地下载，上传到`/usr/local/bin/`目录也可以。

### 2.2.修改文件权限

修改文件权限：

```sh
# 修改权限
chmod +x /usr/local/bin/docker-compose
```

### 2.3.Base自动补全命令：

```sh
# 补全命令
curl -L https://raw.githubusercontent.com/docker/compose/1.29.1/contrib/completion/bash/docker-compose > /etc/bash_completion.d/docker-compose
```

如果这里出现错误，需要修改自己的hosts文件：

```sh
echo "199.232.68.133 raw.githubusercontent.com" >> /etc/hosts
```

## 三、Docker镜像仓库

搭建镜像仓库可以基于Docker官方提供的DockerRegistry来实现。

官网地址：https://hub.docker.com/\_/registry

### 3.1.简化版镜像仓库

Docker官方的Docker Registry是一个基础版本的Docker镜像仓库，具备仓库管理的完整功能，但是没有图形化界面。

搭建方式比较简单，命令如下：

```sh
docker run -d \
    --restart=always \
    --name registry	\
    -p 5000:5000 \
    -v registry-data:/var/lib/registry \
    registry
```

命令中挂载了一个数据卷registry-data到容器内的/var/lib/registry 目录，这是私有镜像库存放数据的目录。

访问http://YourIp:5000/v2/\_catalog 可以查看当前私有镜像服务中包含的镜像

### 3.2.带有图形化界面版本

使用DockerCompose部署带有图象界面的DockerRegistry，命令如下：

```yaml
version: '3.0'
services:
  registry:
    image: registry
    volumes:
      - ./registry-data:/var/lib/registry
  ui:
    image: joxit/docker-registry-ui:static
    ports:
      - 8080:80
    environment:
      - REGISTRY_TITLE=传智教育私有仓库
      - REGISTRY_URL=http://registry:5000
    depends_on:
      - registry
```

### 3.3.配置Docker信任地址

我们的私服采用的是http协议，默认不被Docker信任，所以需要做一个配置：

```sh
# 打开要修改的文件
vi /etc/docker/daemon.json
# 添加内容：
"insecure-registries":["http://192.168.150.101:8080"]
# 重加载
systemctl daemon-reload
# 重启docker
systemctl restart docker
```

## 参考资料

https://www.bilibili.com/video/BV11L411g7U1

[👨‍👦‍👦 多容器通信 - Docker 快速入门 - 易文档 (easydoc.net)](https://docker.easydoc.net/doc/81170005/cCewZWoN/U7u8rjzF)

https://blog.csdn.net/pushiqiang/article/details/78682323

[Ubuntu Docker 安装 | 菜鸟教程 (runoob.com)](https://www.runoob.com/docker/ubuntu-docker-install.html)

修改运行中的docker容器的端口映射：https://blog.csdn.net/lcc2001/article/details/133888294

---

---
url: /Linux/Linux安装软件/Linux安装MongoDB.md
---

# Linux安装MongoDB

## 介绍

MongoDB 是一个由 C++ 语言编写的基于分布式文件存储的数据库，MongoDB 是一个介于关系数据库和非关系数据库之间的产品，是非关系数据库当中功能最丰富，最像关系数据库的。常用用于物流场景-地理位置信息存储、社交场景-储存储用户信息、物联网场景-监控数据、日志记录等，MongoDB在这些场景的应用比其他数据库有这巨大优势。

## 下载MongoDB

1、检查CentOS是否已安装过Mongodb：

```
rpm -qa | grep mongodb
service mongodb status
```

2、查看CentOS版本

```text
cat /etc/redhat-release
```

3、去到Mongodb官网，选择对应版本下载

①：去到官网下载地址：https://www.mongodb.com/try/download/community

②：选择对应版本直接下载或者选择“Copy Link”获取下载地址

## 安装MongoDB

1、去到MongoDB安装目录，下载MongoDB安装包：

```text
wget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel80-4.4.13.tgz
```

2、解压MongoDB安装包：

```text
tar -zxvf mongodb-linux-x86_64-rhel80-4.4.13.tgz
```

3、重命名解压后的MongoDB文件名：

```text
mv mongodb-linux-x86_64-rhel80-4.4.13 mongodb
```

4、在MongoDB文件夹再里创建二个文件夹：

```text
mkdir  data   //用来存放数据库数据
```

5、进入MongoDB文件下面的bin目录创建配置文件：

```text
vi  mongod.conf
dbpath=/usr/local/mongodb/data

logpath=/usr/local/mongodb/logs/mongodb.log
```

6、配置MongoDB环境变量

```
vim /etc/profile
source /etc/profile
```

添加内容

```text
export MONGODB_HOME=/usr/local/mongodb
export PATH=$PATH:$MONGODB_HOME/bin
```

7、启动MongoDB，在bin目录下执行启动命令：

```text
mongod -f /usr/local/MongoDB/mongod.conf
```

出现successfully即证明服务成功启动！或者用`ps aux | grep mongod`查看服务是否运行

8、创建一个对数据库test具有读写权限的用户

```text
db.createUser({
user:"root",
pwd:"123456",
roles:[{role:"readWrite",db:"test"}]
})
```

## 脚本安装

准备mongo安装包后执行脚本

```shell
! /bin/bash
set -e
mongo_dir=mongodb-linux-x86_64-rhel70-4.2.24.tgz
mongo_path=/home/disk2/tools/mongo

#安装Mongo
echo -e "安装mongo-------------"
if [ -e $mongo_name ];then
    if [ -d $mongo_path ];then
        echo "已存在mongo目录${mongo_path},选择执行后续操作： 1、跳过安装包解压 2、退出安装"
        read mongo_option
        if [ $mongo_option -eq 1 ];then
            echo "跳过mongo的解压安装"
        elif [ $mongo_option -eq 2 ];then
            exit            
        else
            echo "所选操作未能识别，退出安装。"
            exit
        fi
    else
        echo "解压安装包-----"
        tar -zxvf $mongo_name $1> /dev/null
        echo "开始安装-----"
        mkdir -p $mongo_path
        mv $mongo_dir/* $mongo_path
        mkdir -p $mongo_path/db
        mkdir -p $mongo_path/logs
        touch $mongo_path/logs/mongodb.log
        touch $mongo_path/bin/mongodb.conf
        echo "开始导入配置----"
cat > $mongo_path/bin/mongodb.conf << EOF
bind_ip=0.0.0.0
port=27017
logappend=true
logpath=$mongo_path/logs/mongodb.log
dbpath=$mongo_path/db
fork=true
#auth=true
EOF
    fi

if [ -e /usr/lib/systemd/system/mongodb.service ];then
    rm -f /usr/lib/systemd/system/mongodb.service
else
cat > /usr/lib/systemd/system/mongodb.service << EOF
[Unit]
Description=mongodb
After=network.target remote-fs.target nss-lookup.target

[Service]
Type=forking
ExecStart=$mongo_path/bin/mongod --config $mongo_path/bin/mongodb.conf
ExecReload=/bin/kill -s HUP \$MAINPID
ExecStop=$mongo_path/bin/mongod --shutdown --config $mongo_path/bin/mongodb.conf
PrivateTmp=true

[Install]
WantedBy=multi-user.target
EOF

chmod 754 /usr/lib/systemd/system/mongodb.service

echo "mongo加入开机自启"
systemctl daemon-reload
systemctl enable mongodb
echo "启动mongodb-------"
systemctl start mongodb
fi

echo "添加mongo用户"
$mongo_path/bin/mongo 127.0.0.1:27017/admin mongo_init.js

authline=$(grep -n "auth" ${mongo_path}/bin/mongodb.conf |cut -f1 -d":")
sed -i "${authline}c auth=true" ${mongo_path}/bin/mongodb.conf

echo "重启mongo"
systemctl restart mongodb
echo "mongo安装完成------"
else
    echo "未检测到mongo安装包，请把安装到放到安装脚本目录下"
fi
```

---

---
url: /Linux/Linux安装软件/Linux安装MySQL.md
---

# Linux安装MySQL

## Linux/UNIX 上安装 MySQL

所有平台的 MySQL 下载地址为： [MySQL 下载](https://dev.mysql.com/downloads/mysql/) 。 挑选你需要的 *MySQL Community Server* 版本及对应的平台。

> \*\*注意：\*\*安装过程我们需要通过开启管理员权限来安装，否则会由于权限不足导致无法安装。

Linux平台上推荐使用RPM包来安装Mysql,MySQL AB提供了以下RPM包的下载地址：

* **MySQL** - MySQL服务器。你需要该选项，除非你只想连接运行在另一台机器上的MySQL服务器。
* **MySQL-client** - MySQL 客户端程序，用于连接并操作Mysql服务器。
* **MySQL-devel** - 库和包含文件，如果你想要编译其它MySQL客户端，例如Perl模块，则需要安装该RPM包。
* **MySQL-shared** - 该软件包包含某些语言和应用程序需要动态装载的共享库(libmysqlclient.so\*)，使用MySQL。
* **MySQL-bench** - MySQL数据库服务器的基准和性能测试工具。

安装前，我们可以检测系统是否自带安装 MySQL:

```
rpm -qa | grep mysql
```

如果你系统有安装，那可以选择进行卸载:

```
rpm -e mysql　　// 普通删除模式
rpm -e --nodeps mysql　　// 强力删除模式，如果使用上面命令删除时，提示有依赖的其它文件，则用该命令可以对其进行强力删除
```

**安装 MySQL：**

接下来我们在 Centos7 系统下使用 yum 命令安装 MySQL，需要注意的是 CentOS 7 版本中 MySQL数据库已从默认的程序列表中移除，所以在安装前我们需要先去官网下载 Yum 资源包，下载地址为：https://dev.mysql.com/downloads/repo/yum/

```
wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm
rpm -ivh mysql-community-release-el7-5.noarch.rpm
yum update
yum install mysql-server
```

权限设置：

```
chown -R mysql:mysql /var/lib/mysql/
```

初始化 MySQL：

```
mysqld --initialize
```

启动 MySQL：

```
systemctl start mysqld
```

查看 MySQL 运行状态：

```
systemctl status mysqld
```

\*\*注意：\*\*如果我们是第一次启动 mysql 服务，mysql 服务器首先会进行初始化的配置。

> 此外,也可以使用 MariaDB 代替，MariaDB 数据库管理系统是 MySQL 的一个分支，主要由开源社区在维护，采用 GPL 授权许可。开发这个分支的原因之一是：甲骨文公司收购了 MySQL 后，有将 MySQL 闭源的潜在风险，因此社区采用分支的方式来避开这个风险。
>
> MariaDB的目的是完全兼容MySQL，包括API和命令行，使之能轻松成为MySQL的代替品。
>
> ```
> yum install mariadb-server mariadb 
> ```
>
> mariadb数据库的相关命令是：
>
> ```
> systemctl start mariadb  #启动MariaDB
> systemctl stop mariadb  #停止MariaDB
> systemctl restart mariadb  #重启MariaDB
> systemctl enable mariadb  #设置开机启动
> ```

## 验证 MySQL 安装

在成功安装 MySQL 后，一些基础表会表初始化，在服务器启动后，你可以通过简单的测试来验证 MySQL 是否工作正常。

使用 mysqladmin 工具来获取服务器状态：

使用 mysqladmin 命令来检查服务器的版本, 在 linux 上该二进制文件位于 /usr/bin 目录，在 Windows 上该二进制文件位于C:\mysql\bin 。

```
[root@host]# mysqladmin --version
```

linux上该命令将输出以下结果，该结果基于你的系统信息：

```
mysqladmin  Ver 8.23 Distrib 5.0.9-0, for redhat-linux-gnu on i386
```

如果以上命令执行后未输出任何信息，说明你的Mysql未安装成功。

***

## 使用 MySQL Client(Mysql客户端) 执行简单的SQL命令

你可以在 MySQL Client(Mysql客户端) 使用 mysql 命令连接到 MySQL 服务器上，默认情况下 MySQL 服务器的登录密码为空，所以本实例不需要输入密码。

命令如下：

```
[root@host]# mysql
```

以上命令执行后会输出 mysql>提示符，这说明你已经成功连接到Mysql服务器上，你可以在 mysql> 提示符执行SQL命令：

```
mysql> SHOW DATABASES;
+----------+
| Database |
+----------+
| mysql    |
| test     |
+----------+
2 rows in set (0.13 sec)
```

***

## Mysql设置密码

Mysql安装成功后，默认的root用户密码为空，你可以使用以下命令来创建root用户的密码：

```
[root@host]# mysqladmin -u root password "new_password";
```

现在你可以通过以下命令来连接到Mysql服务器：

```
[root@host]# mysql -u root -p
Enter password:*******
```

\*\*注意：\*\*在输入密码时，密码是不会显示了，你正确输入即可。

---

---
url: /Linux/Nginx/1_Linux安装Nginx.md
---

# Linux安装Nginx

## 一、Nginx安装

### 1、基于APT源安装

```csharp
sudo apt-get install nginx
```

安装好的文件位置：

* /usr/sbin/nginx：主程序
* /etc/nginx：存放配置文件
* /usr/share/nginx：存放静态文件
* /var/log/nginx：存放日志

基于APT安装会自动创建服务，会自动在/etc/init.d/nginx新建服务脚本

```sh
sudo nginx -s {start|stop|restart|reload|force-reload|status|configtest|rotate|upgrade}
```

### 2、通过源码包编译安装

这种方式可以自定安装指定的模块以及最新的版本，方式更灵活。

> 官方下载页面：http://nginx.org/en/download.html
>
> configure配置文件详解：http://nginx.org/en/docs/configure.html

安装gcc g++的依赖库

```sh
sudo apt-get install build-essential
sudo apt-get install libtool
```

安装pcre依赖库（http://www.pcre.org/）

```sh
sudo apt-get update
sudo apt-get install libpcre3 libpcre3-dev
```

安装zlib依赖库（http://www.zlib.net）

```sh
sudo apt-get install zlib1g-dev
```

安装SSL依赖库

```sh
sudo apt-get install openssl
```

安装Nginx

```sh
#下载最新版本：
wget http://nginx.org/download/nginx-1.13.6.tar.gz
#解压：
tar -zxvf nginx-1.13.6.tar.gz
#进入解压目录：
cd nginx-1.13.6
#配置：
./configure --prefix=/usr/local/nginx 
#编译：
make
#安装：
sudo make install
#启动：
sudo /usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.conf
注意：-c 指定配置文件的路径，不加的话，nginx会自动加载默认路径的配置文件，可以通过-h查看帮助命令。
#查看进程：
ps -ef | grep nginx
```

配置软链接

```sh
sudo ln -s /usr/local/nginx/sbin/nginx /usr/bin/nginx
```

现在就可以不用路径直接输入nginx启动。

### 3、Docker安装

https://gitee.com/xu\_xiaolong/docker-compose/blob/master/compose/nginx/run.md

```nginx
version: '3'
services:
  nginx:
    image: registry.cn-hangzhou.aliyuncs.com/zhengqing/nginx:1.21.1                 # 镜像`nginx:1.21.1`
    container_name: nginx               # 容器名为'nginx'
    restart: unless-stopped                                       # 指定容器退出后的重启策略为始终重启，但是不考虑在Docker守护进程启动时就已经停止了的容器
    volumes:                            # 数据卷挂载路径设置,将本机目录映射到容器目录
      - "./nginx/conf/nginx.conf:/etc/nginx/nginx.conf"
      - "./nginx/conf/conf.d/default.conf:/etc/nginx/conf.d/default.conf"
      - "./nginx/html:/usr/share/nginx/html"
      - "./nginx/log:/var/log/nginx"
    environment:                        # 设置环境变量,相当于docker run命令中的-e
      TZ: Asia/Shanghai
      LANG: en_US.UTF-8
    ports:                              # 映射端口
      - "80:80"
```

## 二、配置开机启动服务

### 1、方法一：

在/etc/init.d/下创建nginx文件，sudo vim /etc/init.d/nginx，内容如下：

```nginx
#!/bin/sh
 
### BEGIN INIT INFO
# Provides:      nginx
# Required-Start:    $local_fs $remote_fs $network $syslog $named
# Required-Stop:     $local_fs $remote_fs $network $syslog $named
# Default-Start:     2 3 4 5
# Default-Stop:      0 1 6
# Short-Description: starts the nginx web server
# Description:       starts nginx using start-stop-daemon
### END INIT INFO
 
PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin
DAEMON=/usr/local/nginx/sbin/nginx
NAME=nginx
DESC=nginx
 
# Include nginx defaults if available
if [ -r /etc/default/nginx ]; then
    . /etc/default/nginx
fi
 
STOP_SCHEDULE="${STOP_SCHEDULE:-QUIT/5/TERM/5/KILL/5}"
 
test -x $DAEMON || exit 0
 
. /lib/init/vars.sh
. /lib/lsb/init-functions
 
# Try to extract nginx pidfile
PID=$(cat /usr/local/nginx/conf/nginx.conf | grep -Ev '^\s*#' | awk 'BEGIN { RS="[;{}]" } { if ($1 == "pid") print $2 }' | head -n1)
if [ -z "$PID" ]; then
    PID=/run/nginx.pid
fi
 
if [ -n "$ULIMIT" ]; then
    # Set ulimit if it is set in /etc/default/nginx
    ulimit $ULIMIT
fi
 
start_nginx() {
    # Start the daemon/service
    #
    # Returns:
    #   0 if daemon has been started
    #   1 if daemon was already running
    #   2 if daemon could not be started
    start-stop-daemon --start --quiet --pidfile $PID --exec $DAEMON --test > /dev/null \
        || return 1
    start-stop-daemon --start --quiet --pidfile $PID --exec $DAEMON -- \
        $DAEMON_OPTS 2>/dev/null \
        || return 2
}
 
test_config() {
    # Test the nginx configuration
    $DAEMON -t $DAEMON_OPTS >/dev/null 2>&1
}
 
stop_nginx() {
    # Stops the daemon/service
    #
    # Return
    #   0 if daemon has been stopped
    #   1 if daemon was already stopped
    #   2 if daemon could not be stopped
    #   other if a failure occurred
    start-stop-daemon --stop --quiet --retry=$STOP_SCHEDULE --pidfile $PID --name $NAME
    RETVAL="$?"
    sleep 1
    return "$RETVAL"
}
 
reload_nginx() {
    # Function that sends a SIGHUP to the daemon/service
    start-stop-daemon --stop --signal HUP --quiet --pidfile $PID --name $NAME
    return 0
}
 
rotate_logs() {
    # Rotate log files
    start-stop-daemon --stop --signal USR1 --quiet --pidfile $PID --name $NAME
    return 0
}
 
upgrade_nginx() {
    # Online upgrade nginx executable
    # http://nginx.org/en/docs/control.html
    #
    # Return
    #   0 if nginx has been successfully upgraded
    #   1 if nginx is not running
    #   2 if the pid files were not created on time
    #   3 if the old master could not be killed
    if start-stop-daemon --stop --signal USR2 --quiet --pidfile $PID --name $NAME; then
        # Wait for both old and new master to write their pid file
        while [ ! -s "${PID}.oldbin" ] || [ ! -s "${PID}" ]; do
            cnt=`expr $cnt + 1`
            if [ $cnt -gt 10 ]; then
                return 2
            fi
            sleep 1
        done
        # Everything is ready, gracefully stop the old master
        if start-stop-daemon --stop --signal QUIT --quiet --pidfile "${PID}.oldbin" --name $NAME; then
            return 0
        else
            return 3
        fi
    else
        return 1
    fi
}
 
case "$1" in
    start)
        log_daemon_msg "Starting $DESC" "$NAME"
        start_nginx
        case "$?" in
            0|1) log_end_msg 0 ;;
            2)   log_end_msg 1 ;;
        esac
        ;;
    stop)
        log_daemon_msg "Stopping $DESC" "$NAME"
        stop_nginx
        case "$?" in
            0|1) log_end_msg 0 ;;
            2)   log_end_msg 1 ;;
        esac
        ;;
    restart)
        log_daemon_msg "Restarting $DESC" "$NAME"
 
        # Check configuration before stopping nginx
        if ! test_config; then
            log_end_msg 1 # Configuration error
            exit $?
        fi
 
        stop_nginx
        case "$?" in
            0|1)
                start_nginx
                case "$?" in
                    0) log_end_msg 0 ;;
                    1) log_end_msg 1 ;; # Old process is still running
                    *) log_end_msg 1 ;; # Failed to start
                esac
                ;;
            *)
                # Failed to stop
                log_end_msg 1
                ;;
        esac
        ;;
    reload|force-reload)
        log_daemon_msg "Reloading $DESC configuration" "$NAME"
 
        # Check configuration before stopping nginx
        #
        # This is not entirely correct since the on-disk nginx binary
        # may differ from the in-memory one, but that's not common.
        # We prefer to check the configuration and return an error
        # to the administrator.
        if ! test_config; then
            log_end_msg 1 # Configuration error
            exit $?
        fi
 
        reload_nginx
        log_end_msg $?
        ;;
    configtest|testconfig)
        log_daemon_msg "Testing $DESC configuration"
        test_config
        log_end_msg $?
        ;;
    status)
        status_of_proc -p $PID "$DAEMON" "$NAME" && exit 0 || exit $?
        ;;
    upgrade)
        log_daemon_msg "Upgrading binary" "$NAME"
        upgrade_nginx
        log_end_msg $?
        ;;
    rotate)
        log_daemon_msg "Re-opening $DESC log files" "$NAME"
        rotate_logs
        log_end_msg $?
        ;;
    *)
        echo "Usage: $NAME {start|stop|restart|reload|force-reload|status|configtest|rotate|upgrade}" >&2
        exit 3
        ;;
esac
```

设置执行权限

```sh
#设置服务脚本有执行权限
sudo chmod +x /etc/init.d/nginx
#注册服务
cd /etc/init.d/
sudo update-rc.d nginx defaults
```

### 2、方法二：

在/usr/lib/systemd/system目录下新建nginx.service文件，内容如下

```nginx
[Unit]
Description=nginx - high performance web server
After=network.target remote-fs.target nss-lookup.target
 
[Service]
Type=forking
ExecStart=/usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.conf
ExecReload=/usr/local/nginx/sbin/nginx -s reload
ExecStop=/usr/local/nginx/sbin/nginx -s stop
 
[Install]
WantedBy=multi-user.target
```

脚本解释：

* Unit：服务的启动顺序和依赖关系；
* Description：对该服务的描述；
* After：在b.target服务组启动后，再启动本服务；
* Service:服务具体执行的方式；
* ExecStart,ExecStop,ExecReload等：启动命令组，分别是服务启动时，停止时，重启时，启动前，启动后，停止后执行的命令；
* Type：服务启动类型。默认simple表示ExecStart为主进程，notify类似于simple，启动结束后会发出通知信号。另外还有forking,oneshot,dbus,idle等类型；
* Install：把服务放在哪个服务组；
* WantedBy：服务所在的服务组。

> Ubuntu：systemd的.service服务文件配置参考：https://blog.csdn.net/qq\_41035283/article/details/122793745

**设置开机自启动并启动nginx**

设置开机自启动：

```sh
systemctl enable nginx.service
```

查看是否正确启动：

```sh
systemctl list-unit-files |grep nginx
```

看下如下图就成功启动了

启动Nginx：

```sh
systemctl start nginx.service
```

其他常用命令：

* 开启开机自启动：systemctl enable nginx.service
* 停止开机自启动：systemctl disable nginx.service
* 启动 nginx 服务：systemctl start nginx.service
* 停止 nginx 服务：systemctl stop nginx.service
* 重启 nginx 服务：systemctl restart nginx.service
* 查看服务当前状态： systemctl status nginx.service
* 查看所有已启动的服务：systemctl list-units --type=service

## 问题

nginx: \[error] open() "/usr/local/nginx/logs/nginx.pid" failed (2: No such file or directory)

https://blog.csdn.net/xinxian1768/article/details/131165831

## 参考资料

\[1]. 安装教程：https://blog.csdn.net/xun527/article/details/131305955

\[2]. nginx启动命令详解：https://www.python100.com/html/89170.html

\[3]. nginx详细参数配置：https://www.cnblogs.com/hanease/p/15890509.html

---

---
url: /Linux/Shell命令/0_Linux常用命令.md
---

# Linux常用命令

### 命令大全

<https://www.runoob.com/linux/linux-command-manual.html>

### 命令操作之文件与目录

* **is**：查看文件和目录列表
* **ls -a**：查看文件和目录列表（包含隐藏文件）
* **ll**：显示出文件的权限、属主、大小等详细信息，是 `ls -l` 的简写。
* **pwd**：看当前所在目录的绝对路径
* **cd**：切换目录，`cd ..`代表切换到上一级，`./user`代表切换到当前目录下的user目录
* **touch**：创建文件
* **mkdir**：创建目录
* **rm**：删除文件
* **rm -f**：强制删除文件
* **rm -rf**：递归删除，例如`rn -rf data/`递归删除data/目录下的所有文件
* **mv**：移动文件或目录，例如`mv test.txt /usr/local`将 text.txt 移动到 /usr/local 路径下
* **解压**：`tar -zxvf 文件地址+名称`
* **解压zip格式**：`unzip 文件地址+名称`
* **重命名**：`mv 旧文件地址加名称 新文件地址加名称`

### 命令操作之文件位置

实际工作中文件多了，可能就会忘掉它的位置，这个时候就可以使用一些出文件所在目录的地址，提升了文件查找的效率。

* which 查看可执行文件的位置。

* whereis 查看文件的位置。

* find  实际搜寻硬盘查询文件名称。

1、whereis

whereis命令是定位可执行文件、源代码文件、帮助文件在文件系统中的位置。

这些文件的属性应属于原始代码，二进制文件，或是帮助文件。

whereis 程序还具有搜索源代码、指定备用搜索路径和搜索不寻常项的能力。

如果省略参数，则返回所有信息。

```
whereis bash
bash: /bin/bash /etc/bash.bashrc /usr/share/man/man1/bash.1.gz
```

说明： 以上输出信息从左至右分别为查询的程序名、bash路径、bash的man 手册页路径。

2、find

Linux下find命令在目录结构中搜索文件，并执行指定的操作。

Linux下find命令提供了相当多的查找条件，功能很强大。由于find具有强大的功能，所以它的选项也很多。

```
sudo find . -name ``"*.log"
```

说明：在当前目录查找 以.log结尾的文件。 "."代表当前目录

```
sudo find / -name ``"*.log"
```

说明：在根目录查找 以.log结尾的文件。 "/"代表当前目录

3、which

which命令的作用是，在PATH变量指定的路径中，搜索某个系统命令的位置，并且返回第一个搜索结果。

也就是说，使用which命令，就可以看到某个系统命令是否存在，以及执行的到底是哪一个位置的命令。

命令行输入`export`可以查看PATH变量

```
which java
```

说明：查看java可执行文件的地址

### 命令操作之进程和磁盘管理

* **查看进程**：`ps -ef|grep java` （查看所有java运行的进程）
* **结束进程**：`kill -9 pid`
* **查看磁盘使用情况**：`df -h`
* **查看目录占用磁盘空间大小**：`du -m | sort -nr`
* **查看内存使用情况**：`free -m|g`（m和g表示单位，二选一）
* **实时显示系统中各个进程的资源占用状况**：top
* **查看单个进程占用资源状况**：`top -d 1 -p pid`（pid 进程id）

### 命令操作之查看占用端口的进程

* lsof命令：lsof(list open files)命令可以列出当前系统中打开的所有文件，包括网络端口。可以使用lsof命令查看某个端口被哪个进程占用。具体的命令为：**sudo lsof -i :端口号**，其中端口号为需要查询的端口号。

  例如查询mysql的路径：

  ```sh
  # 查看所有端口
  netstat -nlp
  # 查看进程
  top
  # 根据端口查看进程
  lsof -i tcp:3306
  # 拿到pid后，由于linux在启动一个进程时，会在/proc下创建一个以PID命名的文件夹，该进程的信息存在该文件夹下。在该文件夹下有一个名为exe的文件，该文件指向了具体的命令文件，所以可以通过ls -l或者ll命令根据ps或top查到的PID查找命令的绝对路径
  cd /proc/15330
  ll
  # 打印的exe就是mysql的绝对路径 exe -> /usr/libexec/mysqld
  ```

* netstat命令：使用netstat命令：netstat命令可以显示网络连接、路由表和网络接口信息等。可以使用netstat命令查看某个端口被哪个进程占用。具体的命令为：**sudo netstat -tlnp | grep 端口号**，其中端口号为需要查询的端口号。

* ss命令：ss命令可以列出当前系统中打开的套接字(socket)信息，包括网络端口。可以使用ss命令查看某个端口被哪个进程占用。具体的命令为：**sudo ss -tlnp | grep 端口号**，其中端口号为需要查询的端口号。

* fuser命令：fuser命令可以查看某个文件或目录被哪个进程占用。对于网络端口，也可以使用fuser命令进行查询，具体的命令为：**sudo fuser 端口号/tcp**，其中端口号为需要查询的端口号。

* ps命令：ps命令可以列出当前系统中正在运行的进程信息。可以使用ps命令结合grep命令来查找某个进程，然后再查看该进程打开的网络端口。具体的命令为：**sudo ps -ef | grep 进程名**，其中进程名为需要查询的进程名。**ps -aux | grep 8090**，-aux 显示所有状态。

* proc文件系统：使用/proc文件系统：在Linux系统中，每个进程都有一个对应的目录，存储了该进程的相关信息。可以使用/proc文件系统来查看某个端口被哪个进程占用。具体的命令为：**sudo ls -l /proc/$(sudo lsof -t -i:端口号) | grep exe**，其中端口号为需要查询的端口号。

> 补充
>
> windows查看进程：https://blog.csdn.net/mrxutada/article/details/119203981

### 防火墙相关

firewalld与iptables命令：https://zhuanlan.zhihu.com/p/452927048

firewalld服务重载、重启、停止

```sh
# 重新加载防火墙配置
firewall-cmd --reload
# 查看状态
systemctl status firewalld.service
# unit is masked 防火墙默认是锁定的，需要取消服务的锁定
systemctl unmask firewalld
# 重启防火墙(redhat系列)
systemctl restart firewalld.service
# 临时关闭防火墙
systemctl stop firewalld.service
# 开机启用防火墙
systemctl enable firewalld.service
# 开机禁止防火墙
systemctl disable firewalld.service
# 查看firewalld的运行状态
firewall-cmd --state
```

firewalld开放端口（public）

```sh
# 公共区域设置开放21端口永久生效并写入配置文件（参数：--permanent）
# 参数：--permanent，设置即立刻生效并且写入配置文件
firewall-cmd --zone=public --add-port=21/tcp --permanent
# 开启防火墙范围
vim /etc/firewalld/zones/public.xml
port="9001-9050"
# 查询防火墙端口21是否开放
firewall-cmd --zone=public --query-port=21/tcp
# 移除开放的端口21
firewall-cmd --zone=public --remove-port=21/tcp --permanent
```

firewalld区域规则修改

```sh
# 查询防火墙规则列表
firewall-cmd --zone=public --list-all
# 查看开启的端口
firewall-cmd --zone=public --list-ports
# 新增一条区域规则httpd服务
firewall-cmd --permanent --zone=internal --add-service=http
# 验证规则
firewall-cmd  --zone=internal --list-all
```

ubuntu中查看防火墙的状态

```sh
# 在 Ubuntu 中查看防火墙的状态，可以使用 ufw 命令。ufw 是 Uncomplicated Firewall 的缩写，是 Ubuntu 默认的防火墙管理工具。
# 如果您想要查看防火墙的状态，可以使用以下命令：
sudo ufw status
# 该命令将显示防火墙的状态，如果防火墙已经开启，则会显示如下信息：
Status: active
To                         Action      From
--                         ------      ----
OpenSSH                    ALLOW       Anywhere
它会显示出所有被允许的端口，及其来源。如果您没有配置防火墙或者所有端口都已经被开放，则状态可能如下所示：
Status: inactive
# 如果防火墙被激活，并且您需要开放某些端口，请参考以下示例，使用 ufw 命令打开和关闭端口：
sudo ufw allow 80/tcp  # 开放TCP 80端口
sudo ufw deny 113     # 拒绝UDP 113端口
sudo ufw delete allow 53/tcp  # 删除TCP 53端口
# 如果您想要关闭防火墙，请使用以下命令：
sudo ufw disable
# 需要注意的是，对防火墙的任何更改都需要使用 sudo 权限进行设置。
```

### 查看Linux系统版本

1. 使用`cat`命令查看`/etc/issue`文件。这种方法适用于所有Linux发行版，但在双核CPU中，`cpuinfo`中会看到两个CPU，可能会让人误以为是两个单核的CPU，实际上应该通过`Physical Processor ID`来区分单核和双核。
2. 使用`uname`命令查看系统信息。例如，`uname -a`命令可以查看系统的内核名/版本、网络主机名、操作系统等信息。
3. 查看配置文件`/etc/issue`或`/etc/*release`文件。这些文件中会写有操作系统和版本号等信息。例如，对于Redhat/Centos系统，可以查看`/etc/redhat_release`文件；对于Debian系统，可以查看`/etc/os-release`文件。
4. 此外，还可以通过查看内存文件`/proc/version`来查看操作系统版本号、内核版本号、网络主机名等信息。

### 使用curl命令在Linux服务器调用接口

一般情况我们测试对方ip端口，都是用**telnet**命令来测试通不通

示例：telnet 127.0.0.1 8080

当服务器不支持 telnet 命令，又无法安装时，我们就可以使用 curl 命令

> curl -X POST -H "Content-Type: application/json" -d 'json请求体的内容' "需要调用的url"

curl命令是一个非常强大的命令行工具，它可以发送各种类型的HTTP请求，并且支持各种协议和认证方式。下面是curl命令的一些常用选项

* -X指定请求方法，如GET、POST、PUT等；
* -H指定请求头，如Content-Type、uthorization等；
* -d指定请求体，如JSON、XML等；
* -u指定认证信息，如用户名、密码等；
* -o指定输出文件名，用于保存响应结果。

GET请求

```sh
curl -X GET http://localhost:8080/api
```

POST请求

```sh
curl -X POST -H "Content-Type: application/json" -d '{"name":"test"}' http://localhost:8080/api
```

发送文件方式

```sh
// myxmlfile.txt为磁盘上面的xml文件，后面为请求路径
curl -X POST -H 'content-type: application/xml'  -d @/home/disk/file/myxmlfile.txt http://192.168.1.1:8080/api/uploadfile
```

### 未知的名称或服务及java.net.UnknownHostException异常

java.net.UnknownHostException异常

https://blog.csdn.net/FMC\_WBL/article/details/135737199

未知的名称或服务 DNS 配置问题

https://www.cnblogs.com/sunny3158/p/16778076.html

https://blog.csdn.net/m0\_72838865/article/details/126784090

Failed to restart network.service: Unit network.service not found.

https://blog.csdn.net/qq\_33468857/article/details/125135211

### 查看所有java应用的内存占用情况

```sh
top -b -n 1 | grep java| awk '{print "PID:"$1",mem:"$6",CPU percent:"$9"%","mem percent:"$10"%"}'
```

结果

```sh
root@qydy:~# top -b -n 1 | grep java| awk '{print "PID:"$1",mem:"$6",CPU percent:"$9"%","mem percent:"$10"%"}'

PID:21509,mem:868988,CPU percent:12.5% mem percent:1.3%
PID:327578,mem:1.9g,CPU percent:6.2% mem percent:3.1%
PID:1581117,mem:4.9g,CPU percent:6.2% mem percent:7.8%
PID:1856910,mem:873336,CPU percent:6.2% mem percent:1.3%
PID:1041,mem:720216,CPU percent:0.0% mem percent:1.1%
PID:9466,mem:1.3g,CPU percent:0.0% mem percent:2.1%
PID:39646,mem:1.6g,CPU percent:0.0% mem percent:2.5%
PID:42408,mem:1.2g,CPU percent:0.0% mem percent:1.9%
PID:108419,mem:1.5g,CPU percent:0.0% mem percent:2.4%
PID:326532,mem:1.2g,CPU percent:0.0% mem percent:2.0%
PID:339477,mem:1.6g,CPU percent:0.0% mem percent:2.6%
PID:342861,mem:1.2g,CPU percent:0.0% mem percent:1.9%
PID:343467,mem:1.5g,CPU percent:0.0% mem percent:2.4%
PID:687881,mem:1.8g,CPU percent:0.0% mem percent:2.8%
PID:719273,mem:1.4g,CPU percent:0.0% mem percent:2.3%
PID:761835,mem:1.8g,CPU percent:0.0% mem percent:2.8%
PID:800379,mem:2.3g,CPU percent:0.0% mem percent:3.7%
```

### 系统清理

当 Linux 系统中`/data`目录空间满了，可以按以下步骤逐步清理，确保安全且高效地释放空间：

#### **1. 确认空间使用情况**

首先通过`df`和`du`命令定位具体占用空间的文件或目录：

```sh
# 查看/data所在分区的总空间、已用和剩余空间
df -h /data

# 查看/data下一级目录的空间占用（从大到小排序）
du -h --max-depth=1 /data | sort -hr
```

```sh
root@ekroot-b760mds3hddr4:/home# du -h --max-depth=1 /data | sort -hr
1.6T    /data
1.5T    /data/home
41G     /data/backup
4.9G    /data/root
182M    /data/mysql
64M     /data/workspace
648K    /data/applogs
16K     /data/lost+found
8.0K    /data/log
4.0K    /data/usershare
```

#### **2. 常见可清理的文件类型**

根据排查结果，优先清理以下几类安全且占用空间大的文件：

##### **(1) 日志文件（Logs）**

日志文件（如`.log`、`.out`）通常会持续增长，且旧日志可清理：

```bash
# 进入日志目录（例如）
cd /data/logs

# 查看大日志文件（大于100M的文件）
find . -type f -size +100M -name "*.log"

# 清理旧日志（保留近7天的，删除更早的）
find . -name "*.log" -mtime +7 -delete

# 或清空正在写入的日志（避免删除文件导致程序报错）
echo "" > 正在写入的日志文件.log
```

##### **(2) 备份文件（Backups）**

旧的备份文件（如`.tar`、`.gz`、`.bak`）若已确认无用，可删除：

```bash
# 查找/data下的备份文件
find /data -type f -name "*.tar*" -o -name "*.bak"

# 删除30天前的备份（确认后执行）
find /data -type f -name "*.tar*" -mtime +30 -delete
```

##### **(3) 临时文件（Temporary Files）**

应用程序可能会遗留临时文件（如`/data/tmp`目录下的文件）：

```bash
# 清理/data下的临时文件（例如7天未修改的）
find /data -type f -path "*/tmp/*" -mtime +7 -delete
```

##### **(4) 大文件（未分类的大文件）**

直接查找`/data`下大于 1G 的文件，手动确认是否可删除：

```bash
find /data -type f -size +1G -exec ls -lh {} \;
```

（例如：过时的数据库备份、无用的视频 / 音频文件等）

#### **3. 注意事项**

* **不要盲目删除系统文件**：`/data`若包含应用程序运行依赖的文件（如数据库数据、配置文件），删除前务必确认用途（可通过`file 文件名`查看文件类型）。
* **避免删除正在使用的文件**：若文件被进程占用，直接删除可能导致程序异常，建议先停止进程再删除，或用`echo "" > 文件名`清空内容。
* **重要文件先备份**：不确定是否有用的文件，可先移动到临时目录（如`/tmp/backup`），观察一段时间后再删除。

#### **4. 长期优化建议**

* 定期通过脚本自动清理日志和旧备份（例如用`crontab`设置每周清理任务）。
* 对大文件目录（如日志、备份）单独挂载分区，避免占满`/data`。
* 启用日志轮转（`logrotate`），自动压缩和删除旧日志。

### yum配置

[CentOS7配置阿里yum源 超详细！！！-阿里云开发者社区 (aliyun.com)](https://developer.aliyun.com/article/1366034)

[CentOS7配置阿里云镜像源（超详细过程）\_centos7 一键配置 配置阿里源脚本-CSDN博客](https://blog.csdn.net/KingveyLee/article/details/114984534)

---

---
url: /Linux/Shell命令/4_Linux服务器抓包分析HTTP请求.md
---

# Linux服务器抓包分析HTTP请求

说到抓包分析，最简单的方法当然是在客户端直接安装Wireshark或Fiddler，这些工具的使用率很高且有很多成熟的教程。但在服务端如何做呢？可以使用tcpdump抓包，并使用Wireshark来分析HTTP请求的简单有效方法。

### 一、服务端抓包

使用 tcpdump 抓包，首先，在服务器上安装 tcpdump，以 Ubuntu 为例运行以下命令

```
apt install -y tcpdump
```

然后，使用 tcpdump 进行抓包

```
tcpdump -tttt -X -vv -s0 -w 80.cap 'tcp port 80'
```

以下是各个参数的说明：

### 二、使用 wireshark 分析

打开 [Wireshark 下载地址](https://www.wireshark.org/download.html)，选择对应的版本下载安装。

我们从服务器上下载这个80.cap文件到自己电脑上，使用 Wireshark 打开，会看到捕获的TCP流量数据。

接下来，可以按照以下步骤进行HTTP请求的分析：

1. 使用Wireshark的过滤功能，只显示HTTP请求。在过滤框中输入`http`，这样Wireshark将只显示与HTTP协议相关的数据包。
2. 在Wireshark的数据包列表中，可以点击选择一个HTTP请求数据包，然后在右侧的详细信息窗口中查看更多的细节。可以展开各个协议分层并查看具体的字段信息，比如源IP和目标IP，源端口和目标端口，HTTP方法和URL路径等。
3. 如果想查看请求的内容，可以在详细信息窗口中找到HTTP协议分层，并展开`Hypertext Transfer Protocol`字段。在这里，将看到请求头和请求体的详细信息，包括请求方法、请求头部、Cookie、请求参数等。
4. 如果想进一步分析响应内容，可以选择一条HTTP响应数据包，然后在详细信息窗口中查看响应的具体信息。可以展开HTTP协议分层的`Hypertext Transfer Protocol with Privacy`字段，其中包含了响应的状态码、响应头部信息以及响应的正文内容。
5. Wireshark还提供了一些强大的统计功能，可帮助分析HTTP请求的性能指标。可以使用`Statistics`菜单中的各项功能，如"HTTP"、"Endpoints"、"Conversations"等，来查看请求和响应的统计数据，如请求数量、包大小、传输时间等。

通过使用Wireshark分析HTTP请求，能够深入了解请求的细节，包括头部信息、参数、Cookie等。这对于调试和性能优化非常有帮助。另外，Wireshark还支持导出分析结果以及生成报告，方便与团队或上级分享分析结果。

### 参考资料

https://cloud.tencent.com/developer/article/2301789

https://zhuanlan.zhihu.com/p/53857010

https://zhuanlan.zhihu.com/p/34839086

---

---
url: /Linux/Linux安装软件/Linux下安装Graylog.md
---

# Linux下安装Graylog

https://www.cnblogs.com/luckyleaf/p/11157905.html

https://blog.csdn.net/qq\_37837722/article/details/90482512

https://blog.csdn.net/liukuan73/article/details/52525431

https://www.yingtwo.com/article/7990658.html

https://www.cnblogs.com/majiang/p/14380803.html

https://www.jianshu.com/p/0af261b76d0c

graylog\_deflector索引作用

```
要配置Graylog以使用graylog_deflector索引并根据策略将数据转发到其他索引中，您需要进行以下步骤：

1. 登录到Graylog的Web界面。
2. 导航到"System"（系统）菜单，然后选择"Indices"（索引）选项。
3. 在索引列表中，找到名为graylog_deflector的索引。如果不存在，请点击"Create index set"（创建索引集）按钮创建一个新的索引集，并将其命名为graylog_deflector。
4. 在graylog_deflector索引的配置页面上，您可以设置以下参数：
- Rotation Strategy（轮询策略）：选择用于切换索引的策略，例如按时间、按大小或其他规则。
- Index Prefix（索引前缀）：指定其他索引的名称前缀。例如，如果您希望创建名为graylog_2022-01-01、graylog_2022-01-02等的索引，可以将前缀设置为graylog_。
- Index Template（索引模板）：选择要应用于新创建的索引的模板。模板定义了索引的设置和映射规则。
- Index Set Configuration（索引集配置）：根据需要配置其他高级设置，例如分片数、副本数等。
5. 完成配置后，点击"Save"（保存）按钮以保存graylog_deflector索引的配置。

一旦graylog_deflector索引配置完成，Graylog将根据您设置的策略自动将数据转发到其他索引中。根据策略的不同，Graylog可能会创建新的索引并将数据写入其中，或者将数据写入已存在的索引中。

请注意，配置graylog_deflector索引和数据转发策略可能需要根据您的具体需求进行调整。建议参考Graylog的官方文档或寻求更详细的配置指南以获得更准确的配置信息。
```

Elasticsearch 健康状态为 yellow 问题的解决

[单节点 Elasticsearch 健康状态为 yellow 问题的解决\_elasticsearch yellow-CSDN博客](https://blog.csdn.net/ale2012/article/details/106992995)

[Elasticsearch(ES)生产集群健康状况为黄色(yellow)的官方详细解释、原因分析和解决方案(实测可用)\_es容器 健康状态yellow-CSDN博客](https://blog.csdn.net/myhes/article/details/106076544)

单机安装

https://blog.csdn.net/qixiaolinlin/article/details/129966703

https://blog.csdn.net/liukuan73/article/details/52525431

https://huaweicloud.csdn.net/637f7accdacf622b8df85933.html

https://www.cnblogs.com/runliuv/p/15475747.html

[在生产环境中使用Graylog日志系统所踩过的坑-阿里云开发者社区 (aliyun.com)](https://developer.aliyun.com/article/738051)

-bash: pwgen: command not found

https://blog.csdn.net/weixin\_37391237/article/details/121389868

查看graylog报错日志：如果提示报错，请查看 **/var/log/graylog-server**/ 下的日志文件，或 journalctl -xe -u graylog

修改用户名密码

https://www.codeleading.com/article/43856151066

graylog查询语法

https://blog.csdn.net/weixin\_43066697/article/details/126303133

## 问题

### 未启动成功

```bash
# 运行如下启动命令未成功
sudo systemctl start graylog-server.service
# 使用journalctl命令来查看Graylog Server服务的日志。这个命令会显示系统日志中与指定服务相关的条目
sudo journalctl -u graylog-server.service
# 如果只想查看最近的日志，可以使用-n选项指定条目数量，例如：
sudo journalctl -u graylog-server.service -n 100
# 如果您想实时查看日志更新（类似于tail -f），可以使用-f选项，这将持续显示新的日志条目，直到停止命令
sudo journalctl -u graylog-server.service -f
# journalctl命令依赖于systemd的日志系统，这是许多现代Linux发行版（如Ubuntu、CentOS 7及更高版本等）的默认日志管理系统。如果您的系统不使用systemd，那么可能需要使用其他工具（如tail命令查看/var/log目录下的日志文件）来查看服务日志
```

### mongo连接问题

```
2024-02-22T14:48:19.007+08:00 INFO  [MongoDBPreflightCheck] MongoDB is not available. Retry #121
2024-02-22T14:48:21.008+08:00 INFO  [MongoDBPreflightCheck] MongoDB is not available. Retry #122
2024-02-22T14:48:23.008+08:00 INFO  [MongoDBPreflightCheck] MongoDB is not available. Retry #123
2024-02-22T14:48:25.009+08:00 INFO  [MongoDBPreflightCheck] MongoDB is not available. Retry #124
2024-02-22T14:48:27.009+08:00 INFO  [MongoDBPreflightCheck] MongoDB is not available. Retry #125
2024-02-22T14:48:29.010+08:00 INFO  [MongoDBPreflightCheck] MongoDB is not available. Retry #126
2024-02-22T14:48:31.011+08:00 INFO  [MongoDBPreflightCheck] MongoDB is not available. Retry #127
2024-02-22T14:48:33.011+08:00 INFO  [MongoDBPreflightCheck] MongoDB is not available. Retry #128
2024-02-22T14:48:35.012+08:00 INFO  [MongoDBPreflightCheck] MongoDB is not available. Retry #129
2024-02-22T14:48:37.012+08:00 INFO  [MongoDBPreflightCheck] MongoDB is not available. Retry #130
2024-02-22T14:48:39.013+08:00 INFO  [MongoDBPreflightCheck] MongoDB is not available. Retry #131
2024-02-22T14:48:41.013+08:00 INFO  [MongoDBPreflightCheck] MongoDB is not available. Retry #132

```

修改配置

```bash
# 配置文件路径：/etc/graylog/server/server.conf
# 105行左右
http_bind_address = 0.0.0.0:9000
# 555行左右
mongodb_uri = mongodb://ekroot:ek070901@localhost:27017/graylog?authSource=admin
```

### 磁盘空间不足

```
2024-02-22T15:01:46.195+08:00 ERROR [PreflightCheckService] Preflight check failed with error: Journal directory </var/lib/graylog-server/journal> has not enough free space (1943 MB) available. You need to provide additional 2518 MB to contain 'message_journal_max_size = 5120 MB'
2024-02-22T15:01:46.195+08:00 ERROR [CmdLineTool] Startup error:
org.graylog2.bootstrap.preflight.PreflightCheckException: Journal directory </var/lib/graylog-server/journal> has not enough free space (1943 MB) available. You need to provide additional 2518 MB to contain 'message_journal_max_size = 5120 MB'
```

1、调低 message\_journal\_max\_size 参数大小

2、清理 dev/vdal

https://wenku.baidu.com/view/52b723d7f405cc1755270722192e453610665bdf.html?fr=sogou&*wkts*=1708585584306

```
[root@iZm5e1egdpr5bckrj8gty5Z ~]# df -h
Filesystem      Size  Used Avail Use% Mounted on
devtmpfs         31G     0   31G   0% /dev
tmpfs            31G     0   31G   0% /dev/shm
tmpfs            31G  608K   31G   1% /run
tmpfs            31G     0   31G   0% /sys/fs/cgroup
/dev/vda1        40G   36G  1.9G  95% /
tmpfs            31G  2.0M   31G   1% /tmp
tmpfs           6.2G     0  6.2G   0% /run/user/0
[root@iZm5e1egdpr5bckrj8gty5Z ~]# cd /
[root@iZm5e1egdpr5bckrj8gty5Z /]# du -sh *
156M    back
0       bin
272M    boot
180M    data
0       dev
12G     ekdir
28M     etc
4.0K    home
0       lib
0       lib64
16K     lost+found
4.0K    media
4.0K    mnt
16K     opt
du: cannot access 'proc/212747/task/212747/fd/3': No such file or directory
du: cannot access 'proc/212747/task/212747/fdinfo/3': No such file or directory
du: cannot access 'proc/212747/fd/3': No such file or directory
du: cannot access 'proc/212747/fdinfo/3': No such file or directory
du: cannot access 'proc/212753': No such file or directory
0       proc
17M     root
608K    run
0       sbin
4.0K    srv
0       sys
896K    tmp
8.2G    usr
16G     var
[root@iZm5e1egdpr5bckrj8gty5Z /]#

```

---

---
url: /Linux/Linux安装软件/Linux下安装Java.md
---

# Linux下安装Java

## OpenJDK安装

### 下载软件包

下载链接：http://jdk.java.net/archive/

找到自己要下载的OpenJDK版本

![image-20240907201149661](/assets/image-20240907201149661.DY5iuDf7.png)

### 下载完成之后上传服务器并解压

```sh
#将下载的包上传到服务器上
#将包放到/usr/local/目录
mv openjdk-11.0.2_linux-x64_bin.tar.gz /usr/local/

#进入/usr/local并解压删除压缩包
cd /usr/local/ && tar xf openjdk-11.0.2_linux-x64_bin.tar.gz && rm -rf openjdk-11.0.2_linux-x64_bin.tar.gz
```

### 添加环境变量

```sh
#添加环境变量（使用追加方式添加）
echo "export PATH=/usr/local/jdk-11.0.2/bin:$PATH" >> /etc/profile

#不放心可以查看一下环境变量
tail -1 /etc/profile

#生效环境变量
source /etc/profile
```

### 验证JDK是否安装成功

```sh
#查看jdk版本
java --version
```

![image-20240907201848702](/assets/image-20240907201848702.B-MSeg1O.png)

显示JDK11，安装成功。

## 参考资料

https://blog.csdn.net/liu\_chen\_yang/article/details/129318513

https://blog.csdn.net/qq\_35112567/article/details/131832497

---

---
url: /Linux/Linux安装软件/Linux下安装Tomcat.md
---

# Linux下安装Tomcat

https://zhuanlan.zhihu.com/p/555992010

---

---
url: /Linux/Shell命令/1_Linux新建用户并授予root权限.md
---

# Linux新建用户并授予root权限

## 一、新建用户

```
adduser xxl
```

## 二、授予root权限

通过修改sudoers文件来赋予新账户root权限

1、查找sudoers文件路径并赋予权限

```
whereis sudoers
```

查看sudoers文件权限

```
ls -l /etc/sudoers
```

只有读权限，所以为其赋予写权限

```
chmod -v u+w /etc/sudoers
```

2、修改sudoers文件

使用vim打开修改sudoers文件：

```
vim /etc/sudoers
```

添加新用户信息：

```
lisong ALL=(ALL) ALL
```

然后输入wq保存
3、收回sudoers文件写权限，防止他人篡改

```
chmod -v u-w /etc/sudoers
```

## 三、新用户登录

1、新建一个连接，使用新用户登录主机

2、切换root权限
（1）sudo su
是当前用户暂时申请root权限，所以输入的不是root用户密码，而是当前用户的密码

（2）su
切换到某某用户模式，提示输入密码时该密码为切换后账户的密码，用法为“su 账户名称”。
如果后面不加账户时系统默认为root账户，密码也为超级账户的密码。没有时间限制。
有些Linux发行版，例如ubuntu，默认没有设置root用户的密码
所以需要我们先使用 sudo passwd root 设置root用户密码。

---

---
url: /StableDiffusion/AIGC/LOFI.md
---

# LOFI音乐频道

## LOFI音乐频道

### 一、制作素材

#### 图生图

使用 leonardo 进行图生图

![image-20240310010618959](/assets/image-20240310010618959.BrBNBzT6.png)

进入image guidance图生图页面

![image-20240310010659091](/assets/image-20240310010659091.Da-vZ8TN.png)

上传图片，调整参数，分辨率、提示词、模型、权重，权重值越高越像原图，

![image-20240310011520420](/assets/image-20240310011520420.BV6mSymr.png)

将生成的图像下载保存。

#### 提示词生图

图片转提示词：https://imagetoprompt.com/

上传图片点击`Generate prompt`，等待几秒即可看到生成的提示词。

![image-20240310140508315](/assets/image-20240310140508315.O1ikpQRB.png)

```
anime scene of a small island with a castle and a tower, anime scenery concept art, anime scenery, anime landscape, studio ghibli landscape, island background, greg rutkowski studio ghibli,
```

可以到翻译软件中优化提示词

将提示词粘贴到提示框中，调整其他参数

![image-20240310201112998](/assets/image-20240310201112998.z2sq_EhX.png)

继续使用 https://www.krea.ai 进行图片增强放大

![image-20240310203110574](/assets/image-20240310203110574.3n40oy_0.png)

上传图片，调整参数

![image-20240310203316669](/assets/image-20240310203316669.CjUXe88m.png)

等待图片生成。

### 二、生成动画

使用runwayml网站生成视频

点击首页的`start generating`

![image-20240310012024181](/assets/image-20240310012024181.C0eZ3Wai.png)

上传图片，Motion值保持5不变，Camera Motion相机运动不变，

点击Motion Brush运动笔刷，涂抹环境，多生成几次选择自己满意的

ambient 调整为1.2，点击确认，生成

![image-20240310012437433](/assets/image-20240310012437433.DFJOIePR.png)

### 三、下载音乐

音频库里筛选不需注明出处的音乐，筛选合适的曲调，下载。

### 四、合成视频

## 涉及的网站

AI生图

Midjourney：https://www.midjourney.com/

Leonardo：https://app.leonardo.ai

Aitubo：https://aitubo.ai/

Dreamlike.art：https://dreamlike.art/

图片动画、运动笔刷

runwayml：https://app.runwayml.com

实时绘画，图片放大

https://www.krea.ai

图片转提示词

https://imagetoprompt.com/

参考资料

https://www.bilibili.com/video/BV18K411v7vr

---

---
url: /Java/中间件/Logstash.md
---
# Logstash

https://blog.csdn.net/wu499920535/article/details/124930208
https://blog.csdn.net/lumengmeng\_csdn/article/details/89482086

---

---
url: /StableDiffusion/StableDiffusion/4_LORA训练.md
---

# LORA原理与训练

## Kohya训练器简介

Kohya GUI的核心是由开源社区开发者@Kohya.S 基于Simo Ryu（@cloneofsimo）的发现编写的LoRA训练脚本sd-scripts 。这是SD社区里出现最早的LoRA训练脚本，后续的大多数训练器与训练脚本均是在这个脚本的基础上衍生开发的。

我们使用的Kohya训练器，来源于另一位开发者@bmaltais 在Kohya脚本的基础上使用Gradio制作的图形界面（GUI），内置了可视化的界面帮助我们合理地输入、调整各项参数，并整合了参数预设、数据整理打标等在内的一系列配套功能（虽然我不常用kk）。

### 下载训练器

新建一个空文件夹目录。清空路径栏并输入“cmd”，回车调出命令行；或右键使用git bash命令行。

输入项目克隆命令

```
git clone https://github.com/bmaltais/kohya_ss.git
```

等待下载完成。

### 运行初始化

初次启动训练器，运行根目录下的“setup.bat”文件。

初次启动，会弹出选项界面。输入选项对应数字回车，完成选择。

![image-20240302004304933](/assets/image-20240302004304933.us7_wy8P.png)

选择1，安装必须依赖。

## 参考资料

[60分钟速通LORA训练！绝对是你看过最好懂的AI绘画模型训练教程！StableDiffusion超详细训练原理讲解+实操教学，LORA参数详解与训练集处理技巧\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV16e411e7Zx/)

---

---
url: /daily/博客文档/VitPress/4_Markdown.md
---

# Markdown

---

---
url: /01.指南/10.使用/05.Markdown 拓展.md
---

# Markdown 拓展

VitePress 使用 `markdown-it` 来对 Markdown 进行解析和渲染，最终转为 Vue 组件。

`markdown-it` 是一款功能强大的 Markdown 解析器，支持丰富的 Markdown 语法，能够轻松将 Markdown 文本转换为 HTML 格式，并提供了许多语法扩展和插件。如果希望文章页拓展一些新的功能、UI，那么可以利用它拦截并处理 Markdown 生成的 HTML。

阅读 VitePress 的代码可以发现，它利用 `markdown-it` 添加了代码块高亮、代码块行号、Tip 容器等功能，在 VitePress 官网的 [Markdown 拓展](https://vitepress.dev/zh/guide/markdown) 里，已经详细介绍了 VitePress 支持 Markdown 额外拓展的功能。

下面介绍的仅仅是 Teek 实现的 Markdown 拓展。

Teek 也提供了几个 Markdown 插件，分别为：

* todo 待办列表
* center 内容居中容器
* right 内容居右容器
* note 容器
* shareCard 分享卡片
* imgCard 图文卡片
* navCard 导航卡片
* demo 容器
* ...

`center`、`right`、`note` 容器是一种简单的 Markdown 容器（不改变内容，只给内容加样式），Teek 支持定义类似的容器，具体请看 [自定义容器](/reference/plugin-config#自定义容器)。

`shareCard`、`imgCard`、`navCard` 是一种较为复杂的 Markdown 容器（改变内容和样式），如果你也想定义类似的容器，可以阅读这三个插件的代码，它们的代码逻辑非常相似且简单，只需要会编写 HTML、CSS，就可以实现一个复杂容器。

## TODO 待办列表

输出：

* \[ ] 吃饭
* \[ ] 睡觉
* \[x] 打豆豆

输入：

```markdown
- [ ] 吃饭
- [ ] 睡觉
- [x] 打豆豆
```

确保 `[ ]` 里有一个空格。

::: tip
支持所有列表语法，如：`1.`、`-`、`+`、`*` 等。
:::

## 内容居中容器

输出：

::: center
Markdown 拓展
:::

::: center

## Markdown 拓展

（这是二级标题）
:::

输入：

```markdown
::: center
Markdown 拓展
:::

::: center

## Markdown 拓展

（这是二级标题）
:::
```

## 内容居右容器

输出：

::: right
@Teek
:::

::: tip 摘要
很久之前，我决定踏上的这条路，映照了我与未来的因果。
::: right
2021-11-13 @Teek
:::

输入：

```markdown
::: right
@Teek
:::

::: tip 摘要
很久之前，我决定踏上的这条路，映照了我与未来的因果。
::: right
2021-11-13 @Teek
:::
```

## 笔记容器

笔记容器和 VitePress 的 Github `NOTE` 容器样式一样，Teek 将其转为 `:::note` 启用。

输出：

::: note
这是一个笔记 Note 容器。
:::

输入：

```markdown
::: note
这是一个笔记 Note 容器。
:::
```

## shareCard 分享卡片

分享卡片容器，可用于 `友情链接、项目推荐、诗词展示` 等。

输出：

::: shareCard

```yaml
- name: George Chan
  desc: 让我给你讲讲他的传奇故事吧
  avatar: https://z3.ax1x.com/2021/09/30/4oKMVI.jpg
  link: https://cyc0819.top/
  bgColor: "#FFB6C1"
  textColor: "#621529"

- name: butcher2000
  desc: 即使再小的帆，也能远航
  avatar: https://gcore.jsdelivr.net/gh/Kele-Bingtang/static/user/20211029181901.png
  link: https://blog.csdn.net/weixin_46827107
  bgColor: "#CBEAFA"
  textColor: "#6854A1"

- name: Evan's blog
  desc: 前端的小学生
  avatar: https://gcore.jsdelivr.net/gh/xugaoyi/image_store/blog/20200103123203.jpg
  link: https://xugaoyi.com/
  bgColor: "#B9D59C"
  textColor: "#3B551F"
```

:::

输入：

````yaml
::: shareCard

```yaml
- name: George Chan
  desc: 让我给你讲讲他的传奇故事吧
  avatar: https://z3.ax1x.com/2021/09/30/4oKMVI.jpg
  link: https://cyc0819.top/
  bgColor: "#FFB6C1"
  textColor: "#621529"

- name: butcher2000
  desc: 即使再小的帆，也能远航
  avatar: https://gcore.jsdelivr.net/gh/Kele-Bingtang/static/user/20211029181901.png
  link: https://blog.csdn.net/weixin_46827107
  bgColor: "#CBEAFA"
  textColor: "#6854A1"

- name: Evan's blog
  desc: 前端的小学生
  avatar: https://gcore.jsdelivr.net/gh/xugaoyi/image_store/blog/20200103123203.jpg
  link: https://xugaoyi.com/
  bgColor: "#B9D59C"
  textColor: "#3B551F"
```

:::
````

### 语法

::: code-group

````markdown [基础语法]
::: shareCard <每行显示数量 | auto>

```yaml
- name: 名称
  desc: 描述
  avatar: https://xxx.jpg # 头像，可选
  link: https://xxx/ # 链接，可选
  bgColor: "#CBEAFA" # 背景色，可选，默认 var(--vp-c-gray-1)。颜色值有 # 号时请添加引号
  textColor: "#6854A1" # 文本色，可选，默认 var(--vp-c-text-1)
```

::: 
````

````markdown [进阶语法]
::: shareCard <每行显示数量 | auto>

```yaml
config:
  cardNum: 2 # 每行显示的卡片数量，默认为 auto，可在容器名字后面添加，如 ::: shareCard 3
  target: _blank # 跳转方式，默认为 _blank，仅支持 _blank | _self
  cardGap: 20 # 每行卡片之间的间隔，默认为 20
  showCode: false # 是否显示代码块，默认为 false

data:
  - name: 名称
    desc: 描述
    avatar: https://xxx.jpg # 头像，可选
    link: https://xxx/ # 链接，可选
    bgColor: "#CBEAFA" # 背景色，可选，默认 var(--vp-c-gray-1)。颜色值有 # 号时请添加引号
    textColor: "#6854A1" # 文本色，可选，默认 var(--vp-c-text-1)
```

::: 
````

```ts [配置项]
export declare namespace ShareCard {
  export interface Config {
    /**
     * 每行显示的卡片数量
     *
     * @default 'auto'
     */
    cardNum?: number | "auto";
    /**
     * 跳转方式
     *
     * @default '_blank'
     */
    target?: "_blank" | "_self";
    /**
     * 每行卡片之间的间隔
     *
     * @default 20
     */
    cardGap?: number;
    /**
     * 是否显示代码块
     */
    showCode?: boolean;
  }

  export interface Item {
    /**
     * 名称
     */
    name: string;
    /**
     * 描述
     */
    desc: string;
    /**
     * 头像
     */
    avatar?: string;
    /**
     * 跳转链接
     */
    link?: string;
    /**
     * 背景色
     * @default var(--vp-c-gray-1)
     */
    bgColor: string;
    /**
     * 文字颜色
     * @default var(--vp-c-text-1)
     */
    textColor: string;
  }
}
```

:::

* `<每行显示数量 | auto>`
  * 当为空或为 `auto` 时，自动根据文档宽度进行适配
  * 为数字时，表示每行最多显示多少个，选值范围 `1 ~ 4`（自带自适应功能：根据屏幕宽度减少每行显示数量）
* 代码块需指定语言为 `yaml`

## imgCard 图文卡片

图文卡片容器，可用于 `项目展示、产品展示` 等。

输出：

::: imgCard

```yaml
- img: https://vp.teek.top/blog/bg1.webp
  link: https://vp.teek.top
  name: 标题
  desc: 描述内容描述内容描述内容描述内容
  author: Teek
  avatar: https://testingcf.jsdelivr.net/gh/Kele-Bingtang/static/user/avatar1.png
- img: https://vp.teek.top/blog/bg2.webp
  link: https://vp.teek.top
  name: 标题
  desc: 描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容
  author: Teek
  avatar: https://testingcf.jsdelivr.net/gh/Kele-Bingtang/static/user/avatar1.png
- img: https://vp.teek.top/blog/bg2.webp
  link: https://vp.teek.top
  name: 标题
  desc: 描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容
  author: Teek
  avatar: https://testingcf.jsdelivr.net/gh/Kele-Bingtang/static/user/avatar1.png
- img: https://vp.teek.top/blog/bg3.webp
  link: https://vp.teek.top
  name: 标题
  desc: 描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容
```

:::

输入：

````yaml
::: imgCard
```yaml
- img: https://vp.teek.top/blog/bg1.webp
  link: https://vp.teek.top
  name: 标题
  desc: 描述内容描述内容描述内容描述内容
  author: Teek
  avatar: https://testingcf.jsdelivr.net/gh/Kele-Bingtang/static/user/avatar1.png
- img: https://vp.teek.top/blog/bg2.webp
  link: https://vp.teek.top
  name: 标题
  desc: 描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容
  author: Teek
  avatar: https://testingcf.jsdelivr.net/gh/Kele-Bingtang/static/user/avatar1.png
- img: https://vp.teek.top/blog/bg2.webp
  link: https://vp.teek.top
  name: 标题
  desc: 描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容
  author: Teek
  avatar: https://testingcf.jsdelivr.net/gh/Kele-Bingtang/static/user/avatar1.png
- img: https://vp.teek.top/blog/bg3.webp
  link: https://vp.teek.top
  name: 标题
  desc: 描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容描述内容
```
:::
````

### 语法

::: code-group

````markdown [基础语法]
::: imgCard <每行显示数量 | auto>

```yaml
- img: https://abc.jpg # 图片地址
  link: https://abc.com # 链接地址
  name: 标题
  desc: 描述 # 可选
  author: 作者名称 # 可选
  avatar: https://abc.jpg # 作者头像，可选
```

::: 
````

````markdown [进阶语法]
::: imgCard <每行显示数量 | auto>

```yaml
config:
  cardNum: 2 # 每行显示的卡片数量，默认为 auto，可在容器名字后面添加，如 ::: imgCard 3
  target: _blank # 跳转方式，默认为 _blank，仅支持 _blank | _self
  lineClamp: 2 # 显示描述信息的行数，默认为 2
  cardGap: 20 # 每行卡片之间的间隔，默认为 20
  imgHeight: auto # 图片宽度，默认为 auto。仅图文卡片支持该配置项
  objectFit: cover # 设置图片的填充方式，支持 cover | fill | contain | scale-down | none，默认为 cover
  showCode: false # 是否显示代码块，默认为 false

data:
  - img: https://abc.jpg # 图片地址
    link: https://abc.com # 链接地址
    name: 标题
    desc: 描述 # 可选
    author: 作者名称 # 可选
    avatar: https://abc.jpg # 作者头像，可选
```

::: 
````

```ts [配置项]
export declare namespace ImgCard {
  export interface Config {
    /**
     * 每行显示的卡片数量
     *
     * @default 'auto'
     */
    cardNum?: number | "auto";
    /**
     * 跳转方式
     *
     * @default '_blank'
     */
    target?: "_blank" | "_self";
    /**
     * 图片宽度
     *
     * @default 'auto'
     */
    imgHeight?: string;
    /**
     * 设置图片的填充方式，为 CSS object-it 属性值
     *
     * @default 'cover'
     */
    objectFit?: "cover" | "fill" | "contain" | "scale-down" | "none";
    /**
     * 显示描述信息的行数
     *
     * @default 2
     */
    lineClamp?: number;
    /**
     * 每行卡片之间的间隔
     *
     * @default 20
     */
    cardGap?: number;
    /**
     * 是否显示代码块
     */
    showCode?: boolean;
  }

  export interface Item {
    /**
     * 图片链接
     */
    img: string;
    /**
     * 跳转链接
     */
    link?: string;
    /**
     * 名称
     */
    name: string;
    /**
     * 描述
     */
    desc?: string;
    /**
     * 作者
     */
    author?: string;
    /**
     * 作者头像
     */
    avatar?: string;
  }
}
```

:::

* `<每行显示数量 | auto>`
  * 当为空或为 `auto` 时，自动根据文档宽度进行适配
  * 为数字时，表示每行最多显示多少个，选值范围 `1 ~ 4`（自带自适应功能：根据屏幕宽度减少每行显示数量）
* 代码块需指定语言为 `yaml`

## navCard 导航卡片

导航卡片容器，可以用于制作 `导航站点`。

输出：

::: navCard

```yaml
- name: 百度
  desc: 百度——全球最大的中文搜索引擎及最大的中文网站，全球领先的人工智能公司
  link: http://www.baidu.com/
  img: https://www.baidu.com/favicon.ico
  badge: 搜索引擎
- name: Google
  desc: 全球最大的搜索引擎公司
  link: http://www.google.com/
  img: https://ts1.cn.mm.bing.net/th/id/R-C.58c0f536ec073452434270fb559c3f8c?rik=SnOUNtUtPLX6ww&riu=http%3a%2f%2fwww.sz4a.cn%2fPublic%2fUploads%2fimage%2f20230303%2f1677839482835474.png&ehk=J1lqoeszPGEWzDOSZQ3JxzXsklfd0QzgrJu6ZVvESKk%3d&risl=&pid=ImgRaw&r=0
  badge: 搜索引擎
  badgeType: tip
```

:::

输入：

````yaml

::: navCard
```yaml
- name: 百度
  desc: 百度——全球最大的中文搜索引擎及最大的中文网站，全球领先的人工智能公司
  link: http://www.baidu.com/
  img: https://www.baidu.com/favicon.ico
  badge: 搜索引擎
- name: Google
  desc: 全球最大的搜索引擎公司
  link: http://www.google.com/
  img: https://ts1.cn.mm.bing.net/th/id/R-C.58c0f536ec073452434270fb559c3f8c?rik=SnOUNtUtPLX6ww&riu=http%3a%2f%2fwww.sz4a.cn%2fPublic%2fUploads%2fimage%2f20230303%2f1677839482835474.png&ehk=J1lqoeszPGEWzDOSZQ3JxzXsklfd0QzgrJu6ZVvESKk%3d&risl=&pid=ImgRaw&r=0
  badge: 搜索引擎
  badgeType: tip
```
:::
````

### 语法

::: code-group

````markdown [基础语法]
::: navCard <每行显示数量 | auto>

```yaml
- name: 标题
  desc: 描述
  link: 链接地址 # 可选
  img: 图片地址 # 可选
  badge: 徽章内容 # 可选
  badgeType: 徽章类型 # 可选
```

::: 
````

````markdown [进阶语法]
::: imgCard <每行显示数量 | auto>

```yaml
config:
  cardNum: 2 # 每行显示的卡片数量，默认为 2，可在容器名字后面添加，如 ::: navCard 3
  target: _blank # 跳转方式，默认为 _blank，仅支持 _blank | _self
  lineClamp: 2 # 显示描述信息的行数，默认为 2
  cardGap: 20 # 每行卡片之间的间隔，默认为 20
  showCode: false # 是否显示代码块，默认为 false

data:
  - name: 标题
  desc: 描述
  link: 链接地址 # 可选
  img: 图片地址 # 可选
  badge: 徽章内容 # 可选
  badgeType: 徽章类型 # 可选
```

::: 
````

```ts [配置项]
export declare namespace NavCard {
  export interface Config {
    /**
     * 每行显示的卡片数量
     *
     * @default 'auto'
     */
    cardNum?: number | "auto";
    /**
     * 跳转方式
     *
     * @default '_blank'
     */
    target?: "_blank" | "_self";
    /**
     * 显示描述信息的行数
     *
     * @default 2
     */
    lineClamp?: number;
    /**
     * 每行卡片之间的间隔
     *
     * @default 20
     */
    cardGap?: number;
    /**
     * 是否显示代码块
     */
    showCode?: boolean;
  }

  export interface Item {
    /**
     * 名称
     */
    name: string;
    /**
     * 描述
     */
    desc: string;
    /**
     * 图片链接
     */
    img?: string;
    /**
     * 跳转链接
     */
    link?: string;
    /**
     * 右上角徽章
     */
    badge?: string;
    /**
     * 右上角徽章类型
     *
     * @default 'info'
     */
    badgeType?: "info" | "tip" | "warning" | "danger";
  }
}
```

:::

* `<每行显示数量 | auto>`
  * 当为空或为 `auto` 时，自动根据文档宽度进行适配
  * 为数字时，表示每行最多显示多少个，选值范围 `1 ~ 4`（自带自适应功能：根据屏幕宽度减少每行显示数量）
* 代码块需指定语言为 `yaml`

## Video 容器

`Video` 容器可以快速嵌入不同平台的视频：

* `bilibili`：Bilibili 视频
* `tencent`：腾讯视频
* `youku`：优酷视频
* `youtube`：YouTube 视频
* `vimeo`：Vimeo 视频
* `xigua`：西瓜视频
* 自定义视频链接

以 `bilibili` 为例，输出：

::: video bilibili
BV11e411m7e8
:::

输入：

```markdown
::: video bilibili
BV11e411m7e8
:::
```

自定义视频链接，输出：

::: video
https://player.bilibili.com/player.html?bvid=BV11e411m7e8\&autoplay=0
:::

输入：

```markdown
::: video
https://player.bilibili.com/player.html?bvid=BV11e411m7e8&autoplay=0
:::
```

### 语法

::: code-group

```markdown [多平台]
::: video <视频平台标识>
<视频 ID>
::: 
```

```markdown [自定义]
::: video
<自定义视频链接>
::: 
```

:::

---

---
url: /daily/日常笔记/Markdown进阶之路.md
---

# Markdown进阶之路

Markdown是一种轻量级标记语言，排版语法简洁，让人们更多地关注内容本身而非排版。它使用易读易写的纯文本格式编写文档，可与HTML混编，可导出 HTML、PDF 以及本身的 .md 格式的文件。因简洁、高效、易读、易写，Markdown被大量使用，如Github、Wikipedia、简书等。

## Markdown笔记软件

* Typora：评价比较高
  * typora\_plugin：推荐搭配扩展插件
* Vnote：推荐比较多
* Obsdian：笔记管理软件
* Marktext：简洁，很久未更新
* Notion

## Markdown语法

Markdown基础语法：https://markdown.com.cn/basic-syntax/headings.html)

扩展

数学公式：<https://zhuanlan.zhihu.com/p/450465546>

数学公式：<https://www.cnblogs.com/bytesfly/p/markdown-formula.html>

数学公式：<https://www.cnblogs.com/XiiX/p/14619121.html>

## 表情大全

https://emoji8.com/zh-hans/

https://emojipedia.org/

## 美化插件

Typora主题：http://theme.typora.io/

VLOOK：https://github.com/MadMaxChow/VLOOK

李笑来老师的文章 Markdownhere 样式：[markdown-here-css (github.com)](https://gist.github.com/xiaolai/aa190255b7dde302d10208ae247fc9f2)

## Markdown工具箱

各种转换工具集合：https://www.markdowntoolbox.com/zh/

图表工具mermaid：https://mermaid.nodejs.cn/

---

---
url: /常用框架/SpringAIAlibaba/Agent Framework/4_Memory短期记忆.md
---

# Memory 短期记忆

## 一、Memory 概述

记忆可以让 Agent 记住之前的会话内容。对于 AI Agent，记忆至关重要，因为它让它们能够记住先前的交互、从反馈中学习并适应用户偏好。随着 Agent 处理更复杂的任务和大量用户交互，这种能力对于效率和用户满意度都变得至关重要。

短期记忆让你的应用程序能够在单个线程或会话中记住先前的交互。

> **注意**：会话可以隔离同一个 Agent 实例中的多个不同交互，类似于电子邮件在单个对话中分组消息的方式。

## 二、理解 ReactAgent 中的短期记忆

Spring AI Alibaba 将短期记忆作为 Agent 状态的一部分进行管理。

通过将这些存储在 Graph 的状态中，Agent 可以访问给定对话的完整上下文，同时保持不同对话之间的分离。状态使用 checkpointer 持久化到数据库（或内存），以便可以随时恢复线程。短期记忆在调用 Agent 或完成步骤（如工具调用）时更新，并在每个步骤开始时读取状态。

## 三、记忆带来的上下文过长问题

保留所有对话历史是实现短期记忆最常见的形式。但较长的对话对历史可能会导致大模型 LLM 上下文窗口超限，导致上下文丢失或报错。

即使你在使用的大模型上下文长度足够大，大多数模型在处理较长上下文时的表现仍然很差。因为很多模型会被过时或偏离主题的内容"分散注意力"。同时，过长的上下文，还会带来响应时间变长、Token 成本增加等问题。

在 Spring AI ALibaba 中，ReactAgent 使用 [messages](https://java2ai.com/docs/frameworks/agent-framework/tutorials/messages) 记录和传递上下文，其中包括指令（SystemMessage）和输入（UserMessage）。在 ReactAgent 中，消息（Message）在用户输入和模型响应之间交替，导致消息列表随着时间的推移变得越来越长。由于上下文窗口有限，许多应用程序可以从使用技术来移除或"忘记"过时信息中受益，即 “上下文工程”。

## 四、使用方法

在 Spring AI Alibaba 中，要向 Agent 添加短期记忆（会话级持久化），你需要在创建 Agent 时指定 `checkpointer`。

```java
/**
 * 配置短期记忆 示例
 */
@SneakyThrows
public static void shortTermMemoryConfiguration() {
    // 初始化 ChatModel
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();
    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();
    // 创建工具
    ToolCallback getUserInfoTool = createGetUserInfoTool();
    // 配置 checkpointer
    ReactAgent agent = ReactAgent.builder()
        .name("my_agent")
        .model(chatModel)
        .tools(getUserInfoTool)
        .saver(new MemorySaver()) // [!code ++]
        .build();
    // 使用 thread_id 维护对话上下文 // [!code ++:5]
    RunnableConfig config = RunnableConfig.builder()
        .threadId("1") // threadId 指定会话 ID
        .build();
    agent.call("你好！我叫 Bob。", config);
}
```

### 在生产环境中

在生产环境中，使用数据库支持的 checkpointer：

**示例：使用 Redis Checkpointer**：

```java
/**
 * 使用 Redis Checkpointer 示例
 */
@SneakyThrows
public static void redisMemoryConfiguration() {
    // 初始化 ChatModel
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();
    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();
    // 创建工具
    ToolCallback getUserInfoTool = createGetUserInfoTool();
    // 配置 Redis checkpointer // [!code ++:2]
    RedisSaver redisSaver = createRedisSaver();
    // 配置 checkpointer
    ReactAgent agent = ReactAgent.builder()
        .name("my_agent")
        .model(chatModel)
        .tools(getUserInfoTool)
        .tools()
        .saver(redisSaver) // [!code ++]
        .build();
    // 使用 thread_id 维护对话上下文 // [!code ++:5]
    RunnableConfig config = RunnableConfig.builder()
        .threadId("1") // threadId 指定会话 ID
        .build();
    AssistantMessage message01 = agent.call("你好！我叫 Bob。", config);
    System.out.println(message01.getText());
    AssistantMessage message02 = agent.call("你好！我叫什么。", config);
    System.out.println(message02.getText());
}

// [!code ++:12]
/**
 * 初始化 RedisSaver
 */
public static RedisSaver createRedisSaver() {
    // 配置 Redisson 客户端
    Config config = new Config();
    config.useSingleServer()
        .setAddress("redis://localhost:6379");  // Redis 地址

    RedissonClient redisson = Redisson.create(config);
    return new RedisSaver(redisson);
}

/**
 * 创建获取用户信息工具
 */
private static ToolCallback createGetUserInfoTool() {
    return FunctionToolCallback.builder("get_user_info", (String userId) -> {
            // 简化的实现
            return "User info for: " + userId;
        })
        .description("Get user information by ID")
        .inputType(String.class)
        .build();
}
```

## 五、自定义 Agent 记忆

默认情况下，Agent 使用状态通过 `messages` 键管理短期记忆，特别是对话历史。

可以通过在工具或 Hook 中访问和修改状态来扩展记忆功能。

```java
package com.xxl.ai.framework.hook;

import com.alibaba.cloud.ai.graph.OverAllState;
import com.alibaba.cloud.ai.graph.RunnableConfig;
import com.alibaba.cloud.ai.graph.agent.hook.HookPosition;
import com.alibaba.cloud.ai.graph.agent.hook.ModelHook;
import org.springframework.ai.chat.messages.Message;

import java.util.List;
import java.util.Map;
import java.util.Optional;
import java.util.concurrent.CompletableFuture;

/**
 * 自定义记忆 Hook
 *
 * @Author xxl
 * @Date 2025/12/2 11:19
 */
public class CustomMemoryHook extends ModelHook {

    @Override
    public String getName() {
        return "custom_memory";
    }

    @Override
    public HookPosition[] getHookPositions() {
        return new HookPosition[]{HookPosition.BEFORE_MODEL};
    }

    /**
     * 在 Hook 中访问和修改状态
     *
     * @param state
     * @param config
     * @return
     */
    @Override
    public CompletableFuture<Map<String, Object>> beforeModel(OverAllState state, RunnableConfig config) {
        // 访问消息历史
        Optional<Object> messagesOpt = state.value("messages");
        if (messagesOpt.isPresent()) {
            List<Message> messages = (List<Message>) messagesOpt.get();
            // 处理消息...
        }

        // 添加自定义状态
        return CompletableFuture.completedFuture(Map.of(
                "user_id", "user_123",
                "preferences", Map.of("theme", "dark")
        ));
    }

    @Override
    public CompletableFuture<Map<String, Object>> afterModel(OverAllState state, RunnableConfig config) {
        return CompletableFuture.completedFuture(Map.of());
    }
}

```

## 六、常见模式

启用短期记忆后，长对话可能超过 LLM 的上下文窗口。常见的解决方案包括：

* 修剪消息。在调用 LLM 之前移除前 N 条或后 N 条消息
* 删除消息。从 Graph 状态中永久删除消息
* 总结消息。总结历史中较早的消息并用摘要替换它们
* 自定义策略。自定义策略（例如消息过滤等）

这允许 Agent 在 reasoning-acting 循环中持续跟踪对话而不超过 LLM 的上下文窗口。

### 修剪消息

大多数 LLM 都有最大支持的上下文窗口（以 token 计）。

决定何时截断消息的一种方法是计算消息历史中的 token 数量，并在接近该限制时进行截断。

要在 Agent 中修剪消息历史，请使用 `ModelHook`：

```java
package com.xxl.ai.framework.hook;

import com.alibaba.cloud.ai.graph.OverAllState;
import com.alibaba.cloud.ai.graph.RunnableConfig;
import com.alibaba.cloud.ai.graph.agent.hook.HookPosition;
import com.alibaba.cloud.ai.graph.agent.hook.ModelHook;
import org.springframework.ai.chat.messages.Message;

import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import java.util.Optional;
import java.util.concurrent.CompletableFuture;

/**
 * 修剪消息
 *
 * @Author xxl
 * @Date 2025/12/2 13:43
 */
public class MessageTrimmingHook extends ModelHook {

    private static final int MAX_MESSAGES = 3;

    @Override
    public String getName() {
        return "message_trimming";
    }

    @Override
    public HookPosition[] getHookPositions() {
        return new HookPosition[]{HookPosition.BEFORE_MODEL};
    }

    @Override
    public CompletableFuture<Map<String, Object>> beforeModel(OverAllState state, RunnableConfig config) {
        Optional<Object> messagesOpt = state.value("messages");
        if (!messagesOpt.isPresent()) {
            return CompletableFuture.completedFuture(Map.of());
        }

        List<Message> messages = (List<Message>) messagesOpt.get();

        if (messages.size() <= MAX_MESSAGES) {
            return CompletableFuture.completedFuture(Map.of()); // 无需更改
        }

        // 保留第一条消息和最后几条消息，并将中间消息标记为删除
        Message firstMsg = messages.get(0);
        int keepCount = messages.size() % 2 == 0 ? 3 : 4;
        List<Message> recentMessages = messages.subList(
                messages.size() - keepCount,
                messages.size()
        );

        List<Object> newMessages = new ArrayList<>();
        // 标记中间消息为删除（使用 RemoveByHash）
        if (messages.size() - keepCount > 1) {
            for (Message msg : messages.subList(1, messages.size() - keepCount)) {
                newMessages.add(com.alibaba.cloud.ai.graph.state.RemoveByHash.of(msg));
            }
        }

        return CompletableFuture.completedFuture(Map.of("messages", newMessages));
    }

    @Override
    public CompletableFuture<Map<String, Object>> afterModel(OverAllState state, RunnableConfig config) {
        return CompletableFuture.completedFuture(Map.of());
    }
}
```

MessageTrimmingHook 修剪消息示例

```java
/**
 * MessageTrimmingHook 修剪消息示例
 */
@SneakyThrows
private static void messageTrimmingConfiguration() {
    // 初始化 ChatModel
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .build();
    // 创建 Agent
    ReactAgent agent = ReactAgent.builder()
        .name("trimming_agent")
        .model(chatModel)
        .hooks(new MessageTrimmingHook()) // [!code ++]
        .saver(new MemorySaver())
        .build();

    RunnableConfig config = RunnableConfig.builder()
        .threadId("1")
        .build();

    agent.call("你好，我叫 bob", config);
    agent.call("写一首关于猫的短诗", config);
    agent.call("现在对狗做同样的事情", config);
    AssistantMessage finalResponse = agent.call("我叫什么名字？", config);

    System.out.println(finalResponse.getText());
    // 输出：
    // 你叫 Bob！很高兴认识你，Bob 😊
    // 我记性还不错吧？要不要给你的名字也写首诗？😄
}
```

### 删除消息

你可以从 Graph 状态中删除消息以管理消息历史。

这在你想要删除特定消息或清除整个消息历史时很有用。

要从 Graph 状态中删除消息，可以在 Hook 中返回新的消息列表：

```java
package com.xxl.ai.framework.hook;

import com.alibaba.cloud.ai.graph.OverAllState;
import com.alibaba.cloud.ai.graph.RunnableConfig;
import com.alibaba.cloud.ai.graph.agent.hook.HookPosition;
import com.alibaba.cloud.ai.graph.agent.hook.ModelHook;
import com.alibaba.cloud.ai.graph.state.RemoveByHash;
import org.springframework.ai.chat.messages.Message;

import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import java.util.Optional;
import java.util.concurrent.CompletableFuture;

/**
 * MessageDeletionHook 删除消息示例
 *
 * @Author xxl
 * @Date 2025/12/2 14:12
 */
public class MessageDeletionHook extends ModelHook {

    @Override
    public String getName() {
        return "message_deletion";
    }

    @Override
    public HookPosition[] getHookPositions() {
        return new HookPosition[]{HookPosition.AFTER_MODEL};
    }

    @Override
    public CompletableFuture<Map<String, Object>> beforeModel(OverAllState state, RunnableConfig config) {
        return CompletableFuture.completedFuture(Map.of());
    }

    @Override
    public CompletableFuture<Map<String, Object>> afterModel(OverAllState state, RunnableConfig config) {
        Optional<Object> messagesOpt = state.value("messages");
        if (!messagesOpt.isPresent()) {
            return CompletableFuture.completedFuture(Map.of());
        }

        List<Message> messages = (List<Message>) messagesOpt.get();

        if (messages.size() > 2) {
            // 将最早的两条消息转为 RemoveByHash 对象以便从状态中删除
            List<Object> removeOldMessages = new ArrayList<>();
            removeOldMessages.add(RemoveByHash.of(messages.get(0)));
            removeOldMessages.add(RemoveByHash.of(messages.get(1)));
            return CompletableFuture.completedFuture(Map.of("messages", removeOldMessages));
        }

        return CompletableFuture.completedFuture(Map.of());
    }
}
```

**删除所有消息**：

```java
package com.xxl.ai.framework.hook;

import com.alibaba.cloud.ai.graph.OverAllState;
import com.alibaba.cloud.ai.graph.RunnableConfig;
import com.alibaba.cloud.ai.graph.agent.hook.HookPosition;
import com.alibaba.cloud.ai.graph.agent.hook.ModelHook;
import com.alibaba.cloud.ai.graph.state.RemoveByHash;
import org.springframework.ai.chat.messages.Message;

import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import java.util.Optional;
import java.util.concurrent.CompletableFuture;

/**
 * ClearAllMessagesHook 删除所有消息
 *
 * @Author xxl
 * @Date 2025/12/2 14:12
 */
public class MessageClearAllHook extends ModelHook {

    @Override
    public String getName() {
        return "clear_all_messages";
    }

    @Override
    public HookPosition[] getHookPositions() {
        return new HookPosition[]{HookPosition.AFTER_MODEL};
    }

    @Override
    public CompletableFuture<Map<String, Object>> afterModel(OverAllState state, RunnableConfig config) {
        Optional<Object> messagesOpt = state.value("messages");
        if (!messagesOpt.isPresent()) {
            return CompletableFuture.completedFuture(Map.of());
        }

        List<Message> messages = (List<Message>) messagesOpt.get();

        // 将所有消息转为 RemoveByHash 对象以便从状态中删除
        List<Object> removeAllMessages = new ArrayList<>();
        for (Message msg : messages) {
            removeAllMessages.add(RemoveByHash.of(msg));
        }

        return CompletableFuture.completedFuture(Map.of("messages", removeAllMessages));
    }
}

```

**警告**：删除消息时，**确保**生成的消息历史有效。检查你使用的 LLM 提供商的限制。例如：

* 某些提供商期望消息历史以 `user` 消息开始
* 大多数提供商要求带有工具调用的 `assistant` 消息后跟相应的 `tool` 结果消息

```java
package com.xxl.ai.framework.hook;

import com.alibaba.cloud.ai.graph.OverAllState;
import com.alibaba.cloud.ai.graph.RunnableConfig;
import com.alibaba.cloud.ai.graph.agent.hook.HookPosition;
import com.alibaba.cloud.ai.graph.agent.hook.ModelHook;
import com.alibaba.cloud.ai.graph.state.RemoveByHash;
import org.springframework.ai.chat.messages.Message;

import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import java.util.Optional;
import java.util.concurrent.CompletableFuture;

/**
 * DeleteOldMessagesHook 删除旧消息
 *
 * @Author xxl
 * @Date 2025/12/2 14:19
 */
public class MessageDeleteOldHook extends ModelHook {

    @Override
    public String getName() {
        return "delete_old_messages";
    }

    @Override
    public HookPosition[] getHookPositions() {
        return new HookPosition[]{HookPosition.AFTER_MODEL};
    }

    @Override
    public CompletableFuture<Map<String, Object>> afterModel(OverAllState state, RunnableConfig config) {
        Optional<Object> messagesOpt = state.value("messages");
        if (!messagesOpt.isPresent()) {
            return CompletableFuture.completedFuture(Map.of());
        }

        List<Message> messages = (List<Message>) messagesOpt.get();
        if (messages.size() > 2) {
            // 将最早的两条消息转为 RemoveByHash 对象以便从状态中删除
            List<Object> removeOldMessages = new ArrayList<>();
            removeOldMessages.add(RemoveByHash.of(messages.get(0)));
            removeOldMessages.add(RemoveByHash.of(messages.get(1)));
            return CompletableFuture.completedFuture(Map.of("messages", removeOldMessages));
        }

        return CompletableFuture.completedFuture(Map.of());
    }
}

```

### 总结消息

如上所示，修剪或删除消息的问题在于你可能会丢失消息队列淘汰的信息。因此，一些应用程序受益于使用聊天模型总结消息历史的更复杂方法。

要在 Agent 中总结消息历史，可以使用自定义 Hook：

```java
package com.xxl.ai.framework.hook;

import com.alibaba.cloud.ai.graph.OverAllState;
import com.alibaba.cloud.ai.graph.RunnableConfig;
import com.alibaba.cloud.ai.graph.agent.hook.HookPosition;
import com.alibaba.cloud.ai.graph.agent.hook.ModelHook;
import com.alibaba.cloud.ai.graph.state.RemoveByHash;
import org.springframework.ai.chat.messages.Message;
import org.springframework.ai.chat.messages.SystemMessage;
import org.springframework.ai.chat.messages.UserMessage;
import org.springframework.ai.chat.model.ChatModel;
import org.springframework.ai.chat.model.ChatResponse;
import org.springframework.ai.chat.prompt.Prompt;

import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import java.util.Optional;
import java.util.concurrent.CompletableFuture;

/**
 * @Classname MessageSummarizationHook
 * @Description MessageSummarizationHook 总结消息
 * @Date 2025/12/7 13:56
 * @Created by xxl
 */
public class MessageSummarizationHook extends ModelHook {

    private final ChatModel summaryModel;
    private final int maxTokensBeforeSummary;
    private final int messagesToKeep;

    public MessageSummarizationHook(
            ChatModel summaryModel,
            int maxTokensBeforeSummary,
            int messagesToKeep
    ) {
        this.summaryModel = summaryModel;
        this.maxTokensBeforeSummary = maxTokensBeforeSummary;
        this.messagesToKeep = messagesToKeep;
    }

    @Override
    public String getName() {
        return "message_summarization";
    }

    @Override
    public HookPosition[] getHookPositions() {
        return new HookPosition[]{HookPosition.BEFORE_MODEL};
    }

    @Override
    public CompletableFuture<Map<String, Object>> beforeModel(OverAllState state, RunnableConfig config) {
        Optional<Object> messagesOpt = state.value("messages");
        if (!messagesOpt.isPresent()) {
            return CompletableFuture.completedFuture(Map.of());
        }

        List<Message> messages = (List<Message>) messagesOpt.get();

        // 估算 token 数量（简化版）
        int estimatedTokens = messages.stream()
                .mapToInt(m -> m.getText().length() / 4)
                .sum();

        if (estimatedTokens < maxTokensBeforeSummary) {
            return CompletableFuture.completedFuture(Map.of());
        }

        // 需要总结
        int messagesToSummarize = messages.size() - messagesToKeep;
        if (messagesToSummarize <= 0) {
            return CompletableFuture.completedFuture(Map.of());
        }

        List<Message> oldMessages = messages.subList(0, messagesToSummarize);

        // 生成摘要
        String summary = generateSummary(oldMessages);

        // 创建摘要消息
        SystemMessage summaryMessage = new SystemMessage("## 之前对话摘要:\n" + summary);

        // 只需要把摘要消息和需要删除的消息保留在状态中，其余未包含的消息将会自动保留
        List<Object> newMessages = new ArrayList<>();
        newMessages.add(summaryMessage);
        // IMPORTANT! Convert summarized messages to RemoveByHash objects so we can remove them from state
        for (Message msg : oldMessages) {
            newMessages.add(RemoveByHash.of(msg));
        }

        return CompletableFuture.completedFuture(Map.of("messages", newMessages));
    }

    private String generateSummary(List<Message> messages) {
        StringBuilder conversation = new StringBuilder();
        for (Message msg : messages) {
            conversation.append(msg.getMessageType())
                    .append(": ")
                    .append(msg.getText())
                    .append("\n");
        }

        String summaryPrompt = "请简要总结以下对话:\n" + conversation;

        ChatResponse response = summaryModel.call(
                new Prompt(new UserMessage(summaryPrompt))
        );

        return response.getResult().getOutput().getText();
    }

    @Override
    public CompletableFuture<Map<String, Object>> afterModel(OverAllState state, RunnableConfig config) {
        return CompletableFuture.completedFuture(Map.of());
    }
}

```

MessageSummarizationHook 总结消息示例

```java
/**
 * MessageSummarizationHook 总结消息示例
 */
@SneakyThrows
private static void messageSummarizationConfiguration() {
    // 初始化 ChatModel
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .build();

    ChatModel summaryModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .build();
    MessageSummarizationHook summarizationHook = new MessageSummarizationHook(
        summaryModel,
        4000,  // 在 4000 tokens 时触发总结
        20     // 总结后保留最后 20 条消息
    );
    // 创建 Agent
    ReactAgent agent = ReactAgent.builder()
        .name("my_agent")
        .model(chatModel)
        .hooks(summarizationHook) // [!code ++]
        .saver(new MemorySaver())
        .build();

    RunnableConfig config = RunnableConfig.builder()
        .threadId("1")
        .build();

    agent.call("你好，我叫 bob", config);
    agent.call("写一首关于猫的短诗", config);
    agent.call("现在对狗做同样的事情", config);
    AssistantMessage finalResponse = agent.call("我叫什么名字？", config);

    System.out.println(finalResponse.getText());
}
```

## 七、访问记忆

你可以通过多种方式访问和修改 Agent 的短期记忆（状态）：

### 工具

#### 在工具中读取短期记忆

使用 `ToolContext` 参数在工具中访问短期记忆（状态）。

`toolContext` 参数从工具签名中隐藏（因此模型看不到它），但工具可以通过它访问状态。

```java
package com.xxl.ai.framework.tool;

import com.alibaba.cloud.ai.graph.RunnableConfig;
import org.springframework.ai.chat.model.ToolContext;

import java.util.function.BiFunction;

/**
 * 用户信息记忆
 *
 * @Author xxl
 * @Date 2025/12/2 17:18
 */
public class UserInfoTool implements BiFunction<String, ToolContext, String> {

    @Override
    public String apply(String query, ToolContext toolContext) {
        // 从上下文中获取用户信息
        RunnableConfig config = (RunnableConfig) toolContext.getContext().get("config");
        String userId = (String) config.metadata("user_id").orElse("");

        if ("user_123".equals(userId)) {
            return "用户是 John Smith";
        } else {
            return "未知用户";
        }
    }
}
```

在工具中读取短期记忆示例

```java
/**
 * 在工具中读取短期记忆示例
 */
private static void userToolConfiguration() {
    // 初始化 ChatModel
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .build();
    // 创建工具  // [!code ++:6]
    ToolCallback getUserInfoTool = FunctionToolCallback
        .builder("get_user_info", new UserInfoTool())
        .description("查找用户信息")
        .inputType(String.class)
        .build();
    // 创建 Agent
    ReactAgent agent = ReactAgent.builder()
        .name("my_agent")
        .model(chatModel)
        .tools(getUserInfoTool) // [!code ++]
        .saver(new MemorySaver())
        .build();

    RunnableConfig config = RunnableConfig.builder()
        .threadId("1")
        .addMetadata("user_id", "user_123")
        .build();

    AssistantMessage response = agent.call("获取用户信息", config);
    System.out.println(response.getText());
}
```

#### 从工具写入短期记忆

要在执行期间修改 Agent 的短期记忆（状态），你可以在 Hook 中更新状态，或者使用工具返回的信息更新状态。

这对于持久化中间结果或使信息对后续工具或提示可访问很有用。

### 提示

在 Hook 中访问短期记忆（状态）以基于对话历史或自定义状态字段创建动态提示。

```java
package com.xxl.ai.framework.interceptor;

import com.alibaba.cloud.ai.graph.agent.interceptor.ModelCallHandler;
import com.alibaba.cloud.ai.graph.agent.interceptor.ModelInterceptor;
import com.alibaba.cloud.ai.graph.agent.interceptor.ModelRequest;
import com.alibaba.cloud.ai.graph.agent.interceptor.ModelResponse;
import org.springframework.ai.chat.messages.SystemMessage;

import java.util.Map;

/**
 * @Classname DynamicPromptNameInterceptor
 * @Description DynamicPromptNameInterceptor 动态提示
 * @Date 2025/12/7 19:55
 * @Created by xxl
 */
public class DynamicPromptNameInterceptor extends ModelInterceptor {

    @Override
    public ModelResponse interceptModel(ModelRequest request, ModelCallHandler handler) {
        // 从上下文中获取用户名
        Map<String, Object> context = request.getContext();
        String userName = (String) context.get("user_name");

        // 创建动态系统提示
        String systemPrompt = "你是一个有帮助的助手。称呼用户为 " + userName + "。";

        // 创建修改后的请求（示例），实际使用中需要根据具体 API 进行调整
        SystemMessage enhancedSystemMessage;
        if (request.getSystemMessage() == null) {
            enhancedSystemMessage = new SystemMessage(systemPrompt);
        } else {
            enhancedSystemMessage = new SystemMessage(request.getSystemMessage().getText() + "\n" + systemPrompt);
        }

        // Create enhanced request
        ModelRequest enhancedRequest = ModelRequest.builder(request)
                .systemMessage(enhancedSystemMessage)
                .build();

        return handler.call(enhancedRequest);
    }

    @Override
    public String getName() {
        return "DynamicPromptNameInterceptor";
    }
}

```

DynamicPromptInterceptor 动态提示示例

```java
/**
 * DynamicPromptInterceptor 动态提示示例
 */
@SneakyThrows
private static void dynamicPromptInterceptorConfiguration() {
    // 初始化 ChatModel
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .build();
    // 定义天气查询工具
    class WeatherTool implements BiFunction<String, ToolContext, String> {
        @Override
        public String apply(String city, ToolContext toolContext) {
            return "It's always sunny in " + city + "!";
        }
    }
    ToolCallback getWeatherTool = FunctionToolCallback.builder("get_weather", new WeatherTool())
        .description("Get weather for a given city")
        .inputType(String.class)
        .build();
    // 创建 Agent
    ReactAgent agent = ReactAgent.builder()
        .name("my_agent")
        .model(chatModel)
        .tools(getWeatherTool)
        .interceptors(new DynamicPromptInterceptor()) // [!code ++]
        .build();
    // 使用时传递上下文
    Map<String, Object> context = Map.of("user_name", "John Smith");
}
```

### Before Model

在 `beforeModel` Hook 中访问短期记忆（状态）以在模型调用之前处理消息。

```java
package com.xxl.ai.framework.hook;

import com.alibaba.cloud.ai.graph.OverAllState;
import com.alibaba.cloud.ai.graph.RunnableConfig;
import com.alibaba.cloud.ai.graph.agent.hook.HookPosition;
import com.alibaba.cloud.ai.graph.agent.hook.ModelHook;
import org.springframework.messaging.Message;

import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import java.util.Optional;
import java.util.concurrent.CompletableFuture;

/**
 * @Classname MessageTrimHook
 * @Description MessageTrimHook Before Model 示例
 * @Date 2025/12/7 20:25
 * @Created by xxl
 */
public class MessageTrimHook extends ModelHook {

    @Override
    public String getName() {
        return "trim_messages";
    }

    @Override
    public HookPosition[] getHookPositions() {
        return new HookPosition[]{HookPosition.BEFORE_MODEL};
    }

    @Override
    public CompletableFuture<Map<String, Object>> beforeModel(OverAllState state, RunnableConfig config) {
        // 访问和修改消息
        Optional<Object> messagesOpt = state.value("messages");
        if (messagesOpt.isPresent()) {
            List<Message> messages = (List<Message>) messagesOpt.get();

            if (messages.size() <= 3) {
                return CompletableFuture.completedFuture(Map.of()); // 无需更改
            }

            // 保留第一条和最后几条消息，并将中间消息标记为删除
            Message firstMsg = messages.get(0);
            List<Message> recentMessages = messages.subList(
                    messages.size() - 3,
                    messages.size()
            );

            List<Object> newMessages = new ArrayList<>();
            newMessages.add(firstMsg);
            newMessages.addAll(recentMessages);
            // 标记中间消息为删除（使用 RemoveByHash）
            if (messages.size() - 3 > 1) {
                for (Message msg : messages.subList(1, messages.size() - 3)) {
                    newMessages.add(com.alibaba.cloud.ai.graph.state.RemoveByHash.of(msg));
                }
            }

            return CompletableFuture.completedFuture(Map.of("messages", newMessages));
        }

        return CompletableFuture.completedFuture(Map.of());
    }

    @Override
    public CompletableFuture<Map<String, Object>> afterModel(OverAllState state, RunnableConfig config) {
        return CompletableFuture.completedFuture(Map.of());
    }
}
```

MessageTrimHook Before Model 示例

```java
/**
 * MessageTrimHook Before Model 示例
 */
@SneakyThrows
private static void messageTrimHookBeforeModelConfiguration() {
    // 初始化 ChatModel
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .build();
    // 创建 Agent
    ReactAgent agent = ReactAgent.builder()
        .name("my_agent")
        .model(chatModel)
        .hooks(new MessageTrimHook()) // [!code ++]
        .saver(new MemorySaver())
        .build();
    AssistantMessage response = agent.call("你好");
    System.out.println(response.getText());
}
```

### After Model

在 `afterModel` Hook 中访问短期记忆（状态）以在模型调用之后处理消息。

```java
package com.xxl.ai.framework.hook;

import com.alibaba.cloud.ai.graph.OverAllState;
import com.alibaba.cloud.ai.graph.RunnableConfig;
import com.alibaba.cloud.ai.graph.agent.hook.HookPosition;
import com.alibaba.cloud.ai.graph.agent.hook.ModelHook;
import org.springframework.ai.chat.messages.AssistantMessage;
import org.springframework.ai.chat.messages.Message;

import java.util.List;
import java.util.Map;
import java.util.Optional;
import java.util.concurrent.CompletableFuture;

/**
 * @Classname ValidateResponseHook
 * @Description ValidateResponseHook After Model 敏感词工具
 * @Date 2025/12/7 20:16
 * @Created by xxl
 */
public class ValidateResponseHook extends ModelHook {

    private static final List<String> STOP_WORDS =
            List.of("password", "secret", "api_key");

    @Override
    public String getName() {
        return "validate_response";
    }

    @Override
    public HookPosition[] getHookPositions() {
        return new HookPosition[]{HookPosition.AFTER_MODEL};
    }

    @Override
    public CompletableFuture<Map<String, Object>> beforeModel(OverAllState state, RunnableConfig config) {
        return CompletableFuture.completedFuture(Map.of());
    }

    @Override
    public CompletableFuture<Map<String, Object>> afterModel(OverAllState state, RunnableConfig config) {
        Optional<Object> messagesOpt = state.value("messages");
        if (!messagesOpt.isPresent()) {
            return CompletableFuture.completedFuture(Map.of());
        }

        List<Message> messages = (List<Message>) messagesOpt.get();
        if (messages.isEmpty()) {
            return CompletableFuture.completedFuture(Map.of());
        }

        Message lastMessage = messages.get(messages.size() - 1);
        String content = lastMessage.getText();

        // 检查是否包含敏感词
        for (String stopWord : STOP_WORDS) {
            if (content.toLowerCase().contains(stopWord)) {
                // 移除包含敏感词的消息
                List<Message> filtered = messages.subList(0, messages.size() - 1);
                filtered.add(new AssistantMessage("\n抱歉，我无法提供该信息。\n"));
                return CompletableFuture.completedFuture(Map.of("messages", filtered));
            }
        }

        return CompletableFuture.completedFuture(Map.of());
    }
}
```

ValidateResponseHook After Model 示例

```java
/**
 * ValidateResponseHook After Model 示例
 */
@SneakyThrows
private static void validateResponseHookAfterModelConfiguration() {
    // 初始化 ChatModel
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .build();
    // 创建 Agent
    ReactAgent agent = ReactAgent.builder()
        .name("secure_agent")
        .model(chatModel)
        .hooks(new ValidateResponseHook()) // [!code ++]
        .saver(new MemorySaver())
        .build();
    AssistantMessage response = agent.call("你好");
    System.out.println(response.getText());
}
```

---

---
url: /常用框架/SpringAIAlibaba/Agent Framework/10_Memory记忆管理.md
---

# Memory 记忆管理

---

---
url: /常用框架/SpringAIAlibaba/Agent Framework/5_Messages消息.md
---

# Messages 消息

Messages 是 Spring AI Alibaba 中模型交互的基本单元。它们代表模型的输入和输出，携带在与 LLM 交互时表示对话状态所需的内容和元数据。

Messages 是包含以下内容的对象：

* **Role（角色）** - 标识消息类型（如 `system`、`user`、`assistant`）
* **Content（内容）** - 表示消息的实际内容（如文本、图像、音频、文档等）
* **Metadata（元数据）** - 可选字段，如响应信息、消息 ID 和 token 使用情况

Spring AI Alibaba 提供了一个标准的消息类型系统，可在所有模型提供商之间工作，确保无论调用哪个模型都具有一致的行为。

## 一、基础使用

使用 messages 最简单的方式是创建消息对象并在调用模型时传递它们。

```java
/**
 * 基础消息使用示例
 */
public static void basicMessageConfiguration() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();

    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();

    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();
    
    // 创建消息对象 // [!code ++:8]
    SystemMessage systemMsg = new SystemMessage("你是一个有帮助的助手。");
    UserMessage userMsg = new UserMessage("你好，你好吗？");

    // 与聊天模型一起使用
    List<org.springframework.ai.chat.messages.Message> messages = List.of(systemMsg, userMsg);
    Prompt prompt = new Prompt(messages);
    ChatResponse response = chatModel.call(prompt);  // 返回 ChatResponse，包含 AssistantMessage
    String answer = response.getResult().getOutput().getText();
    System.out.println(answer);
}
```

### 文本提示

文本提示是字符串 - 适用于简单的生成任务，不需要保留对话历史。

```java
/**
 * 文本提示示例
 */
public static void textConfiguration() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();

    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();

    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();

    // 使用字符串直接调用 // [!code ++:3]
    String response = chatModel.call("写一首关于春天的俳句");
    System.out.println(response);
}
```

**使用文本提示的场景**：

* 有单个独立的请求
* 不需要对话历史
* 想要最小的代码复杂性

### 消息提示

或者，你可以通过提供消息对象列表向模型传递消息列表。

```java
/**
 * 消息提示示例
 */
public static void messageConfiguration() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();

    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();

    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();

    // 创建消息对象 // [!code ++:8]
    List<org.springframework.ai.chat.messages.Message> messages = List.of(
        new SystemMessage("你是一个诗歌专家"),
        new UserMessage("写一首关于春天的俳句"),
        new AssistantMessage("樱花盛开时...")
    );
    Prompt prompt = new Prompt(messages);
    ChatResponse response = chatModel.call(prompt);
    String answer = response.getResult().getOutput().getText();
    System.out.println(answer);
}
```

**使用消息提示的场景**：

* 管理多轮对话
* 处理多模态内容（图像、音频、文件）
* 包含系统指令

## 二、消息类型

* **System Message（系统消息）** - 告诉模型如何行为并为交互提供上下文
* **User Message（用户消息）** - 表示用户输入和与模型的交互
* **Assistant Message（助手消息）** - 模型生成的响应，包括文本内容、工具调用和元数据
* **Tool Response Message（工具响应消息）** - 表示工具调用的输出

### System Message

`SystemMessage` 表示一组初始指令，用于引导模型的行为。你可以使用系统消息来设置语气、定义模型的角色并建立响应指南。

```java
/**
 * SystemMessage 基础指令示例
 */
public static void systemMessageConfiguration() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();

    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();

    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();

    // 基础指令 // [!code ++:9]
    SystemMessage systemMsg = new SystemMessage("你是一个有帮助的编程助手。");

    List<org.springframework.ai.chat.messages.Message> messages = List.of(
        systemMsg,
        new UserMessage("如何创建 REST API？")
    );
    // 调用并获取响应
    ChatResponse response = chatModel.call(new Prompt(messages));
    String answer = response.getResult().getOutput().getText();
    System.out.println(answer);
}
```

```java
/**
 * SystemMessage 详细角色设定示例
 */
public static void systemDetailMessageConfiguration() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();

    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();

    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();

    // 详细的角色设定 // [!code ++:13]
    SystemMessage systemMsg = new SystemMessage("""
         你是一位资深的 Java 开发者，擅长 Web 框架。
         始终提供代码示例并解释你的推理。
         在解释中要简洁但透彻。
         """);

    List<org.springframework.ai.chat.messages.Message> messages = List.of(
         systemMsg,
         new UserMessage("如何创建 REST API？")
    );
    // 调用并获取响应
    ChatResponse response = chatModel.call(new Prompt(messages));
    String answer = response.getResult().getOutput().getText();
    System.out.println(answer);
}
```

### User Message

`UserMessage` 表示用户输入和交互。它们可以包含文本、图像、音频、文件和任何其他数量的多模态内容。

#### 文本内容

```java
/**
 * UserMessage 文本内容示例
 */
@SneakyThrows
public static void userMessageTextConfiguration() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();

    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();

    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();

    // 调用并获取响应
    // 使用消息对象  // [!code ++:4]
    ChatResponse response = chatModel.call(
        new Prompt(List.of(new UserMessage("什么是机器学习？")))
    );
    String answer = response.getResult().getOutput().getText();
    System.out.println(answer);
    // 使用字符串快捷方式   // [!code ++:3]
    // 使用字符串是单个 UserMessage 的快捷方式
    String response2 = chatModel.call("什么是机器学习？");
    System.out.println(response2);
}
```

#### 消息元数据

```java
/**
 * UserMessage 消息元数据示例
 */
@SneakyThrows
public static void userMessageMateConfiguration() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();

    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();

    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();

    // 调用并获取响应  // [!code ++:8]
    UserMessage userMsg = UserMessage.builder()
        .text("你好！")
        .metadata(Map.of(
            "user_id", "alice",  // 可选：识别不同用户
            "session_id", "sess_123"  // 可选：会话标识符
        ))
        .build();
    ChatResponse response = chatModel.call(new Prompt(userMsg));
    String answer0 = response.getResult().getOutput().getText();
    System.out.println(answer0);
}
```

**注意**：元数据字段的行为因提供商而异 - 有些用于用户识别，有些则忽略它。要检查，请参考模型提供商的文档。

#### 多模态内容

`UserMessage` 可以包含多模态内容，如图像：

```java
/**
 * UserMessage 多模态内容示例
 */
@SneakyThrows
public static void userMessageUrlConfiguration() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();

    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();

    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();

    // 调用并获取响应
    // 从 URL 创建图像  // [!code ++:8]
    UserMessage userMsg = UserMessage.builder()
        .text("描述这张图片的内容。")
        .media(Media.builder()
               .mimeType(MimeTypeUtils.IMAGE_JPEG)
               .data(new URL("https://example.com/image.jpg"))
               .build())
        .build();
    ChatResponse response = chatModel.call(new Prompt(userMsg));
    String answer0 = response.getResult().getOutput().getText();
    System.out.println(answer0);
}
```

### Assistant Message

`AssistantMessage` 表示模型调用的输出。它们可以包括多模态数据、工具调用以及你稍后可以访问的提供商特定元数据。

```java
/**
 * AssistantMessage 基础使用示例
 */
@SneakyThrows
public static void assistantMessageBasicConfiguration() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();

    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();

    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();
	 // [!code ++:4]
    ChatResponse response = chatModel.call(new Prompt("解释 AI"));
    AssistantMessage aiMessage = response.getResult().getOutput();
    System.out.println(aiMessage.getText());
}
```

`AssistantMessage` 对象由模型调用返回，其中包含响应中的所有相关元数据。

提供商对消息类型的权重/上下文化方式不同，这意味着有时手动创建新的 `AssistantMessage` 对象并将其插入消息历史中（就像它来自模型一样）会很有帮助。

```java
/**
 * 手动创建 AssistantMessage 示例
 */
@SneakyThrows
public static void assistantMessageManualConfiguration() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();

    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();

    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();

    // 手动创建 AI 消息（例如，用于对话历史） // [!code ++:12]
    AssistantMessage aiMsg = new AssistantMessage("我很乐意帮助你回答这个问题！");

    // 添加到对话历史
    List<org.springframework.ai.chat.messages.Message> messages = List.of(
        new SystemMessage("你是一个有帮助的助手"),
        new UserMessage("你能帮我吗？"),
        aiMsg,  // 插入，就像它来自模型一样
        new UserMessage("太好了！2+2 等于多少？")
    );

    ChatResponse response = chatModel.call(new Prompt(messages));
    AssistantMessage aiMessage = response.getResult().getOutput();
    System.out.println(aiMessage.getText());
}
```

**AssistantMessage 属性**：

* **text**: 消息的文本内容
* **metadata**: 消息的元数据映射
* **toolCalls**: 模型进行的工具调用列表
* **media**: 媒体内容列表（如果有）

#### 工具调用

当模型进行工具调用时，它们包含在 `AssistantMessage` 中：

```java
/**
 * AssistantMessage 工具调用示例
 */
@SneakyThrows
public static void assistantMessageToolConfiguration() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();

    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();

    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();
	// [!code ++:10]
    ChatResponse response = chatModel.call(new Prompt("你好"));
    AssistantMessage aiMessage = response.getResult().getOutput();

    if (aiMessage.hasToolCalls()) {
        for (AssistantMessage.ToolCall toolCall : aiMessage.getToolCalls()) {
            System.out.println("Tool: " + toolCall.name());
            System.out.println("Args: " + toolCall.arguments());
            System.out.println("ID: " + toolCall.id());
        }
    }
}
```

#### Token 使用

Spring AI Alibaba 的 `ChatResponse` 可以在其元数据中保存 token 计数和其他使用元数据：

```java
/**
 * Token 使用信息访问示例
 */
@SneakyThrows
public static void assistantMessageTokenConfiguration() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();

    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();

    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();
    // [!code ++:9]
    ChatResponse response = chatModel.call(new Prompt("你好！"));
    ChatResponseMetadata metadata = response.getMetadata();

    // 访问使用信息
    if (metadata != null && metadata.getUsage() != null) {
        System.out.println("Input tokens: " + metadata.getUsage().getPromptTokens());
        System.out.println("Output tokens: " + metadata.getUsage().getCompletionTokens());
        System.out.println("Total tokens: " + metadata.getUsage().getTotalTokens());
    }
}
```

#### 流式和块

在流式传输期间，你将收到可以组合成完整消息对象的块：

```java
/**
 * 流式输出示例
 */
@SneakyThrows
public static void assistantMessageStreamConfiguration() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();

    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();

    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();
    // [!code ++:10]
    Flux<ChatResponse> responseStream = chatModel.stream(new Prompt("你好"));

    StringBuilder fullResponse = new StringBuilder();
    responseStream.subscribe(
        chunk -> {
            String content = chunk.getResult().getOutput().getText();
            fullResponse.append(content);
            System.out.print(content);
        }
    );
}
```

**了解更多**：

* 从聊天模型流式传输 tokens
* 从 agents 流式传输 tokens 和/或步骤

### Tool Response Message

对于支持工具调用的模型，AI 消息可以包含工具调用。工具消息用于将单个工具执行的结果传回模型。

```java
/**
 * ToolResponseMessage 工具响应消息示例
 */
@SneakyThrows
public static void toolResponseMessageConfiguration() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();

    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();

    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();

    // 在模型进行工具调用后 // [!code ++:28]
    AssistantMessage aiMessage = AssistantMessage.builder()
        .content("")
        .toolCalls(List.of(
            new AssistantMessage.ToolCall(
                "call_123",
                "tool",
                "get_weather",
                "{\"location\": \"San Francisco\"}"
            )
        ))
        .build();

    // 执行工具并创建结果消息
    String weatherResult = "晴朗，22°C";
    ToolResponseMessage toolMessage = ToolResponseMessage.builder()
        .responses(List.of(
            new ToolResponseMessage.ToolResponse("call_123", "get_weather", weatherResult)
        ))
        .build();

    // 继续对话
    List<org.springframework.ai.chat.messages.Message> messages = List.of(
        new UserMessage("旧金山的天气怎么样？"),
        aiMessage,      // 模型的工具调用
        toolMessage     // 工具执行结果
    );
    ChatResponse response = chatModel.call(new Prompt(messages));
    System.out.println(response.getResult().getOutput().getText());
}
```

**ToolResponseMessage 属性**：

* responses: ToolResponse 对象列表，每个包含：
  * **id**: 工具调用 ID（必须与 AIMessage 中的工具调用 ID 匹配）
  * **name**: 调用的工具名称
  * **responseData**: 工具调用的字符串化输出

## 三、多模态内容

**多模态性**指的是处理不同形式数据的能力，如文本、音频、图像和视频。Spring AI Alibaba 包含这些数据的标准类型，可以跨提供商使用。

聊天模型可以接受多模态数据作为输入并生成它作为输出。下面我们展示包含多模态数据的输入消息的简短示例。

### 图像输入

```java
/**
 * 图像输入示例
 */
@SneakyThrows
public static void imageMessageConfiguration() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();

    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();

    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();
    // 从 URL // [!code ++:16]
    UserMessage urlMessage = UserMessage.builder()
        .text("描述这张图片的内容。")
        .media(Media.builder()
               .mimeType(MimeTypeUtils.IMAGE_JPEG)
               .data(new URL("https://example.com/image.jpg"))
               .build())
        .build();
    // 从本地文件
    UserMessage localMessage = UserMessage.builder()
        .text("描述这张图片的内容。")
        .media(new Media(
            MimeTypeUtils.IMAGE_JPEG,
            new ClassPathResource("images/photo.jpg")
        ))
        .build();
    ChatResponse response = chatModel.call(new Prompt(urlMessage, localMessage));
    System.out.println(response.getResult().getOutput().getText());
}
```

### 音频输入

```java
/**
 * 音频输入示例
 */
public static void audioMessageConfiguration() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();

    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();

    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();
    // [!code ++:7]
    UserMessage message = UserMessage.builder()
        .text("描述这段音频的内容。")
        .media(new Media(
            MimeTypeUtils.parseMimeType("audio/wav"),
            new ClassPathResource("audio/recording.wav")
        ))
        .build();
    ChatResponse response = chatModel.call(new Prompt(message));
    System.out.println(response.getResult().getOutput().getText());
}
```

### 视频输入

```java
/**
 * 视频输入示例
 */
@SneakyThrows
public static void videoMessageConfiguration() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();

    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();

    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();
    // [!code ++:7]
    UserMessage message = UserMessage.builder()
        .text("描述这段视频的内容。")
        .media(Media.builder()
               .mimeType(MimeTypeUtils.parseMimeType("video/mp4"))
               .data(new URL("https://example.com/path/to/video.mp4"))
               .build())
        .build();
    ChatResponse response = chatModel.call(new Prompt(message));
    System.out.println(response.getResult().getOutput().getText());
}
```

**警告**：并非所有模型都支持所有文件类型。请查看模型提供商的文档以了解支持的格式和大小限制。

## 四、与 Chat Models 一起使用

Chat models 接受消息对象序列作为输入并返回 `ChatResponse`（包含 `AssistantMessage`）作为输出。交互通常是无状态的，因此简单的对话循环涉及使用不断增长的消息列表调用模型。

### 基础对话示例

```java
/**
 * Chat Models — 基础对话示例
 */
@SneakyThrows
public static void chatModelBasicMessageConfiguration() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();

    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();

    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();
	// [!code ++:15]
    List<Message> conversationHistory = new ArrayList<>();

    // 第一轮对话
    conversationHistory.add(new UserMessage("你好！"));
    ChatResponse response1 = chatModel.call(new Prompt(conversationHistory));
    conversationHistory.add(response1.getResult().getOutput());

    // 第二轮对话
    conversationHistory.add(new UserMessage("你能帮我学习 Java 吗？"));
    ChatResponse response2 = chatModel.call(new Prompt(conversationHistory));
    conversationHistory.add(response2.getResult().getOutput());

    // 第三轮对话
    conversationHistory.add(new UserMessage("从哪里开始？"));
    ChatResponse response3 = chatModel.call(new Prompt(conversationHistory));

    String answer = response3.getResult().getOutput().getText();
    System.out.println(answer);
}
```

### 使用 Builder 模式

Spring AI Alibaba 的消息类提供了 builder 模式以便于构建：

```java
/**
 * Chat Models — Builder 模式示例
 */
@SneakyThrows
public static void chatModelBuildMessageConfiguration() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();

    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();

    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();
    // [!code ++:21]
    List<Message> conversationHistory = new ArrayList<>();

    // UserMessage with builder
    UserMessage userMsg = UserMessage.builder()
        .text("你好，我想学习 Spring AI Alibaba")
        .metadata(Map.of("user_id", "user_123"))
        .build();
    conversationHistory.add(userMsg);

    // SystemMessage with builder
    SystemMessage systemMsg = SystemMessage.builder()
        .text("你是一个 Spring 框架专家")
        .metadata(Map.of("version", "1.0"))
        .build();
    conversationHistory.add(systemMsg);

    // AssistantMessage with builder
    AssistantMessage assistantMsg = AssistantMessage.builder()
        .content("我很乐意帮助你学习 Spring AI Alibaba！")
        .build();
    conversationHistory.add(assistantMsg);

    ChatResponse response = chatModel.call(new Prompt(conversationHistory));
    String answer = response.getResult().getOutput().getText();
    System.out.println(answer);
}
```

### 消息复制和修改

```java
/**
 * Chat Models — 消息复制和修改
 */
public static void messageCopyAndModify() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();

    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();

    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();
    // [!code ++:9]
    // 复制消息
    UserMessage original = new UserMessage("原始消息");
    UserMessage copy = original.copy();

    // 使用 mutate 创建修改的副本
    UserMessage modified = original.mutate()
        .text("修改后的消息")
        .metadata(Map.of("modified", true))
        .build();

    ChatResponse response = chatModel.call(new Prompt(copy, modified));
    String answer = response.getResult().getOutput().getText();
    System.out.println(answer);
}
```

## 五、在 ReactAgent 中使用

ReactAgent 自动管理消息历史，但你也可以直接使用消息：

```java
/**
 * 在 ReactAgent 中使用消息
 */
public static void messagesInReactAgent() throws GraphRunnerException {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();

    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();

    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();
    // [!code ++:19]
    ReactAgent agent = ReactAgent.builder()
        .name("my_agent")
        .model(chatModel)
        .systemPrompt("你是一个有帮助的助手")
        .build();
    
    // 使用字符串
    AssistantMessage response1 = agent.call("你好");

    // 使用 UserMessage
    UserMessage userMsg = new UserMessage("帮我写一首诗");
    AssistantMessage response2 = agent.call(userMsg);

    // 使用消息列表
    List<Message> messages = List.of(
        new UserMessage("我喜欢春天"),
        new UserMessage("写一首关于春天的诗")
    );
    AssistantMessage response3 = agent.call(messages);
}
```

---

---
url: /StableDiffusion/Midjourney/3_Midjourney垫图.md
---

# Midjourney垫图

https://zhuanlan.zhihu.com/p/607038739

https://www.yuque.com/u32937722/qb6y63/sata02aq53qhll78

---

---
url: /StableDiffusion/Midjourney/0_Midjourney基础教程.md
---

# Midjourney基础教程

## 一、使用流程

使用Midjourney绘制你的第一张AI图只需三步：

1. 新建专属服务器

2. 添加Midjourney机器人

3. 开始绘画

## 二、基础创建步骤

### 第一步建立服务器🎨

添加一个自己的服务器（防止刷屏看不到自己的图），从左侧服务器向最下面找到加号。

![1703768758542.png](/assets/1703768758542.BYlHxD6g.png)

创建自己的服务器（服务器名称用自己的）

![1703768337872.png](/assets/1703768337872.Tx-pEeyA.png)

![1703768294285.png](/assets/1703768294285.NFnyoxBa.png)

![1703768326733.png](/assets/1703768326733.DYn4zNF-.png)

### 第二步添加机器人🎨

1、复制邀请链接>粘贴到对话框>回车发送>然后点开链接👇👇👇

https://discord.com/api/oauth2/authorize?client\_id=936929561302675456\&permissions=274877945856\&scope=bot

![image.png](/assets/image-1717346468460.BcfA0csB.png)

2、点击一下链接，确认邀请加入>点击授权即可邀请成功

![image.png](/assets/image-1717346468459.Cv3fvWU3.png)

**3、已经邀请成功可以开始出图。**

![image.png](/assets/image-1717346486707.DVP46lF3.png)

### 第三步出图方法🎨

可以让AI帮你画图啦（搜索/imagine）在后面添加关键词

![image-1717346486708](/assets/image-1717346486708.CHtDDxGx.png)

![image-1717346486709](/assets/image-1717346486709.BTLOJkcm.png)

![image-1717346486710](/assets/image-1717346486710.B6llEe3k.png)

![image-1717346486711](0_Midjourney基础教程.assets/image-1717346486711.png)

![image-1717346486712](/assets/image-1717346486712.B793YW1M.png)

## 资料

Discord网址：<https://discord.com/>

文档资料：<https://www.yuque.com/u32937722/qb6y63/kmhvqbx97gtn7wy5#V7bxK>

---

---
url: /StableDiffusion/AIGC/Midjourney萌宠治愈频道.md
---

# Midjourney萌宠治愈频道

## 生成图片

`/Describe` 上传图片，生成4组提示词

![image-20240314002428219](/assets/image-20240314002428219.haDTvbt4.png)

```
1️⃣ A cute white cat sleeping on clouds, surrounded by stars and a glowing moon in the sky, with a cartoon style and colorful animation stills. The cat has pink ears and is covered in soft fur. It is smiling happily as it sleeps peacefully under twinkling lights, focus stacked, with a character design in the style of Disney, animated gifs, cartoon scene. --ar 128:61

2️⃣ A cute white cat sleeping on clouds, with stars hanging in the sky and colorful lights illuminating its face. The animation style is in the style of Disney Pixar, with high definition details, warm colors, soft lighting effects, close up shots of the cat's face, a smiling expression, soft fur texture, a dreamy atmosphere, and bright moonlight shining through window panes. --ar 128:61

3️⃣ A cute white cat sleeping on clouds, stars hanging in the sky, pink and yellow color scheme, bright background, anime style, colorful animation stills, charming character illustrations, glowing lights, cartoon characters, colorful animated stills, shining text "Walahimall" above it in the style of an anime artist. --ar 128:61

4️⃣ A cute white cat sleeping on clouds, with stars hanging in the sky and glowing moonlight. The animation style is in the style of Disney Pixar, and "minecraft" elements surround it. It has bright colors and dreamy light effects. The cat's expression was smiling sweetly, with soft fur and pink ears. --ar 128:61
```

选择一组提示词，比例改成16:9，点击提交。

![image-20240314002608216](/assets/image-20240314002608216.Bmu24jtz.png)

为了保证风格统一，添加seed参数。

右键图片，添加反应，显示更多，输入envelope，点击第一个信封icon

![image-20240314002958344](/assets/image-20240314002958344.DYu4PosJ.png)

![image-20240314003040296](/assets/image-20240314003040296.x1oErbmB.png)

系统会在左侧菜单发送SEED值

![image-20240314003108387](/assets/image-20240314003108387.VhydHChH.png)

根据seed值重新编写prompt

![image-20240314003530961](/assets/image-20240314003530961.BeDTTW8-.png)

```
/imagine
A cute white penguin sleeping on clouds, surrounded by stars and a glowing moon in the sky, with a cartoon style and colorful animation stills. The penguin has pink ears and is covered in soft fur. It is smiling happily as it sleeps peacefully under twinkling lights, focus stacked, with a character design in the style of Disney, animated gifs, cartoon scene. --seed 3043895473 --ar 16:9
```

![image-20240314004033413](/assets/image-20240314004033413.Dlmdcq4E.png)

## 制作动画

2D转3D

[LeiaPix - Convert 2D to 3D Depth Animation AI Technology](https://www.leiapix.com/)

注册账号后直接上传图片，几秒钟就能生成一个简单的动画。

![image-20240314005100272](/assets/image-20240314005100272.D0HUR7Bq.png)

在线去水印

https://tools.kalvinbg.cn/

## 无版权音乐

1、YouTube音频库

2、Soundcloud、Jamendo等平台

3、AI生成：Soundful、AIVA、Mubert

以[Mubert](https://mubert.com/render)为例

![image-20240314010446427](/assets/image-20240314010446427.gfF5MhJ-.png)

选择音乐类型及生成的时间

![image-20240314010619842](/assets/image-20240314010619842.SPORTgPH.png)

生成完毕后会展示在下方，可以点击试听

![image-20240314010802706](/assets/image-20240314010802706.Cq23x6ij.png)

点击下载，进入我的下载页面后再点击下载按钮，输入自己的频道链接，拥有音乐的试用权

![image-20240314011132343](/assets/image-20240314011132343.vknuthWl.png)

Music generated by Mubert https://mubert.com/render

## 制作视频

添加腮红、睡觉、星星条贴纸

Bling特效

![image-20240314225527243](/assets/image-20240314225527243.Cgx3cJ2i.png)

---

---
url: /StableDiffusion/AIGC/Midjourney制作微信表情包.md
---

# Midjourney制作微信表情包

> 微信开放平台地址：[微信表情 (qq.com)](https://sticker.weixin.qq.com/cgi-bin/mmemoticonwebnode-bin/pages/home)

## 一、制作流程

1. Midjourney根据提示词生成表情包（一整张图）
2. 扣去背景图
3. 制作单个表情，240\*240
4. 添加文字
5. 导出表情包图片

## 二、制作规则

制作规范：[微信表情开放平台 (qq.com)](https://sticker.weixin.qq.com/cgi-bin/mmemoticon-bin/readtemplate?t=guide/index.html#/makingSpecifications#specifications_stickers)

![image-20240320000559936](/assets/image-20240320000559936.BKSRpN_Z.png)

## 三、生成表情包

### 3.1 设置niji版本

输入框输入 `/setting` 调出面板，选择 `Niji Model V5`。

![image-20240320003805403](/assets/image-20240320003805403.DpCRcsn7.png)

### 3.2 生成表情包

官方规则

![image-20240320003017591](/assets/image-20240320003017591.DffC39kd.png)

两种方式

方法1：根据提示词生成表情包

提示词1

```
一只熊猫表情表，16个表情，多个动态姿势，不同的表情，白色背景，8k， -s 250 -niji 5
emoji sheet of a panda, 16 emoticons, mutiple dynamic pose, different expressions, white background, 8k, -s 250 --niji 5
```

提示词2

```
可爱的女孩，表情包，16个表情，表情表，多种姿势和表情，拟人风格，迪士尼风格，黑色笔画，不同的情绪，8k -niji 5可爱
cute girl, emoji pack, 16 emoticons, emoji sheet, mutiple poses and expressions, anthropomorphic style, Disney style, black strokes, different emotions, 8k --niji 5
```

提示词详解

1. emoji sheet of a XXX，这里的XXX可以替换为想要的主题，比如cat、dog等。
2. 16 emoticons，一套微信表情包是8/16/24个。
3. mutiple dynamic pose，different expressions，多动作，不同表情。
4. white background，白色背景图，方便抠图。
5. 8k，高清图。

| 提示词                                                       | 图片                                                         |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| emoji sheet of a panda, 16 emoticons, mutiple dynamic pose, different expressions, white background, 8k,  --niji 5 | ![gungfupandag0163\_emoji\_sheet\_of\_a\_panda\_16\_emoticons\_mutiple\_dy\_94bd1bcc-e7fa-4b9d-99ea-46c036b68e17](/assets/gungfupandag0163_emoji_sheet_of_a_panda_16_emoticons_mutiple_dy_94bd1bcc-e7fa-4b9d-99ea-46c036b68e17.BY6BF7wZ.png) |
| emoji sheet of a cat, 16 emoticons, mutiple dynamic pose, different expressions, white background, 8k,  --niji 5 | ![gungfupandag0163\_emoji\_sheet\_of\_a\_cat\_16\_emoticons\_mutiple\_dyna\_268b171f-e9bc-4063-98f0-2af64cce465b](/assets/gungfupandag0163_emoji_sheet_of_a_cat_16_emoticons_mutiple_dyna_268b171f-e9bc-4063-98f0-2af64cce465b.BGzHc4dC.png) |

### 3.3 无损放大

https://clipdrop.co/image-upscaler

上传照片，放大2倍即可。

![image-20240321003727997](/assets/image-20240321003727997.DIzepJxF.png)

### 3.4 一键去背景

网址一：https://remove.bg，下载时选择高清版本

![image-20240321004129180](/assets/image-20240321004129180.tzo7c3c8.png)

网址二：https://pixian.ai

![image-20240321004304637](/assets/image-20240321004304637.D52EE3VL.png)

### 3.5 抠图

可以使用Photoshop，Sketch，Figma等设计工具。

或者使用在线处理工具：https://mastergo.com

![image-20240321004915581](/assets/image-20240321004915581.BbnQDuKA.png)

新建容器240\*240，然后多复制几个。根据微信表情包的命名规则，重新命名。01,02,03,04...

然后导入表情包图片，双击进行裁剪。

![image-20240321005022820](/assets/image-20240321005022820.BhvwVUJN.png)

![image-20240321005544724](/assets/image-20240321005544724.DO-rqq5p.png)

![image-20240321005719158](/assets/image-20240321005719158.D--XrneN.png)

回车，确认后。将裁剪的图片拖进容器中，调整大小，按住shift使图片不变形。

### 3.6 添加文字

### 3.7 去背景导出

### 3.8 准备其他素材

## 参考资料

https://www.bilibili.com/video/BV1kK42187wb

https://zhuanlan.zhihu.com/p/626044821

---

---
url: /常用框架/SpringAIAlibaba/Agent Framework/6_Models模型.md
---

# Models 模型

## 一、ChatModel API

ChatModel API 为开发者提供了将 AI 驱动的聊天补全功能集成到应用程序中的能力。它利用预训练的语言模型（如 GPT），根据用户的自然语言输入生成类似人类的响应。

该 API 通常通过向 AI 模型发送提示或部分对话来工作，然后模型根据其训练数据和对自然语言模式的理解生成对话的完成或延续。完成的响应随后返回给应用程序，应用程序可以将其呈现给用户或用于进一步处理。

`Spring AI ChatModel API` 被设计为一个简单且可移植的接口，用于与各种 AI 模型交互，允许开发者在不同模型之间切换时只需最少的代码更改。

借助 `Prompt`（用于输入封装）和 `ChatResponse`（用于输出处理）等配套类，ChatModel API 统一了与 AI 模型的通信。它管理请求准备和响应解析的复杂性，提供直接且简化的 API 交互。

## 二、API 概述

本节提供 Spring AI ChatModel API 接口和相关类的指南。

### ChatModel

以下是 [ChatModel](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/model/ChatModel.java) 接口定义：

```java
public interface ChatModel extends Model<Prompt, ChatResponse>, StreamingChatModel {
    default String call(String message) {
        Prompt prompt = new Prompt(new UserMessage(message));
        Generation generation = this.call(prompt).getResult();
        return generation != null ? generation.getOutput().getText() : "";
    }

    default String call(Message... messages) {
        Prompt prompt = new Prompt(Arrays.asList(messages));
        Generation generation = this.call(prompt).getResult();
        return generation != null ? generation.getOutput().getText() : "";
    }

    ChatResponse call(Prompt prompt);

    default ChatOptions getDefaultOptions() {
        return ChatOptions.builder().build();
    }

    default Flux<ChatResponse> stream(Prompt prompt) {
        throw new UnsupportedOperationException("streaming is not supported");
    }
}
```

带有 `String` 参数的 `call()` 方法简化了初始使用，避免了更复杂的 `Prompt` 和 `ChatResponse` 类的复杂性。在实际应用中，更常见的是使用接受 `Prompt` 实例并返回 `ChatResponse` 的 `call()` 方法。

### StreamingChatModel

以下是 [StreamingChatModel](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/model/StreamingChatModel.java) 接口定义：

```java
public interface StreamingChatModel extends StreamingModel<Prompt, ChatResponse> {

    default Flux<String> stream(String message) {...}

    @Override
    Flux<ChatResponse> stream(Prompt prompt);
}
```

`stream()` 方法接受 `String` 或 `Prompt` 参数，类似于 `ChatModel`，但使用响应式 Flux API 流式传输响应。

### Prompt

[Prompt](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-client-chat/src/main/java/org/springframework/ai/chat/prompt/Prompt.java) 是一个封装了 [Message](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/messages/Message.java) 对象列表和可选模型请求选项的 `ModelRequest`。以下是 `Prompt` 类的简化版本，排除了构造函数和其他实用方法：

```java
public class Prompt implements ModelRequest<List<Message>> {

    private final List<Message> messages;

    private ChatOptions modelOptions;

    @Override
    public ChatOptions getOptions() {...}

    @Override
    public List<Message> getInstructions() {...}

    // 构造函数和实用方法省略
}
```

#### Message

`Message` 接口封装了 `Prompt` 文本内容、元数据属性集合以及称为 `MessageType` 的分类。

接口定义如下：

```java
public interface Content {

    String getText();

    Map<String, Object> getMetadata();
    }

    public interface Message extends Content {

    MessageType getMessageType();
}
```

多模态消息类型还实现了 `MediaContent` 接口，提供 `Media` 内容对象列表。

```java
public interface MediaContent extends Content {

    Collection<Media> getMedia();
}
```

`Message` 接口有多种实现，对应于 AI 模型可以处理的消息类别：

* **UserMessage**: 用户消息
* **SystemMessage**: 系统消息
* **AssistantMessage**: 助手消息
* **FunctionMessage**: 函数消息
* **ToolResponseMessage**: 工具响应消息

聊天完成端点根据对话角色区分消息类别，由 `MessageType` 有效映射。

例如，OpenAI 识别不同对话角色的消息类别，如 `system`、`user`、`function` 或 `assistant`。

虽然术语 `MessageType` 可能暗示特定的消息格式，但在此上下文中，它有效地指定了消息在对话中扮演的角色。

对于不使用特定角色的 AI 模型，`UserMessage` 实现充当标准类别，通常表示用户生成的查询或指令。

#### ChatOptions

表示可以传递给 AI 模型的选项。`ChatOptions` 类是 `ModelOptions` 的子类，用于定义可以传递给 AI 模型的少数可移植选项。`ChatOptions` 类定义如下：

```java
public interface ChatOptions extends ModelOptions {

    String getModel();
    Float getFrequencyPenalty();
    Integer getMaxTokens();
    Float getPresencePenalty();
    List<String> getStopSequences();
    Float getTemperature();
    Integer getTopK();
    Float getTopP();
    ChatOptions copy();
}
```

**常用选项说明**：

* **model**: 要使用的模型 ID
* **frequencyPenalty**: 频率惩罚（-2.0 到 2.0），降低重复令牌的可能性
* **maxTokens**: 生成响应的最大令牌数
* **presencePenalty**: 存在惩罚（-2.0 到 2.0），鼓励谈论新主题
* **stopSequences**: 停止序列列表，遇到时停止生成
* **temperature**: 采样温度（0.0 到 2.0），控制随机性
* **topK**: Top-K 采样参数
* **topP**: Top-P（核采样）参数

此外，每个特定模型的 ChatModel/StreamingChatModel 实现都可以有自己的选项。例如，OpenAI Chat Completion 模型有自己的选项，如 `logitBias`、`seed` 和 `user`。

这是一个强大的功能，允许开发者在启动应用程序时使用特定于模型的选项，然后在运行时使用 `Prompt` 请求覆盖它们。

Spring AI 提供了一个复杂的系统来配置和使用 ChatModels。它允许在启动时设置默认配置，同时还提供了在每个请求基础上覆盖这些设置的灵活性。

**选项合并流程**：

1. **启动配置** - ChatModel/StreamingChatModel 使用"启动"ChatOptions 初始化。这些选项在 ChatModel 初始化期间设置，旨在提供默认配置。
2. **运行时配置** - 对于每个请求，Prompt 可以包含运行时 ChatOptions，这些可以覆盖启动选项。
3. **选项合并过程** - "合并选项"步骤结合启动和运行时选项。如果提供了运行时选项，它们优先于启动选项。
4. **输入处理** - "转换输入"步骤将输入指令转换为本地的、特定于模型的格式。
5. **输出处理** - "转换输出"步骤将模型的响应转换为标准化的 `ChatResponse` 格式。

启动和运行时选项的分离允许全局配置和特定于请求的调整。

### ChatResponse

`ChatResponse` 类的结构如下：

```java
public class ChatResponse implements ModelResponse<Generation> {

    private final ChatResponseMetadata chatResponseMetadata;
    private final List<Generation> generations;

    @Override
    public ChatResponseMetadata getMetadata() {...}

    @Override
    public List<Generation> getResults() {...}

    // 其他方法省略
}
```

[ChatResponse](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/model/ChatResponse.java) 类保存 AI 模型的输出，每个 `Generation` 实例包含单个提示可能产生的多个输出之一。

`ChatResponse` 类还携带关于 AI 模型响应的 `ChatResponseMetadata` 元数据。

### Generation

最后，[Generation](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/model/Generation.java) 类从 `ModelResult` 扩展，表示模型输出（助手消息）和相关元数据：

```java
public class Generation implements ModelResult<AssistantMessage> {

    private final AssistantMessage assistantMessage;
    private ChatGenerationMetadata chatGenerationMetadata;

    @Override
    public AssistantMessage getOutput() {...}

    @Override
    public ChatGenerationMetadata getMetadata() {...}

    // 其他方法省略
}
```

## 三、可用实现

Spring AI 提供了与多个 AI 服务提供商的集成，所有这些都通过统一的 `ChatModel` 和 `StreamingChatModel` 接口进行交互：

* **OpenAI Chat Completion** (支持流式、多模态和函数调用)
* **Microsoft Azure OpenAI Chat Completion** (支持流式和函数调用)
* **Alibaba DashScope Chat Completion** (支持流式和函数调用)
* **Ollama Chat Completion** (支持流式、多模态和函数调用)
* **Hugging Face Chat Completion** (不支持流式)
* **Google Vertex AI Gemini Chat Completion** (支持流式、多模态和函数调用)
* **Amazon Bedrock**
* **Mistral AI Chat Completion** (支持流式和函数调用)
* **Anthropic Chat Completion** (支持流式和函数调用)

关于每个模型的具体用法与特性，请查看 Spring AI Alibaba 模型适配文档。

## 四、DashScopeChatModel

DashScope 是阿里云提供的大模型服务平台，提供通义千问等多个大语言模型。Spring AI Alibaba 提供了 DashScopeChatModel 的集成。

### 前置条件

在使用 DashScopeChatModel 之前，你需要：

1. 获取 DashScope API Key：访问 [阿里云百炼](https://www.aliyun.com/product/bailian)

2. 设置环境变量：`export AI_DASHSCOPE_API_KEY=your_api_key`

   windows在环境变量中设置后，即可在代码中使用`System.getenv("AI_DASHSCOPE_API_KEY")`获取，需要重启IDEA

### 添加依赖

```xml
<dependency>
    <groupId>com.alibaba.cloud.ai</groupId>
    <artifactId>spring-ai-alibaba-starter-dashscope</artifactId>
    <version>1.1.0.0-M5</version>
</dependency>
```

### 基础使用

#### 创建 ChatModel

```java
/**
 * 基础模型配置示例
 */
public static void basicModelConfiguration() {
    // 创建 DashScope API 实例 // [!code ++:9]
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();

    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .build();
}
```

#### 简单调用

```java
/**
 * 简单调用示例
 */
public static void simpleCellConfiguration() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();

    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .build();
    // 使用字符串直接调用 // [!code ++:2]
    String response = chatModel.call("介绍一下Spring框架");
    System.out.println(response);
}
```

#### 使用 Prompt

```java
/**
 * 使用 Prompt 调用示例
 */
public static void promptConfiguration() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();

    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .build();

    // 创建 Prompt // [!code ++:5]
    Prompt prompt = new Prompt(new UserMessage("解释什么是微服务架构"));

    // 调用并获取响应
    ChatResponse response = chatModel.call(prompt);
    String answer = response.getResult().getOutput().getText();
    System.out.println(answer);
}
```

### 配置选项

#### 使用 ChatOptions

```java
/**
 * DashScopeChatOptions 配置示例
 */
public static void dashScopeChatOptionsConfiguration() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();

    DashScopeChatOptions options = DashScopeChatOptions.builder()  // [!code ++:12]
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.7)                 // 温度参数
        .withMaxToken(2000)                   // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();

    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();

    // 创建 Prompt
    Prompt prompt = new Prompt(new UserMessage("解释什么是微服务架构"));

    // 调用并获取响应
    ChatResponse response = chatModel.call(prompt);
    String answer = response.getResult().getOutput().getText();
    System.out.println(answer);
}
```

#### 运行时覆盖选项

```java

/**
 * 运行时覆盖选项示例
 */
public static void runtimeOptionsConfiguration() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();

    // 模型配置
    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.7)                 // 温度参数
        .withMaxToken(2000)                   // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();

    // 模型运行时配置（覆盖主配置） // [!code ++:6]
    DashScopeChatOptions runtimeOptions = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .build();

    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();

    // 创建 Prompt // [!code ++:2]
    Prompt prompt = new Prompt(new UserMessage("用一句话总结Java的特点"), runtimeOptions);

    // 调用并获取响应
    ChatResponse response = chatModel.call(prompt);
    String answer = response.getResult().getOutput().getText();
    System.out.println(answer);
}
```

### 流式响应

```java
/**
 * 流式响应示例
 */
public static void streamConfiguration() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();

    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();

    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();

    // 创建 Prompt
    Prompt prompt = new Prompt("详细解释Spring Boot的自动配置原理");

    // 使用流式 API // [!code ++:14]
    Flux<ChatResponse> responseStream = chatModel.stream(prompt);

    // 订阅并处理流式响应
    responseStream.subscribe(
        chatResponse -> {
            String content = chatResponse.getResult()
                .getOutput()
                .getText();
            System.out.print(content);
        },
        error -> System.err.println("错误: " + error.getMessage()),
        () -> System.out.println("流式响应完成")
    );
}
```

### 多轮对话

```java
/**
 * 多轮对话示例
 */
public static void messagesConfiguration() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();

    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();

    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();

    // 创建对话历史 // [!code ++:9]
    List<Message> messages = List.of(
        new SystemMessage("你是一个Java专家"),
        new UserMessage("什么是Spring Boot?"),
        new AssistantMessage("Spring Boot是..."),
        new UserMessage("它有什么优势?")
    );

    Prompt prompt = new Prompt(messages);
    ChatResponse response = chatModel.call(prompt);
    String answer = response.getResult().getOutput().getText();
    System.out.println(answer);
}
```

### 支持的模型

DashScope 支持多个模型，包括：

* **qwen-turbo**: 通义千问超大规模语言模型，支持中文、英文等
* **qwen-plus**: 通义千问增强版
* **qwen-max**: 通义千问旗舰版
* **qwen-max-longcontext**: 支持长文本的通义千问

### 函数调用

DashScopeChatModel 支持函数调用（Function Calling），允许模型调用外部函数：

```java
/**
 * 函数调用示例
 */
public static void functionConfiguration() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();

    // 定义函数工具 // [!code ++:8]
    ToolCallback weatherFunction = FunctionToolCallback.builder("getWeather", (String city) -> {
            // 实际的天气查询逻辑
            return "晴朗，25°C";
        })
        .description("获取指定城市的天气")
        .inputType(String.class)
        .build();

    // 使用函数
    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .withToolCallbacks(List.of(weatherFunction)) // [!code ++]
        .build();

    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();

    Prompt prompt = new Prompt("北京的天气怎么样?", options);
    ChatResponse response = chatModel.call(prompt);
    String answer = response.getResult().getOutput().getText();
    System.out.println(answer);
}
```

## 五、与 ReactAgent 集成

在 Spring AI Alibaba Agent Framework 中使用 DashScopeChatModel：

```java
/**
 * 与 ReactAgent 集成示例
 */
@SneakyThrows
public static void reactAgentConfiguration() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();

    // 使用函数
    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)          // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();

    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();

    ReactAgent agent = ReactAgent.builder()
        .name("my_agent")
        .model(chatModel)
        .systemPrompt("你是一个有帮助的AI助手")
        .build();

    // 调用 Agent
    AssistantMessage response = agent.call("帮我分析这个问题");
    System.out.println(response.getText());
}
```

---

---
url: /数据库/02.MongoDB/1_MongoDB安装.md
---

# MongoDB安装

## Windows安装

### 1. 下载安装包

> <https://www.mongodb.com/try/download/community>

选择zip的格式进行下载

![image-20231221232216135](/assets/image-20231221232216135.BO9xLtNj.png)

附加：mongodb的命名格式: x.y.z

```markdown
- y为奇数表示当前版本为开发版,如:1.5.2、4.1.13
- y为偶数表示当前版本为稳定版,如:1.6.3、4.0.10
- z为修正版本号,越大越好
```

### 2. 解压

下载完成后得到压缩包，解压；其中的bin目录就存放着mongodb相关的命令

### 3. 安装服务

首先要在安装目录里创建两个目录：

* 数据目录：data
* 日志目录：logs

然后以管理员模式，切换到安装目录下的bin目录运行以下格式命令来指定mongdb的数据及日志目录（文件的路径中不能包含中文）

```shell
mongod --install --dbpath 数据目录 --logpath 日志目录\日志名称 
```

具体的代码示例如下所示：

```shell
mongod --install --dbpath D:\Software\mongodb-4.4.26\data --logpath D:\Software\mongodb-4.4.26\logs\mongodb.log
```

没有任何报错和提示，则代表MongoDB服务创建成功

我们可以进行验证，win+r输入`services.msc`

看到MongoDB服务即成功

补充一下：如果想要删除MongoDB服务的话

```shell
SC DELETE MongoDB
```

### 4. 启动服务

输入以下命令启动服务

```shell
net start mongodb
```

输入`http://localhost:27017/`如果看到以下内容,代表启动成功

```
It looks like you are trying to access MongoDB over HTTP on the native driver port.
```

### 5. shell连接登录&退出

输入以下命令进行登录与退出

```shell
#登录
mongo
mongo --host=localhost --port=27017

#退出
exit	
```

补充语法命令：

```shell
mongod --install --dbpath 数据目录 --logpath 日志目录\日志名称	#创建服务
mongod --remove	    #卸载服务		
net start mongodb	#启动服务
net stop mongodb	#关闭服务
mongod #是处理MongoDB系统的主要进程。它处理数据请求，管理数据存储，和执行后台管理操作。当我们运行mongod命令意味着正在启动MongoDB进程,并且在后台运行。
```

### 6. 日志输出配置

1. IDEA插件：MongoQuery

2. 项目配置文件：

   yml配置

   ```yaml
   #mongodb打印日志
   logging:
     level:
       org.springframework.data.mongodb.core.MongoTemplate: DEBUG
   ```

   properties配置

   ```properties
   #mongodb打印日志
   logging.level.org.springframework.data.mongodb.core.MongoTemplate=DEBUG
   ```

   日志示例

   ```sh
   DEBUG 11284 --- [           main] o.s.data.mongodb.core.MongoTemplate      : find using query: { "status" : 1, "$and" : [{ "$or" : [{ "userId" : "abcd" }, { "price" : { "$gte" : 2 } }] }] } fields: Document{{}} for class: class com.example.ademo.model.MongoDbTest in collection: mongoDbTest
   ```

## 参考资料

\[1]. mongodb template打印sql：<https://blog.csdn.net/weixin_35754676/article/details/129072221>

\[2]. MongoDB 性能监控：<https://www.mryunwei.com/371009.html>

\[3]. MongoDB调优-查询优化-MongoDB Profiler：<https://www.cnblogs.com/operationhome/p/10728654.html>

---

---
url: /数据库/02.MongoDB/0_MongoDB简介.md
---

# MongoDB基本概念

## MongoDB基本概念

### 介绍

* MongoDB是一个基于分布式文件存储的数据库。由C++语言编写。旨在为WEB应用提供可扩展的高性能数据存储解决方案。

* MongoDB是一个介于**关系数据库**和**非关系数据库**之间的产品，是非关系数据库当中功能最丰富，最像关系数据库的，它支持的数据结构非常松散，是类似json的bson格式，因此可以存储比较复杂的数据类型。

* MongoDB最大的特点是它支持的查询语言非常强大，其语法有点类似于面向对象的查询语言，几乎可以实现类似关系数据库**单表查询**的绝大部分功能，而且还支持对数据建立索引。

### 应用场景

传统的关系型数据库(如MySQL)，在数据操作的三高需求以及应对Web2.0的网站需求面前，显得力不从心，而 MongoDB可应对“三高”需求。

* High performance：对数据库**高并发读写**的需求。

* Huge Storage：对海量数据的**高效率存储和访问**的需求。

* High Scalability && High Availability：对数据库的**高可扩展性和高可用性**的需求。

具体应用场景：

* 社交场景，使用 MongoDB存储存储用户信息，以及用户发表的朋友圈信息，通过地理位置索引实现附近的人、地点等功能。
* 游戏场景，使用 MongoDB存储游戏用户信息，用户的装备、积分等直接以内嵌文档的形式存储，方便查询、高效率存储和访问。
* 物流场景，使用 MongoDB存储订单信息，订单状态在运送过程中会不断更新，以 MongoDB内嵌数组的形式来存储，一次查询就能将订单所有的变更读取出来。
* 物联网场景，使用 MongoDB存储所有接入的智能设备信息，以及设备汇报的日志信息，并对这些信息进行多维度的分析。
* 视频直播，使用 MongoDB存储用户信息、点赞互动信息等。

这些应用场景中，数据操作方面的共同特点是：

（1）数据量大。

（2）写入操作频繁（读写都很频繁）。

（3）价值较低的数据，对事务性要求不高。

对于这样的数据，我们更适合使用 MongoDB来实现数据的存储。

### 什么时候选择MongoDB

1. 应用不需要事务及复杂join支持。

2. 新应用，需求会变，数据模型无法确定，想快速迭代开发。

3. 应用需要2000-3000以上的读写QPS（更高也可以）。

4. 应用需要TB甚至PB级别数据存储。

5. 应用要求存储的数据不丢失。

6. 应用需要99.999%高可用。

7. 应用需要大量的地理位置查询、文本查。

8. 相对MySQL，在以上以用场景可以以更低的成本解决问题（包括学习、开发、运维等成本）。

### 体系结构

| SQL术语/概念 | MongoDB术语/概念 | 解释/说明                            |
| ------------ | ---------------- | ------------------------------------ |
| database     | database         | 数据库                               |
| table        | collection       | 数据库表/集合                        |
| row          | document         | 数据记录行/文档                      |
| column       | field            | 数据字段/域                          |
| index        | index            | 索引                                 |
| table joins  | --               | 表连接，MongoDB不支持                |
| --           | 嵌入文档         | MongoDB通过嵌入式文档来代替多表连接  |
| primary key  | primary key      | 主键，MongoDB自动将\_id字段设置为主键 |

### 数据类型

MongoDB的最小存储单位就是文档document对象。文档document对象对应于关系型数据库的行。数据在MongoDB中以BSON（Binary-JSON）文档的格式存储在磁盘上。

BSON（Binary Serialized Document Format）是一种类json的一种二进制形式的存储格式，简称 Binary JSON；BSON和JSON一样，支持内嵌的文档对象和数组对象，但是BSON有JSON没有的一些数据类型，如Date和Bin Data类型。

BSON采用了类似于C语言结构体的名称、对表示方法，支持内嵌的文档对象和数组对象，具有轻量性、可遍历性、高效性的三个特点，可以有效描述非结构化数据和结构化数据。这种格式的优点是灵活性高，但它的缺点是空间利用率不是很理想。

BSON中，除了基本JSON类型： string，integer，boolean，double，null，array和object，mongo还使用了特殊的数据类型。这些类型包括 date， object id， binary data， regular expression和code。

BSON数据类型参考列表：

![在这里插入图片描述](/assets/287a55514fe441a69eca966656081761.BaxoAa1v.png)

**提示**：

shell默认使用64位浮点型数值。

```
{“x”:3.14或{“x”:3}
```

对于整型值，可以使用NumberInt（4字节符号整数）或 NumberLong（8字节符号整数）

```
{“x”:NumberInt(“3” ){“x”:NumberLong(“3”)}
```

### 特点

1. 高性能：MongoDB提供高性能的数据持久性。特别是，对嵌入式数据模型的支持减少了数据库系统上I/O活动。

   索引支持更快的查询，并且可以包含来自嵌入式文档和数组的键。（文本索引解决搜索的需求、TTL索引解决历史数据自动过期的需求、地理位置索引可用于构建各种O2O应用）。

   mmapv1、 wiredtiger、 mongorocks（ rocks）、 In-memory等多引擎支持满足各种场景需求。Gridfs解决文件存储的需求。

2. 高可用性：MongoDB的复制工具称为副本集（ replica set），它可提供自动故障转移和数据冗余。

3. 高扩展性：MongoDB提供了水平可扩展性作为其核心功能的一部分。

   分片将数据分布在一组集群的机器上。（海量数据存储，服务能力水平扩展）。

   从3.4开始，MoηgoDB支持基于片键创建数据区域。在一个平衡的集群中， MongoDB将一个区域所覆盖的读写只定向到该区域内的那些片。

4. 丰富的查询支持：MongoDB支持丰富的査询语言，支持读和写操作（CRUD），比如数据聚合、文本搜索和地理空间查询等。

5. 其他特点：如无模式（动态模式）、灵活的文档模型。

## 参考资料

<https://blog.csdn.net/efew212efe/article/details/124524863>

---

---
url: /数据库/02.MongoDB/4_MongoDB数据同步.md
---

# MongoDB数据同步

### Mongo工具包介绍

mongo工具包包括管理数据的一些工具 exe 文件，具体如下：

1. mongoexport.exe：导出数据命令工具
2. mongoimport.exe：导入数据命令工具
3. bsondump.exe： 用于将导出的BSON文件格式转换为JSON格式
4. mongodump.exe： 用于从mongodb数据库中导出BSON格式的文件，类似于mysql的dump工具mysqldump
5. mongofiles.exe： 用于和mongoDB的GridFS文件系统交互的命令，并可操作其中的文件，它提供了我们本地系统与GridFS文件系统之间的存储对象接口
6. mongorestore.exe： 用于恢复导出的BSON文件到 mongodb 数据库中
7. mongostat.exe： 当前 mongod 状态监控工具，像linux中监控linux的vmstat
8. mongotop.exe： 提供了一个跟踪mongod数据库花费在读写数据的时间，为每个collection都会记录，默认记录时间是按秒记录

### Mongo工具包下载

MongoDB安装参考之前的文章

安装后，部分版本会自带该工具包，比如下图的 4.x 版本，我用的 5.0 版本没有自带工具包，所以我需要先去官网下载4.x 的安装包文件，然后把 bin 目录下的工具复制到 5.0 版本的 bin 目录下，才能进行数据的导出、导入操作。

![image-20240423143622398](/assets/image-20240423143622398.CW1pD1og.png)

将下载的 4.x 里的工具文件复制到 5.0 的安装目录

![image-20240423143555034](/assets/image-20240423143555034.BlYFbx-R.png)

### 导出导出实战

涉及命令工具：mongoexport、mongoimport

```markdown
参数释义：
-h ：指的是 host 主机地址
-u ：指的是用户账号
-p ：指的是账户密码
-d ：指的是数据库 database 简称
-c ：指的是表 collection 简称
-o ：指的是导出路径 output 简称
--file ：指的是需要导入的文件
--help : 查看帮助
```

实战示例：将108服务器中examdb库中的exam表复制到106服务器中。

将数据以JSON格式导出到本地

```sh
./mongoexport.exe -h 192.168.100.108:27017 -u "root" -p "password" --authenticationDatabase=admin -d examdb -c exam -o D:\exam001.json --type json
```

![image-20240423142715108](/assets/image-20240423142715108.CnwMltvp.png)

导入到另一条服务器中

```sh
./mongoimport.exe -h 192.168.100.106:27017 -u "root" -p "password" --authenticationDatabase=admin -d examdb -c exam001 --file D:\exam001.json
```

![image-20240423142730623](/assets/image-20240423142730623.BUXEOM2b.png)

---

---
url: /数据库/02.MongoDB/2_MongoTemplate基本用法.md
---

# MongoTemplate基本用法

SpringBoot集成MongoDB使用MongoTemplate

> 官方文档：[Query and Projection Operators — MongoDB Manual](https://www.mongodb.com/docs/v4.4/reference/operator/query/)

## MongoTemplate相关概念

* MongoTemplate：官方提供的操作MongoDB的对象。位于：org.springframework.data.mongodb.core。 使用的时候，需要注入。
* 基本查询
  * Query：用于创建查询条件的对象。 位于：package org.springframework.data.mongodb.core.query。 使用时一般需要传入如"Criteria"构建的查询条件。
  * Criteria: 构建具体查询条件的对象，和Query位于同个包下。
* 管道操作
  * AggregationOperation：聚合管道的操作对象，这是适用于Aggregate Pipeline Stages的操作，比如$group/$lookup/$unwind/$sort.......使用的时候，需要先构建对应的聚合操作，比如$group（需要构建具体操作）， 可以创建多个，最后一并传入到Aggregation对象中，再交给template去执行管道聚合。
  * Aggregation：Pipeline stage的集合，也就是上面AggregationOperation的集合，把上面的所有聚合操作存在一起，template调用aggregate方法的时候，传入该对象。
  * 以上类位于 package org.springframework.data.mongodb.core.aggregation。
* 高级操作
  * Aggregates: Pipeline stage操作对象。 和Aggregation有几乎一样的功能，但是会更加灵活，一般除了预先提供的操作符，还可以自己传入Bson操作对象去灵活实现。 整体的使用难度，比Aggregation可能高一些。
  * Bson、BsonDocument、BsonField: Bson就是灵活的表达式，查询条件、聚合操作符之类的构建定义，都可以由它接收，并最后传给template的aggregate方法去执行聚合操作。BsonDocument则是Bson的具体实现，用于灵活构建表达式的对象。BsonField也是构建灵活的聚合表达式的一个类，比如快速地定义{"count": { $sum: 1 } ，作为聚合操作的一部分传入到具体的聚合阶段中。
  * 以上类位于 package com.mongodb.client.model; Bson/BsonDocument则是另外的包org.bson中。感兴趣自行去源码中查找。

## 引入pxm.xml依赖

```xml
<!--SpringBoot整合MongoDB-->
<dependency>
     <groupId>org.springframework.boot</groupId>
     <artifactId>spring-boot-starter-data-mongodb</artifactId>
</dependency>
<!--MongoDB相关依赖-->
<dependency>
    <groupId>org.mongodb</groupId>
    <artifactId>mongodb-driver-sync</artifactId>
    <version>3.9.1</version>
</dependency>
```

## 配置文件

properties文件

```properties
# MongoDB数据库
spring.data.mongodb.uri=mongodb://127.0.0.1:27017/testMongoDB

# 设置了密码的MongoDB配置方式
# MongoDB服务器连接地址
#spring.data.mongodb.host=127.0.0.1
# MongoDB服务器连接端口
#spring.data.mongodb.port=27017
# MongoDB的验证数据库
#spring.data.mongodb.authentication-database=admin
# MongoDB数据库用户
#spring.data.mongodb.username=root
# MongoDB数据库密码
#spring.data.mongodb.password=123456
# 带连接的数据库
#spring.data.mongodb.database=testMongoDB
```

yml文件

```yaml
spring:
  data:
    mongodb:
      host: 127.0.0.1
      port: 27017
      database: mongodb_database
#      username: admin
#      password: 123456
#      min-connections-per-host: 10
#      max-connections-per-host: 100
#      threads-allowed-to-block-for-connection-multiplier: 5
#      server-selection-timeout: 30000
#      max-wait-time: 120000
#      max-connection-idel-time: 0
#      max-connection-life-time: 0
#      connect-timeout: 10000
#      socket-timeout: 0
#      socket-keep-alive: false
#      ssl-enabled: false
#      ssl-invalid-host-name-allowed: false
#      always-use-m-beans: false
#      heartbeat-socket-timeout: 20000
#      heartbeat-connect-timeout: 20000
#      min-heartbeat-frequency: 500
#      heartbeat-frequency: 10000
#      local-threshold: 15
```

## 实体准备

要有mongodb的Document对应的实体类，标注@Document(collection="")注解。

（collection=""，即为mongodb库中的文档名字，不添加这个注解后面的数据库名，无法对数据进行操作）

```java
@Document(collection = "sys_user")
public class SysUser {

    @Id
    private String id;

    private String userName;

    private String phoneNumber;

    private String address;

    private String idNumber;
    
    private Date birthday;
    
    private Integer money;
}
```

## 初始化测试数据

使用javafaker初始化测试数据

```xml
<!--javafaker数据生成-->
<dependency>
    <groupId>com.github.javafaker</groupId>
    <artifactId>javafaker</artifactId>
    <version>0.17.2</version>
</dependency>
```

实体增加构造方法

```java
public SysUser(String userName) {
    this.userName = userName;
}

public SysUser(String userName, String phoneNumber, String address, String idNumber, Date birthday, Integer money) {
    this.userName = userName;
    this.phoneNumber = phoneNumber;
    this.address = address;
    this.idNumber = idNumber;
    this.birthday = birthday;
    this.money = money;
}
```

初始化方法

```java
/**
 * 初始化数据
 */
@Test
public void init() {
    // 清空数据表
    Query query = new Query();
    mongoTemplate.remove(query, SysUser.class);
    // 构造测试数据
    List<SysUser> sysUserList = Stream.generate(() -> new SysUser(
        FAKER.name().fullName(),
        FAKER.phoneNumber().cellPhone(),
        FAKER.address().city() + FAKER.address().streetAddress(),
        FAKER.idNumber().validSvSeSsn(),
        FAKER.date().birthday(),
        RandomUtil.randomInt(100, 1000000)))
        .limit(10000)
        .collect(Collectors.toList());
    mongoTemplate.insert(sysUserList, SysUser.class);
}
```

## MongoTemplate的基本方法

使用@Autowired注入MongoTemplate。

```java
@Autowired
private MongoTemplate mongoTemplate;
```

### 插入数据

```java
List<SysUser> list = new ArrayList<>();
SysUser user = new SysUser();
user.setUserName("admin");
user.setAddress("地址");
list.add(user);

// 保存对象到mongodb
mongoTemplate.save(user);
mongoTemplate.insert(user);

// 根据集合名称保存对象到mongodb
mongoTemplate.save(user, "sys_user");
mongoTemplate.insert(user, "sys_user");

// 根据集合名称保存list到mongodb
mongoTemplate.save(list, "sys_user");
mongoTemplate.insert(list, "sys_user");
mongoTemplate.insert(list, SysUser.class);
```

**insert**: 若新增数据的主键已经存在，则会抛 `org.springframework.dao.DuplicateKeyException` 异常提示主键重复，不保存当前数据。

**save**: 若新增数据的主键已经存在，则会对当前已经存在的数据进行修改操作。

### 检索数据

```java
// 查询 userName=xxl，结果为集合列表
Query query = Query.query(Criteria.where("userName").is("xxl"));
mongoTemplate.find(query, SysUser.class);
mongoTemplate.find(query, SysUser.class, "sys_user");、

// 查询所有，结果为集合列表
mongoTemplate.findAll(SysUser.class);
mongoTemplate.findAll(SysUser.class, "sys_user");

// 分页查询（page页码，pageSize每页展示几个）
int page = 1;
int pageSize = 10;
Pageable pageable = PageRequest.of(page - 1, pageSize, Sort.by(Sort.Order.desc("date")));
Query query = new Query().with(pageable);
mongoTemplate.find(query, SysUser.class);
mongoTemplate.find(query, SysUser.class, "sys_user");

// 查询多个
Query query = Query.query(Criteria.where("id").in("id1", "id2", "id3")).with(Sort.by(Sort.Order.desc("date")));
mongoTemplate.find(query, SysUser.class);

// 查询数量
Criteria criteria = Criteria.where("userId").is("12345")
    .and("name").is(new ObjectId("张三"))
    .and("address").is("上海");
Query query = Query.query(criteria);
long count = mongoTemplate.count(query, SysUser.class);
```

### 更新数据

```java
Query query = Query.query(Criteria.where("_id").is("658712b96a3c742d4070f6ca"));
Update update = Update.update("userName", "xxl");

// 更新一条数据
mongoTemplate.updateFirst(query, update, SysUser.class);
mongoTemplate.updateFirst(query, update, "sys_user");
mongoTemplate.updateFirst(query, update, SysUser.class, "sys_user");

// 更新多条数据
mongoTemplate.updateMulti(query, update, SysUser.class);
mongoTemplate.updateMulti(query, update, "sys_user");
mongoTemplate.updateMulti(query, update, SysUser.class, "sys_user");

// 更新数据，如果数据不存在就新增
mongoTemplate.upsert(query, update, SysUser.class);
mongoTemplate.upsert(query, update, "sys_user");
mongoTemplate.upsert(query, update, SysUser.class, "sys_user");

// 更新条件不变，更新字段改成了一个我们集合中不存在的，用set方法如果更新的key不存在则创建一个新的key
update = Update.update("userName", "xxl").set("nickName", "xxl");
mongoTemplate.upsert(query, update, SysUser.class);

// update的inc方法用于做累加操作，将money在之前的基础上加上100
update = Update.update("userName", "xxl").inc("money", 100);
mongoTemplate.updateMulti(query, update, SysUser.class);

// update的rename方法用于修改key的名称
update = Update.update("userName", "xxl").rename("nickName", "nickNameNew");
mongoTemplate.updateMulti(query, update, SysUser.class);

// update的unset方法用于删除key
update = Update.update("userName", "xxl").unset("nickNameNew");
mongoTemplate.updateMulti(query, update, SysUser.class);

// update的pull方法用于删除tags数组中的java
List<String> tags = new ArrayList<>();
tags.add("java");
tags.add("python");
update = Update.update("userName", "xxl").set("tags", tags);
mongoTemplate.upsert(query, update, SysUser.class);
update = Update.update("userName", "xxl").pull("tags", "java");
mongoTemplate.updateMulti(query, update, SysUser.class);
```

### 删除数据

```java
// 根据条件删除（可删除多条）
Query query = Query.query(Criteria.where("id").in("65846bbd6a3c7445686c974a", "65846bbd6a3c7445686c974b"));
mongoTemplate.remove(query, SysUser.class); // 指定对象
mongoTemplate.remove(query, "sys_user"); // 直接指定MongoDB集合名称
mongoTemplate.remove(query, SysUser.class, "sys_user");
SysUser user = new SysUser();
user.setId("65846bbd6a3c7445686c974a");
mongoTemplate.remove(user);

// 删除集合，可传实体类，也可以传名称
mongoTemplate.dropCollection(SysUser.class);
mongoTemplate.dropCollection("sys_user");

// 删除数据库；在开发中，开发所使用的数据库是在配置文件中配置的；使用这个方法即可直接删除配置对应的数据库
mongoTemplate.getDb().drop();

// 查询出符合条件的第一个结果，并将符合条件的数据删除,只会删除第一条
query = Query.query(Criteria.where("userName").is("xxl"));
SysUser article = mongoTemplate.findAndRemove(query, SysUser.class);
// 查询出符合条件的所有结果，并将符合条件的所有数据删除
query = Query.query(Criteria.where("userName").is("xxl"));
List<SysUser> articles = mongoTemplate.findAllAndRemove(query, SysUser.class);
```

注意：

mongoTemplate.remove();传入不同类型参数，对于 实体类中有无 `_id`属性的要求不一样。

比如 mongoTemplate.remove(object, collection)方法，如果对应object实体类中没有`_id`属性就会报错：org.springframework.data.mapping.model.MappingException: No id property found for object of type。但是 mongoTemplate.remove(query, entityClass, collectionName)就运行正常；

mongoTemplate.findAllAndRemove();对应的实体类的就需要有`_id`属性；

mongoTemplate.findAndRemove();对应的实体类的不是必须有`_id`属性。

原因在MongoTemplate代码中有的方法调用其中的extractIdPropertyAndValue(Object object)，有的没有。因此，为了方便，建议在实体类添加\_id属性。

### 集合查询

> [收集方法 — MongoDB 手册](https://www.mongodb.com/docs/manual/reference/method/js-collection/)

```java
Query query = Query.query(Criteria.where("userName").exists(true));
// 查询指定字段的list集合，是去重后的结果
// entityClass：实体类，实际上就是实体类.class；如：SysUser.class
// mongoTemplate.getCollectionName(entityClass)：可获取到entityClass实体类所对应的集合名称
// mongoTemplate.getCollection(mongoTemplate.getCollectionName(entityClass))：可通过集合名称获取到对应集合
// mongoTemplate.getCollection(collectionName)：返回的是基本的Driver集合对象，即DBCollection类型
// 因此使用 getCollection() 方法获取到的集合类型，不是我们在开发过程中所使用的集合类型
// key：指定键值，实际上就是MongoDB数据库集合中文档的字段名
// query：查询对象
// query.getQueryObject()：获取对应查询对象的查询条件
// .distinct(key, query.getQueryObject())：在单个集合或视图，查询满足条件的所有文档中，指定字段的不同值
String collectionName = mongoTemplate.getCollectionName(SysUser.class);
MongoCollection<Document> collection = mongoTemplate.getCollection(collectionName);
DistinctIterable<String> userName = collection.distinct("userName", query.getQueryObject(), String.class);
List<String> userNameList = StreamSupport.stream(userName.spliterator(), false).collect(Collectors.toList());
System.out.println(userNameList);
// 统计去重后的数量
int size = this.mongoTemplate.getCollection(collectionName)
    .distinct("userName", query.getQueryObject(), String.class)
    .into(new ArrayList<>())
    .size();
System.out.println(size);
```

补充

```json
{ "_id": 1, "dept": "A", "item": { "sku": "111", "color": "red" }, "sizes": [ "S", "M" ] }
{ "_id": 2, "dept": "A", "item": { "sku": "111", "color": "blue" }, "sizes": [ "M", "L" ] }
{ "_id": 3, "dept": "B", "item": { "sku": "222", "color": "blue" }, "sizes": [ "S" ] }
{ "_id": 4, "dept": "A", "item": { "sku": "333", "color": "black" }, "sizes": [ "S" ] }
```

* mongoTemplate.getCollection("inventory").distinct("dept") ：从inventory集合中的所有文档返回dept字段的不同值；结果为：\[ "A", "B" ]
* mongoTemplate.getCollection("inventory").distinct("item.sku") ：从inventory集合中的所有文档返回sku嵌入字段的不同值；结果为：\[ "111", "222", "333" ]
* mongoTemplate.getCollection("inventory").distinct("sizes") ：从inventory集合中的所有文档返回数组字段的不同值；结果为：\[ "M", "S", "L" ]
* mongoTemplate.getCollection("inventory").distinct("item.sku", { dept: "A" }) ：从inventory集合中 dept字段等于A 的文档中返回sku嵌入字段的不同值；结果为：\[ "111", "333" ]

```java
Query query2 = Query.query(Criteria.where("_id").is("658712b96a3c742d4070f6ca"));
SysUser sysUser = new SysUser("xxl2", "110", "洛杉矶", "911", new Date(), 9999);
Update update = Update.update("userName", "xxl").set("child", sysUser);
mongoTemplate.upsert(query2, update, SysUser.class);

DistinctIterable<String> address = collection.distinct("child.address", query.getQueryObject(), String.class);
List<String> addressList = StreamSupport.stream(address.spliterator(), false).collect(Collectors.toList());
System.out.println(addressList);
```

## 参考资料

<https://blog.csdn.net/Ciel_Y/article/details/121626495>

<https://blog.csdn.net/Java_Rookie_Xiao/article/details/125602833>

<https://blog.csdn.net/harlan95/article/details/129521760>

<https://blog.csdn.net/qq_36826506/article/details/82082988>

mongoTemplate去重排序查询：<https://www.cnblogs.com/guangxiang/p/12366017.html>

---

---
url: /常用框架/SpringAIAlibaba/Agent Framework/11_Multi-agent多智能体.md
---

# Multi-agent 多智能体

---

---
url: /常用框架/Mybatis/Mybatis-Plus-Join连表查询.md
---

# MyBatis-Plus 使用 Join 联表查询

## 介绍

众所周知，Mybatis Plus 封装的 mapper 不支持 join，如果需要支持就必须自己去实现。但是对于大部分的业务场景来说，都需要多表 join，要不然就没必要采用关系型数据库了。

那么有没有一种不通过硬 SQL 的形式，通过框架提供 join 能力呢？答案是，可以有，今天专门拉出来再说一下，确实能提高不少的开发效率！

> 项目地址：<https://gitee.com/best_handsome/mybatis-plus-join>

## 使用

### 安装

* Maven

  ```xml
  <dependency>
      <groupId>com.github.yulichang</groupId>
      <artifactId>mybatis-plus-join-boot-starter</artifactId>
      <version>1.4.4</version>
  </dependency>
  ```

* Gradle

  ```go
  implementation 'com.github.yulichang:mybatis-plus-join-boot-starter:1.4.4'
  ```

或者 clone 代码到本地执行 mvn install，再引入以上依赖。

注意：mybatis plus version >= 3.4.0。

### 使用

* mapper继承MPJBaseMapper (必选)
* service继承MPJBaseService (可选)
* serviceImpl继承MPJBaseServiceImpl (可选)

### SQL准备

```sql
```

### 核心类MPJLambdaWrapper和MPJQueryWrapper

#### MPJLambdaWrapper用法

简单的三表查询

```java
class test {
    @Resource
    private UserMapper userMapper;

    void testJoin() {
        //和Mybatis plus一致，MPJLambdaWrapper的泛型必须是主表的泛型，并且要用主表的Mapper来调用
        MPJLambdaWrapper<UserDO> wrapper = new MPJLambdaWrapper<UserDO>()
                .selectAll(UserDO.class)//查询user表全部字段
                .select(UserAddressDO::getTel)//查询user_address tel 字段
                .selectAs(UserAddressDO::getAddress, UserDTO::getUserAddress)//别名 t.address AS userAddress
                .select(AreaDO::getProvince, AreaDO::getCity)
                .leftJoin(UserAddressDO.class, UserAddressDO::getUserId, UserDO::getId)
                .leftJoin(AreaDO.class, AreaDO::getId, UserAddressDO::getAreaId)
                .eq(UserDO::getId, 1)
                .like(UserAddressDO::getTel, "1")
                .gt(UserDO::getId, 5);

        //连表查询 返回自定义ResultType
        List<UserDTO> list = userMapper.selectJoinList(UserDTO.class, wrapper);

        //分页查询 （需要启用 mybatis plus 分页插件）
        Page<UserDTO> listPage = userMapper.selectJoinPage(new Page<>(2, 10), UserDTO.class, wrapper);
    }
}
```

对应sql

```sql
SELECT  
    t.id, t.name, t.sex, t.head_img, 
    t1.tel, t1.address AS userAddress,
    t2.province, t2.city 
FROM 
    user t 
    LEFT JOIN user_address t1 ON t1.user_id = t.id 
    LEFT JOIN area t2 ON t2.id = t1.area_id 
WHERE (
    t.id = ? 
    AND t1.tel LIKE ? 
    AND t.id > ?)
```

说明:

* UserDTO.class 查询结果返回类(resultType)
* selectAll() 查询指定实体类的全部字段
* select() 查询指定的字段,支持可变参数,同一个select只能查询相同表的字段
* selectAs() 字段别名查询,用于数据库字段与业务实体类属性名不一致时使用
* leftJoin() 参数说明；第一个参数: 参与连表的实体类class 第二个参数: 连表的ON字段,这个属性必须是第一个参数实体类的属性 第三个参数: 参与连表的ON的另一个实体类属性
* 默认主表别名是t,其他的表别名以先后调用的顺序使用t1,t2,t3....
* 条件查询,可以查询主表以及参与连接的所有表的字段,全部调用mp原生的方法,正常使用没有sql注入风险

MPJLambdaWrapper 还有很多其他的功能

* 简单的SQL函数使用：<https://gitee.com/best_handsome/mybatis-plus-join/wikis/selectFunc()?sort_id=4082479>
* ON语句多条件支持：<https://gitee.com/best_handsome/mybatis-plus-join/wikis/leftJoin?sort_id=3496671>

等效于ResultMap

```xml
<resultMap id="xxxxxxxx" type="com.github.yulichang.join.dto.UserDTO">
    <result property="id" column="id"/>
    <result property="name" column="name"/>
    <!--其他属性省略-->
    <collection property="addressList" javaType="java.util.List"
                ofType="com.github.yulichang.join.entity.UserAddressDO">
        <id property="id" column="mpj_id"/>
        <result property="address" column="address"/>
        <result property="userId" column="user_id"/>
        <!--其他属性省略-->
    </collection>
</resultMap>
```

MPJLambdaWrapper其他功能

* 一对一，一对多使用：<https://ylctmh.com/pages/core/lambda/select/selectCollection.html>
* 简单的SQL函数使用：<https://ylctmh.com/pages/core/lambda/select/selectFunc.html>
* ON语句多条件支持：<https://ylctmh.com/pages/core/lambda/join/leftJoin.html>

### String形式用法（MPJQueryWrapper）

#### 简单的连表查询

```java
class test {
    @Resource
    private UserMapper userMapper;

    void testJoin() {
        MPJQueryWrapper wrapper = new MPJQueryWrapper<UserDO>()
                .selectAll(UserDO.class)
                .select("addr.tel", "addr.address", "a.province")
                .leftJoin("user_address addr on t.id = addr.user_id")
                .rightJoin("area a on addr.area_id = a.id")
                .like("addr.tel", "1")
                .le("a.province", "1");

        //列表查询
        List<UserDTO> list = userMapper.selectJoinList(UserDTO.class, wrapper);

        //分页查询 （需要启用 mybatis plus 分页插件）
        Page<UserDTO> listPage = userMapper.selectJoinPage(new Page<>(1, 10), UserDTO.class, wrapper);
    }
}
```

对应sql

```sql
SELECT 
    t.id,
    t.name,
    t.sex,
    t.head_img,
    addr.tel,
    addr.address,
    a.province
FROM 
    user t
    LEFT JOIN user_address addr on t.id = addr.user_id
    RIGHT JOIN area a on addr.area_id = a.id
WHERE (
    addr.tel LIKE ?
    AND a.province <= ?)
```

说明:

* UserDTO.class 查询结果类(resultType)
* selectAll(UserDO.class) 查询主表全部字段(主表实体类)默认主表别名 "t"
* select() mp的select策略是覆盖,以最后一次为准,这里的策略是追加,可以一直select 主表字段可以用lambda,会自动添加表别名,主表别名默认是 t ,非主表字段必须带别名查询
* leftJoin() rightJoin() innerJoin() 传sql片段 格式 (表 + 别名 + 关联条件)
* 条件查询,可以查询主表以及参与连接的所有表的字段,全部调用mp原生的方法,正常使用没有sql注入风险

#### 还可以这么操作,但不建议

```java
class test {
    @Resource
    private UserMapper userMapper;

    void testJoin() {
        List<UserDTO> list = userMapper.selectJoinList(UserDTO.class,
                new MPJQueryWrapper<UserDO>()
                        .selectAll(UserDO.class)
                        .select("addr.tel", "addr.address")
                        //行列转换
                        .select("CASE t.sex WHEN '男' THEN '1' ELSE '0' END AS sex")
                        //求和函数
                        .select("sum(a.province) AS province")
                        //自定义数据集
                        .leftJoin("(select * from user_address) addr on t.id = addr.user_id")
                        .rightJoin("area a on addr.area_id = a.id")
                        .like("addr.tel", "1")
                        .le("a.province", "1")
                        .orderByDesc("addr.id"));
    }
}
```

对应sql

```sql
SELECT 
    t.id,
    t.name,
    t.sex,
    t.head_img,
    addr.tel,
    addr.address,
    CASE t.sex WHEN '男' THEN '1' ELSE '0' END AS sex,
    sum(a.province) AS province
FROM 
    user t
    LEFT JOIN (select * from user_address) addr on t.id = addr.user_id
    RIGHT JOIN area a on addr.area_id = a.id
WHERE (
    addr.tel LIKE ?
    AND a.province <= ?)
ORDER BY
    addr.id DESC
```

这样，我们就能和使用 Mybatis Plus 一样进行表关联操作了！

> 获取本文代码可访问
>
> <https://github.com/Daneliya/springboot_chowder/tree/main/springboot_mybatis_plus_join>

## 参考资料

\[1]. <https://blog.csdn.net/weixin_44421461/article/details/130191133>

---

---
url: /常用框架/Mybatis/Mybatis增强框架Mybatis-Flex.md
---

# Mybatis增强框架Mybatis-Flex

> 官方文档：<https://mybatis-flex.com/>

## 一、Mybatis-Flex 是什么

Mybatis-Flex 是一个优雅的 Mybatis 增强框架，它非常轻量、同时拥有极高的性能与灵活性。我们可以轻松的使用 Mybaits-Flex 链接任何数据库，其内置的 QueryWrapper^亮点 帮助我们极大的减少了 SQL 编写的工作的同时，减少出错的可能性。

总而言之，Mybatis-Flex 能够极大地提高我们的开发效率和开发体验，让我们有更多的时间专注于自己的事情。

## 二、Mybatis-Flex的有什么特点

**1、轻量**：除了 MyBatis，没有任何第三方依赖轻依赖、没有任何拦截器，其原理是通过 SqlProvider 的方式实现的轻实现。同时，在执行的过程中，没有任何的 Sql 解析（Parse）轻运行。 这带来了几个好处：1、极高的性能；2、极易对代码进行跟踪和调试； 3、把控性更高。

**2、灵活**：支持 Entity 的增删改查、以及分页查询的同时，Mybatis-Flex 提供了 Db + Row^灵活 工具，可以无需实体类对数据库进行增删改查以及分页查询。 与此同时，Mybatis-Flex 内置的 QueryWrapper^灵活 可以轻易的帮助我们实现 **多表查询**、**链接查询**、**子查询** 等等常见的 SQL 场景。

**3、强大**：支持任意关系型数据库，还可以通过方言持续扩展，同时支持 **多（复合）主键**、**逻辑删除**、**乐观锁配置**、**数据脱敏**、**数据审计**、 **数据填充** 等等功能。

## 三、Mybatis-Flex和同类框架对比

### 1）功能对比：

| 功能或特点                                                   | MyBatis-Flex | MyBatis-Plus       | Fluent-MyBatis |
| :----------------------------------------------------------- | :----------- | :----------------- | :------------- |
| 对 entity 的基本增删改查                                     | ✅            | ✅                  | ✅              |
| 分页查询                                                     | ✅            | ✅                  | ✅              |
| 分页查询之总量缓存                                           | ✅            | ✅                  | ❌              |
| 分页查询无 SQL 解析设计（更轻量，及更高性能）                | ✅            | ❌                  | ✅              |
| 多表查询：from 多张表                                        | ✅            | ❌                  | ❌              |
| 多表查询：left join、inner join 等等                         | ✅            | ❌                  | ✅              |
| 多表查询：union，union all                                   | ✅            | ❌                  | ✅              |
| 单主键配置                                                   | ✅            | ✅                  | ✅              |
| 多种 id 生成策略                                             | ✅            | ✅                  | ✅              |
| 支持多主键、复合主键                                         | ✅            | ❌                  | ❌              |
| 字段的 typeHandler 配置                                      | ✅            | ✅                  | ✅              |
| 除了 MyBatis，无其他第三方依赖（更轻量）                     | ✅            | ❌                  | ❌              |
| QueryWrapper 是否支持在微服务项目下进行 RPC 传输             | ✅            | ❌                  | 未知           |
| 逻辑删除                                                     | ✅            | ✅                  | ✅              |
| 乐观锁                                                       | ✅            | ✅                  | ✅              |
| SQL 审计                                                     | ✅            | ❌                  | ❌              |
| 数据填充                                                     | ✅            | ✔️ **（收费）**     | ✅              |
| 数据脱敏                                                     | ✅            | ✔️ **（收费）**     | ❌              |
| 字段权限                                                     | ✅            | ✔️ **（收费）**     | ❌              |
| 字段加密                                                     | ✅            | ✔️ **（收费）**     | ❌              |
| 字典回写                                                     | ✅            | ✔️ **（收费）**     | ❌              |
| Db + Row                                                     | ✅            | ❌                  | ❌              |
| Entity 监听                                                  | ✅            | ❌                  | ❌              |
| 多数据源支持                                                 | ✅            | 借助其他框架或收费 | ❌              |
| 多数据源是否支持 Spring 的事务管理，比如 @Transactional 和 TransactionTemplate 等 | ✅            | ❌                  | ❌              |
| 多数据源是否支持 "非Spring" 项目                             | ✅            | ❌                  | ❌              |
| 多租户                                                       | ✅            | ✅                  | ❌              |
| 动态表名                                                     | ✅            | ✅                  | ❌              |
| 动态 Schema                                                  | ✅            | ❌                  | ❌              |

### 2）性能对比：

**这里直接贴测试结果：**

* MyBatis-Flex 的查询单条数据的速度，大概是 MyBatis-Plus 的 5 ~ 10+ 倍。
* MyBatis-Flex 的查询 10 条数据的速度，大概是 MyBatis-Plus 的 5~10 倍左右。
* Mybatis-Flex 的分页查询速度，大概是 Mybatis-Plus 的 5~10 倍左右。
* Mybatis-Flex 的数据更新速度，大概是 Mybatis-Plus 的 5~10+ 倍。

**具体性能对比测试，移步：**

> * <https://mybatis-flex.com/zh/intro/benchmark.html>

## 四、Mybatis-Flex支持的数据库类型

MyBatis-Flex 支持的数据库类型，如下表格所示，我们还可以通过自定义方言的方式，持续添加更多的数据库支持。

| 数据库        | 描述                    |
| :------------ | :---------------------- |
| mysql         | MySQL 数据库            |
| mariadb       | MariaDB 数据库          |
| oracle        | Oracle11g 及以下数据库  |
| oracle12c     | Oracle12c 及以上数据库  |
| db2           | DB2 数据库              |
| hsql          | HSQL 数据库             |
| sqlite        | SQLite 数据库           |
| postgresql    | PostgreSQL 数据库       |
| sqlserver2005 | SQLServer2005 数据库    |
| sqlserver     | SQLServer 数据库        |
| dm            | 达梦数据库              |
| xugu          | 虚谷数据库              |
| kingbasees    | 人大金仓数据库          |
| phoenix       | Phoenix HBase 数据库    |
| gauss         | Gauss 数据库            |
| clickhouse    | ClickHouse 数据库       |
| gbase         | 南大通用(华库)数据库    |
| gbase-8s      | 南大通用数据库 GBase 8s |
| oscar         | 神通数据库              |
| sybase        | Sybase ASE 数据库       |
| OceanBase     | OceanBase 数据库        |
| Firebird      | Firebird 数据库         |
| derby         | Derby 数据库            |
| highgo        | 瀚高数据库              |
| cubrid        | CUBRID 数据库           |
| goldilocks    | GOLDILOCKS 数据库       |
| csiidb        | CSIIDB 数据库           |
| hana          | SAP\_HANA 数据库         |
| impala        | Impala 数据库           |
| vertica       | Vertica 数据库          |
| xcloud        | 行云数据库              |
| redshift      | 亚马逊 redshift 数据库  |
| openGauss     | 华为 openGauss 数据库   |
| TDengine      | TDengine 数据库         |
| informix      | Informix 数据库         |
| greenplum     | Greenplum 数据库        |
| uxdb          | 优炫数据库              |

## 五、快速开始

**第 1 步：创建数据库表**

```sql
CREATE TABLE IF NOT EXISTS `tb_account`
(
    `id`        INTEGER PRIMARY KEY auto_increment,
    `user_name` VARCHAR(100),
    `age`       INTEGER,
    `birthday`  DATETIME
);

INSERT INTO tb_account(id, user_name, age, birthday)
VALUES (1, '张三', 18, '2020-01-11'),
       (2, '李四', 19, '2021-03-21');
```

**第 2 步：创建 Spring Boot 项目，并添加 Maven 依赖**

> TIP：可以使用 Spring Initializer 快速初始化一个 Spring Boot 工程。

需要添加的 Maven 主要依赖示例：

```xml
<dependencies>
    <dependency>
        <groupId>com.mybatis-flex</groupId>
        <artifactId>mybatis-flex-spring-boot-starter</artifactId>
        <version>1.5.3</version>
    </dependency>
    <dependency>
        <groupId>com.mysql</groupId>
        <artifactId>mysql-connector-j</artifactId>
        <scope>runtime</scope>
    </dependency>
    <dependency>
        <groupId>com.zaxxer</groupId>
        <artifactId>HikariCP</artifactId>
    </dependency>
    <!-- for test only -->
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-test</artifactId>
        <scope>test</scope>
    </dependency>
</dependencies>
```

**第 3 步：对 Spring Boot 项目进行配置**

在 application.yml 中配置数据源：

```yaml
# DataSource Config
spring:
  datasource:
    url: jdbc:mysql://localhost:3306/flex_test
    username: root
    password: 12345678
```

在 Spring Boot 启动类中添加 `@MapperScan` 注解，扫描 Mapper 文件夹：

```java
@SpringBootApplication
@MapperScan("com.mybatisflex.test.mapper")
public class MybatisFlexTestApplication {

    public static void main(String[] args) {
        SpringApplication.run(MybatisFlexTestApplication.class, args);
    }

}
```

**第 4 步：编写实体类和 Mapper 接口**

这里使用了 Lombok 来简化代码。

```java
@Data
@Table("tb_account")
public class Account {

    @Id(keyType = KeyType.Auto)
    private Long id;
    private String userName;
    private Integer age;
    private Date birthday;

}
```

* 使用 `@Table("tb_account")` 设置实体类与表名的映射关系
* 使用 `@Id(keyType = KeyType.Auto)` 标识主键为
* 自增

Mapper 接口继承 BaseMapper 接口：

```java
public interface AccountMapper extends BaseMapper<Account> {

}
```

这部分也可以使用 MyBatis-Flex 的代码生成器来生，功能非常强大的。详情进入：

> * <https://mybatis-flex.com/zh/others/codegen.html>

**第 5 步：开始使用**

添加测试类，进行功能测试：

```java
import static com.mybatisflex.test.entity.table.AccountTableDef.ACCOUNT;

@SpringBootTest
class MybatisFlexTestApplicationTests {

    @Autowired
    private AccountMapper accountMapper;

    @Test
    void contextLoads() {
        QueryWrapper queryWrapper = QueryWrapper.create()
                .select()
                .where(ACCOUNT.AGE.eq(18));
        Account account = accountMapper.selectOneByQuery(queryWrapper);
        System.out.println(account);
    }

}
```

控制台输出：

```
Account(id=1, userName=张三, age=18, birthday=Sat Jan 11 00:00:00 CST 2020)
```

以上的 示例 中， `ACCOUNT` 为 MyBatis-Flex 通过 APT 自动生成，只需通过静态导入即可，无需手动编码。

整体来讲，这个框架是Mybatis的增强版，几乎集成了mybatis plus、jooq、fluent mybatis的所有优点。

> 获取本文源码可访问
>
> <https://github.com/Daneliya/springboot_chowder/tree/main/springboot_myBaits_flex>

## 六、参考资料

\[1]. <https://blog.csdn.net/zhipengfang/article/details/132353553>

---

---
url: /Java/系统优化/性能优化/5_MySQL Hints.md
---

# MySQL Hints

https://www.alibabacloud.com/help/zh/rds/apsaradb-rds-for-mysql/inventory-hint

https://cloud.tencent.com/developer/article/2425034

https://blog.csdn.net/qq\_24520119/article/details/147958877

---

---
url: /数据库/01.MySQL/MySQL导入MariaDB.md
---

# MySQL导入MariaDB

https://zhuanlan.zhihu.com/p/59882604

https://www.cnblogs.com/xiaodiky/p/15811760.html

https://www.cnblogs.com/freepc/p/16228954.html

---

---
url: /数据库/01.MySQL/MySQL分词搜索.md
---

# MySQL分词搜索

https://www.cnblogs.com/huanzi-qch/p/15238604.html

```
select name from game WHERE MATCH (name) AGAINST ('刀剑危机')
select english_name from game WHERE MATCH (english_name) AGAINST ('Ac')
explain select english_name from game WHERE MATCH (english_name) AGAINST ('Ac')

show variables like '%token%';

set ngram_token_size=2
SET SESSION ngram_token_size = 1;

CREATE FULLTEXT INDEX index_name ON game (name) WITH PARSER ngram;
CREATE FULLTEXT INDEX index_english_name ON game (english_name) WITH PARSER ngram;


mysqld  --defaults-file="D:\Program Files\MySQL\MySQL Server 8.0\my.ini"
```

window安装的mysql8没有my.ini

https://blog.csdn.net/to\_perfect/article/details/107009110

https://blog.csdn.net/interestANd/article/details/115269428

---

---
url: /常用框架/ShardingJdbc/3_MySQL分库分表原理.md
---

# MySQL分库分表原理

## MySQL分库分表原理

### 1、为什么要分库分表

一般的机器（4核16G），单库的MySQL并发（QPS+TPS）超过了2k，系统基本就完蛋了。最好是并发量控制在1k左右。这里就引出一个问题，为什么要分库分表？

分库分表目的：解决高并发，和数据量大的问题。

1、高并发情况下，会造成IO读写频繁，自然就会造成读写缓慢，甚至是宕机。一般单库不要超过2k并发，NB的机器除外。

2、数据量大的问题。主要由于底层索引实现导致，MySQL的索引实现为B+TREE，数据量其他，会导致索引树十分庞大，造成查询缓慢。第二，innodb的最大存储限制64TB。

要解决上述问题。最常见做法，就是分库分表。

分库分表的目的，是将一个表拆成N个表，就是让每个表的数据量控制在一定范围内，保证SQL的性能。 一个表数据建议不要超过500W。

![img](/assets/a764bf1ba655730832cd5510a24e03a3.Br0rL_hu.png)

### 2、分库分表

又分为垂直拆分和水平拆分。

\*\*水平拆分：\*\*统一个表的数据拆到不同的库不同的表中。可以根据时间、地区、或某个业务键维度，也可以通过hash进行拆分，最后通过路由访问到具体的数据。拆分后的每个表结构保持一致。

\*\*垂直拆分：\*\*就是把一个有很多字段的表给拆分成多个表，或者是多个库上去。每个库表的结构都不一样，每个库表都包含部分字段。一般来说，可以根据业务维度进行拆分，如订单表可以拆分为订单、订单支持、订单地址、订单商品、订单扩展等表；也可以，根据数据冷热程度拆分，20%的热点字段拆到一个表，80%的冷字段拆到另外一个表。

![img](/assets/7f0d86e3bedbf0f6bf87d33056b9b93e.Dcjurlqy.png)

### 3、不停机分库分表数据迁移

一般数据库的拆分也是有一个过程的，一开始是单表，后面慢慢拆成多表。那么我们就看下如何平滑的从MySQL单表过度到MySQL的分库分表架构。

1、利用mysql+canal做增量数据同步，利用分库分表中间件，将数据路由到对应的新表中。

2、利用分库分表中间件，全量数据导入到对应的新表中。

3、通过单表数据和分库分表数据两两比较，更新不匹配的数据到新表中。

4、数据稳定后，将单表的配置切换到分库分表配置上。

![img](/assets/74d7579fbb4fee984ef54d20d6bdf6e3.dxwFtvyW.png)

### 4、小结

垂直拆分：业务模块拆分、商品库，用户库，订单库

水平拆分：对表进行水平拆分（也就是我们说的：分表）

表进行垂直拆分：表的字段过多，字段使用的频率不一。（可以拆分两个表建立1:1关系）

---

---
url: /数据库/01.MySQL/Mysql集群.md
---

# Mysql集群

https://cloud.tencent.com/developer/article/2147091

---

---
url: /数据库/01.MySQL/MySQL常见问题及解决方案/MySQL慢查询优化.md
---

# MySQL慢查询优化

## explain 使用介绍

通过 explain，可以查看 sql 语句的执行情况（比如查询的表，使用的索引以及 mysql 在表中找到所需行的方式等） 用 explain 查询 mysql 查询计划的输出参数有:

| 列名          | 说明                                                         |
| :------------ | :----------------------------------------------------------- |
| id            | 执行编号，标识 select 所属的行。如果在语句中没子查询或关联查询，只有唯一的 select，每行都将显示 1。否则，内层的 select 语句一般会顺序编号，对应于其在原始语句中的位置 |
| select\_type   | 显示本行是简单或复杂 select。如果查询有任何复杂的子查询，则最外层标记为 PRIMARY（DERIVED、UNION、UNION RESUlT） |
| table         | 访问引用哪个表（引用某个查询，如 “derived3”）                |
| type          | 数据访问 / 读取操作类型（ALL、index、range、ref、eq\_ref、const/system、NULL） |
| possible\_keys | 揭示哪一些索引可能有利于高效的查找                           |
| key           | 显示 mysql 决定采用哪个索引来优化查询                        |
| key\_len       | 显示 mysql 在索引里使用的字节数                              |
| ref           | 显示了之前的表在 key 列记录的索引中查找值所用的列或常量      |
| rows          | 为了找到所需的行而需要读取的行数，估算值，不精确。通过把所有 rows 列值相乘，可粗略估算整个查询会检查的行数 |
| Extra         | 额外信息，如 using index、filesort 等                        |

需要注意的是我们重点关注 type 即可！！！

type 显示的是访问类型，是较为重要的一个指标，结果值从好到坏依次是： system > const > eq\_ref > ref > fulltext > ref\_or\_null > index\_merge > unique\_subquery > index\_subquery > range > index > ALL ，一般来说，得保证查询至少达到 range 级别，最好能达到 ref。

| 类型   | 说明                                                         |
| :----- | :----------------------------------------------------------- |
| All    | 最坏的情况，全表扫描                                         |
| index  | 和全表扫描一样。只是扫描表的时候按照索引次序进行而不是行。主要优点就是避免了排序，但是开销仍然非常大。如在 Extra 列看到 Using index，说明正在使用覆盖索引，只扫描索引的数据，它比按索引次序全表扫描的开销要小很多 |
| range  | 范围扫描，一个有限制的索引扫描。key 列显示使用了哪个索引。当使用 =、 <>、>、>=、<、<=、IS NULL、<=>、BETWEEN 或者 IN 操作符，用常量比较关键字列时，可以使用 range |
| ref    | 一种索引访问，它返回所有匹配某个单个值的行。此类索引访问只有当使用非唯一性索引或唯一性索引非唯一性前缀时才会发生。这个类型跟 eq\_ref 不同的是，它用在关联操作只使用了索引的最左前缀，或者索引不是 UNIQUE 和 PRIMARY KEY。ref 可以用于使用 = 或 <=> 操作符的带索引的列。 |
| eq\_ref | 最多只返回一条符合条件的记录。使用唯一性索引或主键查找时会发生 （高效） |
| const  | 当确定最多只会有一行匹配的时候，MySQL 优化器会在查询前读取它而且只读取一次，因此非常快。当主键放入 where 子句时，mysql 把这个查询转为一个常量（高效） |
| system | 这是 const 连接类型的一种特例，表仅有一行满足条件。          |
| Null   | 意味说 mysql 能在优化阶段分解查询语句，在执行阶段甚至用不到访问表或索引（高效） |

参考

不会看 Explain执行计划，劝你简历别写熟悉 SQL优化：https://juejin.cn/post/6844904163969630221
https://juejin.cn/post/7190595503640281145

---

---
url: /数据库/01.MySQL/MySQL日期函数.md
---

# MySQL日期函数

[MYSQL之查询按日期分组统计\_mysql 按日期分组-CSDN博客](https://blog.csdn.net/Alian_1223/article/details/129397114)

[mysql日期范围分组\_mob64ca12e86bd4的技术博客\_51CTO博客](https://blog.51cto.com/u_16213398/7476638)

---

---
url: /数据库/01.MySQL/Mysql索引.md
---

# Mysql索引

## 索引介绍

MySQL 索引是一种数据结构，用于加快数据库查询的速度和性能。通过使用索引，MySQL 可以直接定位到满足查询条件的数据行，而无需逐行扫描整个表。

> MySQL 索引类似于书籍的索引，通过存储指向数据行的指针，可以快速定位和访问表中的特定数据。
>
> 打个比方，如果合理的设计且使用索引的 MySQL 是一辆兰博基尼的话，那么没有设计和使用索引的 MySQL 就是一个人力三轮车。
>
> 拿汉语字典的目录页（索引）打比方，我们可以按拼音、笔画、偏旁部首等排序的目录（索引）快速查找到需要的字。

索引分单列索引和组合索引：

* 单列索引，即一个索引只包含单个列，一个表可以有多个单列索引。
* 组合索引，即一个索引包含多个列。

创建索引时，你需要确保该索引是应用在 SQL 查询语句的条件(一般作为 WHERE 子句的条件)。

实际上，索引也是一张表，该表保存了主键与索引字段，并指向实体表的记录。

索引虽然能够提高查询性能，但也需要注意以下几点：

* 索引需要占用额外的存储空间。
* 对表进行插入、更新和删除操作时，索引需要维护，可能会影响性能。
* 过多或不合理的索引可能会导致性能下降，因此需要谨慎选择和规划索引。

## 添加索引

在mysql中有多种索引，有普通索引，全文索引，唯一索引，多列索引。

### 普通索引

使用 **CREATE INDEX** 语句可以创建普通索引。

```
CREATE INDEX index_name
ON table_name (column1 [ASC|DESC], column2 [ASC|DESC], ...);
```

* `CREATE INDEX`: 用于创建普通索引的关键字。
* `index_name`: 指定要创建的索引的名称。索引名称在表中必须是唯一的。
* `table_name`: 指定要在哪个表上创建索引。
* `(column1, column2, ...)`: 指定要索引的表列名。你可以指定一个或多个列作为索引的组合。这些列的数据类型通常是数值、文本或日期。
* `ASC`和`DESC`（可选）: 用于指定索引的排序顺序。默认情况下，索引以升序（ASC）排序。

假设有一个名为 students 的表，包含 id、name 和 age 列，在 name 列上创建一个普通索引。

```
CREATE INDEX idx_name ON students (name);
```

### 唯一索引

在 MySQL 中，你可以使用 **CREATE UNIQUE INDEX** 语句来创建唯一索引。

唯一索引确保索引中的值是唯一的，不允许有重复值。

```
CREATE UNIQUE INDEX index_name
ON table_name (column1 [ASC|DESC], column2 [ASC|DESC], ...);
```

* `CREATE UNIQUE INDEX`: 用于创建唯一索引的关键字组合。
* `index_name`: 指定要创建的唯一索引的名称。索引名称在表中必须是唯一的。
* `table_name`: 指定要在哪个表上创建唯一索引。
* `(column1, column2, ...)`: 指定要索引的表列名。你可以指定一个或多个列作为索引的组合。这些列的数据类型通常是数值、文本或日期。
* `ASC`和`DESC`（可选）: 用于指定索引的排序顺序。默认情况下，索引以升序（ASC）排序。

以下是一个创建唯一索引的实例： 假设我们有一个名为 employees的 表，包含 id 和 email 列，现在我们想在email列上创建一个唯一索引，以确保每个员工的电子邮件地址都是唯一的。

```
CREATE UNIQUE INDEX idx_email ON employees (email);
```

### 全文索引

```
CREATE FULLTEXT INDEX index_name ON t_dept(name);
```

### 多列索引

```
CREATE INDEX index_name_no ON t_dept(name,no);
```

## 索引失效

[索引失效的情况及解决(超详细)-CSDN博客](https://blog.csdn.net/sy_white/article/details/122112440)

参考资料

https://zhuanlan.zhihu.com/p/293553628

https://www.runoob.com/mysql/mysql-index.html

## 索引下推

https://cloud.tencent.com/developer/article/2398503
https://blog.csdn.net/weixin\_43310500/article/details/135090387

---

---
url: /数据库/01.MySQL/MySQL执行分析.md
---

# MySQL执行分析

https://blog.csdn.net/abc123mma/article/details/127811132

---

---
url: /数据库/01.MySQL/MySQL主从复制.md
---

# MySQL主从复制

## 一、概述

主从复制是指将主数据库的DDL和DML操作通过二进制日志传到从库服务器中，然后在从库上对这些日志重新执行（也叫重做），从而使得从库和主库的数据保持同步。

MySQL支持一台主库同时向多台从库进行复制，从库同时也可以作为其他从服务器的主库，实现链状复制。

MySQL复制的有点主要包含以下三个方面：

1. 主库出现问题，可以快速切换到从库提供服务。
2. 实现读写分离，降低主库的访问压力。
3. 可以在从库中执行备份，以避免备份期间影响主库服务。

## 二、原理

MySQL的主从复制原理如下。
在多个源的复制中，每一个复制源都会打开一个复制通道，这是一个长链接。并且每个复制源都有自己的 IO线程、一个或者多个点 SQL 线程以及 realy log。复制源接收到事务时会将其添加到relay log 中，然后通过SQL thread执行。相关官方文档如下：

```plain
In MySQL multi-source replication, a replica opens multiple replication channels,
one for each replication source server. The replication channels represent the 
path of transactions flowing from a source to the replica. Each replication 
channel has its own receiver (I/O) thread, one or more applier (SQL) threads, and
relay log. When transactions from a source are received by a channel's receiver 
thread, they are added to the channel's relay log file and passed through to the 
channel's applier threads. This enables each channel to function independently.
```

复制分为三步：

1. Master主库在事务提交时，会把数据变更记录在二进制日志文件Binlog中。
2. 从库读取主库的二进制日志文件Binlog，写入到从库的中继日志Relay Log。
3. slave重做中继日志中的事件，将改变反映它自己的数据。

主从复制应该是分为**第一次建立连接**和**增量数据同步**过程。

![image.png](/assets/image-1723364218665.rOI5qR7L.png)

![image.png](/assets/image-1723364227487.DQeAF4VW.png)

### 第一次建立连接

备库 B 跟主库 A 之间维持了一个长连接。主库 A 内部有一个io\_thread线程，专门用于服务备库 B 的这个长连接。一个事务日志同步的完整过程是这样的：

1. 在备库 B 上通过 change master 命令，设置主库 A 的 IP、端口、用户名、密码，以及要从哪个位置开始请求 binlog，这个位置包含文件名和日志偏移量。

   ```shell
   CHANGE MASTER TO 
   MASTER_HOST='192.168.56.104',
   MASTER_USER='root',
   MASTER_PASSWORD='qwer_123',
   MASTER_LOG_FILE='mysql-bin.000001',MASTER_LOG_POS=154;
   ```

2. 备库 B 上执行start slave命令，这时候备库会启动两个线程，就是图中的 io\_thread 和 sql\_thread。其中 io\_thread 负责与主库建立连接。

3. 主库 A 校验完用户名、密码后，开始按照备库 B 传过来的位置，从本地读取 binlog，发给备库 B。

4. 备库 B 拿到 binlog 后，写到relay log（中继日志）中。

5. 备库的 sql\_thread 读取 relay log，解析出日志里的命令，并且回放执行。

### 增量同步详细流程

详细过程如下：

1. 客户端发起 update 请求，MySQL server 端收到请求。
2. 生成被修改数据行对应的 undolog。
3. 执行update成功写入内存。
4. InnboDB 生成 redo log ，此时处于 prepare阶段。
5. server 层生成binlog，事务提交时binlog做持久化，此时binlog便可以开始被同步到从库了。
6. redo log 做磁盘持久化，同时向客户端返回update的执行新结果（默认异步复制）。
7. 主库发送生成的 binlog 数据。
8. 从库的io\_thread处理Maste传输过来的数据，保存为relay log。从库服务器会在一定时间间隔内对master二进制日志进行探测其是否发生改变，如果发生改变，则开始一个I/OThread请求master二进制事件。
9. SQL thread 读取relay log，解析并且在从库中重放执行，数据同步完成。最后I/OThread和SQLThread将进入睡眠状态，等待下一次被唤醒。

## 三、搭建

### 主库配置

1、修改配置文件 /etc/my.cnf

```shell
#mysql服务ID，保证整个集群环境中唯一，取值范围: 1-2^32-1，默认为1
server-id=1
#是否只读,1代表只读,0代表读写
read-only=0
#忽略的数据,指不需要同步的数据库
#binlog-ignore-db=mysql
#指定同步的数据库
#binlog-do-db=db01
```

2、重启MySQL服务器

```shell
systemctl restart mysqld
```

3、登录mysql，创建远程连接的账号，并授予主从复制权限

```shell
#创建用户，并设置密码，该用户可在任意主机连接该MSOL服条
CREATE USER 'write'@'%' IDENTIFIED WITH mysql_native_password BY 'Root@123456';
#为 'read'@'%' 用户分配主从复制权限
GRANT REPLICATION SLAVE ON *.* TO 'write'@'%';
```

4、通过指令，查看二讲制日志坐标

```shell
show master status;
```

字段含义说明：

* file：从哪个日志文件开始推送日志文件

* position：从哪个位置开始推送日志

* binlog\_do\_db：指定需要同步的数据库

* binlog\_ignore\_db：指定不需要同步的数据库

### 从库配置

1、修改配置文件 /etc/my.cnf

```shell
#mysql服务ID，保证整个集群环境中唯一，取值范围: 1-2^32-1，和主库不一样即可
server-id=2
#是否只读,1代表只读,0代表读写
read-only=1
#超级管理员只读
#super-read-only=1
```

2、重启MySQL服务器

```shell
systemctl restart mysqld
```

3、登录mysql，设置主库配置

```sql
CHANGE REPLICATION SOURCE TO SOURCE_HOST='xxx.xxx',SOURCE_USER='XXX',SOURCE_LOG_FILE='xxx',SOURCE_LOG_POS=xxx;
```

上述时8.0.23中的语法，如是时mysql8.0.23之前的版本，执行如下SQL：

```sql
CHANGE MASTER TO MASTER_HOST='xxx.xxx.xxx.xxx',MASTER_USER='xxx',MASTER_PASSWORD='xxx',MASTER_LOG_FILE='xxx',MASTER_LOG_POS=xxx;
```

4、开启同步操作

```shell
start replica; # 8.0.22之后
start slave; # 8.0.22之前
```

5、查看主从同步状态

```shell
show replica status; # 8.0.22之后  show replica status\G;  # 转换成行展示
show slave status; # 8.0.22之前
```

## 四、测试

1.在主库上创建数据库、表，并插入数据

2.在从库中查询数据，验证主从是否同步

## 参考资料

\[1]. [黑马MySQL数据库进阶教程，轻松掌握mysql主从复制从原理到搭建全流程](https://www.bilibili.com/video/BV1jT411r77s)

\[2]. [mysql运维——主从复制](https://www.dianjilingqu.com/710745.html)

\[3]. [主从复制实现原理详解](https://www.cnblogs.com/shoshana-kong/p/17318124.html)

\[4]. [binlog\_ignore\_db 参数的具体使用](https://www.jb51.net/article/201609.htm)

\[5]. <https://blog.csdn.net/qq_41772936/article/details/80380950>

\[6]. <https://blog.csdn.net/m0_62473957/article/details/124140928>

---

---
url: /Java/微服务专栏/03.注册中心Nacos/0_Nacos安装.md
---

# Nacos安装指南

# 1.Windows安装

开发阶段采用单机安装即可。

## 1.1.下载安装包

在Nacos的GitHub页面，提供有下载链接，可以下载编译好的Nacos服务端或者源代码：

GitHub主页：https://github.com/alibaba/nacos

GitHub的Release下载页：https://github.com/alibaba/nacos/releases

如图：

![image-20210402161102887](/assets/image-20210402161102887.BVrTH8Li.png)

本课程采用1.4.1.版本的Nacos，

windows版本使用`nacos-server-1.4.1.zip`包即可。

## 1.2.解压

将这个包解压到任意非中文目录下，如图：

![image-20210402161843337](/assets/image-20210402161843337.Co-ia5ny.png)

目录说明：

* bin：启动脚本
* conf：配置文件

## 1.3.端口配置

Nacos的默认端口是8848，如果你电脑上的其它进程占用了8848端口，请先尝试关闭该进程。

**如果无法关闭占用8848端口的进程**，也可以进入nacos的conf目录，修改配置文件中的端口：

![image-20210402162008280](/assets/image-20210402162008280.C3fdkmZQ.png)

修改其中的内容：

![image-20210402162251093](/assets/image-20210402162251093.B8nquITT.png)

## 1.4.启动

启动非常简单，进入bin目录，结构如下：

![image-20210402162350977](/assets/image-20210402162350977.DYFDHidv.png)

然后执行命令即可：

* windows命令：

  ```
  startup.cmd -m standalone
  ```

执行后的效果如图：

![image-20210402162526774](/assets/image-20210402162526774.D_xu6845.png)

## 1.5.访问

在浏览器输入地址：http://127.0.0.1:8848/nacos即可：

![image-20210402162630427](/assets/image-20210402162630427.LpxqM671.png)

默认的账号和密码都是nacos，进入后：

![image-20210402162709515](/assets/image-20210402162709515.CsW7VTST.png)

# 2.Linux安装

Linux或者Mac安装方式与Windows类似。

## 2.1.安装JDK

Nacos依赖于JDK运行，索引Linux上也需要安装JDK才行。

上传jdk安装包：

上传到某个目录，例如：`/usr/local/`

然后解压缩：

```sh
tar -xvf jdk-8u144-linux-x64.tar.gz
```

然后重命名为java

配置环境变量：

```sh
export JAVA_HOME=/usr/local/java
export PATH=$PATH:$JAVA_HOME/bin
```

设置环境变量：

```sh
source /etc/profile
```

## 2.2.上传安装包

如图：

![image-20210402161102887](/assets/image-20210402161102887.BVrTH8Li.png)

上传到Linux服务器的某个目录，例如`/usr/local/src`目录下：

![image-20210402163715580](/assets/image-20210402163715580.DH19jCg5.png)

## 2.3.解压

命令解压缩安装包：

```sh
tar -xvf nacos-server-1.4.1.tar.gz
```

然后删除安装包：

```sh
rm -rf nacos-server-1.4.1.tar.gz
```

目录中最终样式：

![image-20210402163858429](/assets/image-20210402163858429.LdfHt_RN.png)

目录内部：

![image-20210402164414827](/assets/image-20210402164414827.C-CRAGtd.png)

## 2.4.端口配置

与windows中类似

## 2.5.启动

在nacos/bin目录中，输入命令启动Nacos：

```sh
sh startup.sh -m standalone
```

# 3.Nacos的依赖

父工程：

```xml
<dependency>
    <groupId>com.alibaba.cloud</groupId>
    <artifactId>spring-cloud-alibaba-dependencies</artifactId>
    <version>2.2.5.RELEASE</version>
    <type>pom</type>
    <scope>import</scope>
</dependency>
```

客户端：

```xml
<!-- nacos客户端依赖包 -->
<dependency>
    <groupId>com.alibaba.cloud</groupId>
    <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>
</dependency>

```

https://www.cnblogs.com/ComfortableM/p/17384518.html

---

---
url: /Java/微服务专栏/03.注册中心Nacos/4_Nacos动态读取xml日志配置.md
---

# Nacos动态读取xml日志配置

https://blog.csdn.net/qq\_35429398/article/details/131516382

https://www.cnblogs.com/gaomanito/p/14755232.html

https://blog.csdn.net/qq\_39313596/article/details/131011629

---

---
url: /Java/微服务专栏/03.注册中心Nacos/3_Nacos集群搭建.md
---

# Nacos集群搭建

# 1.集群结构图

官方给出的Nacos集群图：

![image-20210409210621117](/assets/image-20210409210621117.D5ajIBeZ.png)

其中包含3个nacos节点，然后一个负载均衡器代理3个Nacos。这里负载均衡器可以使用nginx。

我们计划的集群结构：

![image-20210409211355037](/assets/image-20210409211355037.hNvI6IUg.png)

三个nacos节点的地址：

| 节点   | ip            | port |
| ------ | ------------- | ---- |
| nacos1 | 192.168.150.1 | 8845 |
| nacos2 | 192.168.150.1 | 8846 |
| nacos3 | 192.168.150.1 | 8847 |

# 2.搭建集群

搭建集群的基本步骤：

* 搭建数据库，初始化数据库表结构
* 下载nacos安装包
* 配置nacos
* 启动nacos集群
* nginx反向代理

## 2.1.初始化数据库

Nacos默认数据存储在内嵌数据库Derby中，不属于生产可用的数据库。

官方推荐的最佳实践是使用带有主从的高可用数据库集群。

这里我们以单点的数据库为例来讲解。

首先新建一个数据库，命名为nacos，而后导入下面的SQL：

```sql
CREATE TABLE `config_info` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT 'id',
  `data_id` varchar(255) NOT NULL COMMENT 'data_id',
  `group_id` varchar(255) DEFAULT NULL,
  `content` longtext NOT NULL COMMENT 'content',
  `md5` varchar(32) DEFAULT NULL COMMENT 'md5',
  `gmt_create` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
  `gmt_modified` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '修改时间',
  `src_user` text COMMENT 'source user',
  `src_ip` varchar(50) DEFAULT NULL COMMENT 'source ip',
  `app_name` varchar(128) DEFAULT NULL,
  `tenant_id` varchar(128) DEFAULT '' COMMENT '租户字段',
  `c_desc` varchar(256) DEFAULT NULL,
  `c_use` varchar(64) DEFAULT NULL,
  `effect` varchar(64) DEFAULT NULL,
  `type` varchar(64) DEFAULT NULL,
  `c_schema` text,
  PRIMARY KEY (`id`),
  UNIQUE KEY `uk_configinfo_datagrouptenant` (`data_id`,`group_id`,`tenant_id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='config_info';

/******************************************/
/*   数据库全名 = nacos_config   */
/*   表名称 = config_info_aggr   */
/******************************************/
CREATE TABLE `config_info_aggr` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT 'id',
  `data_id` varchar(255) NOT NULL COMMENT 'data_id',
  `group_id` varchar(255) NOT NULL COMMENT 'group_id',
  `datum_id` varchar(255) NOT NULL COMMENT 'datum_id',
  `content` longtext NOT NULL COMMENT '内容',
  `gmt_modified` datetime NOT NULL COMMENT '修改时间',
  `app_name` varchar(128) DEFAULT NULL,
  `tenant_id` varchar(128) DEFAULT '' COMMENT '租户字段',
  PRIMARY KEY (`id`),
  UNIQUE KEY `uk_configinfoaggr_datagrouptenantdatum` (`data_id`,`group_id`,`tenant_id`,`datum_id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='增加租户字段';


/******************************************/
/*   数据库全名 = nacos_config   */
/*   表名称 = config_info_beta   */
/******************************************/
CREATE TABLE `config_info_beta` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT 'id',
  `data_id` varchar(255) NOT NULL COMMENT 'data_id',
  `group_id` varchar(128) NOT NULL COMMENT 'group_id',
  `app_name` varchar(128) DEFAULT NULL COMMENT 'app_name',
  `content` longtext NOT NULL COMMENT 'content',
  `beta_ips` varchar(1024) DEFAULT NULL COMMENT 'betaIps',
  `md5` varchar(32) DEFAULT NULL COMMENT 'md5',
  `gmt_create` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
  `gmt_modified` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '修改时间',
  `src_user` text COMMENT 'source user',
  `src_ip` varchar(50) DEFAULT NULL COMMENT 'source ip',
  `tenant_id` varchar(128) DEFAULT '' COMMENT '租户字段',
  PRIMARY KEY (`id`),
  UNIQUE KEY `uk_configinfobeta_datagrouptenant` (`data_id`,`group_id`,`tenant_id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='config_info_beta';

/******************************************/
/*   数据库全名 = nacos_config   */
/*   表名称 = config_info_tag   */
/******************************************/
CREATE TABLE `config_info_tag` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT 'id',
  `data_id` varchar(255) NOT NULL COMMENT 'data_id',
  `group_id` varchar(128) NOT NULL COMMENT 'group_id',
  `tenant_id` varchar(128) DEFAULT '' COMMENT 'tenant_id',
  `tag_id` varchar(128) NOT NULL COMMENT 'tag_id',
  `app_name` varchar(128) DEFAULT NULL COMMENT 'app_name',
  `content` longtext NOT NULL COMMENT 'content',
  `md5` varchar(32) DEFAULT NULL COMMENT 'md5',
  `gmt_create` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
  `gmt_modified` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '修改时间',
  `src_user` text COMMENT 'source user',
  `src_ip` varchar(50) DEFAULT NULL COMMENT 'source ip',
  PRIMARY KEY (`id`),
  UNIQUE KEY `uk_configinfotag_datagrouptenanttag` (`data_id`,`group_id`,`tenant_id`,`tag_id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='config_info_tag';

/******************************************/
/*   数据库全名 = nacos_config   */
/*   表名称 = config_tags_relation   */
/******************************************/
CREATE TABLE `config_tags_relation` (
  `id` bigint(20) NOT NULL COMMENT 'id',
  `tag_name` varchar(128) NOT NULL COMMENT 'tag_name',
  `tag_type` varchar(64) DEFAULT NULL COMMENT 'tag_type',
  `data_id` varchar(255) NOT NULL COMMENT 'data_id',
  `group_id` varchar(128) NOT NULL COMMENT 'group_id',
  `tenant_id` varchar(128) DEFAULT '' COMMENT 'tenant_id',
  `nid` bigint(20) NOT NULL AUTO_INCREMENT,
  PRIMARY KEY (`nid`),
  UNIQUE KEY `uk_configtagrelation_configidtag` (`id`,`tag_name`,`tag_type`),
  KEY `idx_tenant_id` (`tenant_id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='config_tag_relation';

/******************************************/
/*   数据库全名 = nacos_config   */
/*   表名称 = group_capacity   */
/******************************************/
CREATE TABLE `group_capacity` (
  `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT COMMENT '主键ID',
  `group_id` varchar(128) NOT NULL DEFAULT '' COMMENT 'Group ID，空字符表示整个集群',
  `quota` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '配额，0表示使用默认值',
  `usage` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '使用量',
  `max_size` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '单个配置大小上限，单位为字节，0表示使用默认值',
  `max_aggr_count` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '聚合子配置最大个数，，0表示使用默认值',
  `max_aggr_size` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '单个聚合数据的子配置大小上限，单位为字节，0表示使用默认值',
  `max_history_count` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '最大变更历史数量',
  `gmt_create` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
  `gmt_modified` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '修改时间',
  PRIMARY KEY (`id`),
  UNIQUE KEY `uk_group_id` (`group_id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='集群、各Group容量信息表';

/******************************************/
/*   数据库全名 = nacos_config   */
/*   表名称 = his_config_info   */
/******************************************/
CREATE TABLE `his_config_info` (
  `id` bigint(64) unsigned NOT NULL,
  `nid` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `data_id` varchar(255) NOT NULL,
  `group_id` varchar(128) NOT NULL,
  `app_name` varchar(128) DEFAULT NULL COMMENT 'app_name',
  `content` longtext NOT NULL,
  `md5` varchar(32) DEFAULT NULL,
  `gmt_create` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `gmt_modified` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `src_user` text,
  `src_ip` varchar(50) DEFAULT NULL,
  `op_type` char(10) DEFAULT NULL,
  `tenant_id` varchar(128) DEFAULT '' COMMENT '租户字段',
  PRIMARY KEY (`nid`),
  KEY `idx_gmt_create` (`gmt_create`),
  KEY `idx_gmt_modified` (`gmt_modified`),
  KEY `idx_did` (`data_id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='多租户改造';


/******************************************/
/*   数据库全名 = nacos_config   */
/*   表名称 = tenant_capacity   */
/******************************************/
CREATE TABLE `tenant_capacity` (
  `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT COMMENT '主键ID',
  `tenant_id` varchar(128) NOT NULL DEFAULT '' COMMENT 'Tenant ID',
  `quota` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '配额，0表示使用默认值',
  `usage` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '使用量',
  `max_size` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '单个配置大小上限，单位为字节，0表示使用默认值',
  `max_aggr_count` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '聚合子配置最大个数',
  `max_aggr_size` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '单个聚合数据的子配置大小上限，单位为字节，0表示使用默认值',
  `max_history_count` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '最大变更历史数量',
  `gmt_create` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
  `gmt_modified` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '修改时间',
  PRIMARY KEY (`id`),
  UNIQUE KEY `uk_tenant_id` (`tenant_id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='租户容量信息表';


CREATE TABLE `tenant_info` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT 'id',
  `kp` varchar(128) NOT NULL COMMENT 'kp',
  `tenant_id` varchar(128) default '' COMMENT 'tenant_id',
  `tenant_name` varchar(128) default '' COMMENT 'tenant_name',
  `tenant_desc` varchar(256) DEFAULT NULL COMMENT 'tenant_desc',
  `create_source` varchar(32) DEFAULT NULL COMMENT 'create_source',
  `gmt_create` bigint(20) NOT NULL COMMENT '创建时间',
  `gmt_modified` bigint(20) NOT NULL COMMENT '修改时间',
  PRIMARY KEY (`id`),
  UNIQUE KEY `uk_tenant_info_kptenantid` (`kp`,`tenant_id`),
  KEY `idx_tenant_id` (`tenant_id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='tenant_info';

CREATE TABLE `users` (
	`username` varchar(50) NOT NULL PRIMARY KEY,
	`password` varchar(500) NOT NULL,
	`enabled` boolean NOT NULL
);

CREATE TABLE `roles` (
	`username` varchar(50) NOT NULL,
	`role` varchar(50) NOT NULL,
	UNIQUE INDEX `idx_user_role` (`username` ASC, `role` ASC) USING BTREE
);

CREATE TABLE `permissions` (
    `role` varchar(50) NOT NULL,
    `resource` varchar(255) NOT NULL,
    `action` varchar(8) NOT NULL,
    UNIQUE INDEX `uk_role_permission` (`role`,`resource`,`action`) USING BTREE
);

INSERT INTO users (username, password, enabled) VALUES ('nacos', '$2a$10$EuWPZHzz32dJN7jexM34MOeYirDdFAZm2kuWj7VEOJhhZkDrxfvUu', TRUE);

INSERT INTO roles (username, role) VALUES ('nacos', 'ROLE_ADMIN');
```

## 2.2.下载nacos

nacos在GitHub上有下载地址：https://github.com/alibaba/nacos/tags，可以选择任意版本下载。

本例中才用1.4.1版本：

![image-20210409212119411](/assets/image-20210409212119411.Byp_1HBg.png)

## 2.3.配置Nacos

将这个包解压到任意非中文目录下，如图：

![image-20210402161843337](/assets/image-20210402161843337.Co-ia5ny.png)

目录说明：

* bin：启动脚本
* conf：配置文件

进入nacos的conf目录，修改配置文件cluster.conf.example，重命名为cluster.conf：

![image-20210409212459292](/assets/image-20210409212459292.Bfe0T8VK.png)

然后添加内容：

```
127.0.0.1:8845
127.0.0.1.8846
127.0.0.1.8847
```

然后修改application.properties文件，添加数据库配置

```properties
spring.datasource.platform=mysql

db.num=1

db.url.0=jdbc:mysql://127.0.0.1:3306/nacos?characterEncoding=utf8&connectTimeout=1000&socketTimeout=3000&autoReconnect=true&useUnicode=true&useSSL=false&serverTimezone=UTC
db.user.0=root
db.password.0=123
```

## 2.4.启动

将nacos文件夹复制三份，分别命名为：nacos1、nacos2、nacos3

![image-20210409213335538](/assets/image-20210409213335538.nVlNJWOU.png)

然后分别修改三个文件夹中的application.properties，

nacos1:

```properties
server.port=8845
```

nacos2:

```properties
server.port=8846
```

nacos3:

```properties
server.port=8847
```

然后分别启动三个nacos节点：

```
startup.cmd
```

## 2.5.nginx反向代理

进入[官网](https://nginx.org/en/download.html)，下载稳定版本

![image-20230912011918592](/assets/image-20230912011918592.BIeMFA6D.png)

解压到任意非中文目录下：

![image-20210410103322874](/assets/image-20210410103322874.Bm-GRVuY.png)

修改conf/nginx.conf文件，配置如下：

```nginx
upstream nacos-cluster {
    server 127.0.0.1:8845;
	server 127.0.0.1:8846;
	server 127.0.0.1:8847;
}

server {
    listen       80;
    server_name  localhost;

    location /nacos {
        proxy_pass http://nacos-cluster;
    }
}
```

而后在浏览器访问：http://localhost/nacos即可。

代码中application.yml文件配置如下：

```yaml
spring:
  cloud:
    nacos:
      server-addr: localhost:80 # Nacos地址
```

## 2.6.优化

* 实际部署时，需要给做反向代理的nginx服务器设置一个域名，这样后续如果有服务器迁移nacos的客户端也无需更改配置.

* Nacos的各个节点应该部署到多个不同服务器，做好容灾和隔离

---

---
url: /Java/微服务专栏/03.注册中心Nacos/2_Nacos配置管理.md
---

# Nacos配置管理

Nacos除了可以做注册中心，同样可以做配置管理来使用。

## 1.统一配置管理

当微服务部署的实例越来越多，达到数十、数百时，逐个修改微服务配置就会让人抓狂，而且很容易出错。我们需要一种统一配置管理方案，可以集中管理所有实例的配置。

![image-20210714164426792](/assets/image-20210714164426792-1694452160185.CEjo8FcL.png)

Nacos一方面可以将配置集中管理，另一方可以在配置变更时，及时通知微服务，实现配置的热更新。

### 1.1.在nacos中添加配置文件

如何在nacos中管理配置呢？

![image-20210714164742924](/assets/image-20210714164742924-1694452160186.DxFcF70H.png)

然后在弹出的表单中，填写配置信息：

![image-20210714164856664](/assets/image-20210714164856664-1694452160186.DqtGQFpD.png)

> 注意：项目的核心配置，需要热更新的配置才有放到nacos管理的必要。基本不会变更的一些配置还是保存在微服务本地比较好。

### 1.2.从微服务拉取配置

微服务要拉取nacos中管理的配置，并且与本地的application.yml配置合并，才能完成项目启动。

但如果尚未读取application.yml，又如何得知nacos地址呢？

因此spring引入了一种新的配置文件：bootstrap.yaml文件，会在application.yml之前被读取，流程如下：

![img](/assets/L0iFYNF-1694452160186.C9_6LeNO.png)

1）引入nacos-config依赖

首先，在user-service服务中，引入nacos-config的客户端依赖：

```xml
<!--nacos配置管理依赖-->
<dependency>
    <groupId>com.alibaba.cloud</groupId>
    <artifactId>spring-cloud-starter-alibaba-nacos-config</artifactId>
</dependency>
```

2）添加bootstrap.yaml

然后，在user-service中添加一个bootstrap.yaml文件，内容如下：

```yaml
spring:
  application:
    name: userservice # 服务名称
  profiles:
    active: dev #开发环境，这里是dev 
  cloud:
    nacos:
      server-addr: localhost:8848 # Nacos地址
      config:
        file-extension: yaml # 文件后缀名
```

这里会根据spring.cloud.nacos.server-addr获取nacos地址，再根据

`${spring.application.name}-${spring.profiles.active}.${spring.cloud.nacos.config.file-extension}`作为文件id，来读取配置。

本例中，就是去读取`userservice-dev.yaml`：

![image-20210714170845901](/assets/image-20210714170845901-1694452160186.iiCwtZHR.png)

3）读取nacos配置

在user-service中的UserController中添加业务逻辑，读取pattern.dateformat配置：

![image-20210714170337448](/assets/image-20210714170337448-1694452160186.coSC81LC.png)

完整代码：

```java
package cn.itcast.user.web;

import cn.itcast.user.pojo.User;
import cn.itcast.user.service.UserService;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.web.bind.annotation.*;

import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;

@Slf4j
@RestController
@RequestMapping("/user")
public class UserController {

    @Autowired
    private UserService userService;

    @Value("${pattern.dateformat}")
    private String dateformat;
    
    @GetMapping("now")
    public String now(){
        return LocalDateTime.now().format(DateTimeFormatter.ofPattern(dateformat));
    }
    // ...略
}
```

在页面访问，可以看到效果：

![image-20210714170449612](/assets/image-20210714170449612-1694452160186.CgaRrUv8.png)

## 2.配置热更新

我们最终的目的，是修改nacos中的配置后，微服务中无需重启即可让配置生效，也就是**配置热更新**。

要实现配置热更新，可以使用两种方式：

### 2.1.方式一

在@Value注入的变量所在类上添加注解@RefreshScope：

![image-20210714171036335](/assets/image-20210714171036335-1694452160186.BL86lGTX.png)

### 2.2.方式二

使用@ConfigurationProperties注解代替@Value注解。

在user-service服务中，添加一个类，读取patterrn.dateformat属性：

```java
package cn.itcast.user.config;

import lombok.Data;
import org.springframework.boot.context.properties.ConfigurationProperties;
import org.springframework.stereotype.Component;

@Component
@Data
@ConfigurationProperties(prefix = "pattern")
public class PatternProperties {
    private String dateformat;
}
```

在UserController中使用这个类代替@Value：

![image-20210714171316124](/assets/image-20210714171316124-1694452160186.Ef6me41J.png)

完整代码：

```java
package cn.itcast.user.web;

import cn.itcast.user.config.PatternProperties;
import cn.itcast.user.pojo.User;
import cn.itcast.user.service.UserService;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.PathVariable;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RestController;

import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;

@Slf4j
@RestController
@RequestMapping("/user")
public class UserController {

    @Autowired
    private UserService userService;

    @Autowired
    private PatternProperties patternProperties;

    @GetMapping("now")
    public String now(){
        return LocalDateTime.now().format(DateTimeFormatter.ofPattern(patternProperties.getDateformat()));
    }

    // 略
}
```

## 3.配置共享

其实微服务启动时，会去nacos读取多个配置文件，例如：

* `[spring.application.name]-[spring.profiles.active].yaml`，例如：userservice-dev.yaml

* `[spring.application.name].yaml`，例如：userservice.yaml

而`[spring.application.name].yaml`不包含环境，因此可以被多个环境共享。

下面我们通过案例来测试配置共享

### 1）添加一个环境共享配置

我们在nacos中添加一个userservice.yaml文件：

![image-20210714173233650](/assets/image-20210714173233650-1694452160186.CAH8QDoC.png)

### 2）在user-service中读取共享配置

在user-service服务中，修改PatternProperties类，读取新添加的属性：

![image-20210714173324231](/assets/image-20210714173324231-1694452160186.BctAh_bU.png)

在user-service服务中，修改UserController，添加一个方法：

![image-20210714173721309](/assets/image-20210714173721309-1694452160186.C4Az1x9T.png)

### 3）运行两个UserApplication，使用不同的profile

修改UserApplication2这个启动项，改变其profile值：

![image-20210714173538538](/assets/image-20210714173538538-1694452160186.prLUTPFr.png)

![image-20210714173519963](/assets/image-20210714173519963-1694452160186.DlPzkNPf.png)

这样，UserApplication(8081)使用的profile是dev，UserApplication2(8082)使用的profile是test。

启动UserApplication和UserApplication2

访问http://localhost:8081/user/prop，结果：

![image-20210714174313344](/assets/image-20210714174313344-1694452160187.m41qp4U7.png)

访问http://localhost:8082/user/prop，结果：

![image-20210714174424818](/assets/image-20210714174424818-1694452160187.Cxo-CR1L.png)

可以看出来，不管是dev，还是test环境，都读取到了envSharedValue这个属性的值。

### 4）配置共享的优先级

当nacos、服务本地同时出现相同属性时，优先级有高低之分：

![image-20210714174623557](/assets/image-20210714174623557-1694452160187.ChOYnlFH.png)

https://blog.csdn.net/u011066470/article/details/129271345

---

---
url: /Java/微服务专栏/03.注册中心Nacos/Nacos源码分析之ConfigService.md
---

# Nacos源码分析之ConfigService

https://blog.csdn.net/god\_86/article/details/106394522

---

---
url: /Java/微服务专栏/03.注册中心Nacos/1_Nacos注册中心.md
---

# Nacos注册中心

## 1.认识和安装Nacos

[Nacos](https://nacos.io/)是阿里巴巴的产品，现在是[SpringCloud](https://spring.io/projects/spring-cloud)中的一个组件。相比[Eureka](https://github.com/Netflix/eureka)功能更加丰富，在国内受欢迎程度较高。

![image-20210713230444308](/assets/image-20210713230444308.Bu_xI9a3.png)

## 2.服务注册到nacos

Nacos是SpringCloudAlibaba的组件，而SpringCloudAlibaba也遵循SpringCloud中定义的服务注册、服务发现规范。因此使用Nacos和使用Eureka对于微服务来说，并没有太大区别。

主要差异在于：

* 依赖不同
* 服务地址不同

### 1）引入依赖

在cloud-demo父工程的pom文件中的`<dependencyManagement>`中引入SpringCloudAlibaba的依赖：

```xml
<dependency>
    <groupId>com.alibaba.cloud</groupId>
    <artifactId>spring-cloud-alibaba-dependencies</artifactId>
    <version>2.2.6.RELEASE</version>
    <type>pom</type>
    <scope>import</scope>
</dependency>
```

然后在user-service和order-service中的pom文件中引入nacos-discovery依赖：

```xml
<dependency>
    <groupId>com.alibaba.cloud</groupId>
    <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>
</dependency>
```

> **注意**：不要忘了注释掉eureka的依赖。

### 2）配置nacos地址

在user-service和order-service的application.yml中添加nacos地址：

```yaml
spring:
  cloud:
    nacos:
      server-addr: localhost:8848
```

> **注意**：不要忘了注释掉eureka的地址

### 3）重启

重启微服务后，登录nacos管理页面，可以看到微服务信息：

![image-20210713231439607](/assets/image-20210713231439607.CqQVmFmy.png)

## 3.服务分级存储模型

一个**服务**可以有多个**实例**，例如user-service，可以有:

* 127.0.0.1:8081
* 127.0.0.1:8082
* 127.0.0.1:8083

假如这些实例分布于全国各地的不同机房，例如：

* 127.0.0.1:8081，在上海机房
* 127.0.0.1:8082，在上海机房
* 127.0.0.1:8083，在杭州机房

Nacos就将同一机房内的实例 划分为一个**集群**。

也就是说，user-service是服务，一个服务可以包含多个集群，如杭州、上海，每个集群下可以有多个实例，形成分级模型，如图：

![image-20210713232522531](/assets/image-20210713232522531.DIxWh-Og.png)

微服务互相访问时，应该尽可能访问同集群实例，因为本地访问速度更快。当本集群内不可用时，才访问其它集群。例如：

![image-20210713232658928](/assets/image-20210713232658928.BStR9_j0.png)

杭州机房内的order-service应该优先访问同机房的user-service。

### 5.3.1.给user-service配置集群

修改user-service的application.yml文件，添加集群配置：

```yaml
spring:
  cloud:
    nacos:
      server-addr: localhost:8848
      discovery:
        cluster-name: HZ # 集群名称
```

重启两个user-service实例后，我们可以在nacos控制台看到下面结果：

![image-20210713232916215](/assets/image-20210713232916215.DkdZ0QP8.png)

我们再次复制一个user-service启动配置，添加属性：

```sh
-Dserver.port=8083 -Dspring.cloud.nacos.discovery.cluster-name=SH
```

配置如图所示：

![image-20210713233528982](/assets/image-20210713233528982.94wOn6h1.png)

启动UserApplication3后再次查看nacos控制台：

![image-20210713233727923](/assets/image-20210713233727923.CCy4TsUT.png)

### 5.3.2.同集群优先的负载均衡

默认的`ZoneAvoidanceRule`并不能实现根据同集群优先来实现负载均衡。

因此Nacos中提供了一个`NacosRule`的实现，可以优先从同集群中挑选实例。

1）给order-service配置集群信息

修改order-service的application.yml文件，添加集群配置：

```sh
spring:
  cloud:
    nacos:
      server-addr: localhost:8848
      discovery:
        cluster-name: HZ # 集群名称
```

2）修改负载均衡规则

修改order-service的application.yml文件，修改负载均衡规则：

```yaml
userservice:
  ribbon:
    NFLoadBalancerRuleClassName: com.alibaba.cloud.nacos.ribbon.NacosRule # 负载均衡规则 
```

## 4.权重配置

实际部署中会出现这样的场景：

服务器设备性能有差异，部分实例所在机器性能较好，另一些较差，我们希望性能好的机器承担更多的用户请求。

但默认情况下NacosRule是同集群内随机挑选，不会考虑机器的性能问题。

因此，Nacos提供了权重配置来控制访问频率，权重越大则访问频率越高。

在nacos控制台，找到user-service的实例列表，点击编辑，即可修改权重：

![image-20210713235133225](/assets/image-20210713235133225.CC8C_s11.png)

在弹出的编辑窗口，修改权重：

![image-20210713235235219](/assets/image-20210713235235219.CSMPrmu5.png)

> **注意**：如果权重修改为0，则该实例永远不会被访问

## 5.环境隔离

Nacos提供了namespace来实现环境隔离功能。

* nacos中可以有多个namespace
* namespace下可以有group、service等
* 不同namespace之间相互隔离，例如不同namespace的服务互相不可见

![image-20210714000101516](/assets/image-20210714000101516.DOCB2N7n.png)

### 5.5.1.创建namespace

默认情况下，所有service、data、group都在同一个namespace，名为public：

![image-20210714000414781](/assets/image-20210714000414781.BDVNAfu9.png)

我们可以点击页面新增按钮，添加一个namespace：

![image-20210714000440143](/assets/image-20210714000440143.B3P6SOpd.png)

然后，填写表单：

![image-20210714000505928](/assets/image-20210714000505928.Dc2Dqexk.png)

就能在页面看到一个新的namespace：

![image-20210714000522913](/assets/image-20210714000522913.CZC4aYz-.png)

### 5.5.2.给微服务配置namespace

给微服务配置namespace只能通过修改配置来实现。

例如，修改order-service的application.yml文件：

```yaml
spring:
  cloud:
    nacos:
      server-addr: localhost:8848
      discovery:
        cluster-name: HZ
        namespace: 492a7d5d-237b-46a1-a99a-fa8e98e4b0f9 # 命名空间，填ID
```

重启order-service后，访问控制台，可以看到下面的结果：

![image-20210714000830703](/assets/image-20210714000830703.ZcaBBzvT.png)

![image-20210714000837140](/assets/image-20210714000837140.BpedEWbG.png)

此时访问order-service，因为namespace不同，会导致找不到userservice，控制台会报错：

![image-20210714000941256](/assets/image-20210714000941256.BL40SNw8.png)

## 6.Nacos与Eureka的区别

Nacos的服务实例分为两种l类型：

* 临时实例：如果实例宕机超过一定时间，会从服务列表剔除，默认的类型。

* 非临时实例：如果实例宕机，不会从服务列表剔除，也可以叫永久实例。

配置一个服务实例为永久实例：

```yaml
spring:
  cloud:
    nacos:
      discovery:
        ephemeral: false # 设置为非临时实例
```

Nacos和Eureka整体结构类似，服务注册、服务拉取、心跳等待，但是也存在一些差异：

![image-20210714001728017](/assets/image-20210714001728017.Cdc119E9.png)

* Nacos与eureka的共同点
  * 都支持服务注册和服务拉取
  * 都支持服务提供者心跳方式做健康检测

* Nacos与Eureka的区别
  * Nacos支持服务端主动检测提供者状态：临时实例采用心跳模式，非临时实例采用主动检测模式
  * 临时实例心跳不正常会被剔除，非临时实例则不会被剔除
  * Nacos支持服务列表变更的消息推送模式，服务列表更新更及时
  * Nacos集群默认采用AP方式，当集群中存在非临时实例时，采用CP模式；Eureka采用AP方式

---

---
url: /Java/JVM性能调优/01.JVM概念/2_Native、方法区和寄存器.md
---

# Native、方法区和寄存器

## 1. native

凡是使用了native关键字的，说明Java的作用范围已经达不到了，它会去调用底层的C语言的库。

1. 进入本地方法栈。
2. 调用本地方法接口。JNI

JNI的作用：扩展Java的使用，融合不同的语言为Java所用。（最初是为了融合C、C++语言）

因为Java诞生的时候，C和C++非常火，想要立足，就有必要调用C、C++的程序。

所以Java在JVM内存区域专门开辟了一块标记区域Native Method Area Stack，用来登记native方法。
在最终执行（执行引擎执行）的时候，通过JNI来加载本地方法库中的方法。

1. 编写一个多线程启动方法。

   ```java
    public class Test {     
        public static void main(String[] args) {         
            new Thread(()->{},"MyThread").start();     
        } 
    }
   ```

2. 点进去查看start方法。

   ```java
    // Thread类中的start方法，底层是把线程加入到线程组，然后去调用本地方法start0 public class Thread implements Runnable {         
    public synchronized void start() {
        if (threadStatus != 0)
            throw new IllegalThreadStateException();
        group.add(this);
        boolean started = false;
        try {
            start0();
            started = true;
        } finally {
            try {
                if (!started) {
                    group.threadStartFailed(this);
                }
            } catch (Throwable ignore) { /* do nothing. If start0 threw a Throwable then                   it will be passed up the call stack */ }
        }
    }
    private native void start0();
    }
   ```

## 2. 方法区

Method Area方法区（此区域属于共享区间，所有定义的方法的信息都保存在该区域）
方法区是被所有线程共享，所有字段、方法字节码、以及一些特殊方法（如构造函数，接口代码）也在此定义。

**静态变量、常量、类信息（构造方法、接口定义）、运行时的常量池存在方法区中，但是实例变量存在堆内存中，和方法区无关。**

## 3. PC寄存器

程序计数器：Program Counter Register
每个线程都有一个程序计数器，是线程私有的，就是一个指针，指向方法区中的方法字节码（用来存储指向像一条指令的地址，也即将要执行的指令代码），在执行引擎读取下一条指令，是一个非常小的内存空间，几乎可以忽略不计。

#

---

---
url: /数据库/05.Neo4j/1_Neo4j安装.md
---

# Neo4j安装

## Neo4j安装部署

Neo4j是基于Java的图形数据库，运行Neo4j需要启动JVM进程，因此在安装Neo4j前必须安装JAVA SE的JDK。

注意查看Neo4j详情页中与JDK版本对应。5点几版本需要JDK21。

### JDK安装

从Oracle官方网站下载Java SE JDK，地址为：https://www.oracle.com/cn/java/technologies/javase-downloads.html

### Neo4j安装

#### 1、下载

> 官网下载地址：https://neo4j.com/download-center/#community

进入官网，选择社区版下载

![image-20231125203026237](/assets/image-20231125203026237.CkkoPi8l.png)

#### 2、解压安装包

Neo4j应用程序有如下主要的目录结构：

| **文件夹名称** | **相关说明**                |
| -------------- | --------------------------- |
| bin            | 存放Neo4j的可执行程序       |
| conf           | 存放Neo4j启动的相关配置文件 |
| data           | 存放Neo4j数据库的核心文件   |
| lib            | 存放Neo4j所依赖的jar包      |
| logs           | 存放Neo4j的日志文件         |
| plugins        | 存放Neo4j的插件             |

#### 3、配置环境变量

将下载的压缩文件解压到系统合适的位置后

需要**创建**用户主目录环境变量`NEO4J_HOME`，变量值设置为解压后的主目录路径。

**编辑**系统变量区的**Path**，点击新建，然后输入 **%NEO4J\_HOME%\bin**

或者

直接在系统变量Path中新建解压后的bin目录路径。

![image-20231125213410925](/assets/image-20231125213410925.CEJd_NAQ.png)

#### 4、其他配置

Neo4j的配置文件存储在conf目录下，Neo4j通过配置文件neo4j.conf控制服务器的工作。默认情况下，不需要进行任意配置，就可以启动服务器。

Neo4j的核心数据文件默认存储在data/graph.db目录中，要改变默认的存储目录，可以在配置选项更新。下面的代码展示了限定文件存储在data/graph.db目录中的指令。
dbms.active\_database=graph.db
•数据库的安全验证默认是启用的，可以从配置选项中停用该内容。以下代码表示安全验证不启用。

dbms.security.auth\_enabled=false
•在配置选项中还可以配置Java堆内存的大小。下列代码分别是配置Java堆内存的最大值和最小值。

dbms.memory.heap.initial\_size=512m
dbms.memory.heap.max\_size=512m

### 启动neo4j

以系统用户身份，通过命令行**neo4j.bat console**运行Neo4j。

![image-20231125215717202](/assets/image-20231125215717202.Bo7BVovi.png)

显示started即为完成。

Neo4j服务器具有一个集成的浏览器，在一个运行的服务器实例上访问`http://localhost:7474/`，打开浏览器，显示启动页面。

![image-20231125215857448](/assets/image-20231125215857448.hOshAoNd.png)默认的用户名和密码均为**neo4j**

登录后需重置密码（至少八位）

Neo4j-web UI界面

![image-20231125215950959](/assets/image-20231125215950959.DypFTisW.png)

https://blog.csdn.net/sinat\_20471177/article/details/131943497

---

---
url: /数据库/05.Neo4j/2_Neo4j数据库创建.md
---

# Neo4j数据库创建

构建Neo4j数据库

几种构建方式

参考资料

https://wenku.baidu.com/view/42b16dcd561810a6f524ccbff121dd36a32dc4b7.html

https://www.cnblogs.com/Mask-D/p/9267446.html

nlper的自我修养-一文搞定neo4j图数据库

---

---
url: /数据库/05.Neo4j/0_Neo4j概念.md
---

# Neo4j图数据库

## Neo4j的关键概念和特点

* Neo4j是一个开源的NoSQL图形存储数据库，可为应用程序提供支持ACID的后端。Neo4j的开发始于2003年，自2007年转变为开源图形数据库模型。程序员使用的是路由器和关系的灵活网络结构，而不是静态表，但是可以享受企业级质量数据库的所有好处。与关系数据库索引，对于许多应用程序，Neo4j可以提供数量级的性能优势。

* 与传统的数据库按行，列和表排列数据不同，Neo4j具有灵活的结构，该结构由数据记录之间的存储关系定义。

* 使用Neo4j，每个数据记录或节点都存储指向与其连接的所有节点的直接指针。由于Neo4j是围绕此简单而强大的优化设计的，因此与其他数据库相比，它以更快的速度和更大的深度执行复杂连接的查询。

* 之所以说Neo4j是基于图形存储的数据库，是因为它可以有效地将属性图模型实施到存储级别。这就意味着数据存储方式与用户在图形构想上的存储方式是完全一致的，并且数据库使用指针来导航和遍历图形。

* Neo4j具有一些针对图形存储数据库所特有的功能，其中Cypher作为一种类似于SQL的声明性查询语言对图进行了优化。这种查询语言现在也在通过openCypher项目被其他数据库（如SAP HANA Graph和Redis graph）使用。

* 由于Neo4j有效地表示了节点和关系，因此在深度和广度方面在大型的图中可以进行恒定时间遍历。在适度的硬件上可以扩展到数十亿个节点，更好的支持了大数据时代较大数据量的存储分析。

* Neo4j具有可以随时间适应的灵活的属性图架构，可以在后续实现中添加新的关系以实现捷径并在业务需求变化快时加速域数据的速度。

* Neo4j通过分片和联合查询扩展应用程序，以适应用户不断增长的业务需求。同时，该数据库具有细粒度的安全性，LDAP /目录服务，安全性日志记录等等。可以有效地确保数据安全。Neo4j的通用属性图模型使项目可以轻松地随着业务需求的变化而流畅地发展。其本机图形数据库为大型、互连的数据集上的多跳查询提供一致的实时性能。基于筏的因果群集，滚动升级，热备份等带来了高可用性。Neo4j包含功能强大的工具，可帮助开发人员有效地编写，分析和调试查询以及可视化和导航数据。

## 参考资料

\[1]. 视频资料：https://www.bilibili.com/video/BV1sG411s7zV

\[2]. https://blog.csdn.net/qq\_41977838/article/details/123563005

---

---
url: /Linux/Nginx/2_Nginx常用配置.md
---

# Nginx常用配置

## Nginx常用配置

安装Nginx后，`nginx.conf`包含了一些默认配置，下面的示例就是将注释删除后，稍微改造后的配置。

```nginx
worker_processes  1; # 设置工作线程的核心数，工作线程就是用于处理客户请求的
error_log  /opt/homebrew/etc/nginx/log/nginx_error.log  crit; # 错误日志输出路径
events {
    # 每个工作进程允许的最大连接数，举例说明作用：
    # worker_processes设置成1，就是最多能处理1024个并发请求
    # worker_processes设置成2，就是最多能处理2048个并发请求
    worker_connections  1024;
}
http {
    include       mime.types;                # 引入mime.types配置文件，包含了Nginx的一些默认配置
    default_type  application/octet-stream;  # 请求进来后以流形式处理
    sendfile        on;                      # 零拷贝
    keepalive_timeout  65;                   # 请求最大存活时间，65s

    server {
        listen       80;              # 监听服务的端口，http默认80，https默认443
        server_name  localhost;       # 服务名称，一般配置域名，
        client_max_body_size 1024M;

        # 资源的根路径
        root /usr/local/software/code/vueCode/dist;

        location / {
            root    html;
            index  index.html index.htm;
        }
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }
}
```

## 经典https配置

```nginx
worker_processes  1;
error_log  /opt/homebrew/etc/nginx/log/nginx_error.log  crit;
events {
    worker_connections  1024;
}
http {
    include       mime.types;              
    default_type  application/octet-stream;
    sendfile        on;                    
    keepalive_timeout  65;                 

    # 监听80端口，并重定向到443端口；实现http自懂跳转https
	server {
        listen 80;
        server_name docs.oinone.luckilyxxl.com;
        return 301 https://$host$request_uri;
    }

	# 监听443请求，处理https请求，配置SSL政策
    server {
        listen 443 ssl;
        server_name docs.oinone.luckilyxxl.com;
        ssl_certificate ssl/docs.oinone.luckilyxxl.com.pem;      # 当前目录的ssl目录下的证书文件路径
        ssl_certificate_key ssl/docs.oinone.luckilyxxl.com.key;  # 当前目录的ssl目录下的证书key路径
        ssl_session_timeout 5m;
        ssl_protocols TLSv1 TLSv1.1 TLSv1.2;
        ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:HIGH:!aNULL:!MD5:!RC4:!DHE;
        ssl_prefer_server_ciphers on;
        
        # 比如vue下的打包路径就可以通过这种访问方式
        location / {
            root /Users/wuwei/code/book/luckilyxxl/.vitepress/dist;
            index  index.html index.htm;
            try_files $uri $uri/ /index.html;
        }

        # 拦截/api/请求，转发到127.0.0.1:7002，一般配置后端服务
        location /api/ {
            proxy_send_timeout 300;
            proxy_read_timeout 300;
            proxy_connect_timeout 300;
            proxy_pass http://127.0.0.1:7002/;
        }
    }
}
```

## 引入子配置文件

使用`include`引入子配置文件，一般都是读取`conf.d`下面所有.conf结尾的文件，文件的内容就是反向代理的配置信息。

```nginx
http {
    # 引入文件类型映射文件
    include       mime.types;
    # 如果没有找到指定的文件类型映射 使用默认配置
    default_type  application/octet-stream;
    # 日志打印格式
    log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
                      '$status $body_bytes_sent "$http_referer" '
                      '"$http_user_agent" "$http_x_forwarded_for"';
    # 启动零拷贝提高性能
    sendfile        on;
    # 设置keepalive长连接超时时间
    keepalive_timeout  65;
    # 引入子配置文件
    include /usr/local/openresty/nginx/conf/conf.d/*.conf;
}
```

例如这部分数据就可以放到`conf.d`目录下的`api.conf`文件中；

```nginx
server {
    listen 443 ssl;
    server_name docs.oinone.luckilyxxl.com;
    ssl_certificate ssl/docs.oinone.luckilyxxl.com.pem;
    ssl_certificate_key ssl/docs.oinone.luckilyxxl.com.key;
    ssl_session_timeout 5m;
    ssl_protocols TLSv1 TLSv1.1 TLSv1.2;
    ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:HIGH:!aNULL:!MD5:!RC4:!DHE;
    ssl_prefer_server_ciphers on;
    
    location / {
        root /Users/wuwei/code/book/luckilyxxl/.vitepress/dist;
        index  index.html index.htm;
        try_files $uri $uri/ /index.html;
    }

    location /api/ {
        proxy_send_timeout 300;
        proxy_read_timeout 300;
        proxy_connect_timeout 300;
        proxy_pass http://127.0.0.1:7002/;
    }
}
```

---

---
url: /Linux/Nginx/4_Nginx负载均衡.md
---

# Nginx负载均衡

## Nginx负载均衡

负载均衡用于从“upstream”模块定义的后端服务器列表中选取一台服务器接受用户的请求，一个最基本的upstream模块是这样的，模块内的server是服务器列表：

```nginx
upstream dynamicserver {
  server 172.16.44.47:9001; #tomcat 1
  server 172.16.44.47:9002; #tomcat 2
  server 172.16.44.47:9003; #tomcat 3
  server 172.16.44.47:9004; #tomcat 4
}
```

在upstream模块配置完成后，要让指定的访问反向代理到服务器列表：

```nginx
#其他页面反向代理到tomcat容器
location ~.*$ {
  index index.jsp index.html;
  proxy_pass http://dynamicserver;
}
```

这就是最基本的负载均衡实例，但这不足以满足实际需求；目前Nginx服务器的upstream模块支持6种方式的分配；

```nginx
upstream dynamicserver {
  server 192.168.64.1:9001; #tomcat 1
  server 192.168.64.1:9002; #tomcat 2
  server 192.168.64.1:9003; #tomcat 3
  server 192.168.64.1:9004; #tomcat 4
}
server {
        server_name www.luckilyxxl.com;
        default_type text/html;
        charset   utf-8;

        location ~ .*$ {
            index index.jsp index.html;
            proxy_pass http://dynamicserver;
       }
}
```

## 常用参数

| 参数         | 描述                                                         |
| :----------- | :----------------------------------------------------------- |
| server       | 反向服务地址 加端口                                          |
| weight       | 权重                                                         |
| fail\_timeout | 与max\_fails结合使用。                                        |
| max\_fails    | 设置在fail\_timeout参数设置的时间内最大失败次数，如果在这个时间内，所有针对该服务器的请求都失败了，那么认为该服务器会被认为是停机了 |
| max\_conns    | 允许最大连接数                                               |
| fail\_time    | 服务器会被认为停机的时间长度,默认为10s                       |
| backup       | 标记该服务器为备用服务器，当主服务器停止时，请求会被发送到它这里。 |
| down         | 标记服务器永久停机了                                         |
| slow\_start   | 当节点恢复，不立即加入                                       |

## 负载均衡策略

> 在这里，只详细说明Nginx自带的负载均衡策略，第三方不多描述。

| 负载策略           | 描述            |
| :----------------- | :-------------- |
| 轮询               | 默认方式        |
| weight             | 权重方式        |
| ip\_hash            | 依据ip分配方式  |
| least\_conn         | 最少连接方式    |
| fair（第三方）     | 响应时间方式    |
| url\_hash（第三方） | 依据URL分配方式 |

## 实战示例

1、轮训：最基本的方法，上文示例的就是轮训

```nginx
upstream 随便起名 {
  server 192.168.64.1:9001; #tomcat 1
  server 192.168.64.1:9002; #tomcat 2
  server 192.168.64.1:9003; #tomcat 3
  server 192.168.64.1:9004; #tomcat 4
}
```

2、权重：在轮询策略的基础上指定轮询的几率

```nginx
upstream 随便起名 {
    server 192.168.64.1:9001  weight=2;                   #tomcat 1
    server 192.168.64.1:9002;                             #tomcat 2
    server 192.168.64.1:9003;                         	  #tomcat 3
    server 192.168.64.1:9004; 							  #tomcat 4
}
```

3、ip\_hash：基于客户端IP的分配方式，能确保了相同客户端的请求发送到相同的服务器

```nginx
upstream 随便起名 {
    ip_hash;  #保证每个访客固定访问一个后端服务器
    server 192.168.64.1:9001  weight=2;                   #tomcat 1
    server 192.168.64.1:9002;                             #tomcat 2
    server 192.168.64.1:9003;                		      #tomcat 3
    server 192.168.64.1:9004; 							  #tomcat 4
}
```

4、least\_conn：把请求转发给连接数较少的后端服务器，可以达到更好的负载均衡效果

```nginx
upstream 随便起名 {
    ip_hash;  #保证每个访客固定访问一个后端服务器
    server 192.168.64.1:9001  weight=2;                   #tomcat 1
    server 192.168.64.1:9002;                             #tomcat 2
    server 192.168.64.1:9003;                		      #tomcat 3
    server 192.168.64.1:9004; 							  #tomcat 4
}
```

## 重试策略

`max_fails=3 fail_timeout=60s`代表在`60`秒内请求某一应用失败`3`次，认为该应用宕机，后等待`60`秒，这期间内不会再把新请求发送到宕机应用，而是直接发到正常的那一台，时间到后再有请求进来继续尝试连接宕机应用且仅尝试`1`次，如果还是失败，则继续等待`60`秒...以此循环，直到恢复。

nginx

```
upstream dynamicserver {
      server 192.168.64.1:9001 fail_timeout=60s max_fails=3; #Server A
      server 192.168.64.1:9002 fail_timeout=60s max_fails=3; #Server B
}
```

### 重试配置

有时候我们系统出现`500`等异常的情况下，希望nginx能够到其他的服务器进行重试，我们可以配置哪些错误码才进行重试，`proxy_next_upstream`项定义了什么情况下进行重试

```nginx
Syntax: proxy_next_upstream error | timeout | invalid_header | http_500 | http_502 | http_503 | http_504 | http_403 | http_404 | off ...;
Default:    proxy_next_upstream error timeout;
Context:    http, server, location
```

> 默认情况下，当请求服务器发生错误或超时时，会尝试到下一台服务器，还有一些其他的配置项如下：

| 错误状态       | 描述                                                         |
| :------------- | :----------------------------------------------------------- |
| error          | 与服务器建立连接，向其传递请求或读取响应头时发生错误;        |
| timeout        | 在与服务器建立连接，向其传递请求或读取响应头时发生超时;      |
| invalid\_header | 服务器返回空的或无效的响应;                                  |
| http\_500       | 服务器返回代码为500的响应;                                   |
| http\_502       | 服务器返回代码为502的响应;                                   |
| http\_503       | 服务器返回代码为503的响应;                                   |
| http\_504       | 服务器返回代码504的响应;                                     |
| http\_403       | 服务器返回代码为403的响应;                                   |
| http\_404       | 服务器返回代码为404的响应;                                   |
| http\_429       | 服务器返回代码为429的响应（1.11.13）;                        |
| non\_idempotent | 通常，请求与 非幂等 方法（POST，LOCK，PATCH）不传递到请求是否已被发送到上游服务器（1.9.13）的下一个服务器; 启用此选项显式允许重试此类请求; |
| off            | 禁用将请求传递给下一个服务器。                               |

示例：配置了`500`等错误的时候会进行重试

```nginx
upstream dynamicserver {
  server 192.168.64.1:9001 fail_timeout=60s max_fails=3; #tomcat 1
  server 192.168.64.1:9002 fail_timeout=60s max_fails=3; #tomcat 2
}


server {
        server_name www.luckilyxxl.com;
        default_type text/html;
        charset   utf-8;


        location ~ .*$ {
            index index.jsp index.html;
            proxy_pass http://dynamicserver;
            #下一节点重试的错误状态
            proxy_next_upstream error timeout http_500 http_502 http_503 http_504;
       }
}
```

## 备用节点

备用节点在主服务器正常工作时处于待命状态，不处理请求。只有当主服务器故障或不可用时，备用节点才会接管请求。

在 Nginx 的负载均衡配置中，可以通过 `backup` 参数将某个服务器标记为备用节点。

nginx

```nginx
http {
    upstream backend {
        server 192.168.1.101;  # 主服务器
        server 192.168.1.102;  # 主服务器
        server 192.168.1.103 backup;  # 备用节点
    }

    server {
        listen 80;

        location / {
            proxy_pass http://backend;
        }
    }
}
```

---

---
url: /Linux/Nginx/0_Nginx快速入门.md
---

# Nginx快速入门

## 公司产品出现瓶颈？

我们公司项目刚刚上线的时候，并发量小，用户使用的少，所以在低并发的情况下，一个jar包启动应用就够了，然后内部tomcat返回内容给用户。
![img](/assets/kuangstudy98b25257-71be-48f3-8afe-e1226ebc4589.6TFcFNMf.png)
但是慢慢的，使用我们平台的用户越来越多了，并发量慢慢增大了，这时候一台服务器满足不了我们的需求了。
![img](/assets/kuangstudy1d77bc35-df30-46fb-8c41-61ff4468d0c9.B052hXSu.png)
于是我们横向扩展，又增加了服务器。这个时候几个项目启动在不同的服务器上，用户要访问，就需要增加一个代理服务器了，通过代理服务器来帮我们转发和处理请求。
![img](/assets/kuangstudy33ee7313-9356-46e6-a0b2-fc1ed8ef9a62.Cx8VEMr4.png)
我们希望这个代理服务器可以帮助我们接收用户的请求，然后将用户的请求按照规则帮我们转发到不同的服务器节点之上。这个过程用户是无感知的，用户并不知道是哪个服务器返回的结果，我们还希望他可以按照服务器的性能提供不同的权重选择。保证最佳体验！所以我们使用了Nginx。

## 什么是Nginx？

Nginx (engine x) 是一个高性能的HTTP和反向代理web服务器，同时也提供了IMAP/POP3/SMTP服务。Nginx是由伊戈尔·赛索耶夫为俄罗斯访问量第二的Rambler.ru站点（俄文：Рамблер）开发的，第一个公开版本0.1.0发布于2004年10月4日。2011年6月1日，nginx 1.0.4发布。

其特点是占有内存少，并发能力强，事实上nginx的并发能力在同类型的网页服务器中表现较好，中国大陆使用nginx网站用户有：百度、京东、新浪、网易、腾讯、淘宝等。在全球活跃的网站中有12.18%的使用比率，大约为2220万个网站。

Nginx 是一个安装非常的简单、配置文件非常简洁（还能够支持perl语法）、Bug非常少的服务。Nginx 启动特别容易，并且几乎可以做到7\*24不间断运行，即使运行数个月也不需要重新启动。你还能够不间断服务的情况下进行软件版本的升级。

Nginx代码完全用C语言从头写成。官方数据测试表明能够支持高达 50,000 个并发连接数的响应。

## Nginx作用？

> Http代理，反向代理：作为web服务器最常用的功能之一，尤其是反向代理。

正向代理

eg：代理客户端。比如 VPN 就是在客户端进行代理
![img](/assets/kuangstudy46bdad36-d3e0-43b0-a223-43360b7e8fc7.D1MswPaq.png)
反向代理

eg：代理服务器，让客户端无感知的访问资源。比如百度，我们只访问网址就可以。
![img](/assets/kuangstudy62a15097-6e2a-4dbe-bcf5-f0d7cab81089.CF__3s0N.png)

> Nginx提供的负载均衡策略有2种：内置策略和扩展策略。内置策略为轮询，加权轮询，Ip hash。扩展策略，就天马行空，只有你想不到的没有他做不到的。

轮询
![img](/assets/kuangstudy4d33dfac-1949-4b2d-abb8-fe0b6e65b8dc.Zv8NNQvh.png)
加权轮询
![img](/assets/kuangstudyb1e3e440-4159-4259-a174-528b56cb04b2.CMKq7wFT.png)
iphash对客户端请求的ip进行hash操作，然后根据hash结果将同一个客户端ip的请求分发给同一台服务器进行处理，可以解决session不共享的问题。

eg：推荐redis 可以做 session 共享，不建议 nginx 做 session 共享。iphash 弊端：当保存 session 的那一个服务器挂掉，session 就丢失了。
![img](/assets/kuangstudy64acb9a3-cd1a-4c0e-a1fa-9b220046a95a.B_bm_Ycd.png)

> 动静分离，在我们的软件开发中，有些请求是需要后台处理的，有些请求是不需要经过后台处理的（如：css、html、jpg、js等等文件），这些不需要经过后台处理的文件称为静态文件。让动态网站里的动态网页根据一定规则把不变的资源和经常变的资源区分开来，动静资源做好了拆分以后，我们就可以根据静态资源的特点将其做缓存操作。提高资源响应的速度。

![img](/assets/kuangstudyedb1bbd6-e530-4aba-8fde-68658a10e73f.CwRCYCHr.png)

目前，通过使用Nginx大大提高了我们网站的响应速度，优化了用户体验，让网站的健壮性更上一层楼！

---

---
url: /Linux/Nginx/3_Nginx路由匹配.md
---

# Nginx路由匹配

## Nginx路由匹配

我们在conf.d下创建了一个配置文件，配置的内容如下：

```nginx
server {
        server_name www.luckilyxxl.com;
        default_type text/html;
        charset   utf-8;
        location = / {
          echo "规则A";
        }
        location = /login {
            echo "规则B";
        }
        location ^~ /static/ {
            echo "规则C";
        }
        location ^~ /static/files {
            echo "规则X";
        }
        location ~ \.(gif|jpg|png|js|css)$ {
            echo "规则D";
        }
        location ~* \.js$ {
            echo "规则E";
        }
        location /img {
            echo "规则Y";
        }
        location / {
           echo "规则F";
        }
}
```

location语法`location [修饰符] pattern {…}`，修饰符列表如下：

| 修饰符 | 功能                                                         |
| :----- | :----------------------------------------------------------- |
| 空     | 前缀匹配 能够匹配以需要匹配的路径为前缀的uri                 |
| =      | 精确匹配                                                     |
| ~      | 正则表达式模式匹配，区分大小写                               |
| ~\*     | 正则表达式模式匹配，不区分大小写                             |
| ^~     | 精确前缀匹配，类似于无修饰符的行为，也是以指定模块开始，不同的是，如果模式匹配，那么就停止搜索其他模式了，不支持正则表达式 |
| /      | 通用匹配，任何请求都会匹配到。                               |

## 实际使用建议

```nginx
#直接匹配网站根，通过域名访问网站首页比较频繁，使用这个会加速处理，官网如是说。
#这里是直接转发给后端应用服务器了，也可以是一个静态首页
# 第一个必选规则
location = / {
    proxy_pass http://tomcat:8080/index
}
# 第二个必选规则是处理静态文件请求，这是nginx作为http服务器的强项
# 有两种配置模式，目录匹配或后缀匹配,任选其一或搭配使用
location ^~ /static/ {
    alias /webroot/static/;
}
location ~* \.(gif|jpg|jpeg|png|css|js|ico)$ {
    root /webroot/res/;
}
# 第三个规则就是通用规则，用来转发动态请求到后端应用服务器
# 非静态文件请求就默认是动态请求，自己根据实际把握，比如我们后端接口规定统一加上/api前缀，
location /api {
    proxy_pass http://tomcat:8080/
}
```

---

---
url: /Java/JVM性能调优/02.JVM性能监控与调优/OOM分类及排查.md
---

# OOM分类及排查

## OOM分类

* java.lang.OutOfMemoryError: Java heap space

  Java 堆溢出原因： 无法在 Java 堆中分配对象 应用程序保存了无法被GC回收的对象。 应用程序过度使用 finalizer。

* java.lang.OutOfMemoryError: unable to create new native thread

  排查代码，确定是否显示使用死循环创建线程，或者隐式调用第三方接口创建线程

* java.lang.OutOfMemoryError: Metaspace

  方法区溢出：检查JVM元空间设置参数是否过小，检查对应代码,是否使用CGLib生成了大量的代理类

* java.lang.OutOfMemoryError: Direct buffer memory

  本机直接内存溢出：检查JVM参数-Xmx，-XX:MaxDirectMemorySize 是否合理。

* java.lang.OutOfMemoryError: GC overhead limit exceeded

  Sun 官方对此的定义：超过98%的时间用来做GC并且回收了不到2%的堆内存时会抛出此异常。

  一般都是堆太小导致的：检查JVM参数-Xmx -Xms是否合理 dump内存，检查是否存在内存泄露，如果没有，加大内存。

## CPU过高排查流程

1. 利用 top 命令可以查出占用 CPU 最高的进程pid，如果pid为 9876
2. 然后查看该进程下占用最高的线程id【top -Hp 9876】
3. 假设占用率最高的线程 ID 为6900，将其转换为 16 进制形式（因为 java native 线程以 16 进制形式输出）【printf '%x\n' 6900】
4. 利用jstack打印出 java 线程调用栈信息 【jstack 9876 | grep '0x1af4' -A 50 --color】，这样就可以更好的定位问题

## 内存占用过高排查流程

1. 查找进程id：【top -d 2 -c】
2. 查看 JVM 堆内存分配情况：jmap -head pid
3. 查看占用内存比较多的对象：jamp -histo pid | head -n 100
4. 查看占用内存比较多的存活对象：jmap -histo:live pid | head -n 100

## 示例

下面是对常见的 `java.lang.OutOfMemoryError: Java heap space` 排查：

### 使用 ps 命令查看进程

ps -aux|grep java命令查看一下你的java进程，就可以找到你的java进程的进程id。

### 使用 top 命令

top命令显示的结果列表中，会看到%MEM这一列，这里可以看到你的进程可能对内存的使用率特别高。以查看正在运行的进程和系统负载信息，包括cpu负载、内存使用、各个进程所占系统资源等。

![img](/assets/785859-20200313211136187-1482421735.BmeSdO9N.png)

### 使用 jmap 命令查看

* ./jmap -heap PID 打印堆总结
* ./jmap -dump:file=/data/logs/jmap/auto.dump PID，dump内存分析日志

```shell
[root@test bin]# ./jmap -dump:file=/data/logs/jmap/auto.dump 13
13: Unable to open socket file: target process not responding or HotSpot VM not loaded
The -F option can be used when the target process is not responding
[root@test bin]# ./jmap -F -dump:file=/data/logs/jmap/auto.dump 13
Attaching to process ID 13, please wait...
Error attaching to process: sun.jvm.hotspot.debugger.DebuggerException: cannot open binary file
sun.jvm.hotspot.debugger.DebuggerException: sun.jvm.hotspot.debugger.DebuggerException: cannot open binary file
        at sun.jvm.hotspot.debugger.linux.LinuxDebuggerLocal$LinuxDebuggerLocalWorkerThread.execute(LinuxDebuggerLocal.java:163)
        at sun.jvm.hotspot.debugger.linux.LinuxDebuggerLocal.attach(LinuxDebuggerLocal.java:278)
        at sun.jvm.hotspot.HotSpotAgent.attachDebugger(HotSpotAgent.java:671)
        at sun.jvm.hotspot.HotSpotAgent.setupDebuggerLinux(HotSpotAgent.java:611)
        at sun.jvm.hotspot.HotSpotAgent.setupDebugger(HotSpotAgent.java:337)
        at sun.jvm.hotspot.HotSpotAgent.go(HotSpotAgent.java:304)
        at sun.jvm.hotspot.HotSpotAgent.attach(HotSpotAgent.java:140)
        at sun.jvm.hotspot.tools.Tool.start(Tool.java:185)
        at sun.jvm.hotspot.tools.Tool.execute(Tool.java:118)
        at sun.jvm.hotspot.tools.HeapDumper.main(HeapDumper.java:83)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at sun.tools.jmap.JMap.runTool(JMap.java:201)
        at sun.tools.jmap.JMap.main(JMap.java:130)
Caused by: sun.jvm.hotspot.debugger.DebuggerException: cannot open binary file
        at sun.jvm.hotspot.debugger.linux.LinuxDebuggerLocal.attach0(Native Method)
        at sun.jvm.hotspot.debugger.linux.LinuxDebuggerLocal.access$100(LinuxDebuggerLocal.java:62)
        at sun.jvm.hotspot.debugger.linux.LinuxDebuggerLocal$1AttachTask.doit(LinuxDebuggerLocal.java:269)
        at sun.jvm.hotspot.debugger.linux.LinuxDebuggerLocal$LinuxDebuggerLocalWorkerThread
```

参考

常见OOM异常分析：https://blog.csdn.net/u012260238/article/details/110308147

Java性能分析神器-JProfiler详解：https://blog.csdn.net/u013613428/article/details/53926825

如果在线上遇到了OOM，该如何解决？https://blog.csdn.net/Park33/article/details/126379445

内存溢出的原因及解决方式：https://blog.51cto.com/u\_16213603/7622457

Java多线程异常及解决方案，如何合理控制线程数：https://refblogs.com/article/577

---

---
url: /Python/AI大模型应用开发/2_OpenAI大模型API基础.md
---

# OpenAI大模型API基础

## 一、如何用代码与AI对话？模型API

要真正动手构建属于自己的 AI 应用，就不能停留在通过网页与 AI 交互的层面。虽然网页端可以获取 AI 的回答，但这种方式存在明显局限：

1. **数据提取困难**：从网页上“扒”下 AI 的回答用于后续处理非常麻烦，不利于自动化。
2. 控制能力有限：无法灵活调整 AI 回复的关键参数，例如：
   * 回复长度（如限制为 512 个 token）
   * 创造性（temperature）
   * 频率惩罚（frequency penalty）
   * 提示模板的使用效率低

相比之下，**通过代码调用 AI 模型**具有显著优势：

* **精细控制**：可以精确设置请求参数，获得更可靠、可预测的输出。
* **批量处理**：能对大量数据进行自动化处理，例如一次性总结多个文档。
* **系统集成**：可将 AI 能力嵌入到自己的系统或工作流中，实现定制化功能，如自动回复邮件、生成报告等。

### 1、如何用代码与 AI 大模型对话？

最常见的方式是通过 **API（Application Programming Interface，应用程序编程接口）** 调用。

* **API 的作用**：定义了客户端与服务端之间的通信规则，相当于“如何与服务对话的说明书”。
* **底层协议**：大多数 AI API 基于 **HTTP（Hypertext Transfer Protocol，超文本传输协议）**，采用客户端发送请求、服务器返回响应的模式。

工作流程如下：

1. **客户端**（你的代码）将提示（prompt）封装在请求中，发送给 AI 服务提供商（如 OpenAI、百度等）的服务器。
2. **服务器**运行大模型生成回复，并通过 HTTP 响应返回结果。
3. **客户端**接收响应，提取 AI 的回答，用于后续处理。

> 注：HTTP 请求和响应包含许多技术细节（如 headers、status code 等），初学者不必完全理解。

### 2、实际开发中的便利性

为了降低使用门槛，主流 AI 服务商（如 OpenAI、百度）都提供了 **官方封装的 Python 库**：

* 你无需手动构建复杂的 HTTP 请求。
* 只需调用库中的函数或方法，传入相应参数即可完成请求。
* 例如：指定使用哪个模型、输入消息内容、设置 temperature 等。

这些库的使用方法都有详细文档说明，包括：

* 调用哪个方法
* 参数如何设置
* 输入数据结构（如消息列表）
* 响应数据的格式和解析方式

虽然不同平台（如 GPT、文心一言）的 Python 库和代码略有差异，但核心逻辑一致：**发送提示 → 获取回复**，掌握一个即可触类旁通。

### 3、调用前的关键步骤：获取 API 密钥

在调用任何主流 AI 大模型 API 之前，必须先获取 **API Key（API 密钥）**。

* 作用：
  * 身份验证：服务方识别请求来源
  * 使用追踪：记录调用次数和频率
  * 计费依据：按使用量收费
* 因此，API Key 是调用 AI 服务的前提。

## 二、创建OpenAI API密钥，然后藏起来

在通过代码调用 AI 大模型之前，必须获取 API 密钥。没有 API 密钥，无法使用 OpenAI 等平台的模型和服务。本课程以 OpenAI 的 GPT 模型为例进行说明。

***

### 1、创建 OpenAI API 密钥

1. **访问官网**
   前往 OpenAI 官网的 API 密钥管理页面（https://platform.openai.com/api-keys）。
2. **注册或登录**
   如果还没有账号，需先注册并完成验证。
3. **创建密钥**
   * 点击 **"Create new secret key"** 按钮。
   * 可为密钥命名（如用于不同项目），便于识别；不命名则系统自动生成默认名称。
4. **复制并保存密钥**
   * 创建后，系统会显示完整的 API 密钥。
   * **注意：该密钥仅显示一次！关闭后无法再次查看完整内容。**
   * 务必点击复制按钮，将密钥保存到安全位置（如密码管理器或本地文件）。
5. **密钥泄露处理**
   * API 密钥与账户绑定，一旦泄露，他人使用将产生费用由你承担。
   * 若怀疑密钥已泄露，请立即在后台将其删除（Revoke），使其失效，然后重新创建新密钥。

***

### 2、安全使用 API 密钥：避免硬编码

直接将密钥写入代码（硬编码）存在严重安全风险：

* 任何人看到代码即可获取密钥。
* 若代码上传至 GitHub 等公共平台，密钥将公开暴露。

**推荐做法：使用环境变量存储密钥**

这样代码中不包含密钥明文，即使共享代码也不会泄露。

***

### 3、设置环境变量（按操作系统）

#### **Windows 系统**

1. 右键“此电脑”或“我的电脑” → 选择“属性”。
2. 点击“高级系统设置” → 在“高级”选项卡下点击“环境变量”。
3. 在“用户变量”或“系统变量”中点击“新建”。
4. 输入：
   * **变量名**：`OPENAI_API_KEY`（注意拼写和大小写）
   * **变量值**：粘贴你的 API 密钥
5. 依次点击“确定”保存。

#### **macOS 系统**

1. 打开终端，确认 Shell 类型：

   bash深色版本

   ```
   echo $0
   ```

   常见为 `bash` 或 `zsh`。

2. 编辑对应配置文件：

   * Bash：`~/.bash_profile` 或 `~/.bashrc`
   * Zsh：`~/.zshrc`

3. 使用 `nano` 编辑器打开文件（以 Zsh 为例）：

   bash深色版本

   ```
   nano ~/.zshrc
   ```

4. 在文件末尾添加一行：

   bash深色版本

   ```
   export OPENAI_API_KEY="your-api-key-here"
   ```

   将 `your-api-key-here` 替换为实际密钥，注意不要遗漏引号。

5. 保存并退出：

   * 按 `Ctrl + X`
   * 按 `Y` 确认保存
   * 按 `Enter` 确认文件名

6. 使更改立即生效：

   bash深色版本

   ```
   source ~/.zshrc
   ```

***

### 4、使用

设置完成后，代码中可通过读取环境变量的方式获取密钥，例如在 Python 中使用：

python深色版本

```
import os
api_key = os.getenv("OPENAI_API_KEY")
```

这样既保证了安全性，也便于在不同环境中部署。

## 三、发送你对AI大模型的第一个请求

安装和使用OpenAI Python库

### 1、安装OpenAI库

* 通过pip命令安装OpenAI库
  * 打开终端或CMD，输入`pip install openai`（对于macOS可能需要使用`pip3 install openai`）。
  * 或者在Jupyter Notebook中新建一个notebook，并运行`!pip install openai`。感叹号前缀使得该命令在系统shell中执行，效果等同于在终端或CMD中运行。
* 导入并初始化OpenAI库
  * 安装完成后，在Jupyter Notebook中使用`from openai import OpenAI`语句导入OpenAI库。
  * 创建一个`client = OpenAI()`实例，通常不需要手动输入API密钥，因为可以从环境变量中自动获取。

### 2、配置API密钥

* 从环境变量读取API密钥

  * 如果已经将API密钥存储在环境变量中，则可以直接使用，无需再次输入。

* 手动设置API密钥

  * 如果没有存储API密钥或想使用新的密钥，可以通过传递`api_key`参数给OpenAI构造函数来指定密钥值。

    ```python
    client = OpenAI(api_key='密钥')
    ```

### 3、发送请求与接收回复

* 调用chat.completion.create方法
  * 使用`openai.ChatCompletion.create()`方法发起对话请求，需指定`model`参数（如`gpt-3.5-turbo`, `gpt-4`, `gpt-4-turbo`等）。
  * 根据选择的模型版本不同，性能和价格也会有所差异。
* 构建messages列表
  * `messages`参数是一个包含一条或多条消息的列表，每条消息由字典表示。
  * 字典中必须包含`role`键，用于标识消息来源（用户为`user`，AI助手为`assistant`，系统消息为`system`）。
  * 可以包含用户的提示和AI的回答，第一条消息可以是系统消息，用于设定背景或角色。

代码示例

```python
from openai import OpenAI

client = OpenAI()

response = client.chat.completions.create(
  model="gpt-3.5-turbo",
  messages=[
    {"role": "user", "content": "四大文明古国分别有哪些"}
  ]
)
response
# 输出结果
ChatCompletion(id='chatcmpl-8iAPcTm1oItRiinW199PZ1aTPMM0F', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='四大文明古国指的是古代埃及、巴比伦、印度和中国。这四个古国拥有悠久的历史和独特的文明。以下是这些古国的一些特点：\n\n1. 埃及：埃及文明起源于公元前3100年的法老王朝时期。埃及是世界上最早的文明之一，拥有众多的古代建筑和文化遗产，如金字塔、狮身人面像等。埃及人崇拜众神，并相信死后的来世。\n\n2. 巴比伦：巴比伦是古代美索不达米亚地区的一座重要城市。巴比伦文明起源于公元前18世纪的阿卡德帝国时期，盛行于公元前7世纪的新巴比伦帝国时期。巴比伦文明的最大贡献之一是制定了世界上最早的法典——汉谟拉比法典。\n\n3. 印度：印度文明起源于公元前2500年左右的哈拉帕文明，也叫印度河谷文明。印度文明的特点之一是宗教多元化，印度教、佛教、耆那教等宗教都在这里兴起。此外，印度还有许多伟大的文化和科学成就，如印度的数学、医学和文学等。\n\n4. 中国：中国文明起源于公元前21世纪的夏代，是世界上最古老的连续文明之一。中国文明的特点之一是中华传统文化，包括儒家思想、道家思想和佛教等。此外，中国还有众多的发明和科技成就，如造纸术、指南针、火药和活字印刷术等。', role='assistant', function_call=None, tool_calls=None))], created=1705537148, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=566, prompt_tokens=20, total_tokens=586))
```

多Role示例

```python
response = client.chat.completions.create(
  model="gpt-3.5-turbo",
  messages=[
    {"role": "system", "content": "你是一个乐于助人、语气友善的AI聊天助手"},
    {"role": "user", "content": "你是谁"},
    {"role": "assistant", "content": "我是ChatGPT，由OpenAI开发的一款大型语言模型。"},
    {"role": "user", "content": "四大文明古国分别有哪些"}
  ]
)
response
# 输出结果
ChatCompletion(id='chatcmpl-8iAPmOhzCO2DuFGb3D23JBmFSnNPp', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='四大文明古国是指中国、印度、古巴比伦和古埃及。以下是它们的简要介绍：\n\n1. 中国：中国是世界上最古老的文明之一，拥有5000多年的历史。它以其丰富的历史和文化遗产而闻名，包括伟大的长城、秦始皇陵、故宫和中国传统的哲学、文学、艺术和科学。\n\n2. 印度：印度是世界上最古老的宗教之一——印度教的发源地，也是佛教和耆那教的重要发展地。印度拥有丰富的文化遗产，包括泰姬陵、拉贾斯坦邦的古城、印度古典舞蹈和印度瑜伽等。\n\n3. 古巴比伦：古巴比伦位于现在的伊拉克地区，是迦勒底王朝的中心，公元前18至6世纪期间一度是世界上最强大的帝国之一。古巴比伦文明以其先进的法律、政治组织和建筑技术而闻名，其中最著名的是巴比伦的宏伟城市和垂直的花岗岩宏伟塔楼——巴比伦塔。\n\n4. 古埃及：古埃及位于尼罗河流域，拥有悠久的历史和独特的文化。古埃及文明以其庞大的金字塔、神庙和法老王墓葬而闻名，埃及的艺术、文学、数学和天文学也有很高的发展水平。同时，古埃及还有一套独特的象形文字系统——象形文字。\n\n这些古国对人类文明的发展和演进作出了重大贡献，他们的文化和遗产也延续至今。', role='assistant', function_call=None, tool_calls=None))], created=1705537158, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=600, prompt_tokens=81, total_tokens=681))
```

### 4、解析AI返回的消息

* 处理API响应

  * 调用API后，返回的对象是一个`ChatCompletion`类实例，其中AI的回复位于`choices[0].message.content`属性中。
  * 提取并打印出AI的回答内容。

示例代码片段

```python
print(response.choices[0].message.content)
# 输出结果
四大文明古国是指中国、印度、古巴比伦和古埃及。以下是它们的简要介绍：
1. 中国：中国是世界上最古老的文明之一，拥有5000多年的历史。它以其丰富的历史和文化遗产而闻名，包括伟大的长城、秦始皇陵、故宫和中国传统的哲学、文学、艺术和科学。
2. 印度：印度是世界上最古老的宗教之一——印度教的发源地，也是佛教和耆那教的重要发展地。印度拥有丰富的文化遗产，包括泰姬陵、拉贾斯坦邦的古城、印度古典舞蹈和印度瑜伽等。
3. 古巴比伦：古巴比伦位于现在的伊拉克地区，是迦勒底王朝的中心，公元前18至6世纪期间一度是世界上最强大的帝国之一。古巴比伦文明以其先进的法律、政治组织和建筑技术而闻名，其中最著名的是巴比伦的宏伟城市和垂直的花岗岩宏伟塔楼——巴比伦塔。
4. 古埃及：古埃及位于尼罗河流域，拥有悠久的历史和独特的文化。古埃及文明以其庞大的金字塔、神庙和法老王墓葬而闻名，埃及的艺术、文学、数学和天文学也有很高的发展水平。同时，古埃及还有一套独特的象形文字系统——象形文字。
这些古国对人类文明的发展和演进作出了重大贡献，他们的文化和遗产也延续至今。
```

## 四、AI模型咋收费？必了解的token计数

### 1、理解Token

* Token作为文本的基本单位
  * Token是文本处理的基本单位，短单词可能是一个Token，长单词或中文字符则可能被拆分为多个Token。
  * 中文字符通常占用更多的Token数量，某些不常见字可能会被映射成多个Token。
* 查看Token化效果
  * 可以使用OpenAI提供的交互式分词器（<https://platform.openai.com/tokenizer>）来查看任意文本如何被映射为不同的Token。

### 2、Token计费机制

* 基于Token数量的计费
  * 大部分大模型API的计费基于Token的数量，包括用户提示和AI回应的总Token数。
  * 一个Token通常对应约四个英文字符，每100个Token约等于75个单词。
* 具体计费示例
  * GPT-3.5 Turbo: 输入每1000Token 0.1人民币，输出每1000Token 0.2人民币。
  * GPT-4: 输入每1000Token 3美分，输出每1000Token 6美分。

### 3、使用TickToken库估算Token数量

* 安装TickToken库
  * 在Jupyter Notebook中运行`!pip install tiktoken`，或在CMD/终端中运行`pip install tiktoken`（macOS可能需要`pip3 install tiktoken`）。
* 使用TickToken库估算Token数
  * 导入`tiktoken`库并使用`Encoding for Model`方法获取对应模型的编码器。
  * 调用编码器的`encode`方法传入文本内容，返回一个Token ID列表，列表长度即为Token数量。

```python
# 导入库
import tiktoken
# 获取对应模型的编码器
encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")
encoding
# 结果返回编码器
<Encoding 'cl100k_base'>
# 调用编码器中的方法
encoding.encode("黄河之水天上来")
# 返回token id列表结果，每个id对应一个token
[30868, 226, 31106, 111, 55030, 53610, 36827, 17905, 37507]
# 求列表长度
len(encoding.encode("黄河之水天上来"))
# 返回token长度
9
```

### 4、控制API请求中的Token消耗

* 限制AI回应的Token数量
  * 可以通过设置API请求参数来控制AI回应的最大Token数，避免过长回复导致成本增加。
* 上下文窗口的限制
  * AI模型的上下文窗口（Context Window）是有限的，超过该限制的文本会被截断。例如：
    * GPT-3.5 Turbo: 上下文窗口为4096 Token（另有16K版本支持16000 Token）。
    * GPT-4: 上下文窗口为8192 Token（另有32K版本支持32000 Token）。

### 5、优化提示与回应长度

* 简洁提问的重要性
  * 学习如何简洁地构建提示，减少不必要的Token消耗，从而降低成本。
* 应对上下文窗口限制的方法
  * 针对上下文窗口带来的限制，可以采用分段处理、精简提示等策略来优化对话效果。

## 五、定制AI的回复？常用参数详解

* Max Tokens：控制生成长度与成本
* Temperature：调整响应的随机性与创造性
* Top\_p（Nucleus Sampling）：调整响应的随机性与创造性

### 1、Max Tokens参数

* 理解Max Tokens的作用
  * `max_tokens` 参数用于控制AI回复所消耗的Token最大值。
  * 通过限制回复长度，可以有效控制每次API请求的成本上限。
* 使用Max Tokens的注意事项
  * 模型不会根据该参数调整内容篇幅，而是在达到指定Token数时直接截断。
  * 如果设置过小，可能导致回答不完整（如句子中断）。
  * 建议结合提示词（如“请在500字以内回答”）来确保回答完整性。

```python
response = client.chat.completions.create(
  model="gpt-3.5-turbo",
  messages=[
    {
      "role": "user",
      "content": "四大文明古国分别有哪些"
    }
  ],
  max_tokens=100
)
print(response.choices[0].message.content)
# 输出结果
四大文明古国指的是古代埃及、美索不达米亚、印度河流域和黄河流域四个地区。它们分别是：
1. 埃及文明：位于尼罗河流域，是人类历史上最早的文明之一。埃及文明以母系社
```

### 2、Temperature参数

* Temperature的基本概念
  * 取值范围为0到2，默认值为1。
  * 控制AI回答的随机性和创造性。
* 不同Temperature值的影响
  * **低值（接近0）**：输出更确定、可预测，多次请求结果相似。
  * **高值（接近2）**：输出更具随机性和创造性，可能产生出人意料的回答，但也可能偏离逻辑或生成无意义内容。
* Temperature的工作原理
  * 改变Token的概率分布：
    * 温度低时，概率分布更集中，高概率词被优先选择。
    * 温度高时，概率分布更平坦，低概率词也有更高机会被选中。
  * 类比物理中的温度：温度越高，粒子运动越随机。

```python
response = client.chat.completions.create(
  model="gpt-3.5-turbo",
  messages=[
    {
      "role": "user",
      "content": "四大文明古国分别有哪些"
    }
  ],
  max_tokens=100,
  temperature=2
)
print(response.choices[0].message.content)
# 输出结果
四大文明古国分别是：古代埃及文明、幼发拉底河流域文明古印度同时票角utenberg SouthωEurope Mary理streamsJapan modeling forecast TclwaDavis sidl Kenn nobern
```

### 3、Top\_p参数（Nucleus Sampling）

* Top\_p的基本概念
  * 取值范围为0到1。
  * 不改变概率分布，而是从累积概率最高的词中选择一个子集进行采样。
* Top\_p的工作方式
  * 例如，`top_p=0.4` 表示只考虑累积概率≥40%的最可能词组。
  * `top_p=1` 表示不限制，所有词都可选。
* Temperature与Top\_p的使用建议
  * 两者都能控制输出的多样性，但机制不同。
  * OpenAI官方建议：**不要同时调整这两个参数**，只修改其中一个即可。

```python
response = client.chat.completions.create(
  model="gpt-3.5-turbo",
  messages=[
    {
      "role": "user",
      "content": "四大文明古国分别有哪些"
    }
  ],
  max_tokens=300,
  top_p=0.4
)
print(response.choices[0].message.content)
# 输出结果
四大文明古国是指古埃及、古巴比伦、古印度和古中国。
```

### 4、参数选择策略

* 何时使用低Temperature
  * 需要稳定、一致输出的任务，如问答系统、事实性回复。
* 何时使用高Temperature或Top\_p
  * 创意写作、头脑风暴、故事生成等需要多样性的场景。
* 成本与质量的平衡
  * 结合`max_tokens`和提示词设计，既能控制成本，又能获得高质量输出。

## 六、定制AI的回复？更多常用参数详解

### 1、Frequency Penalty（频率惩罚）

* Frequency Penalty的基本概念
  * 取值范围为-2到2，默认值为0。
  * 用于惩罚在已生成文本中频繁出现的词汇，降低其再次被选中的概率。
* 工作原理与效果
  * 值为正时，模型会倾向于避免重复使用高频率词，增加文本多样性。
  * 值为0时，不对重复词进行任何惩罚，模型按原始概率分布选择下一个词。
  * 值过高可能导致不自然的表达（如标点符号频繁切换）。
* 实际应用建议
  * 适用于需要丰富多变输出的场景，如创意写作、故事生成。
  * 一般建议设置在0到1之间，避免过度惩罚导致语义混乱。

frequency\_penalty为负数

```python
response = client.chat.completions.create(
  model="gpt-3.5-turbo",
  messages=[
    {
      "role": "user",
      "content": "生成一个购物清单，包含至少20个物品，每个物品之间用逗号进行分隔，例如：苹果，香蕉，牛奶"
    }
  ],
  max_tokens=300,
  frequency_penalty=-2
)
print(response.choices[0].message.content)
# 输出结果
苹果，香蕉，牛奶，面包，鸡蛋，洗发水，牛肉，蛋糕，薯片，咖啡，牛，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，
```

frequency\_penalty为正数

```python
response = client.chat.completions.create(
  model="gpt-3.5-turbo",
  messages=[
    {
      "role": "user",
      "content": "生成一个购物清单，包含至少20个物品，每个物品之间用逗号进行分隔，例如：苹果，香蕉，牛奶"
    }
  ],
  max_tokens=300,
  frequency_penalty=-2
)
print(response.choices[0].message.content)
# 输出结果
苹果，香蕉，牛奶，面包，鸡蛋，咖啡豆，洗发水，
肥皂, 面巾纸, 茶叶, 巧克力, 红酒,
玉米片, 米饭, 电视机,
手表 ,手机 ,笔记本电脑 ,
运动裤 ，T恤衫
```

### 2、Presence Penalty（存在惩罚）

* Presence Penalty的基本概念
  * 取值范围同样为-2到2，默认值为0。
  * 与Frequency Penalty类似，但机制不同。
* 与Frequency Penalty的区别
  * **Frequency Penalty**：根据词的出现**频率**进行惩罚，出现越多次，惩罚越重。
  * **Presence Penalty**：只关心词是否**出现过**，一旦出现就降低其后续概率，不考虑具体次数。
* 结合使用策略
  * 提高`presence_penalty`可减少整体重复内容，鼓励引入新词。
  * 提高`frequency_penalty`可防止某些高频词反复出现。
  * 两者可结合使用以精细控制文本的多样性和连贯性。

### 3、实用工具推荐：OpenAI API Playground

> <https://platform.openai.com/playround>

API Playground的功能与优势

* 无需编码的参数实验平台
  * 提供可视化界面，支持调整模型、temperature、max\_tokens、top\_p、frequency\_penalty、presence\_penalty等所有关键参数。
  * 实时查看AI响应，快速测试不同配置的效果。
* 重要注意事项
  * 所有操作均通过API执行，**会消耗账户的Token配额**，并非免费试玩工具。
* View Code功能——开发者的福音
  * 点击右上角“View Code”按钮，可自动生成当前设置对应的API调用代码。
  * 支持直接复制代码片段，无缝集成到Python或其他编程环境中，极大提升开发效率。

---

---
url: /Linux/OpenEuler/1_openEuler.md
---

# openEuler

## 一、OpenEuler简介

> 欧拉系统官网：https://www.openeuler.openatom.cn/zh/

### 1、什么是欧拉

```
欧拉是数字基础设施的开源操作系统，可广泛部署于服务器、云计算、边缘计算、嵌入式等各种形态设备，应用场景覆盖IT（Information Technology）、CT（Communication Technology）和OT（Operational Technology），实现统一操作系统支持多设备，应用一次开发覆盖全场景。

openEuler操作系统开源以来，获得了产业界的积极响应，已经有三百家企业、近万名社区开发者加入，成为中国极具活力的开源社区，目前欧拉商用已经突破60万套。2021年9月，华为全新升级欧拉，从服务器操作系统，升级为数字基础设施的操作系统，支持IT、CT、OT等数字基础设施全场景，覆盖服务器、云计算、边缘计算、嵌入式等各种形态的设备。

目前，欧拉和鸿蒙实现了内核技术共享，未来将进一步在分布式软总线、安全OS、设备驱动框架、以及新编程语言等方面实现共享。通过能力共享，实现生态互通及云边端协同，更好地服务数字化全场景。

欧拉的定位是瞄准国家数字基础设施的操作系统和生态底座。捐赠给开放原子开源基金会，汇聚更多产业力量，对于打造数字中国坚实底座具有重要价值，欧拉也将成为全产业共同拥有的开源生态。为了更好的推动数字区域数字经济发展，华为联合北京、广州、深圳、成都、武汉、南京全国6大城市、协同8家操作系统伙伴共同启动首批“欧拉生态创新中心”，进一步深化欧拉生态的全国布局。

                                                            ​ ——华为云开发者社区
```

### 2、欧拉的优势

https://bbs.huaweicloud.com/blogs/327740

![image.png](/assets/1642754357819088619.wj2NhsZx.png)

## 二、OpenEuler安装

vmware安装openEuler：https://blog.csdn.net/qq\_45945548/article/details/120520237

安装openEuler可视化界面：https://blog.csdn.net/qq\_50824019/article/details/124526889

---

---
url: /Linux/OpenEuler/2_openEuler安装Docker.md
---

# openEuler安装Docker

在欧拉系统上安装Docker时，可能会遇到一些问题。

```sh
yum install docker
```

**错误信息**:

```sh
Error: Package: docker-ce-18.09.0-3.el7.x86_64 (docker-ce-stable)

Requires: container-selinux >= 2.9
```

这是由于 OpenEuler 没有官方支持的 docker-ce 仓库

### 1、添加Docker官方仓库

首先，添加Docker官方仓库以确保获取最新版本。

```sh
yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
```

### 2、修改配置文件

进入配置文件目录并编辑*docker-ce.repo*文件，将\*$releasever*替换为*7\*。

```sh
cd /etc/yum.repos.d/

vi docker-ce.repo
```

将以下内容：

```sh
baseurl=https://download.docker.com/linux/centos/$releasever/$basearch/stable
```

替换为：

```sh
baseurl=https://download.docker.com/linux/centos/7/$basearch/stable
```

### 3、安装指定版本的Docker

查询可用的Docker版本并安装最新版本。

```sh
yum list docker-ce --showduplicates | sort -r

yum -y install docker-ce-24.0.2
```

### 4、启动并设置Docker开机自启

安装完成后，启动Docker服务并设置为开机自启。

```sh
systemctl start docker

systemctl enable docker
```

通过以上步骤，可以成功在欧拉系统上安装并运行最新版本的Docker。

---

---
url: /daily/开发文档/PDF转换.md
---

# PDF转换

## POI

[java word转pdf在linux系统上可行\_documents4j linux-CSDN博客](https://blog.csdn.net/Pander_king/article/details/126637932)

## Documents4j

documents4j需要MS Office

依赖

```xml
<dependency>
    <groupId>com.documents4j</groupId>
    <artifactId>documents4j-local</artifactId>
    <version>1.1.1</version>
</dependency>
<dependency>
    <groupId>com.documents4j</groupId>
    <artifactId>documents4j-transformer-msoffice-word</artifactId>
    <version>1.1.1</version>
</dependency>
```

代码

```java
public static void main(String[] args) {
    long startTime = System.currentTimeMillis();

    wordToPdf("docx");

    long endTime = System.currentTimeMillis();
    long totalTime = endTime - startTime;
    System.out.println("程序运行时间：" + totalTime + "毫秒");
}

/**
 * 将之前对应的word文件转换成pdf，然后预览pdf文件
 *
 * @param suffix
 */
public static void wordToPdf(String suffix) {
    // 原始文档
    File inputWord = new File("C:/Users/xxl/Desktop/Generative.docx");
    // 转换之后的pdf文件
    File outputFile = new File("C:/Users/xxl/Desktop/Generative.pdf");
    try {
        InputStream docxInputStream = new FileInputStream(inputWord);
        OutputStream outputStream = new FileOutputStream(outputFile);
        IConverter converter = LocalConverter.builder().build();
        if (suffix.equals("doc")) {
            converter.convert(docxInputStream).as(DocumentType.DOC).to(outputStream).as(DocumentType.PDF).execute();
        } else if (suffix.equals("docx")) {
            converter.convert(docxInputStream).as(DocumentType.DOCX).to(outputStream).as(DocumentType.PDF).execute();
        } else if (suffix.equals("txt")) {
            converter.convert(docxInputStream).as(DocumentType.TEXT).to(outputStream).as(DocumentType.PDF).execute();
        }
        outputStream.close();
    } catch (Exception e) {
        e.printStackTrace();
    }
}
```

在Linux上使用会报错

```
2024-09-07 21:34:01.068 ERROR 35706 --- [nio-8081-exec-2] o.a.c.c.C.[.[.[/].[dispatcherServlet]    : Servlet.service() for servlet [dispatcherServlet] in context with path [] threw exception [Request processing failed; nested exception is java.lang.IllegalStateException: class com.documents4j.conversion.msoffice.MicrosoftWordBridge could not be created by a (File, long, TimeUnit) constructor] with root cause

java.io.IOException: error=2, 没有那个文件或目录

```

这是因为Linux缺少微软的office

### Linux安装Libreoffice

安装：https://blog.csdn.net/a1275302036/article/details/134806486

离线安装：https://blog.csdn.net/nothing\_may/article/details/122175775

首先在官网下载对应的压缩包：[libreoffice压缩包下载链接](https://www.libreoffice.org/download/download-libreoffice/)

解压压缩包（我直接放到opt下解压）

```sh
tar -zxvf LibreOffice_7.1.8_Linux_x86-64_rpm.tar.gz
```

cd到解压的包中的RPMS目录下进行下载

```sh
# 下载
yum localinstall *.rpm
# 启动
/opt/libreoffice7.1/program/soffice --headless --accept="  socket,host=127.0.0.1,port=8100;urp;"- -nofirststartwizard &
```

### 启动报错

#### 报错1

```sh
/opt/libreoffice7.1/program/soffice.bin: error while loading shared libraries: libSM.so.6: cannot open shared object file: No such file or directory
```

这是因为缺少了libSM.so.6包

下载libSM.so.6包：[libSM.so.6压缩包下载地址](https://pkgs.org/download/libSM.so.6)

把包放到系统上直接安装

```sh
yum localinstall libSM-1.2.3-1.el8.x86_64.rpm
```

#### 报错2

```
/opt/libreoffice7.1/program/soffice.bin: error while loading shared libraries: libcairo.so.2: cannot open shared object file: No such file or directory
```

解决

```
yum install -y cairo
```

#### 报错3

```
/opt/libreoffice7.1/program/soffice.bin: error while loading shared libraries: libcups.so.2: cannot open shared object file: No such file or directory
```

解决

```sh
yum install cups-libs
```

https://blog.csdn.net/nothing\_may/article/details/122175775

https://blog.csdn.net/Y\_hanxiong/article/details/124392435

https://www.cnblogs.com/lwjQAQ/p/16505854.html

### 测试

```sh
soffice --headless --convert-to pdf /home/tools/Generative.docx --outdir /home/tools/ 
```

https://blog.csdn.net/apache\_z/article/details/104970280

[Java使用documents4j在linux环境实现word转pdf，并解决中文乱码 - hello龙兄 - 博客园 (cnblogs.com)](https://www.cnblogs.com/qq545505061/p/18345179)

[java 使用documents4j将word转pdf - 快乐小洋人 - 博客园 (cnblogs.com)](https://www.cnblogs.com/1399z3blog/p/17832438.html)

https://blog.csdn.net/qq\_37240161/article/details/124150883

## Aspose

aspose是收费软件，pom依赖无法直接下载

下载：https://blog.csdn.net/cheng137666/article/details/111677549

## docx4j

[使用docx4j 实现word转pdf（linux乱码处理）\_linux导入windows docx4j-CSDN博客](https://blog.csdn.net/winsanity/article/details/122623499)

## Freemarker

[通过FreeMarker生成word文档docx转pdf(二) - 曾经已是追忆 - 博客园 (cnblogs.com)](https://www.cnblogs.com/hehanhan/p/15735637.html)

## jodconverter

[Word转PDF简单示例，分别在windows和centos中完成转换\_documents4j-transformer-msoffice-word-CSDN博客](https://blog.csdn.net/m0_60155232/article/details/134371176)

## 参考资料

https://blog.csdn.net/aley/article/details/127914145

https://blog.csdn.net/wgq2020/article/details/134568346

---

---
url: /Java/架构设计/分布式/06.分布式监控/Pinpoint.md
---

# Pinpoint

## 概述

### 1. 什么是Pinpoint？

Pinpoint是一个开源的应用性能监控工具，专为分布式应用程序而设计。它的目标是帮助开发人员和运维团队深入了解分布式系统中的事务流程和性能表现。Pinpoint跟踪并可视化事务的执行过程，以便用户能够识别问题并改进应用程序性能。

### 2. 为什么需要Pinpoint？

在分布式系统中，一个应用通常由多个不同的组件组成，这些组件之间相互通信，有时还需要与外部服务进行API调用。如何监控这些组件之间的交互以及每个组件的性能变得非常关键。而Pinpoint的出现弥补了这一监控领域的不足，为开发人员和运维人员提供了一个强大的工具，帮助他们理解和改进分布式应用的性能。

### 3. Pinpoint的主要特性

Pinpoint具有许多强大的特性，以下是一些主要功能：

#### 3.1 服务器地图（ServerMap）

Pinpoint通过ServerMap可视化展示组件之间的互联关系，帮助用户理解分布式系统的拓扑结构。用户可以单击节点以查看有关组件的详细信息，如当前状态和事务计数。这有助于快速识别问题所在。

#### 3.2 实时活跃线程图表（Realtime Active Thread Chart）

Pinpoint能够实时监控应用程序内部的活跃线程。这有助于用户了解应用程序的并发情况，识别潜在的性能瓶颈。

#### 3.3 请求/响应分布图表（Request/Response Scatter Chart）

Pinpoint可以可视化展示请求计数和响应模式的变化趋势。这有助于识别潜在的问题，并提供了更详细的信息以进行进一步的分析。

#### 3.4 调用堆栈（CallStack）

Pinpoint提供了代码级别的可视化，允许用户深入了解分布式环境中的每个事务。这有助于识别瓶颈和故障点，从而加速故障排除过程。

#### 3.5 检查器（Inspector）

Pinpoint的Inspector功能允许用户查看有关应用程序的附加信息，如CPU使用率、内存和垃圾收集情况、每秒事务数（TPS）以及JVM参数。这些信息对于性能优化和故障排除非常有帮助。

### 4. 如何使用Pinpoint？

使用Pinpoint来监控分布式应用程序的性能是一个多步骤的过程。

#### 4.1 安装Pinpoint代理

首先，您需要在应用程序的各个组件上安装Pinpoint代理。这些代理将负责收集性能数据并将其发送到Pinpoint服务器。

#### 4.2 配置Pinpoint服务器

您需要设置Pinpoint服务器以接收和存储从代理发送的性能数据。Pinpoint服务器将数据可视化并提供给用户。

#### 4.3 监控和分析

一旦Pinpoint代理和服务器都设置好了，您就可以开始监控分布式应用程序的性能。使用Pinpoint的各种功能来跟踪事务、识别性能问题并进行进一步的分析。

#### 4.4 优化性能

根据Pinpoint提供的数据和分析结果，您可以采取必要的措施来优化应用程序的性能。这可能包括代码优化、资源调整或架构改进。

### 5. Pinpoint的优势

Pinpoint作为一款开源工具，具有以下优势：

#### 5.1 高度可定制性

Pinpoint是开源的，因此用户可以根据自己的需求进行定制。您可以创建自定义插件、规则和仪表板，以满足特定的监控需求。

#### 5.2 社区支持

Pinpoint拥有活跃的开发和用户社区，用户可以在社区中获取支持、分享经验并获取更新和扩展。

#### 5.3 商业友好的开源许可

Pinpoint采用商业友好的开源许可，使其非常适合用于商业环境中。

### 6. 快速安装

看了下最新的安装步骤，竟然有了windwos的安装部署，可以快速安装试用

可以访问如下路径：

**https://pinpoint-apm.gitbook.io/pinpoint/getting-started/quickstart/quickstart.win.en**

github可以访问的直接到如下链接去下载就可以，目前支持windows

**https://github.com/1Remote/1Remote/releases**

github如果无法访问的话，可以后台直接私信

### 7. 总结

Pinpoint是一款强大的分布式应用性能监控工具，旨在帮助开发人员和运维团队深入了解分布式系统中的事务流程和性能表现。它提供了一系列功能，包括服务器地图、实时活跃线程图表、请求/响应分布图表、调用堆栈和检查器，使用户能够识别问题并优化应用程序性能。作为一款开源工具，Pinpoint具有高度可定制性、社区支持和商业友好的开源许可，适用于各种分布式应用监控场景。无论是开发人员还是运维团队，Pinpoint都是一个强大的工具，可帮助他们提升分布式应用的性能和可用性，提供卓越的用户体验。

参考资料

\[1]. [大侠之运维 12.9k star，最强链路监控系统推荐，推荐](https://mp.weixin.qq.com/s/q1fFmwL1HqDNlY87FNLAfw)

\[2]. http://www.rainbond.net/docs/micro-service/tracking/pinpoint/

\[3]. https://blog.csdn.net/zhou920786312/article/details/131891599

\[4]. https://blog.csdn.net/weixin\_43931358/article/details/107671436

\[5]. https://blog.csdn.net/naoju8155/article/details/125857402

\[6]. https://blog.csdn.net/omaidb/article/details/117453528

---

---
url: /常用框架/SpringBoot/SpringBoot程序开发/20_profile环境配置.md
---

# profile环境配置

在项目开发过程中需要考虑不同的运行环境：开发环境（dev）、测试环境（beta）和生产环境（product）。在以往的开发过程中通常使用Maven构建工具进行控制，但却需要进行大量的配置。SpringBoot考虑到此类问题，专门设计了profile支持。

## yml配置

修改application.yml配置文件，让其支持多profile配置

```yaml
spring:
  profiles:
    active: dev # 定义默认生效的环境
---
spring:
  profiles: dev
server:
  port: 7070 # 定义开发服务访问端口配置
---
spring:
  profiles: bate
server:
  port: 8080 # 定义测试服务访问端口配置
---
spring:
  profiles: product
server:
  port: 80 # 定义生产服务访问端口配置
```

在配置文件中一共定义了3个环境（不同的profile之间使用“---”分割）。

* 开发环境（profiles=dev、默认）：端口定义为7070。
* 测试环境（profiles=beta）：端口定义为8080。
* 生产环境（profiles=product）：端口定义为80。

如果要正常进行打包，还需要修改pom.xml文件，追加resource配置，主要的功能是进行源文件夹中内容的打包输出，配置完成后可以将配置文件打包到\*.jar文件中。

```xml
<resources>
    <resource>
        <directory>src/main/plexus</directory>
        <includes>
            <include>**/*.properties</include>
            <include>**/*.yml</include>
            <include>**/*.xml</include>
            <include>**/*.tld</include>
        </includes>
        <filtering>false</filtering>
    </resource>
    <resource>
        <directory>src/main/java</directory>
        <includes>
            <include>**/*.properties</include>
            <include>**/*.xml</include>
            <include>**/*.tld</include>
        </includes>
        <filtering>false</filtering>
    </resource>
</resources>
```

打包完成后一定要运行程序，如果不做出任何的指派，那么默认配置的活跃profile（dev）就将直接起作用

如果要切换到不同的profile环境，可以在启动时动态配置

```sh
java -jar xxx.jar --spring.profiles.active=product
```

## properties

\***.properties与\*.yml配置不同。**

使用application.yml进行多profile配置的时候，只需要在一个配置文件中使用“---”分割不同的profile配置。但是此类模式不适合于application.properties配置，此时应该采用不同的\*.properties保存不同的配置，才可以实现多profile。

定义3个针对不同运行环境的`application.properties`配置文件

【开发】application-dev.properties

```properties
server.port=7070
```

【测试】application-beta.properties

```properties
server.port=8080
```

【生产】application-product.properties

```properties
server.port=80
```

随后还是需要有一个公共的`application.properties`配置文件，用于指派可以使用的profile配置

定义公共的`application.properties`配置文件

```properties
spring.profiles.active=beta
```

随后的使用形式与application.yml配置相同。

---

---
url: /Java/架构设计/分布式/06.分布式监控/Prometheus.md
---

# Prometheus

## 概述

Prometheus 是一个开源的服务监控系统和时间序列数据库。

> 官网：[Prometheus - Monitoring system & time series database](https://prometheus.io/)
>
> 源码：https://github.com/prometheus/prometheus

![img](/assets/082749_47Dp_5189.6l8_Jsh_.png)

特性：

* 高维度数据模型
* 自定义查询语言
* 可视化数据展示
* 高效的存储策略
* 易于运维
* 提供各种客户端开发库
* 警告和报警
* 数据导出

## SpringBoot集成使用

### 1.Spring Boot 工程集成 Micrometer

##### 1.1引入依赖

```xml
<dependency>
  <groupId>org.springframework.boot</groupId>
  <artifactId>spring-boot-starter-actuator</artifactId>
</dependency>
<dependency>
  <groupId>io.micrometer</groupId>
  <artifactId>micrometer-registry-prometheus</artifactId>
</dependency>
```

##### 1.2配置

```properties
management.server.port=9003
management.endpoints.web.exposure.include=*
management.endpoint.metrics.enabled=true
management.endpoint.health.show-details=always
management.endpoint.health.probes.enabled=true
management.endpoint.prometheus.enabled=true
management.metrics.export.prometheus.enabled=true
management.metrics.tags.application=voice-qc-backend
```

这里 `management.endpoints.web.exposure.include=*` 配置为开启 Actuator 服务，因为Spring Boot Actuator 会自动配置一个 URL 为 `/actuator/Prometheus` 的 HTTP 服务来供 Prometheus 抓取数据，不过默认该服务是关闭的，该配置将打开所有的 Actuator 服务。

`management.metrics.tags.application` 配置会将该工程应用名称添加到计量器注册表的 tag 中去，方便后边 Prometheus 根据应用名称来区分不同的服务。

##### 1.3监控jvm信息

然后在工程启动主类中添加 Bean 如下来监控 JVM 性能指标信息：

```java
@SpringBootApplication
public class GatewayDatumApplication {

    public static void main(String[] args) {
        SpringApplication.run(GatewayDatumApplication.class, args);
    }

    @Bean
    MeterRegistryCustomizer<MeterRegistry> configurer(
            @Value("${spring.application.name}") String applicationName) {
        return (registry) -> registry.config().commonTags("application", applicationName);
    }

}
```

##### 1.4创建自定义监控

监控请求次数与响应时间

```
package com.lianxin.gobot.api.monitor;

import io.micrometer.core.instrument.Counter;
import io.micrometer.core.instrument.MeterRegistry;
import io.micrometer.core.instrument.Timer;
import lombok.Getter;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Component;

import javax.annotation.PostConstruct;

/**
 * @Author: GZ
 * @CreateTime: 2022-08-30  10:50
 * @Description: 自定义监控服务
 * @Version: 1.0
 */
@Component
public class PrometheusCustomMonitor {
    /**
     * 上报拨打请求次数
     */
    @Getter
    private Counter reportDialRequestCount;
    /**
     * 上报拨打URL
     */
    @Value("${lx.call-result-report.url}")
    private String callReportUrl;

    /**
     * 上报拨打响应时间
     */
    @Getter
    private Timer reportDialResponseTime;
    @Getter
    private final MeterRegistry registry;


    @Autowired
    public PrometheusCustomMonitor(MeterRegistry registry) {
        this.registry = registry;
    }

    @PostConstruct
    private void init() {
        reportDialRequestCount = registry.counter("go_api_report_dial_request_count", "url",callReportUrl);
        reportDialResponseTime=  registry.timer("go_api_report_dial_response_time", "url",callReportUrl);
    }
}
```

##### 1.5添加具体业务代码监控

```
//统计请求次数
prometheusCustomMonitor.getReportDialRequestCount().increment();
long startTime = System.currentTimeMillis();
String company = HttpUtils.post(companyUrl,"");
//统计响应时间
long endTime = System.currentTimeMillis();
prometheusCustomMonitor.getReportDialResponseTime().record(endTime-startTime, TimeUnit.MILLISECONDS);
```

在浏览器访问 `http://127.0.0.1:9001/actuator/prometheus` ，就可以看到服务的一系列不同类型 metrics 信息，例如`jvm_memory_used_bytes gauge`、`jvm_gc_memory_promoted_bytes_total counter` ，`go_api_report_dial_request_count`等

![图片](https://mmbiz.qpic.cn/mmbiz_png/JfTPiahTHJhrkf2DzYicJ84YjNTO9WGE9TB4sQicDO0fhqPIyJDBFMeZfnzsc9YCGW9ricRgZMfS47A51v4jCosOTw/640?wx_fmt=png\&tp=wxpic\&wxfrom=5\&wx_lazy=1\&wx_co=1)

到此，Spring Boot 工程集成 Micrometer 就已经完成，接下里就要与 Prometheus 进行集成了。

### 2.集成 Prometheus

##### 2.1安装

```
docker pull prom/prometheus
mdkir /usr/local/prometheus
vi prometheus.yml
# my global config
global:
  scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
  # scrape_timeout is set to the global default (10s).

# Alertmanager configuration
alerting:
  alertmanagers:
  - static_configs:
    - targets:
      # - alertmanager:9093

# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.
rule_files:
  # - "first_rules.yml"
  # - "second_rules.yml"

# A scrape configuration containing exactly one endpoint to scrape:
# Here it's Prometheus itself.
scrape_configs:
  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.
  - job_name: 'prometheus'

    # metrics_path defaults to '/metrics'
    # scheme defaults to 'http'.

    static_configs:
    - targets: ['192.168.136.129:9090']
docker run -d --name prometheus -p 9090:9090 -v/usr/local/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml prom/prometheus
```

![图片](/assets/640.BE-3mZVN.png)

##### 2.2集成配置

```
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: "prometheus"
    static_configs:
    - targets: ["localhost:9090"]
  - job_name: "metricsLocalTest"
    metrics_path: "/actuator/prometheus"
    static_configs:
    - targets: ["localhost:9003"]
```

这里 `localhost:9001` 就是上边本地启动的服务地址，也就是 Prometheus 要监控的服务地址。同时可以添加一些与应用相关的标签，方便后期执行 PromSQL 查询语句区分。最后重启 Prometheus 服务

![图片](/assets/640.BE-3mZVN.png)

![图片](/assets/640.BE-3mZVN.png)

### 3.使用 Grafana Dashboard 展示监控项

##### 3.1安装grafana

```
docker pull grafana/grafana
docker run -d --name grafana -p 3000:3000 -v /usr/local/grafana:/var/lib/grafana grafana/grafana
```

默认用户名/密码 admin/admin

![图片](/assets/640-1694531771945.CYXAtjY_.png)

##### 3.2配置prometheus数据源

![图片](https://mmbiz.qpic.cn/mmbiz_png/JfTPiahTHJhrkf2DzYicJ84YjNTO9WGE9TrsYHAibSroeibGz7fcJtbdn9o4GzK1vQ8hSbQiacXm98k6juojWp8KJsg/640?wx_fmt=png\&tp=wxpic\&wxfrom=5\&wx_lazy=1\&wx_co=1)

##### 3.3增加jvm面板

模板编号为4701

![图片](/assets/640.BE-3mZVN.png)

![图片](https://mmbiz.qpic.cn/mmbiz_png/JfTPiahTHJhrkf2DzYicJ84YjNTO9WGE9T1NNuiaed5icDqURydelENgYzVicp6UjRibhlDSiaVoMscosdmiaG5CqYiaGNw/640?wx_fmt=png\&tp=wxpic\&wxfrom=5\&wx_lazy=1\&wx_co=1)

##### 3.4配置业务接口监控面板

![图片](/assets/640.BE-3mZVN.png)

## 参考资料

\[1]. [springboot+Prometheus+grafana 实现自定义监控（请求数、响应时间、JVM性能）](https://blog.csdn.net/GZ946/article/details/126619218)

\[2]. https://blog.51cto.com/u\_16099246/7607251

---

---
url: /StableDiffusion/StableDiffusion/1_Prompt提示词.md
---

# Prompt提示词

## 提示词基本语法

### 提示词类别

内容型提示词（控制画面内容） & 标准化提示词（控制画质和画风）

![img](/assets/prompt01.bhkx52mU.png)

### 提示词权重

控制提示词呈现的优先级的增减

![img](/assets/prompt02.BS5cNJho.png)

### 进阶提示词语法

混合、迁移与迭代

![img](/assets/prompt03.6rVLO2Wb.png)

### 如何书写提示词？

参考下面“通用模版”

![img](/assets/prompt04.jVMAZ1QS.png)

## 在线提示词网站

### PromptTool

链接：<https://www.prompttool.com/home/NovelAI>

国内提示词网站，图文对照，自带翻译

### PromLib

链接：<https://promlib.com>

国内提示词分享社区，横向细分，单词+例图展示，任务相关的描述较多

### 一个工具箱

链接：<http://www.atoolbox.net/Tool.php?Id=1101>

国内提示词网站

### AI词语加速器

链接：<https://ai.dawnmark.cn/>

国内提示词网站

### PromptHero

链接：<http://prompthero.com>

国外优质提示词网站，图文搭配分风格类别罗列，可复制作品提示词。（包含SD、MJ和ChatGPT等提示词）

## 参考资料

\[1]. Nenly同学：[20分钟搞懂Prompt与参数设置，你的AI绘画“咒语”学明白了吗？ | 零基础入门Stable Diffusion·保姆级新手教程 | Prompt关键词教学\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV12X4y1r7QB/)

\[2]. CG迷李辰：[Stable Diffusion提示词全攻略，必备提示词插件，关键词字符解析，写Prompt神器\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1eP411Y7SD/)

---

---
url: /Python/爬虫/2_Python发送HTTP请求.md
---

# Python发送HTTP请求

## 一、什么是HTTP请求和响应

HTTP（Hypertext Transfer Protocol，超文本传输协议）是客户端与服务器之间的请求 - 响应协议，是爬虫获取网页内容的底层原理。以下详细解析 HTTP 的请求、响应结构及核心概念：

### 1、HTTP 请求：客户端向服务器发送的 “指令”

HTTP 请求是客户端（如浏览器、爬虫）向服务器获取数据或提交信息的 “命令”，由**请求行**、**请求头**、**请求体**三部分组成。

#### 1.1. 常见请求方法

* **GET**：主要用于**获取数据**（如访问网页、查询信息），是爬虫最常用的方法。
* **POST**：主要用于**提交数据**（如注册表单、登录信息），数据存放在请求体中。

#### 1.2. 请求的组成部分

##### （1）请求行

包含**请求方法**、**资源路径**、**协议版本**，格式如下：
`方法类型 资源路径 协议版本`

* **资源路径**：指定服务器上的目标资源（如`/movie/top250`），可包含查询参数（`?key=value&key2=value2`），用于传递额外信息（如分页：`?start=75`表示从第 75 条数据开始）。
* **协议版本**：如`HTTP/1.1`（当前主流版本）。

示例：
`GET /movie/top250?start=75 HTTP/1.1`

##### （2）请求头

包含客户端向服务器传递的附加信息，常见字段：

* `Host`：主机域名（如`movie.douban.com`），结合资源路径构成完整 URL。
* `User-Agent`：客户端标识（如浏览器类型、版本，爬虫需模拟浏览器标识以避免被拦截）。
* `Accept`：客户端可接收的响应数据类型（如`text/html,application/json`，`*/*`表示任意类型）。

##### （3）请求体

客户端向服务器提交的额外数据（如表单内容），**GET 方法的请求体通常为空**，POST 方法会在此处携带数据。

### 2、HTTP 响应：服务器给客户端的 “回复”

服务器接收请求后，会返回 HTTP 响应，包含**状态行**、**响应头**、**响应体**三部分。

#### 2.1. 响应的组成部分

##### （1）状态行

包含**协议版本**、**状态码**、**状态消息**，格式如下：
`协议版本 状态码 状态消息`

* 状态码：表示请求处理结果，分为 5 类：
  * `2xx`（成功）：如`200 OK`（请求成功）。
  * `3xx`（重定向）：如`302 Found`（资源临时迁移）。
  * `4xx`（客户端错误）：如`404 Not Found`（资源不存在）、`403 Forbidden`（拒绝访问）。
  * `5xx`（服务器错误）：如`500 Internal Server Error`（服务器故障）。

##### （2）响应头

服务器向客户端传递的附加信息，常见字段：

* `Date`：响应生成的时间。
* `Content-Type`：响应体的数据类型及编码（如`text/html; charset=utf-8`表示 HTML 内容，编码为 UTF-8）。

##### （3）响应体

服务器返回的核心数据，爬虫的目标内容通常在此处：

* 若`Content-Type`为`text/html`，响应体是 HTML 代码（包含网页结构和内容）。
* 若为`application/json`，响应体是 JSON 格式数据（常见于 API 接口）。

### 3、总结

HTTP 请求与响应是爬虫获取数据的基础：

* 爬虫通过发送**GET 请求**向服务器索取网页内容；
* 服务器通过**响应体**返回 HTML 等数据，爬虫解析后提取目标信息。

## 二、如何用Python发送请求

requests 库是 Python 中发送 HTTP 请求的常用工具，能简洁高效地实现网页内容获取。以下是其安装方法、基础用法及实用技巧：

### 1、安装 requests 库

requests 是第三方库，需先安装：

**Windows**：在终端输入

```sh
pip install requests
```

**macOS/Linux**：通常使用

```sh
pip3 install requests
```

**验证安装**：
若终端显示`Successfully installed requests`，或`Requirements already satisfied`（已安装），则安装成功。
若提示 “找不到 pip”，需先按[官方指引](https://pip.pypa.io/en/stable/installation/)安装 pip 工具。

### 2、基础用法：发送 GET 请求获取网页内容

#### 2.1. 引入库并发送请求

```python
import requests

# 发送GET请求（需传入完整URL，包含http/https协议）
url = "http://books.toscrape.com"  # 练习爬虫的测试网站
response = requests.get(url)
```

#### 2.2. 处理响应结果

##### （1）判断请求是否成功

* **状态码（status\_code）**：

  ```python
  print(response.status_code)  # 200表示成功，404表示资源不存在
  ```

* **简化判断（ok 属性）**：

  ```python
  if response.ok:
      print("请求成功")
  else:
      print("请求失败")
  ```

##### （2）获取响应体内容

响应体（网页原始代码）通过`text`属性获取（以字符串形式返回）：

```python
if response.ok:
    html_content = response.text  # 存储网页HTML代码
    print(html_content)  # 打印原始内容（后续将解析这些内容）
```

### 3、进阶技巧：自定义请求头（headers）

requests 会自动生成默认请求头，但部分网站会通过`User-Agent`识别请求来源（区分浏览器与爬虫）。如需伪装成浏览器，可自定义请求头：

#### 示例：设置 User-Agent

```python
import requests

url = "http://books.toscrape.com"

# 自定义请求头：模拟Chrome浏览器
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

# 发送带自定义头的请求
response = requests.get(url, headers=headers)

if response.ok:
    print("伪装浏览器请求成功")
```

**作用**：避免网站因识别到爬虫而拒绝请求（反爬机制的基础应对方法）。

## 三、练习用Python拿到豆瓣源码

通过 requests 库发送 GET 请求获取豆瓣电影 Top250 的网页源码，是爬虫实战的典型案例。以下是具体步骤及解决反爬问题的方法：

### 1、准备工作：安装与引入 requests 库

1. **安装 requests**：在编辑器终端执行以下命令（Windows 用`pip`，macOS/Linux 用`pip3`）：

   ```sh
   pip install requests
   ```

   若显示`Successfully installed requests`，则安装成功。

2. **引入库**：新建 Python 文件（如`scrape_douban.py`），开头导入 requests：

   ```python
   import requests
   ```

### 2、发送请求：初始尝试与问题

#### 2.1. 发送基础 GET 请求

目标 URL：豆瓣电影 Top250 首页（`https://movie.douban.com/top250`）

```python
url = "https://movie.douban.com/top250"
response = requests.get(url)  # 发送GET请求

# 查看状态码
print("状态码：", response.status_code)
```

#### 2.2. 遇到反爬：418 状态码

运行后可能返回状态码`418`（“我是一个茶壶”），这是豆瓣的反爬机制 —— 通过识别请求来源（非浏览器）拒绝服务。

### 3、解决反爬：伪装成浏览器请求

通过自定义请求头（`headers`）中的`User-Agent`，将爬虫伪装成浏览器：

#### 3.1. 获取浏览器的 User-Agent

* 打开任意网页，右键→“检查”→进入 “Network” 面板；
* 刷新网页，点击任意请求→展开 “Request Headers”→复制`User-Agent`的值（如浏览器类型、版本信息）。
  示例：
  `Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36`

#### 3.2. 自定义请求头发送请求

```python
url = "https://movie.douban.com/top250"

# 自定义请求头，模拟浏览器
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
}

# 传入headers参数
response = requests.get(url, headers=headers)

# 验证请求是否成功
if response.ok:
    print("请求成功！")
    # 获取网页源码（HTML）
    html_content = response.text
    print(html_content)  # 打印源码（后续将解析）
else:
    print("请求失败，状态码：", response.status_code)
```

### 4、关键说明

* **状态码判断**：`response.ok`为`True`表示请求成功（状态码 200-299），无需手动判断具体数值。
* **响应体内容**：`response.text`以字符串形式返回网页源码（HTML），是后续解析数据的基础。
* **反爬应对**：`User-Agent`是最基础的反爬绕过方法，复杂网站可能需要更多策略（如 Cookie、IP 代理等）。

---

---
url: /Python/基础语法/Python基础语法.md
---

# Python基础语法

## 学习资料

视频资料：<https://www.bilibili.com/video/BV1944y1x7SW>

## 安装环境

安装Python解释器：[https://www.python.org](about:blank)

测试安装是否成功

![img](/assets/image-1731600886741.BQxMxw8f.png)

安装PyCharm编辑器：<https://www.jetbrains.com/pycharm/download/>

## 创建第一个项目

新建项目

![img](/assets/image-1731600896449.CTXYYE2w.png)

填写项目名称，选择一个项目位置，基础Python选择前面安装的版本，点击创建

![img](/assets/image-1731600905338.DDjWnzWO.png)

项目会默认创建一个venv文件夹，标识这个项目独立的P原合同虚拟环境，它存在的目的是不同的项目使用不同的Python版本。

![img](/assets/image-1731600915076.BrKCzLgy.png)

## 字符串操作

```python
# 字符串连接
print("hello" + "world！" + "!")
print('hello' + 'world！' + '!')

# 单双引号转义 \
print('He said "good"')
print("He said 'good'")
print("He said \"Let\'s go!\"")

# 换行 \n
print("Hello!\nHi!")

# 三引号跨行字符串
# 三组单引号或三组双引号，用三引号包裹住内容，Python会把新的一行当成内容的换行而不是代码语句的结束
print("""君不见高堂明镜悲白发，朝如青丝暮成雪。
人生得意须尽欢，莫使金樽空对月。
天生我材必有用，千金散尽还复来。
烹羊宰牛且为乐，会须一饮三百杯。
岑夫子，丹丘生，将进酒，杯莫停。
与君歌一曲，请君为我倾耳听。
钟鼓馔玉不足贵，但愿长醉不复醒。
古来圣贤皆寂寞，惟有饮者留其名。
陈王昔时宴平乐，斗酒十千恣欢谑。
主人何为言少钱，径须沽取对君酌。
五花马，千金裘，
呼儿将出换美酒，与尔同销万古愁。""")
```

## 变量

```python
greet = "你好！吃了吗，"
greet_chinese = greet
greet_english = "Yo what's up, "
greet = greet_english
print(greet + "张三")
print(greet_chinese + "张三")
```

## 变量命名规则

变量名命名规则

1. 只能由文字、数字、下划线组成
2. 不能有除下划线之外的符号
3. 不能有空格
4. 不能数字开头

Python3开始支持中文变量名，但控制台、日志不一定兼容，不推荐。

Python命名约定俗成是使用下划线命名法：

1. 字母全部小写
2. 不同单词用下划线分隔开

命名注意事项：

1. 变量名大小写敏感，大写跟小写会被看做两个不同的变量
2. 变量名不要占用Python的关键字

## 数学运算

Python中的符号说明：加（+）、减（-）、乘（\*）、除（/）

运算顺序：括号()、乘方\*\*、\*/、+-

复杂运算需要导入math函数库

math函数官方文档：<https://docs.python.org/zh-cn/3/library/math.html>

```python
import math

# 一元二次求根公式计算器
a = -1
b = -2
c = 3
print((-b + (b ** 2 - 4 * a * c) ** (1 / 2)) / (2 * a))
print((-b - (b ** 2 - 4 * a * c) ** (1 / 2)) / (2 * a))

print((-b + math.sqrt(b ** 2 - 4 * a * c)) / (2 * a))
print((-b - math.sqrt(b ** 2 - 4 * a * c)) / (2 * a))

delta = b ** 2 - 4 * a * c
print((-b + math.sqrt(delta)) / (2 * a))
print((-b - math.sqrt(delta)) / (2 * a))
```

## 注释

```python
# 单行注释 快捷键：control+/ 或 command+/

"""
多行注释
多行注释
多行注释
"""
```

## 数据类型

| 数据类型          | 示例           |
| ----------------- | -------------- |
| 字符串 str        | "hello" "呦！" |
| 整数 int          | 6 -32          |
| 浮点数 float      | 6.0 10.07      |
| 布尔类型 bool     | True False     |
| 空值类型 NoneType | None           |

```python
# 对字符串求长度
s = "hello world!"
print(len(s))

# 通过索引获取单个字符
print(s[0])
print(s[11])
print(s[len(s) - 1])

# 布尔类型
b1 = True
b2 = False

# 空值类型
n = None

# type函数
print(type(s))
print(type(b1))
print(type(n))
print(type(1.5))

# 错误测试：对布尔类型使用len函数
len(b1)
```

## 交互模式

不需要创建任何Python文件就可以运行

不需要用print就可以看到返回结果

所有输入的指令都不会被保存

## 获取输入

input函数可以获取输入，并把内容返回

```python
# 写法
input("这里是给用户的一些提示")
# 定义变量接收用户输入
user_age = input("请输入您的年龄")
print("知道了，你今年" + user_age + "岁了！")

# BMI = 体重 / (身高 ** 2)
user_weight = float(input("请输入您的体重（单位：kg）："))
user_height = float(input("请输入您的身高（单位：m）："))
user_BMI = user_weight / user_height ** 2
print("您的BMI值为：" + str(user_BMI))
```

## 条件语句

```python
mood_index = int(input("对象今天的心情指数是："))
if mood_index >= 60:
    print("恭喜，今晚应该可以打游戏！")
    print("去吧，皮卡丘！")
else:  # mood_index < 60
    print("为了自个儿小命，还是别打了。")
```

## 嵌套/多条件判断

```python
# BMI = 体重 / (身高 ** 2)
user_weight = float(input("请输入您的体重（单位：kg）："))
user_height = float(input("请输入您的身高（单位：m）："))
user_BMI = user_weight / user_height ** 2
print("您的BMI值为：" + str(user_BMI))

# 偏瘦：user_BMI <= 18.5
# 正常：18.5 < user_BMI <= 25
# 偏胖：25 < user_BMI <= 30
# 肥胖：user_BMI > 30
if user_BMI <= 18.5:
    print("此BMI值属于偏瘦范围。")
elif 18.5 < user_BMI <= 25:
    print("此BMI值属于正常范围。")
elif 18.5 < user_BMI <= 25:
    print("此BMI值属于偏胖范围。")
else:
    print("此BMI值属于肥胖范围。")
```

## 逻辑运算

Python中只有三个逻辑运算符号：与（and）、或（or）、非（not）

运算优先级：not > and > or

## 列表

```python
# 定义一个空列表
shopping_list = []
# 添加数据
shopping_list.append("键盘")
shopping_list.append("键帽")
print(shopping_list)
# 删除数据
shopping_list.remove("键帽")
print(shopping_list)

shopping_list.append("音箱")
shopping_list.append("电竞椅")
# 覆盖指定下标的数据
shopping_list[1] = "硬盘"
print(shopping_list)
print(len(shopping_list))
print(shopping_list[0])

price = [799, 1024, 200, 800]
# 列表中的最大值
max_price = max(price)
# 列表中的最小值
min_price = min(price)
# 列表排序
socket_price = sorted(price)
print(max_price)
print(min_price)
print(socket_price)

```

## 字典

字典中键的类型必须是**不可变**的。

可变数据类型：列表list；字典dict。

不可变数据类型：字符串str、整数int、浮点数float、布尔类型bool。

如果想在字典里使用列表，Python提供了**元组tuple**数据结构，元组的数据不可变，添加、删除元素等都不能操作。

元组为圆括号：`example_tuple = ("键盘", "键帽")`

字典跟列表一样也是可变的，可以添加、删除键值对

```python
# 字典创建方式
contacts = {"小明": "13700000000",
            "小花": "13700000001",
            "美女A": "18600000002"}
# 获取某个键的值
print(contacts["小明"])
# 某个键是否存在，【键 in 字典】会返回一个布尔值
print("小明" in contacts)
print("小明明" in contacts)
# 删除一个键值对，如果键不存在会报错
del contacts["小明"]
# 获取字典有多少键值对
print(len(contacts))
# 所有键
print(contacts.keys())
# 所有值
print(contacts.values())
# 所有键值对
print(contacts.items())

# 实践
# 结合input、字典、if判断，做一个查询流行语含义的电子词典程序
slang_dict = {
    "i人/e人": "2023年，一款名为MBTI的测试在年轻人中火了起来，取代星座成为最流行的社交标签。MBTI测试，即迈尔斯-布里格斯性格类型指标，是美国布莱格斯母女基于瑞士心理学家卡尔·荣格的人格分类理论研发的性格评估工具，现多被企业用来评估职员的性格特点和发展类型。“i人”和“e人”出自MBTI测试中代表“注意力方向”的两种倾向：内向内倾（Introversion）和外向外倾（Extroversion）。流行语中，“i人”泛指在社交中失去能量、一般来说性格内敛的人，而“e人”泛指在社交中获得能量、一般来说性格外向的人。这两种类型的人在社交行为上有显著差异：“i人”通常更喜欢独处，不太热衷于社交活动；而“e人”则喜欢与外部世界互动和交流。年轻人经常利用MBTI来探讨彼此的性格特点，彰显自我，接纳自我，求同存异。",
    "显眼包": "“显眼包”，原义是一个人过于张扬、爱出风头，甚至到了令人尴尬的地步。在互联网社交语境中，这个词却演变成褒义词，用来形容那些引人注目、个性鲜明的人或物。这个词的流行，部分得益于快手某网红主播。在直播中，该主播的一些出格行为和互动让粉丝们纷纷用“显眼包”来形容。“每个物种每个领域都有自己的显眼包。”这个词通过戏谑和夸张的方式，赞美那些敢于展示自我、不惧他人眼光的行为。"}
slang_dict[
    "特种兵旅游"] = "“特种兵旅游”，也作“特种兵式旅游”，指高强度的旅行方式，即在周末或节假日等有限的时间里，游览尽可能多的景点。其特点在于时间紧凑、游玩的景点多而花费低，因此广泛地受到大学生等年轻人群体的欢迎。“特种兵旅游”借用了“特种兵”这一军事领域的术语，暗示这种旅游方式的“极限挑战”特性：就像特种兵需要在严峻的环境下完成军事任务，“特种兵旅游”要快速、高效地完成旅游打卡的任务。"
slang_dict[
    "×门"] = "“×门”指一些人出于对某样东西超乎寻常的喜欢，遂聚集成一个虚拟阵营。此前多用于亚文化内部的圈子。真正让“×门（永存）”成为网络热词的，是某知名餐饮连锁品牌喜爱者自发形成的“麦门”。之后，“×门”就不再仅限于人物和饮食，诸如“猫门”“鼠门”等也在互联网上有一定数量的拥趸。值得注意的是，源自20世纪70年代的“水门事件”而产生的“名词+门”这一语法构式，此前有另外的语义——专门代指具有轰动效应的事件，大多是丑闻。"
slang_dict[
    "遥遥领先"] = "“遥遥领先”，本义是远远地走在最前面。2020年10月华为Mate40系列发布会上，余承东多次使用“遥遥领先”来形容华为手机在各项技术上与竞争对手的差距。2023年8月，华为宣布推出“HUAWEI Mate 60 Pro先锋计划”时，很多网友都用“遥遥领先”来调侃这一系列的产品。流行语中，这个词一方面被用来称赞技术创新和产品实力，另一方面也被用来微讽某种夸张的说话风格。"
slang_dict[
    "多巴胺××"] = "“多巴胺 ××”是在2023年夏天走红的网络流行语，起源于“多巴胺穿搭”，指运用高饱和色彩展现鲜艳明亮的服装风格。多巴胺原是一种神经传导物质，其特性是：当其分泌旺盛时，人会感到快乐和满足。“多巴胺穿搭”就是通过明亮的色彩搭配，刺激视觉感官，让人产生快乐和自信的感觉。后来出现了“多巴胺美甲”“多巴胺发型”“多巴胺文学”“多巴胺音乐”等多个说法。“多巴胺 ××”的趣味性在于，它将科学术语与时尚生活相结合，引导关注正向的情绪和积极的事物，以此对抗焦虑、阴暗的情绪。秋冬季还出现了借用化学反应名称描述棕色系服装的流行语——“美拉德穿搭”，也有着相似的趣味。"
slang_dict[
    "孔乙己文学"] = "“学历不但是敲门砖，也是我下不来的高台，更是孔乙己脱不下的长衫。”这是网友“失意书生”的独白，引来大量年轻人的共情，且登上了热搜。孔乙己是鲁迅小说中的一个角色，他在“四书”“五经”中耗尽年华，甚至沦落到乞讨度日却仍不肯脱下象征读书人身份的长衫。"
slang_dict[
    "公主/王子，请××"] = "“公主，请××”，源自抖音平台某博主的一个创作。视频里，一位父亲用粉色电瓶车接女儿，女儿要求父亲说一句“公主，请上车”才肯上车，结果父亲觉得是无理取闹，独自骑着电瓶车就走了。“公主，请上车”这句话最早来自电影《罗马假日》记者乔·布莱德利邀请安妮公主时的台词。短视频的这个梗在网络上迅速被网友接受，并进行了广泛的模仿和二次创作，比如 “公主请上班” “公主请下单”等。2023年11月，一款单车在用户扫码后会提示语音“尊贵的公主请上车”，让这个网络语流传得更广。“公主，请××”表达了女性寻求尊重和礼貌的心理。后来还衍生出男性的版本“王子，请××”。"
slang_dict[
    "你人还怪好的（嘞）"] = "“你人还怪好的（嘞）”，最早出自某视频博主。在视频中他模拟大学生心思单纯，轻易相信人贩子说的话，还不忘加上“您人还怪好的嘞”的称赞。之后，多个视频博主创作类似作品，场景大多是在火车站等公共场合，大学生要上洗手间或者有事走开的时候，无条件地信任陌生人，请他们帮忙看包，然后称赞对方一句：“你人还怪好嘞！”这句流行语背后反映的是涉世未深的大学生对于和谐人际关系与美好社会的想象与期待。在评论区走红之后，却常被用来吐槽一些明明伤害自己却又只能称赞对方的尴尬瞬间，比如一网友被蚊子咬了一手包的时候，会这么调侃：“蚊子，你人还怪好的嘞，请我吃自助大餐！”"
slang_dict[
    "挖呀挖呀挖"] = "“在小小的××里挖呀挖呀挖”，是由武汉幼师黄老师在抖音平台上传的童谣《花园种花》中的歌词句式。视频中，黄老师带领幼儿园的小朋友们用甜美的歌声和可爱的动作一起唱这首歌。原歌词“在小小的花园里面挖呀挖呀挖，种小小的种子，开小小的花”，通过重复的“小小”的用词和重复押韵的节奏，容易被人们记忆并模仿。随后各行各业的人开始以这首歌为模板，进行二次创意改编，多用来感叹工作的辛苦与疲惫。例如，打工人版：“在小小的公司里挖呀挖呀挖，挣得少少的工资，根本不够花。”调休版：“在短短的假期里面挖呀挖呀挖，在长长的调休日里乏呀乏呀乏。”此梗还与源于王宝钏故事的网络用语“挖野菜”结合，用以警醒年轻人不能盲目追求爱情，避免因为一些冲动行为而造成不好的结果。"

query = input("请输入您想要查询的流行语")
if query in slang_dict:
    print("您查询的" + query + "含义如下")
    print(slang_dict[query])
else:
    print("您查询的流行语暂未收录。")
    print("当前本词典收录词条数为：" + str(len(slang_dict)) + "条。")

```

## for循环

基本结构：

`for 变量名 in 可迭代对象`

```python
# for循环写法格式
# for 变量名 in 可迭代对象:
#     # 对每个变量做一些事情
#     # ...

# 列表循环
temperature_list = [36.4, 36.6, 36.2, 38.0]
for temperature in temperature_list:
    if temperature >= 38:
        print(temperature)
        print("完球了")

# 字典循环
temperature_dict = {"111": 36.4, "112": 36.6, "113": 36.2, "114": 38.0}
for staff_id, temperature in temperature_dict.items():
    if temperature >= 38:
        print(staff_id)

# 字典循环
temperature_dict = {"111": 36.4, "112": 36.6, "113": 36.2, "114": 38.0}
for temperature_tuple in temperature_dict.items():
    staff_id = temperature_tuple[0]
    temperature = temperature_tuple[1]
    if temperature >= 38:
        print(staff_id)

# for 结合 range
for i in range(5, 10):
    print(i)  # 打印5到9，不包括10

# for 结合 range(起始值, 结束值, 步长)
for i in range(5, 10, 2):
    print(i)  # 打印5、7、9

# 计算1到100，秒杀高斯
total = 0
for i in range(1, 100):
    total = total + i
print("1到100的和为：" + str(total))

```

## while循环

for与while区别

1. for：有明确循环对象或次数
2. while：循环次数未知

```python
# while 基本格式
# while 条件A:
#     行动B
from unittest import removeResult

from setuptools.package_index import user_agent


# measure_brightness 函数返回当前测量的天空亮度
def measure_brightness():
    return 0


# 拍照片
def take_photo():
    pass


# for 循环写法
for i in range(100):
    # measure_brightness 函数返回当前测量的天空亮度
    if measure_brightness() >= 500:
        # 拍照片
        take_photo()

# while 改造后写法
# measure_brightness 函数返回当前测量的天空亮度
while measure_brightness() >= 500:
    # 拍照片
    take_photo()

# 使用while编写求平均值的计算器
print("哈喽呀！我是一个求平均值的程序。")
total = 0
count = 0
user_input = input("请输入数字（完成所有数字输入后，请输入q终止程序）：")
while user_input != "q":
    num = float(user_input)
    total += num
    count += 1
    user_input = input("请输入数字（完成所有数字输入后，请输入q终止程序）：")
if count == 0:
    result = 0
else:
    result = total / count
print("您输入的数字平均值为" + str(result))

```

## format格式化

```python
year = "虎"
name = "小明"

# format 方法，指定替换位置
message_content = """
新岁甫至，福气东来。
金{0}贺岁，欢乐祥瑞。
金{0}敲门，五福临门。
给{1}及家人拜年啦!
新春快乐，{0}年大吉!
""".format(year, name)
print(message_content)

# format 方法，指定替换对象
message_content = """律回春渐，新元肇启。
新岁甫至，福气东来。
金{current_year}贺岁，欢乐祥瑞。
金{current_year}敲门，五福临门。
给{current_name}及家人拜年啦!
新春快乐，{current_year}年大吉!
""".format(current_year=year, current_name=name)
print(message_content)

# f 字符串
current_year = "虎"
current_name = "王"
message_content = f"""律回春渐，新元肇启。
新岁甫至，福气东来。
金{current_year}贺岁，欢乐祥瑞。
金{current_year}敲门，五福临门。
给{current_name}及家人拜年啦!
新春快乐，{current_year}年大吉!
"""
print(message_content)

# 数字对字符串进行格式化
gpa_dict = {"小明": 3.251, "小花": 3.869, "小李": 2.683, "小张": 3.685}
for name, gpa in gpa_dict.items():
    print("{0}你好，你的当前绩点为：{1:.2f}".format(name, gpa))
print("=====================")
for name, gpa in gpa_dict.items():
    print(f"{name}你好，你的当前绩点为：{gpa:.2f}")
```

## function函数

函数没有`return`语句，默认为`return None`

```python
# 定义一个函数，计算扇形的面积
def calculate_sector(central_angle, radius):
    # 接下来是一些定义函数的代码
    sector_area = central_angle / 360 * 3.14 * radius ** 2
    print(f"此扇形面积为：{sector_area}")
    return sector_area


# 调用函数
sector_area_1 = calculate_sector(160, 30)
sector_area_2 = calculate_sector(60, 15)
sector_area_3 = calculate_sector(30, 16)

"""
写一个计算BMI的函数，函数名为 calculate_BMI。
1. 可以计算任意体重和身高的BMI值
2. 执行过程中打印一句话，“您的BMI分类为：xx”
3. 返回计算出的BMI值

BMI = 体重 / (身高 ** 2)

BMI分类
偏瘦: BMI <= 18.5
正常: 18.5 < BMI <= 25
偏胖: 25 < BMI <= 30
肥胖: BMI > 30
"""


def calculate_BMI(weight, height):
    BMI = weight / height ** 2
    if BMI <= 18.5:
        category = "偏瘦"
    elif 18.5 < BMI <= 25:
        category = "正常"
    elif 25 < BMI <= 30:
        category = "偏胖"
    else:
        category = "肥胖"
    print(f"您的BMI分类为：{category}")
    return BMI


calculate_BMI(70, 1.8)
```

## 引入模块

内置函数、内置类型：无需引入，只需`import`

内置模块：无需引入，只需`import`

第三方模块：需要安装+引入，需要`pip install`

Python官方标准库：https://docs.python.org/zh-cn/3/library/index.html

第三方库：https://pypi.org/

引入模块方式如下

```python
# 引入模块方式一：import 语句
import statistics

print(statistics.median([19, -5, 36]))
print(statistics.mean([19, -5, 36]))

# 引入模块方式二：from...import... 语句
from statistics import median, mean

print(median([19, -5, 36]))
print(mean([19, -5, 36]))

# 引入模块方式三：from...import* 语句
from statistics import *

print(median([19, -5, 36]))
print(mean([19, -5, 36]))

# 引入第三方库 akshare 财经数据库
# 提前 pip install akshare
import akshare

# 使用 get_czce_daily 函数获取2024年2月22日中国金融期货交易所交易数据
print(akshare.get_czce_daily("20240222"))
```

## 面向对象

面向对象编程，简称OOP（Object Oriented Programming）

与之相对的是面向过程编程（函数）：将事情拆分成步骤，依次完成。

面向对象编程以对象为核心，并不会聚焦于每一步，而是模拟真实世界，考虑各个对象有什么性质、能做什么事情，提取性质，定义类，使用类创建对象。

类与对象之间的关系是：**类是创建对象的模板，对象是类的实例**。

对象能够绑定属性（Attribute）、方法（Method）。

属性对应对象拥有的性质，方法对应对象能做些什么。

属性就是放在类里面的变量，方法就是放在类里面的函数。

面向过程是编年体，面向对象是纪传体。

面向对象三大特性：**封装、继承、多态**。

## 类

Python定义类名不同于定义普通变量用的下划线命名法，使用的是**Pascal命名法**，例子：UserAccount、CustomerOrder

基本 语法

```python
# 基本语法
# class NameOfClass:
#     # 接下来是一些定义类的代码
#     # ...

# 示例一
# 定义可爱猫猫类
class CuteCat:
    # 构造函数，self 表示对象自身
    def __init__(self):
        # 定义猫猫的名字属性，一定要加self，不然会被Python认为是变量
        self.name = "Lambton"


# 创建对象
cat1 = CuteCat()
print(cat1.name)


# 示例二
# 定义可爱猫猫类，构造函数增加参数
class CuteCat2:
    def __init__(self, cat_name):
        self.name = cat_name


# 创建对象
cat1 = CuteCat2("Jojo")
print(cat1.name)


# 示例三
# 定义可爱猫猫类，构造函数增加更多属性
class CuteCat3:
    def __init__(self, cat_name, cat_age, cat_color):
        self.name = cat_name
        self.age = cat_age
        self.color = cat_color


# 创建对象
cat1 = CuteCat3("Jojo", 2, "橙色")
print(f"小猫{cat1.name}的年龄是{cat1.age}岁，花色是{cat1.color}")
```

类中定义方法

```python
# 定义对象中的方法
class CuteCat:
    def __init__(self, cat_name, cat_age, cat_color):
        self.name = cat_name
        self.age = cat_age
        self.color = cat_color

    def speak(self):
        print("喵" * self.age)

    def think(self, content):
        print(f"小猫{self.name}在思考{content}...")


cat1 = CuteCat("Jojo", 3, "橙色")
cat1.speak()
cat1.think("现在去抓沙发还是去撕纸箱")


# 实践
# 定义一个学生类
# 要求：
# 1. 属性包括学生姓名、学号，以及语数英三科成绩
# 2. 能够设置学生某科目的成绩
# 3. 能够打印出该学生所有科目的成绩

class Student:
    def __init__(self, name, student_id):
        self.name = name
        self.student_id = student_id
        self.grades = {"语文": 0, "数学": 0, "英语": 0}

    def set_grade(self, course, grade):
        if course in self.grades:
            self.grades[course] = grade

    def print_grades(self):
        print(f"学生{self.name}（学号：{self.student_id}）的成绩为：")
        for course in self.grades:
            print(f"{course}：{self.grades[course]}分")


chen = Student("小陈", "100618")
print(chen.name)
zeng = Student("小曾", "100622")
zeng.set_grade("语文", 92)
zeng.set_grade("数学", 94)
zeng.print_grades()
```

## 类继承

什么时候用继承？

A是B，class A(B)

人类是动物，class Human(Animal)

新能源是车，class ElectricCar(Car)

```python
# 类继承练习：人力系统
# - 员工分为两类：全职员工 FullTimeEmployee、兼职员工 PartTimeEmployee。
# - 全职和兼职都有"姓名 name"、"工号 id"属性，
#   都具备"打印信息 print_info"（打印姓名、工号）方法。
# - 全职有"月薪 monthly_salary"属性，
#   兼职有"日薪 daily_salary"属性、"每月工作天数 work_days"的属性。
# - 全职和兼职都有"计算月薪 calculate_monthly_pay"的方法，但具体计算过程不一样。

class Employee:
    def __init__(self, name, id):
        self.name = name
        self.id = id

    def print_info(self):
        print(f"员工名字：{self.name}，工号：{self.id}")


class FullTimeEmployee(Employee):
    def __init__(self, name, id, monthly_salary):
        super().__init__(name, id)
        self.monthly_salary = monthly_salary

    def calculate_monthly_pay(self):
        return self.monthly_salary


class PartTimeEmployee(Employee):
    def __init__(self, name, id, daily_salary, work_days):
        super().__init__(name, id)
        self.daily_salary = daily_salary
        self.work_days = work_days

    def calculate_monthly_pay(self):
        return self.daily_salary * self.work_days


zhangsan = FullTimeEmployee("张三", "1001", 6000)
lisi = PartTimeEmployee("李四", "1002", 230, 15)
zhangsan.print_info()
lisi.print_info()
print(zhangsan.calculate_monthly_pay())
print(lisi.calculate_monthly_pay())
```

## 文件路径

绝对路径：从根目录出发的路径，以斜杠开头（windows是以分区名+反斜杠开头，D:\）

相对路径：从一个参考位置出发

## 文件操作

使用open函数打开文件

open参数中的mode模式

* r：读取模式，只读，默认模式
* w：写入模式，只写
* a：附加模式，只写
* r+：读写模式

读取方法

* read：返回全部文件内容的字符串。
* readline：返回一行文件内容的字符串。自带换行
* readlines：返回全部文件内容组成的列表。

使用with关键字会自动关闭文件资源，不需要再f.close手动关闭

```python
# 需自行创建data.txt文件，并放在和此代码文件同一目录下，才能读出内容
# 也可以自行更改文件路径

# 1. read方法读文件
with open("./data.txt", "r", encoding="utf-8") as f:
    content = f.read()
    print(content)

# 2. readline方法读文件
with open("./data.txt", "r", encoding="utf-8") as f:
    line = f.readline()
    while line != "":
        print(line)
        line = f.readline()

# 3. readlines方法读文件
with open("./data.txt", "r", encoding="utf-8") as f:
    lines = f.readlines()
    for line in lines:
        print(line)
```

使用写入模式，如果文件不存在，会自动创建。

如果已经存在，会把原来的内容清空。

如果不想清空使用附件模式（a）

```python
# 任务1：在一个新的名字为"poem.txt"的文件里，写入以下内容：
# 我欲乘风归去，
# 又恐琼楼玉宇，
# 高处不胜寒。
with open("./poem.txt", "w", encoding="utf-8") as f:
    f.write("我欲乘风归去，\n又恐琼楼玉宇，\n高处不胜寒。\n")

# 任务2：在上面的"poem.txt"文件结尾处，添加以下两句：
# 起舞弄清影，
# 何似在人间。
with open("./poem.txt", "a", encoding="utf-8") as f:
    f.write("起舞弄清影，\n")
    f.write("何似在人间。")
```

## 异常处理

```python
try:  # 有可能产生错误的代码
    user_weight = float(input("请输入您的体重（单位：kg）："))
    user_height = float(input("请输入您的身高（单位：m）："))
    user_BMI = user_weight / user_height ** 2
except ValueError:  # 产生值错误时会运行
    print("输入不为合理数字，请重新运行程序，并输入正确的数字。")
except ZeroDivisionError:  # 产生除零错误时会运行
    print("身高不能为零，请重新运行程序，并输入正确的数字。")
except:  # 产生其它错误时会运行
    print("发生了未知错误，请重新运行程序。")
else:  # 没有错误时会运行
    print("您的BMI值为：" + str(user_BMI))
finally:  # 不管发生错误与否都会运行
    print("程序结束运行。")
```

## 测试

`assert` 断言语句，后面跟上布尔表达式，如果有错，会产生`AssertionError`断言错误。

但是断言有异常会直接中止，后面的不会执行，并不能知道后面的代码还有没有其他问题。

使用专门做测试的库，一次性跑多个测试用例。

`unittest` 测试库

测试类、测试方法以`test_`开头，因为`unittest`只会把`test_`开头的作为测试用例。

`unittest.TestCase`类的常见测试方法

| 方法                | 类似于            |
| ------------------- | ----------------- |
| assertEqual(A,B)    | assert A==B       |
| assertTrue(A)       | assert A is True  |
| assertIn(A,B)       | assert A in B     |
| assertNotEqual(A,B) | assert A != B     |
| assertFalse(A)      | assert A is False |
| assertNotIn(A,B)    | assert A not in B |

为了减少重复创建对象，可以使用`TestCase`类里的`setUp`方法。在运行各个测试方法，也就是`test_`开头的方法前，`setUp`方法都会先被运行一次

```python
'''
注意：此文件是针对以下类的测试文件。
可以在此文件同一文件夹下新建shopping_list.py，并复制以下内容到该文件：

class ShoppingList:
    """初始化购物清单，shopping_list是字典类型，包含商品名和对应价格
    例子：{"牙刷": 5, "沐浴露": 15, "电池": 7}"""
    def __init__(self, shopping_list):
        self.shopping_list = shopping_list

    """返回购物清单上有多少项商品"""
    def get_item_count(self):
        return len(self.shopping_list)

    """返回购物清单商品价格总额数字"""
    def get_total_price(self):
        total_price = 0
        for price in self.shopping_list.values():
            total_price += price
        return total_price
'''

import unittest
from shopping_list import ShoppingList


class TestShoppingList(unittest.TestCase):
    def setUp(self):
        self.shopping_list = ShoppingList({"纸巾": 8, "帽子": 30, "拖鞋": 15})

    def test_get_item_count(self):
        self.assertEqual(self.shopping_list.get_item_count(), 3)

    def test_get_total_price(self):
        self.assertEqual(self.shopping_list.get_total_price(), 55)
```

在程序目录下打开命令行，执行`python -m unittest`，就可以看到测试结果

```shell
.F
======================================================================
FAIL: test_get_total_price (test_shopping_list.TestShoppingList)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "D:\Program Files\JetBrains\python_project\python_study_project\1_basic\test_shopping_list.py", line 35, in test_get_total_price
    self.assertEqual(self.shopping_list.get_total_price(), 55)
AssertionError: 53 != 55

----------------------------------------------------------------------
Ran 2 tests in 0.001s

FAILED (failures=1)
```

## 高阶和匿名函数

高阶函数：把函数作为参数的函数，给函数提供更多灵活性。

注意：作为参数的函数，直接写函数名进行传入，表示函数其本身，不要带括号和参数，一旦有括号这个函数就被调用了，传入的就是函数的执行结果，而不是函数本身了。

```python
# 普通函数
def calculate_and_print(num, power):
    if power == 2:
        result = num * num
    elif power == 3:
        result = num * num * num
    else:
        print("只支持计算二次方和三次方")
        return
    print(f"""
    | 数字参数 | {num} |
    | 计算结果 | {result} |""")


calculate_and_print(3, 3)


# 改造后
def calculate_and_print(num, calculator):
    result = calculator(num)
    print(f"""
    | 数字参数 | {num} |
    | 计算结果 | {result} |""")


def calculate_square(num):
    return num * num


def calculate_cube(num):
    return num * num * num


def calculate_plus_10(num):
    return num + 10


def calculate_times_5(num):
    return num * 5


calculate_and_print(3, calculate_square)
calculate_and_print(7, calculate_plus_10)
calculate_and_print(7, calculate_times_5)


# 进一步改造，把格式作为参数，扩展不同打印格式
def calculate_and_print(num, calculator, formatter):
    result = calculator(num)
    formatter(num, result)


def print_with_vertical_bar(num, result):
    print(f"""
    | 数字参数 | {num} |
    | 计算结果 | {result} |""")


calculate_and_print(7, calculate_times_5, print_with_vertical_bar)
```

匿名函数

匿名函数（LAMBDA）不需要起名字，像个一次性物品一样，即用即扔

匿名函数基本格式：

```python
# 关键字 函数参数 : 函数返回表达式
lambda num1,, num2: num1 + num2

# 直接调用
(lambda num1,, num2: num1 + num2)(2, 3)
```

匿名函数局限性

冒号后面没法有多个语句表达式，只适用于比较简单的场景。

---

---
url: /Python/爬虫/3_Python解析HTML.md
---

# Python解析HTML

## 一、什么是HTML网页结构

在爬虫中，我们主要通过 HTML 获取网页数据，因其定义了网页的核心结构和信息。以下是 HTML 的基础概念及与爬虫相关的核心知识：

### 1、网页三大技术要素

* **HTML**：定义网页的**结构和内容**（如标题、段落、列表等），是爬虫的主要处理对象。
* **CSS**：定义网页的**样式**（如颜色、布局、字体），爬虫通常无需关注。
* **JavaScript**：定义网页的**交互逻辑**（如按钮点击、动态加载），复杂爬虫可能需要处理。

### 2、HTML 的基本结构

一个简单的 HTML 文档结构如下：

```html
<!DOCTYPE html>  <!-- 声明文档类型为HTML -->
<html>  <!-- 根元素，包含所有其他元素 -->
  <body>  <!-- 主体元素，包含网页可见内容 -->
    <h1>这是一级标题</h1>  <!-- 标题元素（h1-h6，字号递减） -->
    <p>这是一个段落。</p>  <!-- 段落元素 -->
  </body>
</html>
```

#### 核心概念

1. 标签：用`<>`包围的关键词（如`<html>`、`<body>`），分为：

   * **起始标签**：`<标签名>`（如`<h1>`），表示元素开始。
   * **闭合标签**：`</标签名>`（如`</h1>`），表示元素结束。

2. **元素**：起始标签 + 内容 + 闭合标签的整体（如`<p>内容</p>`）。

3. 层级关系：

   * 子元素：嵌套在其他元素内部的元素（如`是`的子元素）。

* 兄弟元素：同一层级的元素（如`和`是兄弟元素）。

### 3、查看网页的 HTML 源码

要分析目标网页的结构，可通过以下方式查看 HTML：

1. **查看网页源码**：右键网页→“查看网页源代码”，获取完整 HTML 文本。

2. 开发者工具：右键网页→“检查”（或按 F12），进入 “Elements” 面板：

   * 可实时查看鼠标点击元素对应的 HTML 标签。

* 便于定位目标数据在 HTML 中的位置（爬虫解析的关键）。

### 4、爬虫为何关注 HTML？

HTML 直接包含网页的**文本、链接、图片路径**等核心数据，例如：

* 电影名称可能在`<h3 class="title">`标签内；
* 商品价格可能在`<span class="price">`标签内。

## 二、HTML有哪些常用标签

HTML 标签定义了网页的信息结构，是爬虫提取数据的核心依据。以下是爬虫中常见的 HTML 标签及其功能，帮助快速定位目标数据：

### 1、文本与标题标签

#### 1.1. 标题标签（`<h1>`-`<h6>`）

* **功能**：定义不同层级的标题，`层级最高（通常为页面主标题），`层级最低。

* **示例**：

  ```html
  <h1>豆瓣电影Top250</h1>  <!-- 一级标题 -->
  <h2>剧情片</h2>          <!-- 二级标题 -->
  ```

#### 1.2. 段落标签（`<p>`）

* **功能**：定义文本段落，不同`<p>`标签默认换行。

* **示例**：

  ```html
  <p>这是电影简介内容...</p>
  <p>导演：张艺谋</p>
  ```

#### 1.3. 换行与文本格式化标签

* `<br>`：强制换行（单标签，无闭合标签）。

  ```html
  <p>第一行<br>第二行</p>
  ```

* `<b>`：加粗文本；`<i>`：斜体文本；`<u>`：下划线文本。

  ```html
  <b>重点内容</b>、<i>强调内容</i>
  ```

### 2、媒体与链接标签

#### 2.1. 图片标签（`<img>`）

* **功能**：插入图片（单标签，无闭合标签）。

* 核心属性：

  * `src`：图片路径（必填，可为 URL 或本地路径）；
  * `alt`：图片加载失败时的替代文本；
  * `width`/`height`：图片宽高。

* **示例**：

  ```html
  <img src="movie.jpg" alt="电影海报" width="300">
  ```

#### 2.2. 链接标签（`<a>`）

* **功能**：定义超链接，实现页面跳转。

* 核心属性：

  * `href`：目标 URL（必填，如`https://movie.douban.com`）；
  * `target`：打开方式（`_self`默认当前窗口，`_blank`新窗口）。

* **示例**：

  ```html
  <a href="https://movie.douban.com/top250" target="_blank">豆瓣Top250</a>
  ```

### 3、容器标签（`<div>`与`<span>`）

* **功能**：用于分组或包裹其他元素，便于统一样式或结构管理。

* 区别：

  * `<div>`：块级元素，独占一行（如页面分区）；
  * `<span>`：内联元素，同行显示（如段落内部分文本）。

* **示例**：

  ```html
  <div class="movie-info">  <!-- 块级容器，包含电影信息 -->
    <span class="title">肖申克的救赎</span>  <!-- 内联容器，包裹标题 -->
    <span class="rating">9.7</span>         <!-- 内联容器，包裹评分 -->
  </div>
  ```

### 4、列表标签

#### 4.1. 有序列表（`<ol>`+`<li>`）

* **功能**：定义有顺序的列表（如排名），`为列表容器，`为列表项。

* **示例**：

  ```html
  <ol>
    <li>第一名：肖申克的救赎</li>
    <li>第二名：霸王别姬</li>
  </ol>
  ```

#### 4.2. 无序列表（`<ul>`+`<li>`）

* **功能**：定义无顺序的列表（如标签、类别），列表项前通常为符号（圆点、方块等）。

* **示例**：

  ```html
  <ul>
    <li>剧情</li>
    <li>犯罪</li>
  </ul>
  ```

### 5、表格标签（`<table>`系列）

* **功能**：定义表格，常用于展示结构化数据（如统计信息）。

* 核心标签：

  * `<table>`：表格容器；
  * `<thead>`：表格头部（通常为表头行）；
  * `<tbody>`：表格主体（数据行）；
  * `<tr>`：表格行；
  * `<td>`：单元格数据。

* **示例**：

  ```html
  <table border="1">  <!-- border属性添加边框 -->
    <thead>
      <tr><td>排名</td><td>电影名</td></tr>  <!-- 表头行 -->
    </thead>
    <tbody>
      <tr><td>1</td><td>肖申克的救赎</td></tr>  <!-- 数据行 -->
    </tbody>
  </table>
  ```

### 6、通用属性：`class`

* **功能**：为元素定义类名，用于分组或区分同类元素（所有标签均可使用）。

* **示例**：

  ```html
  <p class="intro">电影简介...</p>
  <p class="review">用户评论...</p>
  ```

* **爬虫关注点**：通过类名快速定位特定类型的元素（如提取所有`class="review"`的评论内容）。

## 三、练习HTML常见标签

### 1、准备工作：选择编辑器与创建 HTML 文件

1. **编辑器选择**：推荐使用 WebStorm 或 VS Code（支持代码高亮、自动缩进），也可使用记事本（需手动保存为`.html`后缀）。
2. **新建文件**：创建名为`practice.html`的文件，所有代码将写入此文件。

### 2、基础结构：HTML 文档的 “骨架”

所有 HTML 内容需包裹在固定的基础结构中，确保浏览器正确识别：

```html
<!DOCTYPE html>  <!-- 声明文档类型为HTML -->
<html>  <!-- 根元素，包含所有其他元素 -->
  <head>  <!-- 文档头部：存放网页元信息（非可见内容） -->
    <title>HTML实战练习</title>  <!-- 浏览器选项卡标题 -->
  </head>
  <body>  <!-- 文档主体：存放网页可见内容（标签、文本、图片等） -->
    <!-- 以下将写入各类标签 -->
  </body>
</html>
```

* **效果验证**：保存文件后，双击或拖入浏览器，可看到浏览器选项卡显示 “HTML 实战练习”，页面主体为空白（待添加内容）。
* **缩进规则**：子元素缩进比父元素多 2-4 个空格（如`head`和`body`缩进于`html`，`title`缩进于`head`），层级更清晰。

### 3、实战 1：文本与标题标签

#### 3.1. 标题标签（`<h1>`-`<h6>`）

在`body`中添加不同层级的标题：

```html
<body>
  <h1>一级标题（最大）</h1>
  <h2>二级标题</h2>
  <h3>三级标题</h3>
  <h6>六级标题（最小）</h6>
  <h7>七级标题（无效，显示为普通文本）</h7>  <!-- 无此标签 -->
</body>
```

* **效果**：刷新浏览器，标题字号随层级递减，`h7`无特殊样式（视为普通文本，建议删除）。

#### 3.2. 段落与换行标签（`<p>`、`<br>`）

添加文本段落并控制换行：

```html
<body>
  <!-- 标题标签省略，接下文 -->
  <p>这是第一个段落。即使在代码里换行，网页也不会换行，只会显示为空格。</p>
  <p>这是第二个段落（自动换行）。</p>
  <p>这是第三个段落<br>用BR标签强制换行（单标签，无闭合）。</p>
</body>
```

* **效果**：`p`标签自动分隔段落，`br`标签实现段落内强制换行。

#### 3.3. 文本格式化标签（`<b>`、`<i>`、`<u>`）

对文本进行加粗、斜体、下划线处理：

```html
<body>
  <!-- 前文标签省略，接下文 -->
  <p>这是<b>加粗文本</b>，这是<i>斜体文本</i>，这是<u>下划线文本</u>。</p>
</body>
```

* **效果**：对应文本分别呈现加粗、斜体、下划线样式。

### 4、实战 2：媒体与链接标签

#### 4.1. 图片标签（`<img>`）

插入图片并控制尺寸（需指定`src`属性，单标签）：

```html
<body>
  <!-- 前文标签省略，接下文 -->
  <!-- src为图片URL或本地路径，width/height控制尺寸（单位：像素px） -->
  <img src="https://picsum.photos/200/300" alt="示例图片" width="200" height="150">
</body>
```

* 参数说明：
  * `src`：图片来源（示例使用免费图片链接，本地图片需写路径如`./images/pic.jpg`）；
  * `alt`：图片加载失败时显示的替代文本；
  * `width/height`：可选，控制图片大小。
* **效果**：刷新后显示指定尺寸的图片。

#### 4.2. 链接标签（`<a>`）

添加超链接，控制跳转方式：

```html
<body>
  <!-- 前文标签省略，接下文 -->
  <!-- 1. 当前窗口跳转（默认） -->
  <a href="https://www.baidu.com">百度（当前窗口打开）</a>
  <br>
  <!-- 2. 新窗口跳转（target="_blank"） -->
  <a href="https://www.douban.com" target="_blank">豆瓣（新窗口打开）</a>
</body>
```

* **注意**：`href`必须为完整 URL（含`http/https`），否则无法正常跳转。
* **效果**：点击链接，分别在当前窗口 / 新窗口打开目标网页。

### 5、实战 3：容器标签（`<div>`与`<span>`）

容器用于分组元素，配合简单 CSS 可直观区分（仅演示功能，不深入 CSS）：

```html
<body>
  <!-- 前文标签省略，接下文 -->
  <!-- 1. div：块级元素，独占一行 -->
  <div style="background-color: lightcoral; padding: 10px;">
    <p>这是div容器内的文本（独占一行）</p>
    <img src="https://picsum.photos/100/100" alt="小图">
  </div>

  <br>  <!-- 换行分隔 -->

  <!-- 2. span：内联元素，同行显示 -->
  <p>
    <span style="background-color: lightblue; padding: 5px;">这是第一个span</span>
    <span style="background-color: lightgreen; padding: 5px;">这是第二个span（同行）</span>
  </p>
</body>
```

* **效果**：`div`区域背景为浅红色，独占一行；两个`span`背景分别为浅蓝色、浅绿色，在同一行显示。

### 6、实战 4：列表标签（有序列表、无序列表）

#### 6.1. 有序列表（`<ol>`+`<li>`）

定义有顺序的列表（如排名）：

```html
<body>
  <!-- 前文标签省略，接下文 -->
  <h3>电影排名（有序列表）</h3>
  <ol>
    <li>肖申克的救赎</li>
    <li>霸王别姬</li>
    <li>阿甘正传</li>
  </ol>
</body>
```

* **效果**：列表项前自动添加数字（1、2、3）。

#### 6.2. 无序列表（`<ul>`+`<li>`）

定义无顺序的列表（如标签）：

```html
<body>
  <!-- 前文标签省略，接下文 -->
  <h3>电影类型（无序列表）</h3>
  <ul>
    <li>剧情</li>
    <li>犯罪</li>
    <li>爱情</li>
  </ul>
</body>
```

* **效果**：列表项前显示圆点（默认样式），无顺序标识。

### 7、实战 5：表格标签（`<table>`系列）

创建结构化表格，包含表头、表体和边框：

```html
<body>
  <!-- 前文标签省略，接下文 -->
  <h3>学生信息表</h3>
  <!-- border="1" 添加边框，1为边框宽度（单位：像素） -->
  <table border="1">
    <thead>  <!-- 表格头部（表头） -->
      <tr>  <!-- 表头行 -->
        <td>学号</td>  <!-- 表头单元格 -->
        <td>姓名</td>
        <td>班级</td>
      </tr>
    </thead>
    <tbody>  <!-- 表格主体（数据） -->
      <tr>  <!-- 数据行1 -->
        <td>2023001</td>
        <td>小明</td>
        <td>一班</td>
      </tr>
      <tr>  <!-- 数据行2 -->
        <td>2023002</td>
        <td>小红</td>
        <td>二班</td>
      </tr>
      <tr>  <!-- 数据行3（单元格数量可不同） -->
        <td>2023003</td>
        <td>小刚</td>
      </tr>
    </tbody>
  </table>
</body>
```

* **效果**：表格带边框，表头与数据清晰区分，第三行仅 2 个单元格（允许行数与列数不统一）。

### 8、实战 6：通用属性`class`

为元素添加类名（后续结合 CSS/JavaScript 有用，此处仅演示定义）：

```html
<body>
  <!-- 给表格添加class="student-table" -->
  <table border="1" class="student-table">
    <!-- 表格内容省略，同上文 -->
  </table>
</body>
```

### 9、关键总结

1. **效果更新**：每次修改 HTML 后，需刷新浏览器才能看到最新效果。
2. **标签规则**：大部分标签需成对（起始 + 闭合），单标签（`<br>`、`<img>`）无需闭合。
3. **爬虫关联**：实战中的标签（如`<h3>`电影名、`<table>`数据、`<a>`链接）正是爬虫后续要提取的核心目标，理解结构即可，无需记忆所有细节。

## 四、如何解析HTML内容

在获取网页 HTML 内容后，需要必要从中精准提取目标数据。Beautiful Soup 是 Python 中强大的 HTML 解析库，能高效处理复杂的 HTML 结构，轻松实现数据提取。以下是其使用方法及实战案例：

### 1、安装与引入 Beautiful Soup

#### 1.1. 安装库

Beautiful Soup 是第三方库，需先安装：

* **Windows**：在终端输入

  ```sh
  pip install bs4
  ```

* **macOS/Linux**：通常使用

  ```sh
  pip3 install bs4
  ```

**验证安装**：
若终端显示`Successfully installed bs4`，或`Requirements already satisfied`（已安装），则安装成功。

#### 1.2. 引入库

在 Python 代码中引入 Beautiful Soup 类：

```python
from bs4 import BeautifulSoup
```

### 2、解析 HTML：创建 Beautiful Soup 的基本用法

#### 2.1. 创建 Beautiful Soup 对象

假设已通过 requests 库获取网页 HTML 内容（字符串格式），存储在`content`变量中，解析步骤如下：

```python
# 假设content是通过requests.get().text获取的HTML字符串
soup = BeautifulSoup(content, "html.parser")  # 第二个参数指定解析器（解析HTML用html.parser）
```

* `soup`是解析后的的 HTML 对象，将复杂的 HTML 转换为树形结构，便于查找和提取数据。

#### 2.2. 查找元素的核心方法

##### （1）获取单个元素：`find()`

返回第一个符合条件的第一个元素，例如：

```python
# 获取第一个<p>标签
first_p = soup.find("p")
# 获取第一个<img>标签
first_img = soup.find("img")
```

##### （2）获取多个元素：`find_all()`

返回所有符合条件的元素（可迭代对象），例如：

```python
# 获取所有<h3>标签
all_h3 = soup.find_all("h3")
# 获取所有<a>标签
all_links = soup.find_all("a")
```

##### （3）按属性筛选元素

通过`attrs`参数指定标签的属性（如`class`、`id`等），精准定位元素：

```python
# 示例：获取所有class为"price_color"的<p>标签（假设价格信息在这类标签中）
price_tags = soup.find_all("p", attrs={"class": "price_color"})
```

#### 2.3. 提取元素内容

通过元素的`string`属性获取标签内的文本：

```python
# 遍历价格标签，提取文本（例如价格）
for tag in price_tags:
    price_text = tag.string  # 获取文本内容（如"£51.77"）
    pure_price = price_text[1:]  # 去除货币符号，仅保留数字（如"51.77"）
    print(pure_price)
```

### 3、实战案例：提取书名（多层级元素）

若目标数据嵌套在多层标签中（如书名在`→`标签内），需分层提取：

#### 步骤分析

1. 找到所有`标签（假设书名的父元素是`）；
2. 在每个`中找到`标签（书名所在标签）；
3. 提取\`\`标签内的文本。

#### 代码实现

```python
# 1. 获取所有<h3>标签
all_h3 = soup.find_all("h3")

# 2. 遍历每个<h3>，提取其中的<a>标签文本
book_titles = []
for h3 in all_h3:
    # 在当前<h3>中找第一个<a>标签（使用find()，因每个<h3>通常只有一个<a>）
    a_tag = h3.find("a")
    # 提取文本并添加到列表
    if a_tag:
        title = a_tag.string
        book_titles.append(title)

# 打印所有书名
for title in book_titles:
    print(title)
```

### 4、关键技巧

1. **利用浏览器开发者工具**：通过 “检查” 功能定位目标数据的标签及属性（如`class`值），为`find_all()`提供筛选条件。
2. **灵活处理嵌套结构**：若数据在多层标签中，需逐层查找（如先找父标签，再在父标签内找子标签）。
3. **字符串处理**：提取的文本可能包含多余符号（如货币符号、空格），可通过切片、`strip()`等方法清洗。

---

---
url: /Python/爬虫/5_Python使用API.md
---

# Python使用API

当爬虫因网页结构变化失效、或面临合规风险时，**API（Application Programming Interface，应用程序编程接口）** 是更可靠的数据获取方案。它是官方定义的 “软件对话说明书”，能让程序直接、高效地获取结构化数据。以下是 API 的核心知识、使用方法及与爬虫的对比：

## 一、API 是什么？

API 本质是**两个软件之间的通讯规则**，定义了 “如何通过请求获取服务端数据”。通俗来说，它就像餐厅的 “菜单”—— 用户（程序）按菜单（API 规则）点单（发送请求），厨房（服务端）按规则上菜（返回数据）。

例如：某论坛提供 API，程序可通过 API 直接获取 “最热主题”“用户信息”，无需爬取网页 HTML 再解析。

## 二、使用 API 的核心要素

要通过 API 获取数据，需明确 4 个关键信息（均来自官方 API 文档）：

### 1. API 端点（Endpoint）

* **定义**：发送请求的目标 URL，不同功能对应不同端点。
* 示例：
  * 论坛 “获取最热主题” 的端点：`https://api.v2ex.com/topics/hot.json`
  * 论坛 “获取最新主题” 的端点：`https://api.v2ex.com/topics/latest.json`

### 2. 请求方法（HTTP Method）

API 基于 HTTP 协议，常用方法与功能对应：

| 方法     | 功能                   | 示例场景             |
| -------- | ---------------------- | -------------------- |
| `GET`    | **获取数据**（最常用） | 查热门主题、用户信息 |
| `POST`   | **提交数据**           | 发布新主题、用户登录 |
| `PUT`    | **更新数据**           | 修改已发布的主题内容 |
| `DELETE` | **删除数据**           | 删除某条评论         |

### 3. 查询参数与请求体

* **查询参数**：URL 中通过`?key=value`传递的额外信息（常用于`GET`请求）。
  示例：`https://api.v2ex.com/users/show.json?username=test`（通过`username=test`指定查询用户）。
* **请求体（Body）**：包含更多数据的载体（常用于`POST`/`PUT`请求），如提交表单时的 “用户名 + 密码”。
  注：`GET`请求的请求体通常为空（数据通过查询参数传递）。

### 4. 响应格式

API 返回的数据多为**结构化格式**，便于程序直接解析：

* **JSON**（最常用）：文本格式，可直接转为 Python 的字典 / 列表；
* **XML**：标签式格式，需专用库解析（如`xml.etree.ElementTree`）。

示例 JSON 响应（论坛热门主题）：

```json
[
  {"id": 123, "title": "Python学习推荐", "content": "分享几个学习资源..."},
  {"id": 456, "title": "API vs 爬虫", "content": "哪个更适合数据获取？..."}
]
```

### 额外注意：认证与授权

部分 API 需 “身份验证”（如 API 密钥、Token），仅授权用户可访问。例如：

* 请求头中携带密钥：`headers={"Authorization": "Bearer your_token"}`
* 官方文档会明确认证方式（如 API Key、OAuth2.0）。

## 三、用 Python 调用 API 的实战步骤

API 基于 HTTP 协议，可直接用`requests`库发送请求，步骤如下：

### 1. 准备工作

* 查看官方 API 文档，确认端点、请求方法、参数；

* 导入所需库（`requests`用于发请求，`json`用于解析响应）：

  ```python
  import requests
  import json
  ```

### 2. 发送 API 请求（以`GET`为例）

以 “获取 V2EX 热门主题” 为例：

```python
# 1. API端点（从文档获取）
url = "https://www.v2ex.com/api/topics/hot.json"

# 2. 发送GET请求（若需查询参数，可加params参数）
response = requests.get(url)

# 3. 判断请求是否成功（2xx状态码表示成功）
if response.status_code >= 200 and response.status_code < 300:
    print("请求成功！")
    # 4. 解析JSON响应（response.text是JSON字符串）
    data = json.loads(response.text)  # 转为Python列表/字典
else:
    print(f"请求失败，状态码：{response.status_code}")
```

### 3. 提取目标数据

解析后的`data`是列表 / 字典，直接用 Python 语法提取信息：

```python
# 提取第3个热门主题的标题和内容（索引从0开始，索引2对应第3个）
third_topic = data[2]
print("第3个热门主题标题：", third_topic["title"])
print("第3个热门主题内容：", third_topic["content"])
```

## 四、API vs 爬虫：核心优势

相比爬虫，API 是更优的选择，核心优势如下：

| 对比维度         | API                                       | 爬虫                                              |
| ---------------- | ----------------------------------------- | ------------------------------------------------- |
| **可靠性**       | 官方提供，不受网页外观更新影响            | 网页结构变化（如标签改 class）则失效              |
| **数据解析难度** | 结构化格式（JSON/XML），直接转列表 / 字典 | 需从 HTML 中定位标签（如 Beautiful Soup），易出错 |
| **合规性**       | 官方允许，合法合规                        | 可能违反网站使用条款（如反爬规则）                |
| **数据完整性**   | 可能提供网页未显示的隐藏数据              | 仅能获取网页可见内容                              |
| **效率**         | 请求直接返回目标数据，速度快              | 需下载完整 HTML，解析步骤多                       |

## 五、为什么还要学爬虫？

API 虽好，但并非万能：

1. **并非所有网站都提供 API**：多数中小型网站或小众服务无官方 API；
2. **API 可能有限制**：即使有 API，部分关键数据（如付费内容）可能不开放；
3. **API 可能有调用门槛**：部分 API 需申请密钥、限制调用频率（如每日 1000 次）。

因此，**API 是首选，但爬虫仍是必要补充**—— 当 API 不可用时，爬虫是获取数据的备选方案。

通过 API 获取数据，能规避爬虫的诸多痛点，是企业和开发者的主流选择。掌握 API 调用，可大幅提升数据获取的效率与可靠性。

---

---
url: /Python/数据分析/1_Python数据分析.md
---

# Python数据分析

## Jupyter Notebook介绍

### 一、定义

Jupyter Notebook是一个基于网页的交互式计算环境，是数据分析、数据科学，甚至机器学习领域里非常流行的一款工具。

可以用来编写代码、运行代码、查看输出、可视化数据，并分享输出的报告文档。

### 二、优点

#### 1、Jupyter Notebook可以按单元格运行代码

对于搞数据的人来说，不是所有时候都想从头运行到结尾，比如数据量特别大的时候，假如读取数据要等几秒，清洗数据要等几秒，那在我们每次修改分析公式，想反复运行看效果的时候，不希望前面没有改动的步骤，比如说读取数据，还要反复被运行，因为这会浪费很多等待时间。

用Jupyter Notebook就很简单了，我们可以把不同步骤放在不同单元格里，每次运行一个单元格的代码，这样我们可以只读取一遍数据，当反复修改和运行分析代码时，读数据的代码就不会再被运行了。

#### 2、可展示的信息格式更丰富

我们用常规编辑器时，注释和代码一样都是纯文本，但分析数据时，有时需要记录和解释更多东西，比如数据的背景、使用的公式、分析思路等等。

用Jupyter Notebook可以用Markdown标记语言，让注释更加清晰、有层次，还可以用LaTex插入公式。

当你把Jupyter Notebook上的内容，以HTML等格式分享给其他人的时候，这些效果丰富的文字，也会原封不动地展示给对方，帮助对方更好地理解你思考和分析的过程，也节约了你解答疑问的时间。

#### 3、交互式运行环境

交互模式相比命令行模式的好处是，当我们想查看输出的时候，不需要加上打印语句就能看到。

那我们就可以很方便地查看变量的值,输出中间结果，有利于快速探索数据，试验不同分析方法。

## Jupyter Notebook安装

命令行下使用命令安装`pip install notebook`

macos使用`pip3 install notebook`

安装完成后，运行启动命令`jupyter notebook`，会自动在浏览器打开一个窗口

![](/assets/image-1733845385949.bW6HQ5eO.png)

jupyter notebook界面

![](/assets/image-1733845392947.C-Kq7-xc.png)

## Jupyter Notebook使用

### 一、启动Jupyter Notebook

1. 启动
   1. Windows系统，在菜单栏搜索CMD，点击命令提示符。
   2. macOS系统，点击顶部菜单栏的放大镜，输入"终端"或"terminal"，回车进入。
   3. 出现大黑窗口后，输入Jupyter Notebook的启动命令："Jupyter-Notebook"。
   4. 默认浏览器会自动打开一个网页，展示Notebook的主面板。
2. 如果不小心关闭了JN的网页，地址可以再CMD或终端里找到，其中某一行："The Jupyter Notebook is running at..."，后面跟着的就是页面地址，复制到浏览器即可。
3. 接下来的时间里，记得不要关闭这个输入了启动命令的CMD或终端，否则JN会被终止。

### 二、创建Jupyter Notebook文件

#### 1、创建文件

希望文件在什么位置，就点进那个文件夹，然后点击New，Notebook，一个新的编辑界面就会被打开，而且在桌面上也能看到一个全新的文件出现了。

#### 2、重命名文件

在编辑界面，点下标题，输入想要的名字。

### 三、Jupyter Notebook编辑界面

标题下面分别是菜单栏、工具条以及单元格。

工具条就是把菜单栏里一些最常用的操作摆出来，所以大部分时候我们只需要通过工具条和单元格打交道，单元格主要用来写Python代码和文字。

#### (一)、编辑模式和命令模式

##### 1、编辑模式

在我们点击单元格里面后，外框会变成绿色，表示当前是编辑模式，

##### 2、命令模式

完成输入后，点Esc键，或者鼠标点下其它地方，外框会变成蓝色，表示当前是命令模式

#### (二)、工具栏

![](/assets/image-1733845404862.DbBMYnX2.png)

1. 第一个按钮，表示保存文件内容。
2. 第二个加号按钮，表示在当前选中的单元格下面，新建一个单元格。
3. 接下来三个按钮，分别表示剪切选中的单元格、复制选中的单元格，以及粘贴选中的单元格，还可以按住Shift键选中多个单元格，然后同一进行操作。
4. 上箭头表示把选中的单元格往上移动一格，下箭头表示往下移动一格，来更改单元格顺序。
5. 运行按钮
   1. 会执行这个单元格里面所有Python代码。
   2. 执行时，左边方括号会展示星号，表示正在运行。
   3. 执行完毕后，方括号里面会变成数字。
   4. 数字表示的是执行顺序，比如运行完第一个单元格后，旁边数字显示1；继续运行下一个单元格，旁边数字就会显示2。
   5. JN很灵活的一点是，你可以用任意顺序运行单元格
      1. 比如可以运行第三格后，回到第一格再执行一遍；也可以多次反复运行同一个单元格。旁边的数字，会帮忙记录和告知执行过的顺序。
   6. 顺序是很关键的
      1. 比如你分别在第二和第三个单元格里，写了读取和查看数据的代码，想要修改读取的文件，需要修改和再次运行读取数据的代码。
      2. 这时，第二个单元格的数字大于第三个单元格，就能侧面提醒我们，第二个单元格里，查看数据来输出的代码还没有被更新，查看的还是之前的数据文件。
      3. 所以应该把第三个单元格也运行一次。
      4. 另：代码单元格里的代码是通过交互模式运行的，也就是说可以不需要print语句，就能直接看到执行输出的结果。
      5. 但是如果单元格里有多条输出语句，只有最后一项的输出会被展示，我们还是要借助print，才能同时展示多项输出结果。
6. 终止执行按钮
   1. 执行单元格里代码的过程中，想要中断的话，就可以点击它。
7. 重启按钮
   1. 这会帮我们清空所有定义过的变量，而且单元格旁边的数字也会重新从1开始，表明重启过。
   2. 举个例子，假如第一格定义了一个变量，第二格输出这个变量的值，那运行第一格后变量的值就已经被储存到内存里了，每次输出第二格就会输出对应的值。
   3. 但重启后，再运行第二格，就会提醒我们变量不存在了。
8. 重启并重新运行所有单元格的按钮
   1. 非常使用，如果你想看自己写的所有代码，从上往下完整执行一遍的输出，就用这个操作。
   2. 它可以帮我们检查单元格顺序是否有问题。
9. 下拉框，可以让我们切换单元格里的内容。最常用的就是代码和Markdown，单元格并不限于写代码，也可以写文字。
   1. Markdown
      1. 是一种帮助我们为内容增加样式的标记语言，语法简单。
      2. 通过在前面添加1~6个#和1个空格，可以把文字设置成一至六级标题。
   2. 公式
      1. 在行内插入公式，就用1个美元符号，包裹住那个公式。
      2. 要插入一个独占一行的公式，就用2个美元符号，包裹住那个公式。
      3. 复杂的公式，可以用LaTex语法来表示。
10. 键盘按钮，是快捷键配置
    1. 掌握快捷键的使用，可以大大提升我们使用JN的效率，\*在命令模式下使用。
    2. A键，可以在当前单元格上方插入一个新的单元格。
    3. B键，可以在当前单元格下方插入。
    4. 连按两次D，可以删除当前选中的单元格。
    5. Shift+Enter，运行当前单元格，并跳到下一个单元格。

### 三、分享Jupyter Notebook

#### 1、可以自行编辑和运行

如果对方使用JN，可以把这个以.ipynb为后缀的文件，直接发给对方。

#### 2、只读

点击File，选择Save and Export Notebook as，有很多选项。

比如HTML，这是针对网页的标记语言，所以对方可以直接用浏览器打开，所有代码以及Markdown文字都会原封不动得展示出来。

### 四、打开之前创建过的Jupyter Notebook

启动JN，进入存放notebook的目录，点击.ipynb的文件。

## Markdown语法

### 一、优势

#### 1、格式比纯文本更丰富

代码中的注释没法添加任何格式，或添加丰富的信息。

Markdown支持标题、粗体、引用、列表、代码块等常用格式。

#### 2、体积比富文本更轻量

Word或者一些网站支持的富文本编辑器，可以让我们更改内容样式或排版。但它的功能过于强大多样，让产出文件更加臃肿。

不支持自定义字体、颜色等操作，所有样式都是通过简单的符号来添加的。因此Markdown文件非常轻量，和纯文本差不了多少。

代码包的README，文件后缀一般都是.md，说明是一个Markdown文件。

### 二、语法

#### 1、标题

通过在前面添加1~6个#和1个空格，可以把文字设置成一至六级标题。

#### 2、加粗、斜体、删除样式

用两个\*把文字包住，把文字变成粗体。

用一个\*把文字包住，把文字变成斜体。

用两个~把文字包住，把文字用删除线划掉。\*注意是英文输入法下的小波浪。

#### 3、普通文字

不把文字用任何符号包围的话，那就默认是普通的段落文字。

特点：加的换行，只会在文字之间出现一个空格。

如果想让文字分隔在不同行，一个方法是多打一次换行，另一个方法是在第一行后面额外加两个空格。

#### 4、列表

无序列表：在每个列表元素前面，加上短横杠、空格"- "。

有序列表：在每个列表元素前面，加上数字、英文句号、空格"1. "。

#### 5、链接

完整的链接是要带协议名的，比如前面的`https://`

1）展示链接

把链接直接像普通文字那样放进去，如果Markdown识别出来这是个链接，就会把它变成可跳转的。

2）展示链接标题 更直观地告知读者链接指向的内容

方括号把链接包围起来，在后面紧跟着的括号里面，放上链接。。

```markdown
[必应]([https://cn.bing.com/)](https://cn.bing.com/))
```

#### 6、图片

方括号里放文字，圆括号里面放图片链接，同时在方括号前面加上一个英文感叹号。

```markdown
![城市景观](C:/Users/xxl/Downloads/[https:/img0.baidu.com/it/u=25183460,870873689&fm=253)](https://img0.baidu.com/it/u=25183460,870873689&fm=253))
```

插入图片，方括号里放文字的意义是，如果图片加载不出来的话，就会显示那个文字内容作为替代。

#### 7、引用

插入一个引用段落，用右书名号、空格，后面紧跟着引用内容。

引用段落里的文字，和普通段落里的文字一样，不会因为你在内容里加了换行，展示效果里就有换行。

```markdown
>蒹葭苍苍
白露为霜
```

#### 8、代码

要在文字里插入代码，就用反引号包裹住代码，`import math`

如果我们要插入独占一段的代码段落，就用三个反引号包裹住代码段落。

在开头的三个反引号后面，还可以跟上代码语言的名字，这样Markdown就会展示针对那个语言的语法高亮。

````markdown
​```python
import math
print("Hello World!")
print(math.pi)
````

#### 9、公式

1. 在行内插入公式，就用1个$，包裹住那个公式。
2. 要插入一个独占一行的公式，就用2个$，包裹住那个公式。
3. 复杂的公式，可以用LaTex语法来表示。

## LaTeX语法

LaTex是一个排版系统，可以负责定义书籍、简历、论文等格式和布局，不局限于数学公式。

在JN的使用场景里，用LaTex就是为了在Markdown里插入公式，所以只需学会公式相关语法即可

### 一、加减乘除

表示加减的符号，就是键盘上的加减。

乘号和除号是没有的，用`\times`表示乘号，`\div`表示除号。

```markdown
"""
$$x + y$$
$$x - y$$
$$x \times y$$
$$x \div y$$
"""
```

### 二、上标下标

要加上标，用插入符`^`，跟上作为上标的内容；要加下标，用下划线`_`，后面跟上作为下标的内容。

LaTex默认上标或下标只包含1位字符，如果想把多个字符作为上标或下标，用花括号把它们组合起来，就可以了

```markdown
"""
$$x^3$$
$$H_2O$$ 
$$S_{input}$$
"""
```

### 三、求和求根

求根符号，`\sqrt`，后面可以跟上方括号，里面的数字表示求几次方根。

LaTex默认求根符号的横线只拉到第1个字符，如果对一个长公式求根，可以在方括号后面跟上花括号，把要求根的内容全部包围起来。

```markdown
"""
$$\sum(x^2 + y^2)$$
$$\sqrt[3]x$$
$$\sqrt[3]{a^2m^2}$$
"""
```

$$\sum(x^2 + y^2)$$
$$\sqrt\[3]x$$
$$\sqrt\[3]{a^2m^2}$$

### 四、分数线

用`\frac`表示，后面跟着两个花括号，在第一个花括号里面，放分数线上面的内容；在第二个花括号里，放分数线下面的内容。

```markdown
"""
$$\frac{x+y}{x-y}$$
"""
```

$$\frac{x+y}{x-y}$$

### 五、其他

除上面之外，LaTex还能搞定其它无数的公式符号，参考官方文档

其他公式符号语法：<https://oeis.org/wiki/List_of_LaTeX_mathematical_symbols>

## 参考资料

原资料：<https://www.bilibili.com/cheese/play/ss2298>

下载资料：<https://www.hezuclub.com/1775.html>

下载链接：[下载链接](https://since1982-my.sharepoint.com/personal/hezuclub2024_since1982_org/_layouts/15/onedrive.aspx?ga=1\&id=%2Fpersonal%2Fhezuclub2024%5Fsince1982%5Forg%2FDocuments%2FP1775%20%E5%B0%8F%E7%99%BD%E7%8E%A9%E8%BD%ACPython%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E8%AE%AD%E7%BB%83%E8%90%A5\&sortField=LinkFilename\&isAscending=true)

代码：<https://github.com/NaturalCutie/Python-Data-Analysis-Notes/tree/main>

---

---
url: /Python/数据分析/10_Python数据分析—保存数据.md
---

# Python数据分析—保存数据

当完成数据清洗后，可以把干净整洁的版本先保存一下，假如会进行多次数据分析，可以有效节约下一次分析的时间，因为不需要再重复评估和清洗步骤了。

保存数据，本质上就是把DataFrame，写入到一个新建的空白文件里。

## 写入CSV文件

DataFrame的`to_csv`方法，参数传入文件路径，调用后就能把DataFrame转换成CSV格式，并且保存在路径对应的文件里面。

**如果那个文件本身不存在，新文件会自动帮我们创建出来；**

**但如果路径对应的文件已经存在，方法运行后，就会把原始内容给覆盖掉。**

`to_csv`方法，会默认把DataFrame的列名和索引都写入到文件里

```python
import pandas as pd
import numpy as np

# 创建示例 DataFrame
df1 = pd.DataFrame({
    '日期': ['2022-01-01', '2022-01-02', '2022-01-03', '2022-01-04'],
    '销售额': [1000, 1500, 800, 1200],
    '销售人员': ['Alice', 'Bob', 'Charlie', 'David'],
    '城市': ['New York', 'San Francisco', 'New York', 'San Francisco']
})
df1 = df1.rename(index={
    0: '38H9',
    1: 'W9F1',
    2: 'KD82',
    3: '004U'
})
df1
```

|          | **日期**   | **销售额** | **销售人员** | **城市**      |
| -------- | :--------- | :--------- | :----------- | :------------ |
| **38H9** | 2022-01-01 | 1000       | Alice        | New York      |
| **W9F1** | 2022-01-02 | 1500       | Bob          | San Francisco |
| **KD82** | 2022-01-03 | 800        | Charlie      | New York      |
| **004U** | 2022-01-04 | 1200       | David        | San Francisco |

```python
df1.to_csv('cleaned_sales_data.csv')
cleaned_df = pd.read_csv('cleaned_sales_data.csv')
cleaned_df
```

|       | **Unnamed: 0** | **日期**   | **销售额** | **销售人员** | **城市**      |
| ----- | :------------- | :--------- | :--------- | :----------- | :------------ |
| **0** | 38H9           | 2022-01-01 | 1000       | Alice        | New York      |
| **1** | W9F1           | 2022-01-02 | 1500       | Bob          | San Francisco |
| **2** | KD82           | 2022-01-03 | 800        | Charlie      | New York      |
| **3** | 004U           | 2022-01-04 | 1200       | David        | San Francisco |

以上运行结果是因为，to\_csv方法会默认帮我们把索引进行保存，但读取的时候，read\_csv不知道那是索引，所以它们又被当成了第一列数据，而且因为这列上面没有名字，列名变成Unnamed: 0，表示那是没有名字的第一列。

调整方法

### 1、默认to\_csv和read\_csv后

先调用清理数据中所学的rename方法，给列换一个有意义的名字

```python
cleaned_df.rename(columns={'Unnamed: 0': '销售ID'}, inplace=True)
cleaned_df
```

|       | **销售ID** | **日期**   | **销售额** | **销售人员** | **城市**      |
| ----- | :--------- | :--------- | :--------- | :----------- | :------------ |
| **0** | 38H9       | 2022-01-01 | 1000       | Alice        | New York      |
| **1** | W9F1       | 2022-01-02 | 1500       | Bob          | San Francisco |
| **2** | KD82       | 2022-01-03 | 800        | Charlie      | New York      |
| **3** | 004U       | 2022-01-04 | 1200       | David        | San Francisco |

然后调用`set_index`方法，把那一列设置为DataFrame的索引

```python
cleaned_df.set_index('销售ID', inplace=True)
cleaned_df
```

|            | **日期**   | **销售额** | **销售人员** | **城市**      |
| ---------- | ---------- | ---------- | ------------ | ------------- |
| **销售ID** |            |            |              |               |
| **38H9**   | 2022-01-01 | 1000       | Alice        | New York      |
| **W9F1**   | 2022-01-02 | 1500       | Bob          | San Francisco |
| **KD82**   | 2022-01-03 | 800        | Charlie      | New York      |
| **004U**   | 2022-01-04 | 1200       | David        | San Francisco |

### 2、写入CSV文件时，不保存DataFrame的索引

to\_csv方法，放入可选参数`index=False`，写入时会自动忽略索引

**如果DataFrame的索引，只是位置索引，一般我们不会专门写入，也就是说会指定**`**index=False**`**；但如果是有关键信息的标签索引，就写入到CSV文件里**

```python
df1.to_csv('cleaned_sales_data2.csv', index=False)
cleaned_df_without_index = pd.read_csv('cleaned_sales_data2.csv')
cleaned_df_without_index
```

|       | **日期**   | **销售额** | **销售人员** | **城市**      |
| ----- | :--------- | :--------- | :----------- | :------------ |
| **0** | 2022-01-01 | 1000       | Alice        | New York      |
| **1** | 2022-01-02 | 1500       | Bob          | San Francisco |
| **2** | 2022-01-03 | 800        | Charlie      | New York      |
| **3** | 2022-01-04 | 1200       | David        | San Francisco |

### 3、可选参数index\_col=0

读取CSV文件时，放入可选参数`index_col=0`，表示把CSV第一列数据作为索引

```python
cleaned_df2 = pd.read_csv('cleaned_sales_data.csv', index_col=0)
cleaned_df2
```

|          | **日期**   | **销售额** | **销售人员** | **城市**      |
| -------- | :--------- | :--------- | :----------- | :------------ |
| **38H9** | 2022-01-01 | 1000       | Alice        | New York      |
| **W9F1** | 2022-01-02 | 1500       | Bob          | San Francisco |
| **KD82** | 2022-01-03 | 800        | Charlie      | New York      |
| **004U** | 2022-01-04 | 1200       | David        | San Francisco |

## 数据评估和清洗整体流程

**下载数据；**

**读取数据；**

**评估数据；**

**根据对数据的评估，指定清洗的步骤；清洗数据；每清洗一步，就再次查看清洗后的数据，来确保问题已经解决；**

**保存清洗后的数据**

在Jupyter Notebook里进行数据清晰，并以报告的形式呈现内容。

运用标题，使内容结构清晰；

运用Markdown对代码进行注释，方便读者理解；

并且运用Markdown，总结评估结论，使内容逻辑清晰。

### 1、下载数据

1. 每次清洗数据项目开始前，都新建一个新的文件夹，将数据文件和Jupyter Notebook放入。

这样可以保持文件结构的整洁；数据文件和Jupyter Notebook放入同一文件夹，也方便读取

1. 对于公开数据集，我们并不了解它的背景，在正式分析前，最好了解一下它的介绍，和数据每列的含义。

### 2、读取数据

### 3、评估数据

#### 1）缺失数据

发现缺失数据后，需要深入探索缺失数据：应把该变量存在缺失的观察值，筛选出来，进一步观察数据特征，(提出猜测，验证猜测)

#### 2）重复数据

首先判断哪些变量不应该重复，再去评估数据。

**唯一标识符不一定不能重复，具体数据具体分析**

#### 3）不一致数据

先观察哪个变量可以判断是否存在不一致数据，再去评估数据。

#### 4）无效/错误数据

先观察哪个变量可以判断是否存在不一致数据。

然后通过某些方法，结合常识，评估无效/错误数据。

最后，与缺失数据一样，发现无效/错误数据后，需要深入探索无效/错误数据：应把该变量无效/错误的观察值，筛选出来，进一步观察数据特征，(提出猜测，验证猜测)

### 4、根据对数据的评估，指定清洗的步骤

#### 清洗数据

1. 在清洗前，先创建一个新变量，用于储存清洗过程中的数据。

这样原始的数据和经过清理的数据，分别储存在两个变量中

2. 清洗数据过程中，遇到现学处理方法无法很好清理数据的情况，可以查询官方文档或查询搜索引擎。

#### 每清洗一步，就再次查看清洗后的数据，来确保问题已经解决

### 5、保存清洗后的数据

新的CSV文件名，可以是在原始CSV文件名后面，添加`'_cleaned'`。

## 分享到GitHub

Notebook文件格式不方便直接分享

用文本编辑器打开，文本内容并不直观，也不可读。

notebook文件需要用Jupyter Notebook或代码编辑器打开；或者把notebook上传到GitHub，然后分享项目链接，因为GitHub能直接渲染Notebook的内容并进行展示。

GitHub是世界上最大的代码托管网站，和开源社区。可以在上面管理代码和追踪代码历史记录，也可以在上面搜索和查看无数其它开发者的项目代码

### (一)、创建代码仓库

1. license

也就是软件许可证，是用来规定和限制用户，使用这个仓库里面的代码的权利的。

不同许可证的含义：<https://choosealicense.com/>

### (二)、Git

1. Git是GitHub背后的版本控制系统，git可以记录项目每次做了什么改动，是由谁改动的，也可以随时切换到之前某个版本的状态
2. 学习Git命令：<https://www.runoob.com/git/git-tutorial.html>

---

---
url: /Python/数据分析/7_Python数据分析—读取CSV.md
---

# Python数据分析—读取CSV

CSV是数据分析师最喜欢的数据格式。

与JSON一样，CSV也是纯文本文件，也就是说文字内容不存在粗体、下划线、字号、颜色等特征

如果把逗号对齐，CSV的结构基本上就是一个表格。

## 一、CSV

### 1、概念

CSV，全称是Comma-Separated Values，表示逗号分隔值。

### 2、结构

1）表头

a、有表头：CSV文件第一行，也就是展示列名那行，大部分情况下是表格头，包含了许多属性名

b、无表头：也可以无表头，以数据直接开始

2）数据

a、在表头下面，每一条数据都是独占一行的，因此当我们把CSV文件转换成DataFrame后，CSV的行与DataFrame的行之间能够直接对应上

b、每行数据里所包含的值的数量应该是相同的，逗号分隔符的数量也必须相同，但凡哪行多了或少了，说明那就不是一个合格有效的CSV

### 3、特殊情况

1）当数据值里面包含英文逗号，可以用引号围完整的值。里面的逗号，就不会被当成分隔符的逗号了

2）当某个值为空缺，可以让两个分隔逗号相邻，表示中间那个值不存在

### 4、CSV与JSON对比

CSV本身是一个非常规整的二维结构，能一眼就知道它所对应的表格长什么样子

JSON则不同，它的结构能非常灵活，也可以层层嵌套，很难直观看出对应表格长什么样

因此，JSON是通用编程时受欢迎的数据结构，而CSV是数据分析时受欢迎的数据结构

### 5、编写CSV

打开代码编辑器或文本编辑器，按照CSV格式的规则写好后，把文件保存为以.csv结尾的文件即可

### 6、默认规范

用代码分析数据时，尽量让源数据文件，以CSV、JSON等纯文本格式，或者sqlite等数据库文件格式

### 7、优点

体积小，结构工整，容易让人理解，能非常直接地转换成表格。

可以用Excel软件去读取、修改或导出CSV。

### 8、读取

在实际的数据分析中，由于一般数据量比较大，动辄1G以上的CSV数据集是很常见的，

1）Excel

可能会卡住。因为Excel不止要展示数据，还要试图展示格式(虽然文件里可能就没有额外格式)，还得加载一系列功能等

2）代码编辑器或者纯文本编辑器

3）更好的方法

用代码读取，转换成DataFrame，然后用Pandas库里的方法，想看几行看几行，而不用等海量的数据全部加载出来

## 二、读取CSV

### 1、导入Pandas

`import pandas as pd`

### 2、read\_csv函数

顾名思义是用来读CSV文件的

把文件的路径作为参数

**以下用代码而不是软件去处理数据，能更快处理完成**

```python
import pandas as pd
df1 = pd.read_csv('fifa_players_22.csv')
# fifa_players_22.csv文件，里面记录了2万条国际足联球员的信息，文件体积是12MB左右
df1.head()
```

![](/assets/image-1733845825106.DUtYtCCR.png)

#### 1）可选参数header

a、csv文件有表头，省略可选参数header，默认会把第一行内容作为列名

b、csv文件无表头，设置'header=None'，表示不要把第一行当做列名。那解析出来的DataFrame就会把第一行视为第一条数据，列名用从0开始的数据代替

```python
df = pd.read_csv('fifa_players_22.csv', header=None)
df.head()
```

![](/assets/image-1733845825205.C7LHPWnz.png)

#### 2）可选参数index\_col

a、只用位置索引，省略可选参数index\_col，默认用位置索引

b、把某一列作为标签索引，设置'index\_col=列名/列的位置索引'，返回的DataFrame就会把列名的值作为标签索引。如下例子，可以通过标签索引'球员ID'，而不是仅通过位置索引，来提取数据了

```python
df1 = pd.read_csv('fifa_players_22.csv', index_col='sofifa_id')
df1.head()
df1.loc[261667]
```

![](/assets/image-1733845825230.Bd01Ppz0.png)

## 三、展示CSV的方法

### (一)、set\_option函数

1、更改展示列数上限

默认只展示20列。

当我们遇到的数据包含非常多列的时候，输入的DataFrame里，中间有些列就会被自动忽略，被用'...'表示。

假如想查看所有列，可以通过set\_option函数，提升展示的列数的上限。

第一个参数传入'display.max\_columns'，表示展示列数的上限。

第二个参数传入，你希望最多看到的列数

```python
pd.set_option('display.max_columns', 150)
df1
```

![](/assets/image-1733845825217.yWpoIi3D.png)

2、更改展示值的字符上限

默认的字符上限是50

当我们遇到值很长的时候，文字不会展示全，如上例子里，'player\_tags'和'player\_traits'两列

假如想让值展示的长度更大一些，可以通过set\_option函数，提升值展示的字符上限。

第一个参数传入'display.max\_colwidths'，每列里的值最多包含几个字符限。

第二个字符上限到的列数

```python
pd.set_option('display.max_colwidth', 500)
df1
```

![](/assets/image-1733845825233.hPNXRjmp.png)

### (二)、head/tail/sample方法

可以得到开头/结尾/随机的N行

```python
df1.head()
df1.tail()
# 默认随机返回一行的数据
df1.sample()
```

### (三)、info方法

快速了解一个DataFrame，除了看它实际的数据之外，还可以通过info，获得DataFrame的概况信息

1、当列数太多时，只会展示部分信息

```python
df1.info()
```

结果

```python
<class 'pandas.core.frame.DataFrame'>
Index: 19239 entries, 158023 to 264540
Columns: 106 entries, player_url to nation_flag_url
dtypes: float64(12), int64(44), object(50)
memory usage: 16.2+ MB
```

从上到下分别是：类型、索引的范围、列的数量、列的数据类型、占用memory

2、当列的数量较少时，还会返回每列具体的列名，排除掉空值后实际值的数量，和每一列的数据类型

```python
df3 = pd.read_csv('fifa_players_22_simple.csv')
df3.head(2)
df3.info()
```

![](/assets/image-1733845825238.Dacpux3C.png)

### (四)、descirbe方法

针对数字类型的列，一次性展示多种统计信息

```python
df1.describe()
```

![](/assets/image-1733845825257.S1hslSbe.png)

---

---
url: /Python/数据分析/6_Python数据分析—读取JSON.md
---

# Python数据分析—读取JSON

## 一、数据格式

JSON是程序员非常喜欢的数据格式。

不同文件格式有不同的读取方法，一般通过文件后缀来分辨文件格式。

**文件名后缀只是文件名的一部分，更改后缀并不改变文件里面的内容，因此更改后缀不影响实际的文件格式，文件名后缀会影响电脑选择用什么软件去打开它。**

当我们聊数据格式时，不仅聊的是这个文件以什么后缀结尾，更重要的是它里面的内容遵循怎样的格式规则。

## 二、JSON

### 1、概念

JSON，全称为**JaveScript Object Notation**，意思是JavaScript的对象表示法，可见这种数据格式和JavaScript的语法是有些关联的。

JSON，与Python的字典或列表，有非常类似之处；并且被无数主流变成语言支持。

### 2、优点

1）用API获得的数据，很多时候数据都是一JSON格式进行返回的

2）占用体积小

3）非常容易被转换成程序语言自己的结构

### 3、数据结构

JSON对象和JSON数组，可以分别被转换成Python字典和列表

#### 1）JSON对象

JSON对象，以大括号开头和结尾，然后里面都是键值对，每个键值对之间用逗号进行分隔。

```json
{
  "id": "1", 
  "type": "article", 
  "title": "working with JSON data", 
  "created": "2099-12-18T14:56:29.000Z"
}
```

#### 2）JSON数组

JSON数组，以中括号开头和结尾，然后里面是一个个值，每个值之间用逗号进行分隔。

```json
[
  {
    "title": "A Light in the Attic"
    "price": "£51.77"
  },
  {
    "title": "Tipping the Velvet"
    "price": "£53.74"
  }
]
```

### 4、JSON值的类型

需要属于以下几种：

1. 字符串 "star"
2. 数字 31
3. 布尔值 true
4. 数组 \["hi", 7]
5. 对象 {"age": 25}
6. 空值 null
7. 支持嵌套， 数组里面的值可以是对象，对象里的值可以是数组

### 5、与Python语法的区别

1. Python字典，可以用整数等不可变数据类型作为键；但在JSON对象里，只能是字符串作为键，不能是其他类型。
2. JSON对象的键，不能存在重复，因为值要靠键提取；Python字典里，键重复不会报错，但在有两个键相同的情况下，后来赋给键的值将成为键的真实值。
3. JSON里，字符串必须被双引号包围，不能用单引号。
4. Python的布尔值，都是以大写开头；因为本质是JavaScript对象，而JavaScript的布尔值是以小写开头，JSON里布尔值也是以小写开头。
5. JSON里空值是null。

### 6、数据转换

因为JSON和Python的字典或列表还是存在差别的，因此获取JSON数据后，要进行解析+转换，才能去分析数据。

## 三、读取JSON

用Python的Pandas库，可以丝滑读取JSON

1. 导入Pandas

`import pandas as pd`

2. read\_json函数

顾名思义是用来读JSON文件的

把文件的路径作为参数

函数会完成文件读取、JSON解析、转成DataFrame的全流程，会直接把JSON文件转成DataFrame。

如果不用Pandas，每一个步骤都可能需要一段单独的代码。

## 四、JSON与DataFrame的对应

### 1、普通JSON数据

```json
[
  {
    "questionType": "yes/no",
    "asin": "1466736038",
    "answerTime": "Mar 8, 2014",
    "unixTime": 1394265600,
    "question": "Is there a SIM card in it?",
    "answerType": "Y",
    "answer": "Yes. The Galaxy SIII accommodates a micro SIM card."
  },
  {
    "questionType": "yes/no",
    "asin": "1466736038",
    "answerTime": "Jan 29, 2015",
    "unixTime": 1422518400,
    "question": "Is this phone new, with 1 year manufacture warranty?",
    "answerType": "?",
    "answer": "It is new but I was not able to get it activated with AT&T."
  },
  {
    "questionType": "yes/no",
    "asin": "1466736038",
    "answerTime": "Nov 30, 2014",
    "unixTime": 1417334400,
    "question": "can in it be used abroad with a different carrier?",
    "answerType": "Y",
    "answer": "Yes"
  },
  {
    "questionType": "open-ended",
    "asin": "1466736038",
    "answerTime": "Nov 3, 2014",
    "unixTime": 1415001600,
    "question": "What is the warranty on this?",
    "answer": "No warranty"
  },
  {
    "questionType": "yes/no",
    "asin": "1466736038",
    "answerTime": "Oct 2, 2014",
    "unixTime": 1412233200,
    "question": "Does this phone use the regular Sim card (the bigger Sim card)?",
    "answerType": "?",
    "answer": "it takes mini sim"
  },
  {
    "questionType": "open-ended",
    "asin": "1466736038",
    "answerTime": "Sep 11, 2014",
    "unixTime": 1410418800,
    "question": "how much time you need to send me this product to miami?",
    "answer": "If you choose expedited shipping you will have the phone in 2-3 days"
  },
  {
    "questionType": "yes/no",
    "asin": "1621911888",
    "answerTime": "Dec 13, 2013",
    "unixTime": 1386921600,
    "question": "Is it unlocked?",
    "answerType": "Y",
    "answer": "yes"
  }
]
```

读取数据

```python
import pandas as pd
survey_df = pd.read_json("./data_cell_phones_survey.json")
survey_df
```

结果

![](/assets/image-1733845754343.DhdgPSgx.png)

解析

1. 原始文件是一个JSON数组，数组里的元素都是JSON对象，每一个JSON对象与DataFrame中的一行相对应。JSON里数据实例一般会用对象表示，而DataFrame里一般**每一行代表一个实例**，正好相互对应。
2. JSON对象的键值对，与DataFrame的列名和下面的数据相对应，DataFrame的**列名表示数据实例的各个属性**，而JSON对象里的键值对也是这个作用，正好相互对应。
3. JSON中有些对象没有answerType这个键，与DataFrame里answerType列下部分值是NaN相对应，由于JSON并不要求所有对象的键都相同，Pandas也很好地在DataFrame里对应了这种情况，**用NaN来表示数据空缺**。

### 2、带子集的JSON数据

```json
{
  "owner": "pelmers",
  "name": "text-rewriter",
  "stars": 11,
  "forks": 4,
  "watchers": 3,
  "isFork": false,
  "languages": [
    {
      "name": "JavaScript",
      "size": 21769
    },
    {
      "name": "HTML",
      "size": 2096
    },
    {
      "name": "CSS",
      "size": 2081
    }
  ],
  "description": "Webextension to rewrite phrases in pages",
  "createdAt": "2015-03-14T22:35:11Z",
  "pushedAt": "2022-02-11T14:26:00Z",
  "license": null
}
```

读取数据

```python
github_df = pd.read_json("./3.21_data_github.json")
github_df
```

结果

![](/assets/image-1733845754420.BVVPAF3d.png)

解析

JSON文件里JSON对象中，languages的值是一个长度为3的数组，与DataFrame的三行相对应。当键值对里的值是数组时，Pandas会把数组元素视为属于不同数据实例，拆分成单独的一行，因此languages属性下面就应该有3行。但因为表格里每列下面的行数都得是一样的，所以DataFrame其它属性值就被复制成了三个，从而得到一个结构规整的表格。

---

---
url: /Python/数据分析/13_Python数据分析—可视化数据.md
---

# Python数据分析—可视化数据

## 一、可视化图表—单变量

可视化的作用不仅在于最后步骤。在数据分析前，图表能在我们自行探索数据的时候，帮助发现隐藏的关系、趋势、影响，高效找到下一步的分析方向。甚至在数据评估与清理步骤，可视化也能帮助我们直观发现异常数据。因此，数据可视化是一个万金油，它不仅是数据分析全流程中的一个步骤，更是一种方式，可以任何步骤里使用。

如果我们的数据包含一个数值变量，可以绘制直方图、密度图、箱型图、小提琴图。

### (一)、直方图

![](/assets/image-1733846545568.C3UMVM-5.png)

1、特点

展示数据的频率分布

2、表示

横轴表示某数据范围，而纵轴表示个数

### (二)、密度图(Kernel Density)

![](/assets/image-1733846545577.CM763P4-.png)

1、特点

同样用来表示数据的分布，更容易看出分布形状

2、不同

不同于直方图用一个个条柱表示频率，密度图会用一条平滑的曲线

纵轴表示的是概率密度，纵轴的最大值不会超过1

### (三)、箱型图

![](/assets/image-1733846545584.BoYEQtdh.png)

1、特点

看出中位数、上下四分位数、四分位距大小、分布是否对称、是否紧密，以及有没有异常值

2、表示

纵轴表示数据值；箱子的边界分别是第一和第三四分位数，所以箱子的长度就是四分位距；而箱子中间那条线是中位数；箱子上下的横线分别是上界和下界；上界和下界外的点是异常值。

3、上界&下界

上界 = (数据最大值 or 第三四分位数 + 1.5倍四分位距)，其中的最小值

下界 = (数据最小值 or 第一四分位数 - 1.5倍四分位距)，其中的最大值

如果有数据的值，(大于第三四分位数 + 1.5倍四分位距 or 小于第一四分位数 - 1.5倍四分位距)，会被用单独的点表示出来，被算作异常值

### (四)、小提琴图

![](/assets/image-1733846545592.CtqlwgB6.png)

1、特点

能同时展现箱型图和密度图的信息

2、表示

中间的小圆点表示中位数；黑色条的边界和箱型图的箱子一样，分别是第一和第三四分位数；而小提琴的长度，表示95%置信区间；小提琴的体型，对应了密度图，越胖的地方那个值频率越高。

## 二、可视化图表—数据包含两个变量

如果我们的数据包含两个变量，比如两列DataFrame的话，可以绘制散点图、折线图、条形图、饼图。其中散点图和折线图，主要针对两个数值变量；条形图和饼图主要针对一个分类变量加一个数值变量。

### (一)、散点图

1. 特点

可以从散点图，看出变量之间的相关性，比如是否相关、呈正比还是呈反比、线性还是非线性等等，也可以帮我们发现异常值的存在

2. 表示

X轴表示一个变量的值，Y轴表示另一个变量的值

### (二)、折线图

1. 特点

用于展示连续间隔或时间跨度上数值的变化，从而展示趋势变化

### (三)、条形图

1. 特点

用来展示一个分类变量所对应的数值变量

2. 条形图与直方图的不同
   * 直方图只针对一个数值变量，而条形图则是针对一个分类变量和一个数值变量
   * 直方图的各个条柱分隔除了不同的数字区间，而条形图的各个条柱分隔这不同的分类变量

### (四)、饼图

1. 特点

用来展示各个分类对应的数值之间的比例，可以直观了解不同类别在整体中的占比

2. 表示

每个圆弧的长度/面积，代表每个分类所占的百分比，全圆则表示所有分类占比的总和，也就是100%

## 三、可视化图表—数据包含多个变量

通过添加颜色或尺寸，在图表上表示新的变量

### (一)、散点图

1. 2个数值变量，1个分类变量

如果要引入新的分类变量，可以把点绘制成不同的颜色，让颜色表示不同分类

2. 气泡图，3个数值变量

如果要引入新的数值变量，可以把点绘制成不同的大小，让面积去表示不同数值

3. 气泡图，3个数值变量，1个分类变量

把气泡绘制成不同的颜色

### (二)、折线图

1. 2个数值变量，1个分类变量

可以绘制多条折线，不同颜色的折线代表不同分类

### (三)、条形图

1. 复式条形图，2个分类变量，1个数值变量

绘制多个条柱，不同颜色的条柱代表不同分类

### (四)、热力图

2个分类变量，1个数值变量

1. 特点

通过颜色来展示不同变量之间的数值差异

2. 表示

横轴是分类变量，纵轴也是分类变量，每行或每列都表示一个分类种类，通过热力图里单元格的颜色或数值，表示数值变量数值的大小

通过把多个图放在一起，互相对比来挖掘信息

1. 通过把两个直方图叠在一张图上，可以直观的看出，它们集中位置的差异、分散程度的差异等等
2. 通过把多个小提琴图并排放，可以直观的比较它们的四分位距、密度概率等等

## 四、Seaborn和Matplotlib

用代码绘制包含一个变量的数据

绘制图表很多人会选择用Seaborn，并和Matplotlib配合使用

Seaborn

一个可视化库，操作简单，又广泛支持多种图表类型

Matplotlib

也是一个可视化库。Seaborn是基于Matplotlib实现的。Matplotlib虽然操作起来没有Seaborn简单，但是它更灵活和自由，所以让我们能对图表做各种自定义操作

### (一)、安装Seaborn

在CMD或终端，输入`pip install seaborn`

macOS有可能使用`pip3 install seaborn`

### (二)、安装Matplotlib

在CMD或终端，输入`pip install matplotlib`

macOS有可能使用`pip3 install matplotlib`

### (三)、导入Seaborn和Matplotlib

Matplotlib中大部分我们会用到的功能都在pyplot的子模块下

```python
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
```

```python
# 导入实例数据
df = sns.load_dataset('penguins').query('species == "Adelie"')
bill = df.bill_length_mm
bill
```

```python
0      39.1
1      39.5
2      40.3
3       NaN
4      36.7
... 
147    36.6
148    36.0
149    37.8
150    36.0
151    41.5
Name: bill_length_mm, Length: 152, dtype: float64
```

## 五、绘制图表—对单个变量绘制图表

### (一)、对Series进行绘图

**如果用Jupyter Notebook运行绘图函数的话，可以在输出里看到直方图；**

**如果在编辑器里运行的，需要调用Matplotlib的show函数，才能展示图片**

#### 1、直方图

英文是Histogram，Seaborn绘制直方图的函数叫做histplot

```python
sns.histplot(bill) 

<Axes: xlabel='bill_length_mm', ylabel='Count'>
```

![](/assets/output_10_1.CT2Ubd0O.png)

```python
sns.histplot(bill)
plt.show()
```

![](/assets/output_11_0.C1Mo1rEv.png)

#### 2、密度图

统计学里密度叫做Kernel Density，Seaborn绘制密度图的函数叫kdeplot

```python
sns.kdeplot(bill) 

<Axes: xlabel='bill_length_mm', ylabel='Density'>
```

![](/assets/output_13_1.CcoBGV9o.png)

#### 3、箱型图

英文是Box Plot，Seaborn绘制箱型图的函数叫`boxplot`

```python
sns.boxplot(bill)
plt.show()
```

![](/assets/output_15_0.DpHYw0rU.png)

#### 4、小提琴图

英文是violin plot，Seaborn绘制小提琴图的函数叫`violinplot`

```python
sns.violinplot(bill) 

<Axes: ylabel='bill_length_mm'>
```

![](/assets/output_17_1.BZGTZTpL.png)

### (二)、对DataFrame进行绘图

```python
df
```

|         | **species** | **island** | **bill\_length\_mm** | **bill\_depth\_mm** | **flipper\_length\_mm** | **body\_mass\_g** | **sex** |
| ------- | :---------- | :--------- | :----------------- | :---------------- | :-------------------- | :-------------- | :------ |
| **0**   | Adelie      | Torgersen  | 39.1               | 18.7              | 181.0                 | 3750.0          | Male    |
| **1**   | Adelie      | Torgersen  | 39.5               | 17.4              | 186.0                 | 3800.0          | Female  |
| **2**   | Adelie      | Torgersen  | 40.3               | 18.0              | 195.0                 | 3250.0          | Female  |
| **3**   | Adelie      | Torgersen  | NaN                | NaN               | NaN                   | NaN             | NaN     |
| **4**   | Adelie      | Torgersen  | 36.7               | 19.3              | 193.0                 | 3450.0          | Female  |
| **...** | ...         | ...        | ...                | ...               | ...                   | ...             | ...     |
| **147** | Adelie      | Dream      | 36.6               | 18.4              | 184.0                 | 3475.0          | Female  |
| **148** | Adelie      | Dream      | 36.0               | 17.8              | 195.0                 | 3450.0          | Female  |
| **149** | Adelie      | Dream      | 37.8               | 18.1              | 193.0                 | 3750.0          | Male    |
| **150** | Adelie      | Dream      | 36.0               | 17.1              | 187.0                 | 3700.0          | Female  |
| **151** | Adelie      | Dream      | 41.5               | 18.5              | 201.0                 | 4000.0          | Male    |

#### 1、直接传入DataFrame

`data=DataFrame`，参数名'data='可以省略；然后参数`x='X轴所对应列的列名'`，(`y='Y轴所对应列的列名'`)

```python
sns.histplot(df, x='bill_length_mm')
plt.show()
```

![](13_Python数据分析—可视化数据.assets/output_22_0.png)

#### 2、传入DataFrame的列

特别适用于，两个变量不属于同一DataFrame时

参数`x=df1.列名a`，(`y=df2['列名b']`)

```python
sns.histplot(x=df['bill_length_mm'])
plt.show()
```

![](/assets/output_25_0.B_eqBG-c.png)

### (三)、给图表添加标题和轴标签

#### 1、给图表添加标题

可以调用`plt.title`，里面传入标题字符串

#### 2、给图表添加/更改轴标签

添加/更改X轴标签，可以调用plt.xlabel，里面传入标签字符串

添加/更改Y轴标签，可以调用plt.ylabel，里面传入标签字符串

**标题和轴标签的添加，都需要在show函数被调用之前运行**

**Matplotlib当前默认的字体并不支持中文展示，所以我们需要把字体换成其它支持中文的字体。\*\*\*\*由于每台电脑上有的字体并不相同，你可以先查询当前系统所有字体，然后把字体替换成其中某个支持中文的字**体。

```python
# 查询当前系统所有字体
import matplotlib
from matplotlib.font_manager import FontManager

# FontManager是matplotlib.font_manager中的一个类，FontManager()创建一个类实例，FontManager().ttflist返回FontManager类实例的ttflist属性，ttflist包含Matplotlib的字体列表
# set()和f.name部分是生成式表达式
mpl_fonts = set(f.name for f in FontManager().ttflist)
for f in sorted(mpl_fonts):
    print(f)
```

```python
Agency FB
Algerian
Arial
Arial Rounded MT Bold
Bahnschrift
Baskerville Old Face
Bauhaus 93
Bell MT
Berlin Sans FB
Berlin Sans FB Demi
Bernard MT Condensed
Blackadder ITC
Bodoni MT
Book Antiqua
Bookman Old Style
Bookshelf Symbol 7
Bradley Hand ITC
Britannic Bold
Broadway
Brush Script MT
Calibri
Californian FB
Calisto MT
Cambria
Candara
Castellar
Centaur
Century
Century Gothic
Century Schoolbook
Chiller
Colonna MT
Comic Sans MS
Consolas
Constantia
Cooper Black
Copperplate Gothic Bold
Copperplate Gothic Light
Corbel
Courier New
Curlz MT
DejaVu Sans
DejaVu Sans Display
 DejaVu Sans Mono
 DejaVu Serif
 DejaVu Serif Display
 DengXian
 Dubai
 Ebrima
 Edwardian Script ITC
 Elephant
 Engravers MT
 Eras Bold ITC
 Eras Demi ITC
 Eras Light ITC
 Eras Medium ITC
 FZShuTi
 FZYaoTi
 FangSong
 Felix Titling
 Fences
 Footlight MT Light
 Forte
 Franklin Gothic Book
 Franklin Gothic Demi
 Franklin Gothic Demi Cond
 Franklin Gothic Heavy
 Franklin Gothic Medium
 Franklin Gothic Medium Cond
 Freestyle Script
 French Script MT
 Gabriola
 Gadugi
 Garamond
 Georgia
 Gigi
 Gill Sans MT
 Gill Sans MT Condensed
 Gill Sans MT Ext Condensed Bold
 Gill Sans Ultra Bold
 Gill Sans Ultra Bold Condensed
 Gloucester MT Extra Condensed
 Goudy Old Style
 Goudy Stout
 HGB1X_CNKI
 HGB1_CNKI
 HGB2X_CNKI
 HGB2_CNKI
 HGB3X_CNKI
 HGB3_CNKI
 HGB4X_CNKI
 HGB4_CNKI
 HGB5X_CNKI
 HGB5_CNKI
 HGB6X_CNKI
 HGB6_CNKI
 HGB7X_CNKI
 HGB7_CNKI
 HGB8X_CNKI
 HGB8_CNKI
 HGBD_CNKI
 HGBKBX_CNKI
 HGBKB_CNKI
 HGBKHX_CNKI
 HGBKH_CNKI
 HGBS1_CNKI
 HGBS2_CNKI
 HGBS_CNKI
 HGBTH_CNKI
 HGBTS_CNKI
 HGBX_CNKI
 HGBZ_CNKI
 HGCH_CNKI
 HGCUH_CNKI
 HGCY_CNKI
 HGDBS_CNKI
 HGDGY_CNKI
 HGDH2_CNKI
 HGDH_CNKI
 HGDYS_CNKI
 HGDY_CNKI
 HGF1X_CNKI
 HGF1_CNKI
 HGF2X_CNKI
 HGF2_CNKI
 HGF3_CNKI
 HGF4X_CNKI
 HGF4_CNKI
 HGF5X_CNKI
 HGF5_CNKI
 HGF6X_CNKI
 HGF6_CNKI
 HGF7X_CNKI
 HGF7_CNKI
 HGF8_CNKI
 HGF9X_CNKI
 HGF9_CNKI
 HGFS1_CNKI
 HGFS2_CNKI
 HGFSH_CNKI
 HGFS_CNKI
 HGFX_CNKI
 HGFZ_CNKI
 HGGG_CNKI
 HGH1X_CNKI
 HGH1_CNKI
 HGH2X_CNKI
 HGH2_CNKI
 HGH3X_CNKI
 HGH3_CNKI
 HGH4X_CNKI
 HGH4_CNKI
 HGH5X_CNKI
 HGH5_CNKI
 HGH6X_CNKI
 HGH6_CNKI
 HGH7X_CNKI
 HGH7_CNKI
 HGHB_CNKI
 HGHD_CNKI
 HGHP_CNKI
 HGHT1_CNKI
 HGHT2_CNKI
 HGHT_CNKI
 HGHUATI_CNKI
 HGHX_CNKI
 HGHZ_CNKI
 HGKT1_CNKI
 HGKT2_CNKI
 HGKT_CNKI
 HGKY_CNKI
 HGLB_CNKI
 HGLS_CNKI
 HGMH_CNKI
 HGNBS_CNKI
 HGOCR_CNKI
 HGPH_CNKI
 HGPTY_CNKI
 HGSS1_CNKI
 HGSS2_CNKI
 HGSS_CNKI
 HGSXT_CNKI
 HGTT_CNKI
 HGTX_CNKI
 HGWT_CNKI
 HGWYS_CNKI
 HGX1X_CNKI
 HGX1_CNKI
 HGXBS_CNKI
 HGXC_CNKI
 HGXF1_CNKI
 HGXFX_CNKI
 HGXFZ_CNKI
 HGXH1_CNKI
 HGXH_CNKI
 HGXK_CNKI
 HGXL_CNKI
 HGXS_CNKI
 HGXT_CNKI
 HGXY_CNKI
 HGY1_CNKI
 HGY2_CNKI
 HGY3_CNKI
 HGY4_CNKI
 HGYB_CNKI
 HGYT1_CNKI
 HGYT2_CNKI
 HGYT_CNKI
 HGYX_CNKI
 HGZCS_CNKI
 HGZDX_CNKI
 HGZK_CNKI
 HGZYT_CNKI
 HGZY_CNKI
 Haettenschweiler
 Harlow Solid Italic
 Harrington
 High Tower Text
 HoloLens MDL2 Assets
 Impact
 Imprint MT Shadow
 Informal Roman
 Ink Free
 Javanese Text
 Jokerman
 Juice ITC
 KaiTi
 Kristen ITC
 Kunstler Script
 Leelawadee UI
 LiSu
 Lucida Bright
 Lucida Calligraphy
 Lucida Console
 Lucida Fax
 Lucida Handwriting
 Lucida Sans
 Lucida Sans Typewriter
 Lucida Sans Unicode
 MS Gothic
 MS Outlook
 MS Reference Sans Serif
 MS Reference Specialty
 MT Extra
 MV Boli
 Magneto
 Maiandra GD
 Malgun Gothic
 Matura MT Script Capitals
 Microsoft Himalaya
 Microsoft JhengHei
 Microsoft New Tai Lue
 Microsoft PhagsPa
 Microsoft Sans Serif
 Microsoft Tai Le
 Microsoft YaHei
 Microsoft Yi Baiti
 MingLiU-ExtB
 Mistral
 Modern No. 20
 Mongolian Baiti
 Monotype Corsiva
 Myanmar Text
 Niagara Engraved
 Niagara Solid
 Nirmala UI
 OCR A Extended
 Old English Text MT
 Onyx
 Origin
 Palace Script MT
 Palatino Linotype
 Papyrus
 Parchment
 Perpetua
 Perpetua Titling MT
 Playbill
 Poor Richard
 Pristina
 Rage Italic
 Ravie
 Rockwell
 Rockwell Condensed
 Rockwell Extra Bold
 STCaiyun
 STFangsong
 STHupo
 STIXGeneral
 STIXNonUnicode
 STIXSizeFiveSym
 STIXSizeFourSym
 STIXSizeOneSym
 STIXSizeThreeSym
 STIXSizeTwoSym
 STKaiti
 STLiti
 STSong
 STXihei
 STXingkai
 STXinwei
 STZhongsong
 Sans Serif Collection
 Script MT Bold
 Segoe Fluent Icons
 Segoe MDL2 Assets
 Segoe Print
 Segoe Script
 Segoe UI
 Segoe UI Emoji
 Segoe UI Historic
 Segoe UI Symbol
 Segoe UI Variable
 Showcard Gothic
 SimHei
 SimSun
 SimSun-ExtB
 Sitka
 Snap ITC
 Stencil
 Sylfaen
 Symbol
 Tahoma
 Tempus Sans ITC
 Times New Roman
 Trebuchet MS
 Tw Cen MT
 Tw Cen MT Condensed
 Tw Cen MT Condensed Extra Bold
 Verdana
 Viner Hand ITC
 Vivaldi
 Vladimir Script
 Webdings
 Wide Latin
 Wingdings
 Wingdings 2
 Wingdings 3
 YouYuan
 Yu Gothic
 ZWAdobeF
 cmb10
 cmex10
 cmmi10
 cmr10
 cmss10
 cmsy10
 cmtt10
```

```python
# 替换成Microsoft YaHei字体
# 调用的Matplotlib的rc函数，rc意思是'runtime configuration',rc函数可以配置各种Matplotlib的设置
matplotlib.rc("font",family='Microsoft YaHei')
```

```python
sns.histplot(df, x='bill_length_mm')
plt.title('Adelie企鹅样本的嘴长度分布')
plt.xlabel('嘴长度(单位：mm)')
plt.ylabel('样本数量')
plt.show()
```

![](/assets/output_35_0.B5gYpYg6.png)

## 六、绘制图表—对两个变量绘制图表

通过Matplotlib和Seaborn，对两个变量绘制图表

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
```

### (一)、散点图

英文是Scatter Plot，Seaborn绘制散点图的函数叫做scatterplot

```python
# 当本地没有此数据集，会自动从线上Seaborn数据库加载，Seaborn数据库数据集https://github.com/mwaskom/seaborn-data
df1 = sns.load_dataset('tips')
df1
```

|         | **total\_bill** | **tip** | **sex** | **smoker** | **day** | **time** | **size** |
| ------- | :------------- | :------ | :------ | :--------- | :------ | :------- | :------- |
| **0**   | 16.99          | 1.01    | Female  | No         | Sun     | Dinner   | 2        |
| **1**   | 10.34          | 1.66    | Male    | No         | Sun     | Dinner   | 3        |
| **2**   | 21.01          | 3.50    | Male    | No         | Sun     | Dinner   | 3        |
| **3**   | 23.68          | 3.31    | Male    | No         | Sun     | Dinner   | 2        |
| **4**   | 24.59          | 3.61    | Female  | No         | Sun     | Dinner   | 4        |
| **...** | ...            | ...     | ...     | ...        | ...     | ...      | ...      |
| **239** | 29.03          | 5.92    | Male    | No         | Sat     | Dinner   | 3        |
| **240** | 27.18          | 2.00    | Female  | Yes        | Sat     | Dinner   | 2        |
| **241** | 22.67          | 2.00    | Male    | Yes        | Sat     | Dinner   | 2        |
| **242** | 17.82          | 1.75    | Male    | No         | Sat     | Dinner   | 2        |
| **243** | 18.78          | 3.00    | Female  | No         | Thur    | Dinner   | 2        |

```python
sns.scatterplot(df1, x='total_bill', y='tip')
plt.show()
```

![](/assets/output_6_0.B7EtDbKi.png)

```python
sns.scatterplot(x=df1.total_bill, y=df1['tip'])

<Axes: xlabel='total_bill', ylabel='tip'>
```

![](13_Python数据分析—可视化数据.assets/output_7_1.png)

### (二)、折线图

英文是`Line Plot`，`Seaborn`绘制折线图的函数叫做`lineplot`

```python
df2 = sns.load_dataset('flights').query('month == "Jan"')
df2
```

|         | **year** | **month** | **passengers** |
| ------- | :------- | :-------- | :------------- |
| **0**   | 1949     | Jan       | 112            |
| **12**  | 1950     | Jan       | 115            |
| **24**  | 1951     | Jan       | 145            |
| **36**  | 1952     | Jan       | 171            |
| **48**  | 1953     | Jan       | 196            |
| **60**  | 1954     | Jan       | 204            |
| **72**  | 1955     | Jan       | 242            |
| **84**  | 1956     | Jan       | 284            |
| **96**  | 1957     | Jan       | 315            |
| **108** | 1958     | Jan       | 340            |
| **120** | 1959     | Jan       | 360            |
| **132** | 1960     | Jan       | 417            |

```python
sns.lineplot(x=df2.year, y=df2.passengers) 
<Axes: xlabel='year', ylabel='passengers'>
```

![](/assets/output_11_1.RkEYvKqx.png)

### (三)、条形图

英文是`Bar Plot`，`Seaborn`绘制条形图的函数叫做`barplot`

**默认情况下，条柱的高度对应所属分类下的所有值的平均值。如果希望展示其他聚合值，可选参数**`**estimator=聚合函数**`

```python
df3 = sns.load_dataset('penguins')
df3
```

|         | **species** | **island** | **bill\_length\_mm** | **bill\_depth\_mm** | **flipper\_length\_mm** | **body\_mass\_g** | **sex** |
| ------- | :---------- | :--------- | :----------------- | :---------------- | :-------------------- | :-------------- | :------ |
| **0**   | Adelie      | Torgersen  | 39.1               | 18.7              | 181.0                 | 3750.0          | Male    |
| **1**   | Adelie      | Torgersen  | 39.5               | 17.4              | 186.0                 | 3800.0          | Female  |
| **2**   | Adelie      | Torgersen  | 40.3               | 18.0              | 195.0                 | 3250.0          | Female  |
| **3**   | Adelie      | Torgersen  | NaN                | NaN               | NaN                   | NaN             | NaN     |
| **4**   | Adelie      | Torgersen  | 36.7               | 19.3              | 193.0                 | 3450.0          | Female  |
| **...** | ...         | ...        | ...                | ...               | ...                   | ...             | ...     |
| **339** | Gentoo      | Biscoe     | NaN                | NaN               | NaN                   | NaN             | NaN     |
| **340** | Gentoo      | Biscoe     | 46.8               | 14.3              | 215.0                 | 4850.0          | Female  |
| **341** | Gentoo      | Biscoe     | 50.4               | 15.7              | 222.0                 | 5750.0          | Male    |
| **342** | Gentoo      | Biscoe     | 45.2               | 14.8              | 212.0                 | 5200.0          | Female  |
| **343** | Gentoo      | Biscoe     | 49.9               | 16.1              | 213.0                 | 5400.0          | Male    |

```python
sns.barplot(data=df3, x='species', y='body_mass_g') 
<Axes: xlabel='species', ylabel='body_mass_g'> 
```

![](/assets/output_15_1.BueSIFpl.png)

```python
sns.barplot(data=df3, x='species', y='body_mass_g', estimator=np.max) 
<Axes: xlabel='species', ylabel='body_mass_g'>
```

![](/assets/output_16_1.vEfCqNIM.png)

### (四)、计数图

如果希望条柱的高度对应个数，需要使用计数图。

英文是`Count Plot`，`Seaborn`绘制计数图的函数叫做`countplot`

与条形图区别：只需要用一个分类变量绘制图表

```python
sns.countplot(df3, x='species')
plt.show()
```

![](/assets/output_19_0.q08o5VZ8.png)

### (五)、饼图

**Seaborn没有绘制饼图的函数，需要调用Matplotlib的pie函数**

1. 参数

* **直接传入需要计算比例的Series,**`**DataFrame.列名1**`**，Matplotlib中的函数不支持传入整个DataFrame再指定变量**
* 指定标签，可选参数`labels=DataFrame.列名2`

**标签与数据的个数和顺序都要对应**

* 显示比例数字标签, 可选参数`autopct='文字格式'`，例：`autppct='%.1f%%'`

```python
- autopct是用来设置每个扇形里文字标签格式的参数

- 文字格式要以`%`开头；`.1f`表示小数点后保留1位小数；`%%`表示数字标签要以'%'结尾，之所以用两个百分比符号，表示文字标签里的一个百分比，是因为程序需要区分于表示文字格式起始的一个百分比

- 以此类推，如果纸箱保留整数比例数字，可以改成`.0f`;如果想移除百分比符号，可以把格式字符串中，最后两个百分比符号去掉
```

2. **大多数情况，数据集里没有各个类别数量的数据，需要自己获得**

* 对分类变量的Series，调用`value_counts`方法，会获得一个Series。这个Series可以直接传入`pie`函数中，`labels=Series.index`
* `df.groupby('分类变量')['无缺失值的列'].count()`，会返回一个Series。这个Series可以直接传入`pie`函数中，`labels=Series.index`

```python
# 设置示例数据
df4 = pd.DataFrame({'fruit': ['apple', 'orange', 'banana', 'pear'],
                    'vote': [32, 22, 19, 7]})
df4
```

|       | **fruit** | **vote** |
| ----- | :-------- | :------- |
| **0** | apple     | 32       |
| **1** | orange    | 22       |
| **2** | banana    | 19       |
| **3** | pear      | 7        |

```python
plt.pie(df4.vote)
plt.show()
```

![](/assets/output_23_0.Z9hjd6Ub.png)

```python
plt.pie(df4.vote, labels=df4.fruit, autopct='%.1f%%')
plt.show()
```

![](/assets/output_24_0.BivgC9Bc.png)

### (六)、更改图表颜色

许多绘图函数都支持color这个可选参数，`color='颜色的英文'`或者`color='表示颜色的16进制值'`

```python
sns.scatterplot(x=df1.total_bill, y=df1['tip'], color='violet') 

<Axes: xlabel='total_bill', ylabel='tip'>
```

![](/assets/output_27_1.Bim1zDp-.png)

```python
sns.histplot(data=df3, x="body_mass_g",
             color="#c287c7")
plt.show()
```

![](/assets/output_28_0.CQ7X5HGH.png)

### (七)、更改图表色盘

适用于包含分类数据的图表，意思是更改一整套颜色主题

调用`Seaborn`的`set_palette`函数

```python
sns.set_palette('crest')
sns.barplot(data=df3, x='species', y='body_mass_g')
```

![](/assets/output_31_1.Cxn9HyUH.png)

```python
sns.set_palette('pastel')
sns.barplot(data=df3, x='species', y='body_mass_g')
plt.show()
```

![](/assets/output_32_0.BaUYZu2p.png)

## 七、绘制图表—对多个变量绘制图表

```python
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
```

通过Matplotlib和Seaborn，对多个变量绘制图表

### (一)、更改图例的位置

有时图例太长，会挡住图表里的元素，那我们可以把图例的位置进行更改。

调用`Matplotlib`的`legend`函数，传入参数`bbox_to_anchor=(横向位置, 纵向位置)`

`bbox_to_anchor`会把图例放在图表的边框外面去。

元组第一个元素：放入`0`表示图例会在图表左边，`1`表示图表右边

元组第二个元素：放入`0`表示图例顶部和接近图表下边框的位置对齐，`1`表示图例顶部和接近图表上边框的位置对齐

```python
sns.scatterplot(iris, x='sepal_length', y='sepal_width', hue='species', size='petal_length')
plt.legend(bbox_to_anchor=(1, 1))
plt.show()
```

![](/assets/output_5_0.D5Rb3XQ5.png)

### (二)、增加变量

#### 1、给散点图增加变量

```python
iris = sns.load_dataset('iris')
iris
```

|         | **sepal\_length** | **sepal\_width** | **petal\_length** | **petal\_width** | **species** |
| ------- | :--------------- | :-------------- | :--------------- | :-------------- | :---------- |
| **0**   | 5.1              | 3.5             | 1.4              | 0.2             | setosa      |
| **1**   | 4.9              | 3.0             | 1.4              | 0.2             | setosa      |
| **2**   | 4.7              | 3.2             | 1.3              | 0.2             | setosa      |
| **3**   | 4.6              | 3.1             | 1.5              | 0.2             | setosa      |
| **4**   | 5.0              | 3.6             | 1.4              | 0.2             | setosa      |
| **...** | ...              | ...             | ...              | ...             | ...         |
| **145** | 6.7              | 3.0             | 5.2              | 2.3             | virginica   |
| **146** | 6.3              | 2.5             | 5.0              | 1.9             | virginica   |
| **147** | 6.5              | 3.0             | 5.2              | 2.0             | virginica   |
| **148** | 6.2              | 3.4             | 5.4              | 2.3             | virginica   |
| **149** | 5.9              | 3.0             | 5.1              | 1.8             | virginica   |

```python
sns.scatterplot(iris, x='sepal_length', y='sepal_width')
plt.show()
```

![](/assets/output_9_0.D0vGZoHm.png)

##### 1）增加1个分类变量

可以传入可选参数`hue='分类变量'`，散点图会根据不同变量把点画成不同的颜色

```python
sns.scatterplot(iris, x='sepal_length', y='sepal_width', hue='species')
plt.show()
```

![](/assets/output_11_0.C1Mo1rEv.png)

##### 2）增加1个数值变量

通过添加颜色，表示不同数值

* 可以传入可选参数`hue='数值变量'`，深色和浅色分别表示大的数值和小的数值。

```python
sns.scatterplot(iris, x='sepal_length', y='sepal_width', hue='petal_length')
plt.show()
```

![](/assets/output_13_0.DZbK2GFr.png)

通过添加尺寸，表示不同数值

* 可以传入可选参数`size='数值变量'`，通过点的大小表示数值，也即绘制了一个气泡图

```python
sns.scatterplot(iris, x='sepal_length', y='sepal_width', size='petal_length')
plt.show()
```

![](/assets/output_15_0.DpHYw0rU.png)

##### 3）增加1个分类变量，1个数值变量

可选参数`hue='分类变量', size='数值变量'`

用颜色表示种类，用点的大小表示数值，也即绘制了一个彩色气泡图

```python
sns.scatterplot(iris, x='sepal_length', y='sepal_width', hue='species', size='petal_length')
plt.show()
```

![](/assets/output_17_0.DH6lV4Qr.png)

#### 2、给折线图增加变量

增加1个分类变量

可以传入可选参数`hue='分类变量'`，让不同颜色的先分别代表不同变量

```python
flights = sns.load_dataset('flights')
flights
```

|         | **year** | **month** | **passengers** |
| ------- | :------- | :-------- | :------------- |
| **0**   | 1949     | Jan       | 112            |
| **1**   | 1949     | Feb       | 118            |
| **2**   | 1949     | Mar       | 132            |
| **3**   | 1949     | Apr       | 129            |
| **4**   | 1949     | May       | 121            |
| **...** | ...      | ...       | ...            |
| **139** | 1960     | Aug       | 606            |
| **140** | 1960     | Sep       | 508            |
| **141** | 1960     | Oct       | 461            |
| **142** | 1960     | Nov       | 390            |
| **143** | 1960     | Dec       | 432            |

```python
sns.lineplot(x=flights.month, y=flights.passengers, hue=flights.year)
plt.legend(bbox_to_anchor=(1, 1))
plt.show()
```

![](/assets/output_21_0.GwFmxahF.png)

#### 3、给条形图增加变量

增加1个分类变量，变成复式条形图

可以传入可选参数`hue='分类变量'`，让不同颜色的先分别代表不同变量

```python
penguins = sns.load_dataset('penguins')
penguins
```

|         | **species** | **island** | **bill\_length\_mm** | **bill\_depth\_mm** | **flipper\_length\_mm** | **body\_mass\_g** | **sex** |
| ------- | :---------- | :--------- | :----------------- | :---------------- | :-------------------- | :-------------- | :------ |
| **0**   | Adelie      | Torgersen  | 39.1               | 18.7              | 181.0                 | 3750.0          | Male    |
| **1**   | Adelie      | Torgersen  | 39.5               | 17.4              | 186.0                 | 3800.0          | Female  |
| **2**   | Adelie      | Torgersen  | 40.3               | 18.0              | 195.0                 | 3250.0          | Female  |
| **3**   | Adelie      | Torgersen  | NaN                | NaN               | NaN                   | NaN             | NaN     |
| **4**   | Adelie      | Torgersen  | 36.7               | 19.3              | 193.0                 | 3450.0          | Female  |
| **...** | ...         | ...        | ...                | ...               | ...                   | ...             | ...     |
| **339** | Gentoo      | Biscoe     | NaN                | NaN               | NaN                   | NaN             | NaN     |
| **340** | Gentoo      | Biscoe     | 46.8               | 14.3              | 215.0                 | 4850.0          | Female  |
| **341** | Gentoo      | Biscoe     | 50.4               | 15.7              | 222.0                 | 5750.0          | Male    |
| **342** | Gentoo      | Biscoe     | 45.2               | 14.8              | 212.0                 | 5200.0          | Female  |
| **343** | Gentoo      | Biscoe     | 49.9               | 16.1              | 213.0                 | 5400.0          | Male    |

```python
sns.barplot(penguins, x='species', y='body_mass_g', hue='sex')
plt.legend(bbox_to_anchor=(1, 1))
plt.show()
```

![](/assets/output_25_0.B_eqBG-c.png)

#### 4、热力图

英文叫Heat Map，Seaborn绘制热力图的函数叫做heatmap

```python
# 通过透视表导入示例数据
glue = sns.load_dataset('glue').pivot(index='Model', columns='Task', values='Score')
glue
```

| **Task**        | **CoLA** | **MNLI** | **MRPC** | **QNLI** | **QQP** | **RTE** | **SST-2** | **STS-B** |
| :-------------- | -------- | -------- | -------- | -------- | ------- | ------- | --------- | --------- |
| **Model**       |          |          |          |          |         |         |           |           |
| **BERT**        | 60.5     | 86.7     | 89.3     | 92.7     | 72.1    | 70.1    | 94.9      | 87.6      |
| **BiLSTM**      | 11.6     | 65.6     | 81.8     | 74.6     | 62.5    | 57.4    | 82.8      | 70.3      |
| **BiLSTM+Attn** | 18.6     | 67.6     | 83.9     | 74.3     | 60.1    | 58.4    | 83.0      | 72.8      |
| **BiLSTM+CoVe** | 18.5     | 65.4     | 78.7     | 70.8     | 60.6    | 52.7    | 81.9      | 64.4      |
| **BiLSTM+ELMo** | 32.1     | 67.2     | 84.7     | 75.5     | 61.1    | 57.4    | 89.3      | 70.3      |
| **ERNIE**       | 75.5     | 92.3     | 93.9     | 97.3     | 75.2    | 92.6    | 97.8      | 93.0      |
| **RoBERTa**     | 67.8     | 90.8     | 92.3     | 95.4     | 74.3    | 88.2    | 96.7      | 92.2      |
| **T5**          | 71.6     | 92.2     | 92.8     | 96.9     | 75.1    | 92.8    | 97.5      | 93.1      |

```python
sns.heatmap(glue) 

<Axes: xlabel='Task', ylabel='Model'>
```

![](/assets/output_29_1.B2dzerUL.png)

如果要在格子上把对应数值作为标签进行展示的话，传入可选参数`annot=True`，annot是annotation的简写

```python
sns.heatmap(glue, annot=True) 
<Axes: xlabel='Task', ylabel='Model'>
```

![](/assets/output_31_1.Cxn9HyUH.png)

### (三)、图上绘制多个图表

#### 1、图上绘制多个直方图

```python
setosa = iris.query('species == "setosa"')
versicolor = iris.query('species == "versicolor"')
virginica = iris.query('species == "virginica"')
```

##### 1）图上绘制多个直方图

调用多次`histplot`函数，最后`plt.show`展示出来的图片里，多个直方图就都在里面了

```python
sns.histplot(setosa.petal_length)
sns.histplot(versicolor.petal_length)
sns.histplot(virginica.petal_length)
plt.show()
```

![](/assets/output_36_0.blc_2dnq.png)

##### 2）统一条柱宽度

由于调用多次`histplot`函数进行绘制的时候，条柱是按照各自适合的粗细画的，我们只是把它们放到了同一张图上，所以会导致条柱的粗细不统一。

所以我们需要在调用`histplot`函数进行绘制的时候，指定条柱宽度。传入可选参数`binwidth=数值间距`

```python
sns.histplot(setosa.petal_length, binwidth=0.1)
sns.histplot(versicolor.petal_length, binwidth=0.1)
sns.histplot(virginica.petal_length, binwidth=0.1)
plt.show()
```

![](/assets/output_38_0.D_uRKsH2.png)

##### 3）设置图例

另外一个问题是，这个图片没有图例，无法分清哪个颜色对应哪个种类。

为了解决这个问题，我们可以在调用`histplot`函数进行绘制时，传入可选参数`label="标签"`；然后需要调用下`plt.legend()`，手动让图例展示出来

```python
sns.histplot(setosa.petal_length, binwidth=0.1, label='Setosa')
sns.histplot(versicolor.petal_length, binwidth=0.1, label='Versicolor')
sns.histplot(virginica.petal_length, binwidth=0.1, label='Virginica')
plt.legend()
plt.show()
```

![](/assets/output_40_0.CmA1subV.png)

#### 2、图上绘制多个密度图

图上绘制多个密度图与直方图是一样的步骤，唯一的区别是因为密度图是一条平滑的曲线，所以我们不需要可选参数`binwidth`去指定条柱的宽度了

```python
sns.kdeplot(setosa.petal_length, label='Setosa')
sns.kdeplot(versicolor.petal_length, label='Versicolor')
sns.kdeplot(virginica.petal_length, label='Virginica')
plt.legend()
plt.show()
```

![](/assets/output_43_0.DYCO5pVW.png)

#### 3、图上绘制多个箱型图/小提琴图

如果和直方图及密度图一样，在图上绘制多个箱型图/小提琴图，这样它们会都挤在同一条纵线上，因为和前面不同的是，这次横轴并不表示数值

```python
sns.boxplot(setosa.petal_length)
sns.boxplot(versicolor.petal_length)
sns.boxplot(virginica.petal_length)
plt.show()
```

![](/assets/output_46_0.BC5dhofv.png)

```python
sns.violinplot(setosa.petal_length)
sns.violinplot(versicolor.petal_length)
sns.violinplot(virginica.petal_length)
plt.show()
```

![](/assets/output_47_0.BFJ-wBM5.png)

这种情况下，更好的选择是，通过添加坐标轴，在图表上表示分类变量。传入可选参数`x='分类变量'`

```python
sns.boxplot(iris, y='petal_length', x='species')
plt.show()
```

![](/assets/output_49_0.BVThrfKP.png)

```python
sns.violinplot(iris, y='petal_length', x='species')
plt.show()
```

![](/assets/output_50_0.D7AoghSN.png)

总之，当我们想在图上表示多个变量的时候，要根据图表的具体种类选择合适的操作方法。

### (四)、图上绘制多个子图

除了把不同图形放到同一个图里，还有的时候我们是想把单独的图，并排放在一起，这样的话可以让图表排版得更加紧密，因为默认的情况下，Seaborn一行只会放一张图

#### 1、图上绘制多个子图

需要用`Matplotlib`的`subplots`函数，`plt.subplot(行数, 列数)`

```python
plt.subplots(1, 3)
plt.show()
```

![](/assets/output_55_0.BOr5JpUF.png)

#### 2、指定子图的宽和高

传入可选参数`figsize=(宽, 高)`

```python
plt.subplots(1, 3, figsize=(15, 5))
plt.show()
```

![](/assets/output_57_0.SSRPs_5D.png)

#### 3、把图表画进空白的子图

* 把`subplots`返回的结果，赋值为`fig`和`axes`两个变量，`fig`对应整个大图，而`axes`对应一系列子图，所以我们主要用到的是`axes`
* 正常调用绘图函数，调用绘图函数时，传入可选参数`ax=axes[n]`

```python
fig, axes = plt.subplots(1, 3, figsize=(15, 5))
sns.boxplot(iris, y='sepal_length', x='species', ax=axes[0])
sns.boxplot(iris, y='sepal_width', x='species', ax=axes[1])
sns.boxplot(iris, y='petal_length', x='species', ax=axes[2])
plt.show()
```

![](/assets/output_59_0.RHi-puFN.png)

### (五)、配对图

`Seaborn`的`pairplot`函数，传入`DataFrame`后，它会把数据里面数值变量之间的，所有两两关系都绘制出来，一共得到`N*N`个图，N表示传入`DataFrame`里数值变量的个数。

具体来说，它会把各变量的分布用直方图绘制出来，然后把不同变量两两之间的关系绘制成散点图，所以非常适合一次性探索变量分布，以及不同变量之间的关系

```python
sns.pairplot(iris)
plt.show()
```

![](/assets/output_62_0.Cp-sTHF5.png)

也可以传入可选参数`hue='分类变量'`，因此可以一次性对比不同分类变量的数值关系

```python
sns.pairplot(iris, hue='species')
plt.show()
```

![](/assets/output_64_0.Dqgu7GqZ.png)

***

**数据可视化环节，可以用在数据分析流程的方方面面**

1. 清洗环节

可以通过绘制箱型图来寻找异常值的存在，然后根据具体情况考虑是否删除异常值，

2. 假设检验之前

可以先通过直方图或密度图查看它们之间的关系，对比差异从图上来看是否显著，然后再进行科学化的数值检验

3. 绘制图表本身也是一种探索数据、分析数据的方式

---

---
url: /Python/数据分析/8_Python数据分析—评估数据.md
---

# Python数据分析—评估数据

## 一、评估数据

评估数据时，我们主要看两方面：结构和内容

### （一）、结构

1. 结构方面需要清理的数据叫做乱数据；结构方面不需要清理的数据叫做整洁数据。
2. 整洁数据，根据埃德加科德的第三范式，包括以下三个特点：

1）每列是一个变量

2）每行是一个观察值

3）每个单元格是一个值

任何不符合以上三个特点的数据都是乱数据。

不整洁的数据存在多种多样的例子：比如存在合并单元格的表格或者每列存在多个变量。

很多方便人类理解的数据，并不是整洁数据；

而整洁数据有时不太直观。

因为整洁数据并不是方便人类理解，而是让代码更加高效处理和达成分析目的的。

### （二）、内容

#### 1、概念

内容方面需要清理的数据叫脏数据，内容方面不需要清理的数据叫干净数据。

#### 2、脏数据可能存在问题

1. 丢失数据
   1. 有些值为空缺。
   2. 空缺值的影响大小取决于具体情况，有些时候允许某些列的数据不全。
2. 有些值为空缺。
   1. 但我们仍然要进行评估，否则可能导致错误的分析，比如:如果我们没有考虑到有同学缺考，存在成绩缺失，直接用总分数除以总人头数求平均，就会导致计算结果被缺失值拉低。
3. 重复数据
   1. 即数据中有些观察值重复出现。
   2. 有些值的重复不是问题，比如说班级里学生的性别。
   3. 但假如数据中作为唯一表示符的属性存在重复，或者数据中存在有数据实例所有属性值均相同的情况，就是有问题
4. 不一致数据
   1. 即不同数据值实际含义相同，或不同数据值实际指代统一目标
   2. 比如：数字单位不一致、全称和简写混用、中文数字和阿拉伯数字混用等等
5. 无效或错误数据
   1. 脱离现实规则的数据都是无效数据，比如：负数的身高
   2. 虽然符合规则但并不准确的数据是错误数据，比如：一个成年人的体重是3公斤

## 二、评估数据整洁度

数据的整洁度比干净度更好评估，因为结构性问题相比内容性问题，更直观和容易被发现。

通过DataFrame的概况信息，和实际数据，来评估数据的整洁度。

### (一)、info方法列名

DataFrame的info方法，它能提供数据的概况信息，其中列名首先能透露一些关于结构的信息。

比如：如果有列叫做'销售额\_2010'，说明这列应该是同时包含了销售额和年份两个变量，不符合每列只能表示一个变量的整洁性规则。

### (二)、head/tail/sample方法

1. 通过head/tail/sample方法，来获取开头/结尾/随机N行。

通过部分数据来评估，是否符合每列是一个变量，每行是一个观察值，每个单元格是一个值的规则。

2. 在输出部分行检查结构时，如果DataFrame由于列数太多，或者值太长，而展示不全的话，可以通过set\_option函数，调整列数或值长度展示的上限。

## 三、评估数据的干净度

准备数据

```python
import pandas as pd
import numpy as np
scores = pd.DataFrame({"001": {"姓名": "小陈", "考试1": 85, "考试2": 95, "考试3": 92},  "002": {"姓名": "小李", "考试1": 91, "考试2": 92, "考试3": 94},  "003": {"姓名": "小王", "考试1": 86, "考试2": 81, "考试3": np.nan},  "004": {"姓名": "小张", "考试1": 79, "考试2": 89, "考试3": 95},  "005": {"姓名": "小赵", "考试1": 96, "考试2": 91, "考试3": 91},  "006": {"姓名": "小周", "考试1": 81, "考试2": np.nan, "考试3": 92} }).T
scores
```

|         | **姓名** | **考试1** | **考试2** | **考试3** |
| ------- | :------- | :-------- | :-------- | :-------- |
| **001** | 小陈     | 85        | 95        | 92        |
| **002** | 小李     | 91        | 92        | 94        |
| **003** | 小王     | 86        | 81        | NaN       |
| **004** | 小张     | 79        | 89        | 95        |
| **005** | 小赵     | 96        | 91        | 91        |
| **006** | 小周     | 81        | NaN       | 92        |

### (一)、评估丢失数据

#### 1、info方法

将行的数量，与每列非空缺值的数量进行对比，就能知道这列是否存在空缺值。

#### 2、isnull方法

Series和DataFrame都有一个方法叫isnull，作用是检查值是否为空缺值

##### 1）Series中的isnull方法

用在Series上，它会返回一个布尔值组成的Series，告诉我们原Series里的各个值是否为空缺值，是的话就是True，不是就是False

```python
scores.考试2

001     95
002     92
003     81
004     89
005     91
006    NaN
Name: 考试2, dtype: object
```

```python
scores.考试2.isnull()

001    False
002    False
003    False
004    False
005    False
006     True
Name: 考试2, dtype: bool
```

##### 2）DataFrame中的isnull方法

用在DataFrame上，会返回一个布尔值组成的DataFrame，告诉我们原DataFrame里各个值是否为空缺值。

```python
scores
```

|         | **姓名** | **考试1** | **考试2** | **考试3** |
| ------- | :------- | :-------- | :-------- | :-------- |
| **001** | 小陈     | 85        | 95        | 92        |
| **002** | 小李     | 91        | 92        | 94        |
| **003** | 小王     | 86        | 81        | NaN       |
| **004** | 小张     | 79        | 89        | 95        |
| **005** | 小赵     | 96        | 91        | 91        |
| **006** | 小周     | 81        | NaN       | 92        |

```python
scores.isnull()
```

|         | **姓名** | **考试1** | **考试2** | **考试3** |
| ------- | :------- | :-------- | :-------- | :-------- |
| **001** | False    | False     | False     | False     |
| **002** | False    | False     | False     | False     |
| **003** | False    | False     | False     | True      |
| **004** | False    | False     | False     | False     |
| **005** | False    | False     | False     | False     |
| **006** | False    | False     | True      | False     |

**直接对DataFrame调用isnull方法，并不能直观的评估丢失数据**

##### 3）提取丢失数据的行

根据条件筛选DataFrame中的行的原理是：DataFrame的列是Series类型，而Series和条件结合起来，会返回一个布尔值组成的Series，它的长度和DataFrame的行数相对应。DataFrame会用布尔值的Series进行索引，保留True所对应的索引的行。

而在这里，对DataFrame的列调用isnull方法，也会返回一个布尔值组成的Series。将其放在方括号里，DataFrame会用布尔值的Series进行索引，保留True所对应的索引的行，即提取出那列丢失数据的行。

```python
scores[scores["考试3"].isnull()]
```

|         | **姓名** | **考试1** | **考试2** | **考试3** |
| ------- | :------- | :-------- | :-------- | :-------- |
| **003** | 小王     | 86        | 81        | NaN       |

#### 3、连续调用isnull方法和sum方法

由于True和False布尔值，用在数学计算里，会被分别视为数字1和数字0，所以再调用求和方法，就是在算True的数量

```python
True + 5
6
False + 5
5
```

##### 1）Series

计算空缺值的数量

```python
scores['考试2'].isnull().sum()
1
```

##### 2）DataFrame

由于sum方法默认沿着纵向操作，会返回一个列名作为标签索引的Series；

又由于DataFrame的列就是Series，所以返回的Series的值，就是各个列里空缺值的数量

a、计算某列空缺值的个数

```python
scores.考试3.isnull().sum()
1
```

b、计算每列空缺值的个数

```python
scores.isnull().sum()

姓名     0
考试1    0
考试2    1
考试3    1
dtype: int64
```

### (二)、评估重复数据

Series和DataFrame都有一个方法叫duplicated，作用是检查值或行是否存在重复

```python
name = pd.Series(["小陈", "小李", "小王", "小张", "小赵", "小李", "小周"])
gender = pd.Series(["女", "女", "男", "男", "女", "女", "女"])
height = pd.Series([172.5, 168.0, 178.2, 181.3, 161.7, 168.0, 158.5])
student_number = pd.Series(["001", "002", "003", "002", "005", "002", "006"])
students = pd.DataFrame({"学号": student_number, "姓名": name, "性别": gender, "身高": height})
students
```

|       | **学号** | **姓名** | **性别** | **身高** |
| ----- | :------- | :------- | :------- | :------- |
| **0** | 001      | 小陈     | 女       | 172.5    |
| **1** | 002      | 小李     | 女       | 168.0    |
| **2** | 003      | 小王     | 男       | 178.2    |
| **3** | 002      | 小张     | 男       | 181.3    |
| **4** | 005      | 小赵     | 女       | 161.7    |
| **5** | 002      | 小李     | 女       | 168.0    |
| **6** | 006      | 小周     | 女       | 158.5    |

#### 1、Series中的duplicated方法

会返回布尔值组成的Series，表明当前值是否在前面存在过，某个值第一次出现时是False，说明前面不存在重发；但当它第二次、第三次出现时是True，说明这个属于重复值

```python
students.学号

0    001
1    002
2    003
3    002
4    005
5    002
6    006
Name: 学号, dtype: object
```

```python
students.学号.duplicated()

0    False
1    False
2    False
3     True
4    False
5     True
6    False
Name: 学号, dtype: bool
```

#### 2、DataFrame中的duplicated方法

会返回布尔值组成的Series

##### 1）查看所有属性是否存在重复

默认查看所有属性是否存在重复，当这一行里所有值和前面某行完全一样时，才会是True

```python
students.duplicated()

0    False
1    False
2    False
3    False
4    False
5     True
6    False
dtype: bool
```

##### 2）查看任意属性是否存在重复

给duplicated传入可选参数subset，赋值给一个列表，里面放上我们想查看是否存在重复的任意属性。当这些属性同时出现重复的时候，才会是True

```python
students.duplicated(subset=['学号', '性别'])

0    False
1    False
2    False
3    False
4    False
5     True
6    False
dtype: bool
```

##### 3）提取重复数据的行

原理是根据条件筛选DataFrame中的行。

在这里，对DataFrame调用duplicated方法时，会返回一个布尔值组成的Series。将其放在方括号里，DataFrame会用布尔值的Series进行索引，保留True所对应的索引的行，即提取出包含重复数据的行。

**对于长数据，评估重复数据，无法直接查看，只能提取重复数据的行**

```python
students[students.duplicated(subset=['学号', '性别'])]
```

|       | **学号** | **姓名** | **性别** | **身高** |
| ----- | :------- | :------- | :------- | :------- |
| **5** | 002      | 小李     | 女       | 168.0    |

### (三)、评估不一致数据

Series的value\_counts方法

会返回Series里各个值的个数。

我们可以用value\_counts，看到DataFrame某列下面所有值的种类，以及各个种类出现的次数，帮我们确定有没有不同值在表示同一目标的情况。

```python
name = pd.Series(["小陈", "小李", "小李", "小王", "小张", "小赵", "小周"])
gender = pd.Series(["女", "女", "女", "男", "男", "女", "女"])
class_number = pd.Series(["1 班", "1班", "1 班", "一班", "二班", "2 班", "2 班"])
student_number = pd.Series(["001", "002", "002", "003", "004", "005", "006"])
students2 = pd.DataFrame({"学号": student_number, "姓名": name, "性别": gender, "班级": class_number})
students2
```

|       | **学号** | **姓名** | **性别** | **班级** |
| ----- | :------- | :------- | :------- | :------- |
| **0** | 001      | 小陈     | 女       | 1 班     |
| **1** | 002      | 小李     | 女       | 1班      |
| **2** | 002      | 小李     | 女       | 1 班     |
| **3** | 003      | 小王     | 男       | 一班     |
| **4** | 004      | 小张     | 男       | 二班     |
| **5** | 005      | 小赵     | 女       | 2 班     |
| **6** | 006      | 小周     | 女       | 2 班     |

```python
students2['班级']

0    1 班
1     1班
2    1 班
3     一班
4     二班
5    2 班
6    2 班
Name: 班级, dtype: object
```

```python
students2['班级'].value_counts()

1 班    2
2 班    2
1班     1
一班     1
二班     1
Name: 班级, dtype: int64
```

### (四)、评估无效/错误数据

评估无效/错误数据的评估难度很大，比如拿到一个陌生数据集的时候，无法知道上面的地址是不是有效的，某个人的身高是不是准确的，这些都超出了我们能够评估的范围。

但对于特别离谱的数据，还是可以通过某些方法，结合常识，来检查是否存在无效数据

```python
name = pd.Series(["小陈", "小李", "小李", "小王", "小张", "小赵", "小周"])
gender = pd.Series(["女", "女", "女", "男", "男", "女", "女"])
height = pd.Series([172.5, 168.0, 1168.0, 178.2, 181.3, -161.7, 158.5])
students3 = pd.DataFrame({"姓名": name, "性别": gender, "身高": height})
students3
```

|       | **姓名** | **性别** | **身高** |
| ----- | :------- | :------- | :------- |
| **0** | 小陈     | 女       | 172.5    |
| **1** | 小李     | 女       | 168.0    |
| **2** | 小李     | 女       | 1168.0   |
| **3** | 小王     | 男       | 178.2    |
| **4** | 小张     | 男       | 181.3    |
| **5** | 小赵     | 女       | -161.7   |
| **6** | 小周     | 女       | 158.5    |

#### 1、Series的sort\_values方法

可以对某列的值进行排序，能帮我们发现异常的小值或异常的大值

```python
students3.身高.sort_values()
```

```python
5    -161.7
6     158.5
1     168.0
0     172.5
3     178.2
4     181.3
2    1168.0
Name: 身高, dtype: float64
```

#### 2、describe方法

会展示数字列的最小值、最大值等，可以帮我们确实是否有异常数据的存在

```python
students3.describe()
```

|           | **身高**    |
| --------- | :---------- |
| **count** | 7.000000    |
| **mean**  | 266.400000  |
| **std**   | 416.596447  |
| **min**   | -161.700000 |
| **25%**   | 163.250000  |
| **50%**   | 172.500000  |
| **75%**   | 179.750000  |
| **max**   | 1168.000000 |

**除了这些之外，数据还可能存在其它类型的问题，或者需要其它方法才能评估。**

---

---
url: /Python/数据分析/9_Python数据分析—清理数据.md
---

# Python数据分析—清理数据

## 一、清理数据注意事项

在评估之后，下一步是根据评估结果，对数据进行清洗。

### （一）、清洗数据之前

在清洗数据之前，我们要先看看索引或列名是否有意义。

如果索引或列名都是乱七八糟的，应该对它们进行重命名，或重新排序，以便我们理解数据。

### （二）、结构性问题

清洗数据，我们一般会先解决结构性问题，再处理内容性问题。

整洁数据，根据埃德加科德的第三范式，包括以下三个特点：

1. 每列是一个变量
2. 每行是一个观察值
3. 每个单元格是一个值

任何不符合以上三个特点的数据都是乱数据。

#### 1、每列是观察值，每行是变量

对行和列进行转置

#### 2、每列包含多个变量

* 对列进行拆分，把多的变量分到其它列去
* 有的时候光拆分还不够，还要进行重塑，确保每列只包含一种变量

比如：许多列同时包含两个或多个变量的时候

#### 3、每行包含多个观察值

* 对行进行拆分，让每个观察值为独立的一行
* 有的时候光拆分还不够，还要进行重塑，确保每列只包含一种观察值

比如：许多行同时包含两个或多个观察值的时候

很多时候，清理前的数据是宽数据，清理后的数据是长数据。

我们清理的目的，是为了后续能更高效地用程序处理数据，而不是更方便地让人类理解，所以清理前的宽数据更直观易懂也是正常的。

### （三）、内容性问题

在确保结构不存在问题后，我们再去深入到内容，处理脏数据。

#### 1、缺失数据

针对缺失数据，处理方式需要具体情况具体分析。

* 如果恰好知道空缺值的实际值，可以更新表格数据，人工把那个值填进去。
* 如果我们不知道空缺值的实际值，而缺失值并不影响此次分析，最直接的办法是不处理缺失值。

Pandas在计算的时候，会自动忽略缺失值，所以很多时候放着不管不会造成什么问题。

* 如果是关键变量缺失，我们可以把变量为空缺的行删掉，只留下对分析结果有意义的数据。
* 如果是关键变量缺失，我们也可以用填充值的方式去处理，比如说把平均数、中位数、众数等填充进去，来代替空缺值

#### 2、重复数据

针对重复数据，我们的处理方式就很简单了。

找到后删除即可，不删除的话，重复数据可能影响分析结论

#### 3、不一致数据

针对不一致数据，我们的目标是对它们进行统一。针对同一含义，只保留一种表达方式，把其余的都进行替换

#### 4、无效/错误数据

针对无效/错误数据，也有不同的清洗途径。

比如删除/替换，否则留下无效/错误数据，也可能影响分析结论。比如说一个负数的身高记录值，可以严重拉低平均值的分析。

1. 把那条记录值进行删除。因为Pandas会自动忽略空缺值，所以NaN值反而不影响平均值计算
2. 替换成其他值。比如说平均数。

### （四）、其他问题

除了数据本身的问题以外，我们清理数据时，有时候还要针对编程语言或库，做一些其他的处理，包括对数据类型进行转换。

比如：把手机号从数字类型转换成字符串类型；把'是'和'否'转换成布尔值True和False，能让我们之后针对这个变量的分析更加方便，包括能更简洁地进行逻辑判断

## 二、清理索引和列名

### （一）、 重命名索引和列名(使用字典)

DataFrame的`rename`方法，`rename`方法会返回一个新的DataFrame

适用于列名或索引数量少的情况

```python
import pandas as pd
df1 = pd.read_csv('example1.csv', index_col=0)
df1
```

|        | **客户\_姓名** | **客户 性别** | **age** | **邮箱**             |
| ------ | :------------ | :------------ | :------ | :------------------- |
| **1**  | 李十          | 女            | 58      | yVaQXekW@example.com |
| **2\_** | 冯三          | 男            | 27      | LctXbrEM@example.com |
| **3**  | 吴十          | 男            | 32      | UyTYBzUZ@example.com |
| **4**  | 王十          | 女            | 58      | KCixgciF@example.com |
| **\_5** | 钱十          | 女            | 27      | nqRBXOtA@example.com |
| **6**\* | 赵八          | 女            | 51      | GyKwILAL@example.com |

#### 1、重命名索引

放入可选参数index，并赋值为一个字典。

字典中，把需要修改的老的名字作为键，把对应的新的名字作为值。

```python
df1 = df1.rename(index={'2_': '2', '_5': '5', '6*': '6'})
df1
```

|       | **客户\_姓名** | **客户 性别** | **age** | **邮箱**             |
| ----- | :------------ | :------------ | :------ | :------------------- |
| **1** | 李十          | 女            | 58      | yVaQXekW@example.com |
| **2** | 冯三          | 男            | 27      | LctXbrEM@example.com |
| **3** | 吴十          | 男            | 32      | UyTYBzUZ@example.com |
| **4** | 王十          | 女            | 58      | KCixgciF@example.com |
| **5** | 钱十          | 女            | 27      | nqRBXOtA@example.com |
| **6** | 赵八          | 女            | 51      | GyKwILAL@example.com |

#### 2、重命名列名

放入可选参数columns，并赋值为一个字典

字典中，把需要修改的老的名字作为键，把对应的新的名字作为值。

```python
df1 = df1.rename(columns={'客户_姓名': '客户姓名', '客户 性别': '客户性别', 'age': '客户年龄', '邮箱': '客户邮箱'})
df1
```

|       | **客户姓名** | **客户性别** | **客户年龄** | **客户邮箱**         |
| ----- | :----------- | :----------- | :----------- | :------------------- |
| **1** | 李十         | 女           | 58           | yVaQXekW@example.com |
| **2** | 冯三         | 男           | 27           | LctXbrEM@example.com |
| **3** | 吴十         | 男           | 32           | UyTYBzUZ@example.com |
| **4** | 王十         | 女           | 58           | KCixgciF@example.com |
| **5** | 钱十         | 女           | 27           | nqRBXOtA@example.com |
| **6** | 赵八         | 女           | 51           | GyKwILAL@example.com |

### （二）、重命名索引和列名(函数/方法)

rename方法，除了传入字典，还可以传入一个针对Series的函数或方法，可以是Pandas库里面的，也可以是我们自行定义的

```python
df2.rename(index=函数/方法)
df2.rename(columns=函数/方法)
```

这个函数/方法会把原本索引或列名作为参数，然后返回新的名字。

举个例子:

#### 1、把所有列名变成大写

可以给columns这个可选参数，传入str.upper方法

str是Series类自带的一个属性，会返回一个包含了很多字符串相关操作方法的,StringMethods类的实例(返回实例才可以调用方法)，对这个StringMethods实例调用upper方法，就会把Series里所有字符变成大写。

```python
s1 = pd.Series(["hello", "this", "is", "Zheng"])
s1

0    hello
1     this
2       is
3    Zheng
dtype: object
```

```python
s1.str
<pandas.core.strings.accessor.StringMethods at 0x1e6708d9710>
```

```python
s1.str.upper()

0    HELLO
1     THIS
2       IS
3    ZHENG
dtype: object
```

作为columns参数的值，传给rename，执行后就会返回一个列名全变成了大写的DataFrame

```python
df2 = pd.read_csv("example2.csv")
df2
```

|       | **Date**   | **Amount** | **Salesperson** | **Location**  |
| ----- | :--------- | :--------- | :-------------- | :------------ |
| **0** | 2022-01-01 | 1000       | Alice           | New York      |
| **1** | 2022-01-02 | 1500       | Bob             | San Francisco |
| **2** | 2022-01-03 | 800        | Charlie         | New York      |
| **3** | 2022-01-04 | 1200       | David           | San Francisco |

```python
df2.rename(columns=str.upper)
```

|       | **DATE**   | **AMOUNT** | **SALESPERSON** | **LOCATION**  |
| ----- | :--------- | :--------- | :-------------- | :------------ |
| **0** | 2022-01-01 | 1000       | Alice           | New York      |
| **1** | 2022-01-02 | 1500       | Bob             | San Francisco |
| **2** | 2022-01-03 | 800        | Charlie         | New York      |
| **3** | 2022-01-04 | 1200       | David           | San Francisco |

#### 2、其他操作

1. 如果想了解其它对字符串Series调用的方法，可以查看Pandas官方文档

像这些所有以`pandas.Series.str`开头的，都是可以用在`Pandas Series`上的字符串操作

所有Series的方法相关文档：

<https://pandas.pydata.org/docs/reference/api/pandas.Series.html>

2. 所有DataFrame的方法相关文档：

<https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html>

3. Pandas 官万文档: <https://pandas.pydata.org/docs/reference>

里面有所有 Pandas 库里面的类、类方法、类属性介绍等等。包括Series、DataFrame等

### （三）、设置索引

#### 1、把某列设为索引

set\_index方法，可以直接把某列属性的值用来作为索引，里面直接传入列名，会返回一个新的DataFrame，其索引变成了那列的值

```python
df3 = df2.set_index('Salesperson')
df3
```

|                 | **Date**   | **Amount** | **Location**  |
| --------------- | ---------- | ---------- | ------------- |
| **Salesperson** |            |            |               |
| **Alice**       | 2022-01-01 | 1000       | New York      |
| **Bob**         | 2022-01-02 | 1500       | San Francisco |
| **Charlie**     | 2022-01-03 | 800        | New York      |
| **David**       | 2022-01-04 | 1200       | San Francisco |

#### 2、重置索引

reset\_index方法，相当于做的和set\_index相反的事，即把索引重设为初始默认的位置索引，并且把原本作为index的值变成单独一列

```python
df3.reset_index()
```

|       | **Salesperson** | **Date**   | **Amount** | **Location**  |
| ----- | :-------------- | :--------- | :--------- | :------------ |
| **0** | Alice           | 2022-01-01 | 1000       | New York      |
| **1** | Bob             | 2022-01-02 | 1500       | San Francisco |
| **2** | Charlie         | 2022-01-03 | 800        | New York      |
| **3** | David           | 2022-01-04 | 1200       | San Francisco |

### （四）、对索引和列名重新排序

sort\_index方法，适用于数据的索引或列名符合要求，但它的顺序是乱的情况

```python
df4 = pd.read_csv('example3.csv', index_col=0)
df4
```

|       | **客户姓名** | **客户性别** | **客户年龄** | **客户邮箱**         |
| ----- | :----------- | :----------- | :----------- | :------------------- |
| **a** | 李十         | 女           | 58           | yVaQXekW@example.com |
| **c** | 冯三         | 男           | 27           | LctXbrEM@example.com |
| **b** | 吴十         | 男           | 32           | UyTYBzUZ@example.com |
| **e** | 王十         | 女           | 58           | KCixgciF@example.com |
| **d** | 钱十         | 女           | 27           | nqRBXOtA@example.com |
| **f** | 赵八         | 女           | 51           | GyKwILAL@example.com |

#### 1、对索引进行排序

默认axis=0,沿着索引纵向操作，对索引排序

```python
df4 = df4.sort_index()
df4
```

|       | **客户姓名** | **客户性别** | **客户年龄** | **客户邮箱**         |
| ----- | :----------- | :----------- | :----------- | :------------------- |
| **a** | 李十         | 女           | 58           | yVaQXekW@example.com |
| **b** | 吴十         | 男           | 32           | UyTYBzUZ@example.com |
| **c** | 冯三         | 男           | 27           | LctXbrEM@example.com |
| **d** | 钱十         | 女           | 27           | nqRBXOtA@example.com |
| **e** | 王十         | 女           | 58           | KCixgciF@example.com |
| **f** | 赵八         | 女           | 51           | GyKwILAL@example.com |

#### 2、对列名进行排序

指定axis=1,沿着列名纵向操作，对列名排序

```python
df4.sort_index(axis=1)
```

|       | **客户姓名** | **客户年龄** | **客户性别** | **客户邮箱**         |
| ----- | :----------- | :----------- | :----------- | :------------------- |
| **a** | 李十         | 58           | 女           | yVaQXekW@example.com |
| **b** | 吴十         | 32           | 男           | UyTYBzUZ@example.com |
| **c** | 冯三         | 27           | 男           | LctXbrEM@example.com |
| **d** | 钱十         | 27           | 女           | nqRBXOtA@example.com |
| **e** | 王十         | 58           | 女           | KCixgciF@example.com |
| **f** | 赵八         | 51           | 女           | GyKwILAL@example.com |

### （五）、可选参数inplace

适用于rename、set\_index、reset\_index、sort\_index方法

传入'inplace=True'，表示直接原地修改原始DataFrame，就不用进行赋值了；

并且此时方法不返回新的DataFrame，如果赋值给变量，则变量变为空值。

## 三、清理乱数据

整洁数据，根据埃德加科德的第三范式，包括以下三个特点：

1. 每列是一个变量
2. 每行是一个观察值
3. 每个单元格是一个值

任何不符合以上三个特点的数据都是乱数据。

### （一）、对数据进行转置

当数据的列和行是反的，也就是每列是观察值，每行是变量，需要对数据进行转置

DataFrame的T属性，能返回一个行和列调换后的DataFrame

```python
import pandas as pd
df1 = pd.read_csv('example4.csv', index_col=0)
df1
```

|          | **0** | **1** | **2** |
| -------- | :---- | :---- | :---- |
| **姓名** | 张三  | 李四  | 王五  |
| **年龄** | 30    | 25    | 28    |
| **地址** | 北京  | 上海  | 广州  |

```python
df1 = df1.T
df1
```

|       | **姓名** | **年龄** | **地址** |
| ----- | :------- | :------- | :------- |
| **0** | 张三     | 30       | 北京     |
| **1** | 李四     | 25       | 上海     |
| **2** | 王五     | 28       | 广州     |

### （二）、对列进行拆分

当一列有多个变量时，把一列拆分成多列

```python
df2 = pd.read_csv("example5.csv")
df2
```

|       | **城市** | **人口密度**           |
| ----- | :------- | :--------------------- |
| **0** | 城市A    | 28.53万人/0.87平方公里 |
| **1** | 城市B    | 37.83万人/2.19平方公里 |
| **2** | 城市C    | 15.3万人/1.57平方公里  |

#### 1、拆分列

1）针对Python字符串

split方法，参数可以指定分隔符，就会得到拆分后字符串所组成的列表

```python
"28.53万人/0.87平方公里".split("/")

['28.53万人', '0.87平方公里']
```

2）针对Series

str.split方法，参数可以指定分隔符，拆分字符串，调用后返回一个新的Series，每一个元素是一个列表。

但这没有达到拆分列的效果，因为Series只能表示DataFrame的一列，而我们希望拆分成多列。

```python
df2['人口密度'].str.split('/')
```

```python
0    [28.53万人, 0.87平方公里]
1    [37.83万人, 2.19平方公里]
2     [15.3万人, 1.57平方公里]
Name: 人口密度, dtype: object
```

3）拆分列(其实也是针对Series)

str.split方法，额外指定一个可选参数expand=True，表示把分隔后的结果分别用单独的Series表示，这样输出结果就会变成一个包含多列的DataFrame

```python
df2['人口密度'].str.split('/', expand=True)
```

|       | **0**     | **1**        |
| ----- | :-------- | :----------- |
| **0** | 28.53万人 | 0.87平方公里 |
| **1** | 37.83万人 | 2.19平方公里 |
| **2** | 15.3万人  | 1.57平方公里 |

#### 2、把新生成的DataFrame添加进原先的DataFrame

其实就是添加多列

在DataFrame后面方括号里放入列表，列表里放入多个列名，赋值给要加入的新列所组成的DataFrame

```python
df2[['人口', '面积']] = df2['人口密度'].str.split('/', expand=True)
df2
```

|       | **城市** | **人口密度**           | **人口**  | **面积**     |
| ----- | :------- | :--------------------- | :-------- | :----------- |
| **0** | 城市A    | 28.53万人/0.87平方公里 | 28.53万人 | 0.87平方公里 |
| **1** | 城市B    | 37.83万人/2.19平方公里 | 37.83万人 | 2.19平方公里 |
| **2** | 城市C    | 15.3万人/1.57平方公里  | 15.3万人  | 1.57平方公里 |

#### 3、删除拆分前的列

drop方法，指定axis=1

```python
df2 = df2.drop('人口密度', axis=1)
df2
```

|       | **城市** | **人口**  | **面积**     |
| ----- | :------- | :-------- | :----------- |
| **0** | 城市A    | 28.53万人 | 0.87平方公里 |
| **1** | 城市B    | 37.83万人 | 2.19平方公里 |
| **2** | 城市C    | 15.3万人  | 1.57平方公里 |

### （三）、把不同列合并成一列

```python
df3 = pd.DataFrame({'姓': ['张', '李', '王'], '名': ['三', '四', '五'], '年龄': [30, 25, 35]})
df3
```

|       | **姓** | **名** | **年龄** |
| ----- | :----- | :----- | :------- |
| **0** | 张     | 三     | 30       |
| **1** | 李     | 四     | 25       |
| **2** | 王     | 五     | 35       |

1. 拼接字符串

str.cat方法，参数传入拼接的Series，可以拼接2个Series。

可以传入可选参数sep，来指定拼接时的分隔符

2. 添加列
3. 删除拼接前的列

df3.姓.str.cat(df3.名)

```python
0    张三
1    李四
2    王五
Name: 姓, dtype: object
```

df3\['姓'].str.cat(df3\['名'], sep='-')

```python
0    张-三
1    李-四
2    王-五
Name: 姓, dtype: object
```

```python
df3['姓名'] = df3.姓.str.cat(df3.名)
df3 = df3.drop(['姓', '名'], axis=1)
df3
```

|       | **年龄** | **姓名** |
| ----- | :------- | :------- |
| **0** | 30       | 张三     |
| **1** | 25       | 李四     |
| **2** | 35       | 王五     |

### （四）、把宽数据转换成长数据

```python
df4 = pd.read_csv('example6.csv')
df4
```

|       | **国家代码** | **年份** | **男性年龄组（0-4岁）** | **男性年龄组（5-14岁）** | **男性年龄组（15-24岁）** | **女性年龄组（0-4岁）** | **女性年龄组（5-14岁）** | **女性年龄组（15-24岁）** |
| ----- | :----------- | :------- | :---------------------- | :----------------------- | :------------------------ | :---------------------- | :----------------------- | :------------------------ |
| **0** | CN           | 2019     | 100                     | 200                      | 500                       | 80                      | 150                      | 400                       |
| **1** | US           | 2019     | 50                      | 150                      | 300                       | 40                      | 100                      | 200                       |
| **2** | JP           | 2019     | 30                      | 120                      | 250                       | 20                      | 80                       | 150                       |
| **3** | IN           | 2019     | 80                      | 180                      | 400                       | 60                      | 120                      | 300                       |

即重塑列，具体来说是在把一部分列名的值，转换为变量的值

pd.melt方法，可以帮我们进行这方面的转换。

第一个参数传入要转换的DataFrame

可选参数id\_vars传入一个列表，里面放入想保持原样的列，那么除了id\_vars列表里面之外的列，都会被视为要进行转换的列

可选参数var\_name，要被赋值为在转换后的DataFrame中，包含原本列名值的新列列名

可选参数value\_name，要被赋值为在转换后的DataFrame中，包含原本变量值的新列列名

运行后，就会返回一个从原本宽数据转换成长数据的DataFrame

```python
df4 = pd.melt(df4, 
              id_vars=['国家代码', '年份'], 
              var_name='年龄组', 
              value_name='肺结核病例数')
df4
```

|        | **国家代码** | **年份** | **年龄组**            | **肺结核病例数** |
| ------ | :----------- | :------- | :-------------------- | :--------------- |
| **0**  | CN           | 2019     | 男性年龄组（0-4岁）   | 100              |
| **1**  | US           | 2019     | 男性年龄组（0-4岁）   | 50               |
| **2**  | JP           | 2019     | 男性年龄组（0-4岁）   | 30               |
| **3**  | IN           | 2019     | 男性年龄组（0-4岁）   | 80               |
| **4**  | CN           | 2019     | 男性年龄组（5-14岁）  | 200              |
| **5**  | US           | 2019     | 男性年龄组（5-14岁）  | 150              |
| **6**  | JP           | 2019     | 男性年龄组（5-14岁）  | 120              |
| **7**  | IN           | 2019     | 男性年龄组（5-14岁）  | 180              |
| **8**  | CN           | 2019     | 男性年龄组（15-24岁） | 500              |
| **9**  | US           | 2019     | 男性年龄组（15-24岁） | 300              |
| **10** | JP           | 2019     | 男性年龄组（15-24岁） | 250              |
| **11** | IN           | 2019     | 男性年龄组（15-24岁） | 400              |
| **12** | CN           | 2019     | 女性年龄组（0-4岁）   | 80               |
| **13** | US           | 2019     | 女性年龄组（0-4岁）   | 40               |
| **14** | JP           | 2019     | 女性年龄组（0-4岁）   | 20               |
| **15** | IN           | 2019     | 女性年龄组（0-4岁）   | 60               |
| **16** | CN           | 2019     | 女性年龄组（5-14岁）  | 150              |
| **17** | US           | 2019     | 女性年龄组（5-14岁）  | 100              |
| **18** | JP           | 2019     | 女性年龄组（5-14岁）  | 80               |
| **19** | IN           | 2019     | 女性年龄组（5-14岁）  | 120              |
| **20** | CN           | 2019     | 女性年龄组（15-24岁） | 400              |
| **21** | US           | 2019     | 女性年龄组（15-24岁） | 200              |
| **22** | JP           | 2019     | 女性年龄组（15-24岁） | 150              |
| **23** | IN           | 2019     | 女性年龄组（15-24岁） | 300              |

### （五）、对行进行拆分

```python
df5 = pd.read_csv("example7.csv")
df5
```

|       | **学生姓名** | **学号** | **课程列表**                     |
| ----- | :----------- | :------- | :------------------------------- |
| **0** | 张三         | 1        | \['数学', '物理']                 |
| **1** | 李四         | 2        | \['英语', '化学', '历史']         |
| **2** | 王五         | 3        | \['语文', '数学', '英语', '政治'] |
| **3** | 赵六         | 4        | \['物理', '生物']                 |

虽然上面的课程列表变量看起来像是列表，但实际上是字符串

比如第一行里的课程列表实际为字符串"\['数学', '物理']"，而不是列表\['数学', '物理']

所以我们需要先将字符串形式的列表转换为实际的列表对象，这样explode才能对列表进行拆分

```python
import json
df5['课程列表'] = df5['课程列表'].apply(lambda x: json.loads(x.replace("'", "\"")))
df5
```

|       | **学生姓名** | **学号** | **课程列表**             |
| ----- | :----------- | :------- | :----------------------- |
| **0** | 张三         | 1        | \[数学, 物理]             |
| **1** | 李四         | 2        | \[英语, 化学, 历史]       |
| **2** | 王五         | 3        | \[语文, 数学, 英语, 政治] |
| **3** | 赵六         | 4        | \[物理, 生物]             |

当某列的值都是列表，而不是一个个独立的值

df.explode方法，传入需要拆分的变量名，就会返回一个把该列列表中每一个元素，转换为单独一行的新DataFrame

这个方法很实用，当每个单元格是一个值而不是列表时，更好进行统计

```python
df5 = df5.explode('课程列表')
df5
```

|       | **学生姓名** | **学号** | **课程列表** |
| ----- | :----------- | :------- | :----------- |
| **0** | 张三         | 1        | 数学         |
| **0** | 张三         | 1        | 物理         |
| **1** | 李四         | 2        | 英语         |
| **1** | 李四         | 2        | 化学         |
| **1** | 李四         | 2        | 历史         |
| **2** | 王五         | 3        | 语文         |
| **2** | 王五         | 3        | 数学         |
| **2** | 王五         | 3        | 英语         |
| **2** | 王五         | 3        | 政治         |
| **3** | 赵六         | 4        | 物理         |
| **3** | 赵六         | 4        | 生物         |

### （六）、对行或列进行删除

当我们想清理无效或无用的行或列

drop方法

**基本上，对DataFrame的操作方法，都是默认不改变原始DataFrame的，而是返回一个新的DataFrame**

**要么就得进行重新赋值，要么指定可选参数inplace=True**

## 四、清理脏数据

```python
import pandas as pd
import numpy as np
```

### （一）、处理缺失数据

#### 1、对整列缺失值进行填充

```python
# 创建示例 DataFrame
df1 = pd.DataFrame({
    '日期': ['2005-01-01', '2005-01-02', '2005-01-03', '2005-01-03'],
    '销售额': [1000, 1500, 800, 1200],
    '销售人员': ['李华', '王磊', '刘娜', '张洋'],
    '地址': ['苏州', '郑州', '南京', '西安']
})
df1 = df1.rename(index={
    0: '001',
    1: '002',
    2: '003',
    3: '004'
})
df1['国家'] = np.nan
df1
```

|         | **日期**   | **销售额** | **销售人员** | **地址** | **国家** |
| ------- | :--------- | :--------- | :----------- | :------- | :------- |
| **001** | 2005-01-01 | 1000       | 李华         | 苏州     | NaN      |
| **002** | 2005-01-02 | 1500       | 王磊         | 郑州     | NaN      |
| **003** | 2005-01-03 | 800        | 刘娜         | 南京     | NaN      |
| **004** | 2005-01-03 | 1200       | 张洋         | 西安     | NaN      |

1. 对全是缺失值的列填入统一值，可以通过针对列的赋值操作
2. 对全是缺失值的列填入不同值

见更新DataFrame的一列值 2.15.一.1.

```python
df1["国家"] = "中国"
df1
```

|         | **日期**   | **销售额** | **销售人员** | **地址** | **国家** |
| ------- | :--------- | :--------- | :----------- | :------- | :------- |
| **001** | 2005-01-01 | 1000       | 李华         | 苏州     | 中国     |
| **002** | 2005-01-02 | 1500       | 王磊         | 郑州     | 中国     |
| **003** | 2005-01-03 | 800        | 刘娜         | 南京     | 中国     |
| **004** | 2005-01-03 | 1200       | 张洋         | 西安     | 中国     |

#### 2、对整行缺失值进行填充

1. 对全是缺失值的行填入统一值，可以通过针对行的赋值操作
2. 对全是缺失值的行填入不同值，见更新DataFrame的一行值

```python
df1.iloc[3] = np.nan
df1
```

|         | **日期**   | **销售额** | **销售人员** | **地址** | **国家** |
| ------- | :--------- | :--------- | :----------- | :------- | :------- |
| **001** | 2005-01-01 | 1000.0     | 李华         | 苏州     | 中国     |
| **002** | 2005-01-02 | 1500.0     | 王磊         | 郑州     | 中国     |
| **003** | 2005-01-03 | 800.0      | 刘娜         | 南京     | 中国     |
| **004** | NaN        | NaN        | NaN          | NaN      | NaN      |

```python
df1.iloc[3] = "test"
df1
```

|         | **日期**   | **销售额** | **销售人员** | **地址** | **国家** |
| ------- | :--------- | :--------- | :----------- | :------- | :------- |
| **001** | 2005-01-01 | 1000.0     | 李华         | 苏州     | 中国     |
| **002** | 2005-01-02 | 1500.0     | 王磊         | 郑州     | 中国     |
| **003** | 2005-01-03 | 800.0      | 刘娜         | 南京     | 中国     |
| **004** | test       | test       | test         | test     | test     |

#### 3、对某个缺失值进行填充

```python
# 创建示例 DataFrame
df2 = pd.DataFrame({
    '日期': ['2005-01-01', '2005-01-02', '2005-01-03', '2005-01-03'],
    '销售额': [1000, 1500, np.nan, 1200],
    '销售人员': ['李华', '王磊', '刘娜', '张洋'],
    '地址': ['苏州', '郑州', '南京', '西安']
})
df2 = df2.rename(index={
    0: '001',
    1: '002',
    2: '003',
    3: '004'
})
df2
```

|         | **日期**   | **销售额** | **销售人员** | **地址** |
| ------- | :--------- | :--------- | :----------- | :------- |
| **001** | 2005-01-01 | 1000.0     | 李华         | 苏州     |
| **002** | 2005-01-02 | 1500.0     | 王磊         | 郑州     |
| **003** | 2005-01-03 | NaN        | 刘娜         | 南京     |
| **004** | 2005-01-03 | 1200.0     | 张洋         | 西安     |

可以先用loc或iloc先去定位到那个值，然后重新赋值

用loc或iloc先去定位值，见Dataframe章节

```python
df2.loc['003', '销售额'] = 800
df2
```

|         | **日期**   | **销售额** | **销售人员** | **地址** |
| ------- | :--------- | :--------- | :----------- | :------- |
| **001** | 2005-01-01 | 1000.0     | 李华         | 苏州     |
| **002** | 2005-01-02 | 1500.0     | 王磊         | 郑州     |
| **003** | 2005-01-03 | 800.0      | 刘娜         | 南京     |
| **004** | 2005-01-03 | 1200.0     | 张洋         | 西安     |

#### 4、对部分缺失值进行填充

```python
df3 = pd.DataFrame({
    '日期': ['2005-01-01', '2005-01-02', np.nan, np.nan],
    '销售额': [1000, 1500, 800, 1200],
    '销售人员': ['李华', '王磊', '刘娜', '张洋'],
    '地址': ['苏州', '郑州', '南京', '西安']
})
df3 = df3.rename(index={
    0: '001',
    1: '002',
    2: '003',
    3: '004'
})
df3
```

|         | **日期**   | **销售额** | **销售人员** | **地址** |
| ------- | :--------- | :--------- | :----------- | :------- |
| **001** | 2005-01-01 | 1000       | 李华         | 苏州     |
| **002** | 2005-01-02 | 1500       | 王磊         | 郑州     |
| **003** | NaN        | 800        | 刘娜         | 南京     |
| **004** | NaN        | 1200       | 张洋         | 西安     |

##### 1）填入统一值

如果要替换某一部分的值，也是一样的道理，能用loc或iloc提取出来的部分，都可以通过赋值来填充或替换值。

```python
df3.loc['003': '004', '日期'] = '2005-01-03'
df3
```

|         | **日期**   | **销售额** | **销售人员** | **地址** |
| ------- | :--------- | :--------- | :----------- | :------- |
| **001** | 2005-01-01 | 1000       | 李华         | 苏州     |
| **002** | 2005-01-02 | 1500       | 王磊         | 郑州     |
| **003** | 2005-01-03 | 800        | 刘娜         | 南京     |
| **004** | 2005-01-03 | 1200       | 张洋         | 西安     |

##### 2）分别填入值

如果要对部分缺失值，分别填入不同值，可以用元组或列表的形式来赋值

**缺失值的个数和元组或列表里元素的个数，要一致**

```python
df3.loc['003': '004', '日期'] = ['2005-01-03', '2023-9-15']
df3
```

|         | **日期**   | **销售额** | **销售人员** | **地址** |
| ------- | :--------- | :--------- | :----------- | :------- |
| **001** | 2005-01-01 | 1000       | 李华         | 苏州     |
| **002** | 2005-01-02 | 1500       | 王磊         | 郑州     |
| **003** | 2005-01-03 | 800        | 刘娜         | 南京     |
| **004** | 2023-9-15  | 1200       | 张洋         | 西安     |

#### 5、自动找到缺失值进行填充

以上方法都需要我们自行定位缺失值的位置，但在数据量很大，缺失值很多的情况下，更需要程序能自动找到所有为NaN的值，然后根据我们的指示进行填充

```python
df4 = pd.DataFrame({'A': [1, 2, np.nan, 4],
                    'B': [5, np.nan, 7, np.nan],
                    'C': [8, 9, 10, 11]})
df4
```

|       | **A** | **B** | **C** |
| ----- | :---- | :---- | :---- |
| **0** | 1.0   | 5.0   | 8     |
| **1** | 2.0   | NaN   | 9     |
| **2** | NaN   | 7.0   | 10    |
| **3** | 4.0   | NaN   | 11    |

##### 1）Series的fillna方法

Series有个叫fillna的方法，调用时列里面所有NaN值都会被替换成传入的参数，就不需要一个个去定位缺失值的位置了

a、对缺失值填入确定的值

```python
df4['B'].fillna(0)

0    5.0
1    0.0
2    7.0
3    0.0
Name: B, dtype: float64
```

b、对缺失值填入某个计算结果

fillna方法里，除了传入确定的参数，还可以替换成某个计算结果，如平均值

```python
df4['B'].fillna(df4['B'].mean())

0    5.0
1    6.0
2    7.0
3    6.0
Name: B, dtype: float64
```

##### 2）DataFrame的fillna方法

DataFrame也有fillna方法，所以除了能填充某列的缺失值，还能直接对整个DataFrame里的缺失值进行填充。

a、对缺失值填入统一值

传入某个值后，返回的新DataFrame里，所有缺失值都会变成那个值

```python
df4.fillna(0)
```

|       | **A** | **B** | **C** |
| ----- | :---- | :---- | :---- |
| **0** | 1.0   | 5.0   | 8     |
| **1** | 2.0   | 0.0   | 9     |
| **2** | 0.0   | 7.0   | 10    |
| **3** | 4.0   | 0.0   | 11    |

b、对缺失值按列填入不同值

还可以传入一个字典，键为列名，值为替换值，能实现把不同列里的空缺值，替换成不同值的效果。这个相比Series的fillna方法逐列操作要高效得多

```python
df4.fillna({'A': 0, 'B': 10, 'C': 20})
```

|       | **A** | **B** | **C** |
| ----- | :---- | :---- | :---- |
| **0** | 1.0   | 5.0   | 8     |
| **1** | 2.0   | 10.0  | 9     |
| **2** | 0.0   | 7.0   | 10    |
| **3** | 4.0   | 10.0  | 11    |

**只返回替换好的新DataFrame**

#### 6、删除存在缺失值的行

如果丢失的是对分析目标有关键意义的数据，同时又没有合适的值进行填充，这时我们或许需要抛弃存在空缺值的观察值，调用dropna方法，被用于删除有缺失值的数据

```python
df5 = pd.DataFrame({
    '姓名': ['John', 'Alice', 'Bob', 'Mary'],
    '年龄': [25, 30, np.nan, 40],
    '工资': [50000, np.nan, 70000, 60000],
    '性别': ['M', 'F', 'M', 'F']
})
df5
```

|       | **姓名** | **年龄** | **工资** | **性别** |
| ----- | :------- | :------- | :------- | :------- |
| **0** | John     | 25.0     | 50000.0  | M        |
| **1** | Alice    | 30.0     | NaN      | F        |
| **2** | Bob      | NaN      | 70000.0  | M        |
| **3** | Mary     | 40.0     | 60000.0  | F        |

##### 1）删除所有有缺失值的行

DataFrame在dropna方法直接调用时，会返回没有任何缺失值的行，所组成的新DataFrame。也就是说，只要某行任何一个值是空缺的，调用dropna后就见不到它了。

```python
df5.dropna()
```

|       | **姓名** | **年龄** | **工资** | **性别** |
| ----- | :------- | :------- | :------- | :------- |
| **0** | John     | 25.0     | 50000.0  | M        |
| **3** | Mary     | 40.0     | 60000.0  | F        |

##### 2）删除关键值缺失的行

但我们一般清理数据的时候，更关注的是某些有关键信息的列，不太关键的变量，即使缺失也问题不大

那我们调用dropna方法时，可以传入一个可选参数subset=\['关键值缺失的列的列名1'，...]。

```python
df5.dropna(subset=['工资'])
```

|       | **姓名** | **年龄** | **工资** | **性别** |
| ----- | :------- | :------- | :------- | :------- |
| **0** | John     | 25.0     | 50000.0  | M        |
| **2** | Bob      | NaN      | 70000.0  | M        |
| **3** | Mary     | 40.0     | 60000.0  | F        |

##### 3）删除有缺失值的列

指定axis=1

```python
df5.dropna(axis=1)
```

|       | **姓名** | **性别** |
| ----- | :------- | :------- |
| **0** | John     | M        |
| **1** | Alice    | F        |
| **2** | Bob      | M        |
| **3** | Mary     | F        |

```python
df5.dropna(axis=1, subset=[0, 1])
```

|       | **姓名** | **年龄** | **性别** |
| ----- | :------- | :------- | :------- |
| **0** | John     | 25.0     | M        |
| **1** | Alice    | 30.0     | F        |
| **2** | Bob      | NaN      | M        |
| **3** | Mary     | 40.0     | F        |

### （二）、删除重复数据

```python
df6 = pd.DataFrame({
    '姓名': ['John', 'Alice', 'Bob', 'Alice', 'John'],
    '年龄': [25, 30, 35, 30, 40],
    '性别': ['M', 'F', 'M', 'F', 'M']
})
df6
```

|       | **姓名** | **年龄** | **性别** |
| ----- | :------- | :------- | :------- |
| **0** | John     | 25       | M        |
| **1** | Alice    | 30       | F        |
| **2** | Bob      | 35       | M        |
| **3** | Alice    | 30       | F        |
| **4** | John     | 40       | M        |

drop\_duplicates方法，可以用于删除Series里的重复值，或是DataFrame里的重复行

#### 1、删除所有变量的值都重复的行

直接调用DataFrame的drop\_duplicates时，只有当所有变量都一样的时候，才会被视为重复，也因此才会被删除掉。

```python
df6['姓名'].drop_duplicates()

0     John
1    Alice
2      Bob
Name: 姓名, dtype: object
```

```python
df6.drop_duplicates()
```

|       | **姓名** | **年龄** | **性别** |
| ----- | :------- | :------- | :------- |
| **0** | John     | 25       | M        |
| **1** | Alice    | 30       | F        |
| **2** | Bob      | 35       | M        |
| **4** | John     | 40       | M        |

#### 2、删除特定变量的值重复的行

也可以用subset=\['变量1', '变量2'...],只要这个列表里的变量同时重复，就会被删除

```python
df6.drop_duplicates(subset=['姓名', '性别'])
```

|       | **姓名** | **年龄** | **性别** |
| ----- | :------- | :------- | :------- |
| **0** | John     | 25       | M        |
| **1** | Alice    | 30       | F        |
| **2** | Bob      | 35       | M        |

#### 3、可选参数keep

删除的时候，默认是删除第二次及之后出现的值。

但我们也可以把让可选参数keep='last'，这样会保留最后一个出现的值，删除掉之前出现的重复值

```python
df6.drop_duplicates(subset=['姓名', '性别'], keep='last')
```

|       | **姓名** | **年龄** | **性别** |
| ----- | :------- | :------- | :------- |
| **2** | Bob      | 35       | M        |
| **3** | Alice    | 30       | F        |
| **4** | John     | 40       | M        |

### （三）、处理不一致数据

不一致数据是指，存在不同的值实际含义相同，或指代的是同一目标

处理方法是，把数值都替换成统一的。

```python
data = {'姓名': ['小明', '小红', '小张', '小李'],
        '家乡': ['北京', '上海', '广州', '深圳'],
        '学校': ['北京大学', '清华大学', '华南理工', '清华']}
df7 = pd.DataFrame(data)
df7
```

|       | **姓名** | **家乡** | **学校** |
| ----- | :------- | :------- | :------- |
| **0** | 小明     | 北京     | 北京大学 |
| **1** | 小红     | 上海     | 清华大学 |
| **2** | 小张     | 广州     | 华南理工 |
| **3** | 小李     | 深圳     | 清华     |

可以用Series或DataFrame的replace方法，在Series和DataFrame上用法差不多

#### 1、replace方法参数是两个数字或字符串

表示想把前面值全部替换成后面值

```python
df7['学校'].replace('清华', '清华大学')

0    北京大学
1    清华大学
2    华南理工
3    清华大学
Name: 学校, dtype: object
```

```plain
df7.replace('清华', '清华大学')
```

|       | **姓名** | **家乡** | **学校** |
| ----- | :------- | :------- | :------- |
| **0** | 小明     | 北京     | 北京大学 |
| **1** | 小红     | 上海     | 清华大学 |
| **2** | 小张     | 广州     | 华南理工 |
| **3** | 小李     | 深圳     | 清华大学 |

#### 2、replace方法第一个参数是一个列表

那么列表里面所有值都会被替换成后面那个，这样就可以一次性，对多个不同指代词进行统一

```python
df7['学校'].replace(['清华', '五道口职业技术学院', 'Tsinghua University'],'清华大学')

0    北京大学
1    清华大学
2    华南理工
3    清华大学
Name: 学校, dtype: object
```

```python
df7.replace(['清华', '五道口职业技术学院', 'Tsinghua University'],'清华大学')
```

|       | **姓名** | **家乡** | **学校** |
| ----- | :------- | :------- | :------- |
| **0** | 小明     | 北京     | 北京大学 |
| **1** | 小红     | 上海     | 清华大学 |
| **2** | 小张     | 广州     | 华南理工 |
| **3** | 小李     | 深圳     | 清华大学 |

#### 3、replace方法参数是单个字典

去指定不同的值要被什么值替换

```python
replace_dict = {'华南理工': '华南理工大学', 
                '清华': '清华大学', 
                '北大': '北京大学', 
                '中大': '中山大学'}
df7.replace(replace_dict)
```

|       | **姓名** | **家乡** | **学校**     |
| ----- | :------- | :------- | :----------- |
| **0** | 小明     | 北京     | 北京大学     |
| **1** | 小红     | 上海     | 清华大学     |
| **2** | 小张     | 广州     | 华南理工大学 |
| **3** | 小李     | 深圳     | 清华大学     |

### （四）、进行数据类型转换

数据类型转换，在很多时候是有必要，因为某些方法只能用在特定类型的数据上。比如说：不能对字符串求平均值

#### 1、Series的astype方法

参数为数据类型，比如int、float、str、bool等，调用后，返回的新Series里的数据就被转换成了那个类型

```python
s1 = pd.Series([1, 2, 3])
s1.astype(float)
```

```python
0    1.0
1    2.0
2    3.0
dtype: float64
```

#### 2、type函数

在Python里面，如果你想知道数据类型，可以调用type函数

#### 3、Pandas的数据类型category

数据可以被划分为两种类型，分类数据和数值数据。

分类数据，指的是包含有限数量的不同类别的数据，比如：出生时的性别，就两种，是有限的；奥运会的奖牌，金银铜，也是有限的

数值数据，指的是测量出的观测值，是某个具体的数值，种类不受限制。比如：0到1之间就有无限个数字，那对它进行求和或求平均值等数学运算也是有意义的

当分类数据种类有限时，Pandas会推荐把它们转换成category这个数据类型。

好处：节约内存空间，并且在后续分析或可视化的时候，也有利于让Pandas自动选用合适的统计方法或图标类型

**category不是Python自带的类型名称，而是Pandas库里面的，所以我们在用astype进行转换的时候，要传入一个用引号包围的category，不然Python不认**

```python
s2 = pd.Series(["红色", "红色", "橙色", "蓝色"])
s2.astype('category')
```

```python
0    红色
1    红色
2    橙色
3    蓝色
dtype: category
Categories (3, object): ['橙色', '红色', '蓝色']
```

#### 4、转换成datetime日期时间类型

可以用Pandas的to\_datetime方法，参数传入Series，可以返回转换成日期时间类型的Series

```python
df3['日期'] = pd.to_datetime(df3['日期'])
df3.日期
```

```python
001   2005-01-01
002   2005-01-02
003   2005-01-03
004   2023-09-15
Name: 日期, dtype: datetime64[ns]
```

---

---
url: /Python/数据分析/5_Python数据分析—数据收集.md
---

# Python数据分析—数据收集

**数据分析流程：获取数据、读取数据、评估数据、清洗数据、整理数据、分析数据、可视化数据**

## 一、获取私密数据

获取私密数据没有通用方法，方法取决于具体情况。

## 二、获取公开数据

### (一)、下载

公开数据集

网络上有一些提供公开数据集的网站，可以在后续掌握分析技巧后，探索和下载感兴趣的数据集。

飞桨（百度旗下深度学习平台）数据集：<https://aistudio.baidu.com/aistudio/datasetoverview>

天池（阿里云旗下开发者竞赛平台）：<https://tianchi.aliyun.com/dataset/>

和鲸社区（数据科学开源社区）数据集：<https://www.heywhale.com/home/dataset>

### (二)、爬虫

获取数据的过程

1. 获取网页内容

通过发送请求，获取网页源代码。

2. 解析网页内容

解析源代码内容，提取出想要的内容。这些数据就可以作为后续分析的原料了...

爬虫的红线

1. 不要爬取公民隐私数据
2. 不要爬取受著作权保护的内容
3. 不要爬取国家事务、国防建设、尖端科学技术领域的计算机系统等

### (三)、API

1. 概念

API, 全称是**Application Programming Interface**，表示应用程序编程接口。

API定义了两个程序之间的服务合约，即双方是如何使用请求和响应来进行通讯的。

2. 爬虫和API获取数据的区别

当我们爬虫时，发送请求后，获取的是网页源码，但网页源码本身是用来给浏览器渲染的，而不是直接的信息。要从中寻找特定数据，还要解析网页源码。

如果网站直接提供给我们API，我们就能按照对方规定好的服务合约，根据规则发送请求，然后直接获得想要的数据，不需要经过解析源码这一步骤。

因此，当一个网站提供了公开API时，通过API而不是爬虫去获取数据，肯定是更高效的方法。

---

---
url: /Python/数据分析/12_Python数据分析—统计学基础.md
---

# Python数据分析—统计学基础

我们用Python做数据分析时，要用代码编写统计分析方法，运用在数据上。

数据分析和统计学是无法分隔的。

统计学的本质是对数据进行描述和推断，主要涉及描述统计学知识。

描述，指的是对数据提供特征的概述。

## 一、数据的分类

### (一)、分类数据

分类数据指的是包含有限数量的不同类别的数据。

#### 1、定序数据

表示数据是可以有顺序的，比如：金银铜可以按顺序排

#### 2、定类数据

表示数据没有顺序，比如：狗子的种类

### (二)、数值数据

数值数据，指的是测量出的观测值，是个具体的数值，对它进行求和或求平均值等数学运算是有意义的。

#### 1、连续数据

表示数据没有最小的表示单位，两个数值之间可以取无数不同的值

#### 2、离散数据

表示数据只能以整数或自然数为单位

## 二、分析维度

数值数据，通常是分析的重点，可以有三个分析维度。

1. 集中趋势，看的是数据集中在何处
2. 离散趋势，看的是数据偏离中心的散布情况
3. 分布形状，看的是分布的对称程度，峰值高低等等情况

### (一)、集中趋势指标

#### 1、平均数

表示数据相对集中较多的中心位置，平均数很容易受到极端值的影响

#### 2、中位数

表示数据的中间位置，中位数不容易受到极端值的影响，有时会和平均数一起纳入统计

#### 3、众数

表示数据最普遍的倾向

### (二)、离散趋势指标

#### 1、极差

表示数据波动的范围

#### 2、方差/标准差

方差和标准差可以互相转换

平均数是用来描述集中趋势的，所以数值和平均数的差距，就是在描述偏离中心值的离散趋势

之所以要对各个数据和平均数的差进行平方，是因为我们只在乎值和平均数的距离，并不在乎这个距离是正距离还是负距离

#### 3、四分位距

四分位距=第三四分位数-第一四分位数

表示中间一半数值的离散程度，越大说明数据越分散

### (三)、分布形状指标

#### 1、直方图

直方图常被用于发现数据的分布情况，包括偏态、峰度、异常值等信息

将数据按照一定的区间范围进行分组，计算每个区间内的数据数量，然后用矩形条表示每个区间内数据的数量

#### 2、正态分布

#### 3、偏态分布

* 正偏态/右偏态

长尾巴在右边，说明大部分数值比平均值更低，比如：平均月薪，被高工资拉高了平均值。

* 负偏态/左偏态

长尾巴在左边，说明大部分数值比平均值更高。

## 三、描述统计学分析

如何用Python，对数值数据进行描述统计学分析。

描述统计学的分析对象，很多时候是一维数据，像Python的列表，NumPy的一维数组，Pandas的Series，DataFrame的某列。

```python
import pandas as pd
import numpy as np
```

```python
# 全班某天测量的体温数据
df = pd.read_csv('temperature.csv')
df
```

|        | **姓名** | **体温** |
| ------ | :------- | :------- |
| **0**  | Tom      | 37.8     |
| **1**  | Jerry    | 36.8     |
| **2**  | Lucy     | 37.0     |
| **3**  | Emma     | 37.2     |
| **4**  | John     | 36.6     |
| **5**  | Alice    | 37.0     |
| **6**  | Bob      | 37.0     |
| **7**  | David    | 36.1     |
| **8**  | Sam      | 37.5     |
| **9**  | Alex     | 37.3     |
| **10** | Lisa     | 36.7     |
| **11** | Frank    | 36.9     |
| **12** | Grace    | 37.3     |
| **13** | Mary     | 36.9     |
| **14** | Ben      | 36.9     |
| **15** | Kate     | 36.3     |
| **16** | Oliver   | 37.3     |
| **17** | Sophie   | 37.1     |
| **18** | Ella     | 37.1     |
| **19** | Hannah   | 36.2     |
| **20** | James    | 37.8     |
| **21** | Leo      | 37.1     |
| **22** | Luke     | 36.8     |
| **23** | Max      | 38.0     |
| **24** | Mia      | 37.0     |
| **25** | Noah     | 36.3     |
| **26** | Peter    | 36.8     |
| **27** | Sarah    | 39.2     |
| **28** | Tim      | 38.6     |

### (一)、计算集中趋势指标

#### 1、计算平均数

调用Series类的`mean`方法

```python
df.体温.mean() 

37.12413793103448
```

说明这个班的体温大概集中在37.1度

#### 2、计算中位数

调用Series类的`median`方法

```python
df['体温'].median() 

37.0
```

数据中有两个人体温显著更高，拉高了平均值，中位数不太会收到异常值的影响

#### 3、计算众数

调用Series类的`mode`方法

**mode方法返回的不是一个数字，而是一个Series，因为众数可能有多个，所以用Series去囊括多个数字结果**

```python
df.体温.mode()

0    37.0
Name: 体温, dtype: float64
```

说明办理人的体温倾向于37度

### (二)、计算离散趋势指标

#### 1、极差

* 可以用Series的max和min函数去做减法
* 用NumPy数组的ptp方法，会返回极差结果，需要先用NumPy库的array方法将Pandas的Series转换成NumPy数组

```python
df.体温.max() - df.体温.min() 
3.1000000000000014 
np.array(df.体温).ptp() 
3.1000000000000014
```

**浮点数精度误差**

**以上计算结果多了0.0000000000000014，这个误差源自浮点数的精度问题，是二进制计算和转换造成的**

#### 2、方差/标准差

* 方差：调用Series的var方法
* 标准差：调用Series的std方法

```python
df.体温.var() 
0.4518965517241383 
df.体温.std() 
0.6722325131412035
```

#### 3、四分位距

Series的quantile方法，可以用来计算任意百分位数，比如传入10%，就能得到刚好大于数据里10%数值的那个数字

计算四分位距，用75%分位数减去25%分位数

```python
df.体温.quantile(0.75) - df.体温.quantile(0.25) 

0.5
```

### (三)、分布形状

如果我们把各个数值出现的次数，绘制成直方图，就能看见数据的分布模式

调用Series的plot方法，可选参数`kind = 'hist'`，表示直方图

```python
df['体温'].plot(kind='hist') 

<Axes: ylabel='Frequency'>
```

![](/assets/output_26_1-1733846279625.6WqmHRse.png)

现在只能很勉强的看出分布形状，当数据观察值很少的时候，分布形状不会特别明显，而且很容易受到异常值的影响；但数据观察值数量越大，越容易看出分布形状。

---

---
url: /Python/数据分析/14_Python数据分析—统计学进阶.md
---

# Python数据分析—统计学进阶

### 一、有关假设检验的一些概念

1、描述统计学与推断统计学

* 描述统计学：对数据进行描述和总结
* 推断统计学：通过样本做出关于总体的推断和预测

2、对象和整体

* 对象：我们想要观测的**具体事物**叫做对象
* 整体：我们想观测的**整个对象**的集合

3、样本和整体

* 样本是我们收集数据的**对象**
* 总体是我们想要得到结论的**群体**

4、统计量和参数

* 统计量：描述样本特征的数值
* 参数：描述总体特征的数值
* 在统计推断中，我们会基于样本的统计量，对总体的参数进行推断，从而得到对总体的结论

### 二、独立双样本t检验步骤

1、概念

* 独立：说明样本来自不同的总体，彼此没有关联
* 双样本：比较两个不同样本的数据
* t检验：一种统计方法，用于确定样本的平均值之间是否存在统计显著的差异

2、前提条件

* 随机抽样
* 总体大致呈正态分布

#### (一)、建立假设

1、原假设(H0)

2、备择假设(H1)

一般我们进行假设检验的时候，是想反驳原假设，以及支持备择假设

#### (二)、选择单尾或双尾

1、双尾只推断总体之间是否有差异，不在意是正差还是负差

2、单尾推断只看是否存在正差异，或者只看是否存在负差异

#### (三)、确定显著水平

显著水平反应了检验的严格程度。

样本抽样存在随机性，检验结果没有可能保证100%符合现实，只能通过显著水平来调整严格程度

常见的双尾检验显著水平是0.05，也就是说如果检验结果是拒绝原假设，原假设实际为真的概率是5%；再换句话说，如果检验结果是拒绝原假设，结论95%概率是对的。

常见的单尾检验显著水平是0.025。

**之所以单尾检验显著水平定为双尾检验显著水平的一半，可以想象两样本数据的正态分布图，一样本的平均值落在另一样本的一边区间中，说明差异显著性，说明观察正差异或负差异，说明是单尾假设检验；一样本的平均值落在另一样本的两边区间中，说明差异显著性，说明观察正差异和负差异，说明是双尾假设检验**

#### (四)、计算t值

1、t值：表示两个样本之间均值差异的大小，越大说明差异约显著

#### (五)、计算自由度

自由度 = 样本1的数量 + 样本2的数量 -2

#### (六)、查看t值临界值表

根据单双尾、自由度和显著水平，去查t值临界值表

#### (七)、比较临界值和t值

t值 >= 临界值，拒绝原假设，说明存在显著差异

t值 < 临界值，接受原假设，说明不存在显著差异

### 三、Z检验

#### 1、前提条件的区别

* 增加了要求：总体方差已知/样本容量大于30

#### 2、步骤的区别

* 不需要计算自由度
* z值计算公式，相比于t值计算公式，把样本方差换成了总体方差
* 查看z值临界值表

#### 3、应用场景

* Z检验适用于已知总体方差，或者样本的数量大于30的情况
* t检验适用于总体方差未知，且样本的数量小于等于30的情况。
* 在实际应用中，t检验比Z检验更加常见

#### 4、检验结果差异

* Z检验可以提供更高的准确性和敏感性

### 四、用代码实现t检验

```python
import numpy as np
import pandas as pd

# 引入绘图相关库
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib

# 引入t检验相关函数
from scipy.stats import ttest_ind

# 引入Z检验相关函数
from statsmodels.stats.weightstats import ztest
```

#### (一)、Knowledge

1、SciPy

* 做t检验，可以用SciPy这个库。SciPy和Pandas一样，都是建立在NumPy之上的
* `SciPy.stats`模块，有很多和统计分析有关的函数和类
* `SciPy.stats`模块中的`ttest_ind`函数，就是一个专门用来做独立双样本t检验的函数，'ind'是independent的简写

2、p值：如果总体之间不存在显著差异，那样本之间存在当前这种差异或更极端的差异，有多大概率。

* p值小，假设总体之间没有显著差异的话，样本有这样的差异是小概率事件，应该拒绝原假设
* p值大，假设总体之间没有显著差异的话，样本有这样的差异是大概率事件，应该接受原假设

#### (二)、步骤

```python
height_df = pd.read_csv('height.csv')
height_df
```

|        | **身高** | **地区** |
| ------ | :------- | :------- |
| **0**  | 165      | A        |
| **1**  | 167      | A        |
| **2**  | 172      | A        |
| **3**  | 176      | A        |
| **4**  | 178      | A        |
| **5**  | 180      | A        |
| **6**  | 182      | A        |
| **7**  | 183      | A        |
| **8**  | 185      | A        |
| **9**  | 188      | A        |
| **10** | 155      | B        |
| **11** | 158      | B        |
| **12** | 160      | B        |
| **13** | 162      | B        |
| **14** | 165      | B        |
| **15** | 168      | B        |
| **16** | 172      | B        |
| **17** | 176      | B        |
| **18** | 179      | B        |
| **19** | 182      | B        |

##### 1、引入t检验相关函数

##### 2、筛选两独立样本的Series

```python
region_a_height = height_df.query('地区 == "A"')['身高']
region_b_height = height_df.query('地区 == "B"')['身高']
```

```python
region_a_height

0    165
1    167
2    172
3    176
4    178
5    180
6    182
7    183
8    185
9    188
Name: 身高, dtype: int64
```

```python
region_b_height

10    155
11    158
12    160
13    162
14    165
15    168
16    172
17    176
18    179
19    182
Name: 身高, dtype: int64
```

```python
# 替换成Microsoft YaHei字体
matplotlib.rc("font",family='Microsoft YaHei')
```

```python
sns.kdeplot(region_a_height)
sns.kdeplot(region_b_height)
plt.show()
```

![](/assets/output_14_0.CaHajyga.png)

##### 3、建立假设

原假设(H0)：地区A和地区B的人平均身高没有差异

备择假设(H1)：地区A和地区B 的人平均身高有差异

##### 4、选择单尾/双尾

选择双尾检验

##### 5、确定显著水平

alpha = 0.05

##### 6、计算t值和p值

* 调用`ttest_ind`函数，分别传入两样本的Series，`ttest_ind`函数会返回t检验的结果，里面包含两个数字，第一个表示t值，第二个表示p值
* `**ttest_ind**`**函数默认计算的是双尾的p值，如果是单尾检验，单尾的p值等于返回的p值除以二**
* **之所以双尾检验p值是单尾检验p值的2倍，是人为x2的结果。可以想象单尾检验的p值为，一样本平均值，落在另一样本正态分布图中的面积；而由于双尾检验显著水平为，一样本平均值落入另一样本两边的面积，所以人为将p值x2，用两倍的面积和两边的面积去比较**

```python
t_stat, p_value = ttest_ind(region_a_height, region_b_height)
print(t_stat, p_value)

2.608375959216796 0.017783305969556976
```

##### 7、比较p值和显著水平

```python
# (把p值和显著水平进行比较，这里计算出的p值是0.018左右，小于0,05。
# 说明当原假设为真时，当前样本出现这种差异的概率很小，而且比我们选择的显著水平更小，因此应该拒绝原假设，换句话说，我们推断总体的平均值确实存在差异。)
if p_value < alpha:
    print('两组数据有显著差异')
else:
    print('两组数据无显著差异')
```

两组数据有显著差异

**用代码做t检验的步骤，比上一节学的更加简洁和高效。我们不需要去看t值具体是多少，不需要去查t值临界值表，也不需要对比t值和临界值。只需要直接对比p值和显著水平，就能得到结论，到底是拒绝还是接受原假设**

### 五、用代码实现Z检验

#### (一)、Knowledge

1、statsmodels

计算Z值和p值，可以用`statsmodels.stats.weightstats`模块的`ztest`函数

#### (二)、步骤

用Python做Z检验，整个步骤和t检验很相似

```python
height_df2 = pd.read_csv('height2.csv')
height_df2
```

|         | **身高** | **地区** |
| ------- | :------- | :------- |
| **0**   | 175      | A        |
| **1**   | 169      | A        |
| **2**   | 176      | A        |
| **3**   | 185      | A        |
| **4**   | 168      | A        |
| **...** | ...      | ...      |
| **61**  | 173      | B        |
| **62**  | 164      | B        |
| **63**  | 163      | B        |
| **64**  | 183      | B        |
| **65**  | 189      | B        |

##### 1、引入Z检验相关函数

##### 2、筛选两独立样本的Series

```python
region_a_height2 = height_df2.query('地区 == "A"')['身高']
region_b_height2 = height_df2.query('地区 == "B"')['身高']
```

```python
sns.kdeplot(region_a_height2)
sns.kdeplot(region_b_height2)
plt.show()
```

![](/assets/output_35_0.B5gYpYg6.png)

##### 3、建立假设

##### 4、选择单尾/双尾

##### 5、确定显著水平

alpha = 0.05

##### 6、计算Z值和p值

* 调用ztest函数，分别传入两样本，ztest函数返回两个数字，第一个表示Z值，第二个表示p值
  * 传入可选参数`alternative='two-sided'`，表示双尾
  * 如果是单尾的话，传入可选参数`alternative='larger'`，表示想推断第一个的均值，是否显著大于第二个
  * 传入可选参数`alternative='smaller'`，表示想推断第一个的均值，是否显著小于第二个

```python
z_stat, p_value = ztest(region_a_height2, region_b_height2, alternative='two-sided')
print(z_stat, p_value)

-1.9906963757270788 0.04651427741343414
```

##### 7、比较p值和显著水平

```python
if p_value < alpha:
    print('两组数据有显著差异')
else:
    print('两组数据无显著差异')
```

两组数据有显著差异

```python
# 建立假设
# 原假设：
# 备择假设：地区A的人平均身高小于地区B 的人的平均身高

alpha = 0.025

# 计算z值和p值（单尾负差异）
z_stat, p_value = ztest(region_a_height2, region_b_height2, 
                        alternative='smaller')
print(z_stat, p_value)

if p_value < alpha:
    print('两组数据有显著差异')
else:
    print('两组数据无显著差异')
```

```python
-1.9906963757270788 0.02325713870671707
两组数据有显著差异
```

---

---
url: /Python/数据分析/11_Python数据分析—整理数据.md
---

# Python数据分析—整理数据

在数据清洗干净后，很多时候，我们可以直接开始可视化或分析了，但也有些时候，我们还需要一些额外步骤。

比如：当我们从多个数据源获取相关数据，或者数据集本身包括了多个表格，那就可能涉及一些数据连接或合并等操作。

## 一、对DataFrame进行拼接

其中，最简单的操作，就是把两个DataFrame，纵向或横向连接到一起。

### (一)、对DataFrame进行纵向拼接

1. 可以用Pandas的concat函数，给concat函数传入一个列表，列表里面放上想要拼接的DataFrame，会返回一个前面两个DataFrame纵相拼接后的新DataFrame
2. 索引

**concat函数，默认保留DataFrame索引，包括位置索引。**

**但位置索引没有啥意义，且拼接后索引会乱掉**

**可以指定可选参数ignore\_index=True，这样拼接时就会对原本DataFrame索引进行忽略，返回的新结果里，索引就是从0开始排序的位置索引**

```python
import pandas as pd
df1 = pd.DataFrame({
    '商品名': ['iPhone 12', 'MacBook Air', 'iPad', 'Apple Watch Series 6'],
    '单价（元）': [6799, 8499, 3199, 2699],
    '颜色': ['蓝色', '金色', '灰色', '粉色'],
    '库存数量': [100, 50, 150, 80]
})


df2 = pd.DataFrame({
    '商品名': ['AirPods Pro', 'HomePod mini', 'Apple TV 4K', 'Beats Flex'],
    '单价（元）': [1599, 749, 1499, 399],
    '颜色': ['白色', '空间灰色', '黑色', '黄色'],
    '库存数量': [120, 80, 60, 200]
})
```

```python
df1
```

|       | **商品名**           | **单价（元）** | **颜色** | **库存数量** |
| ----- | :------------------- | :------------- | :------- | :----------- |
| **0** | iPhone 12            | 6799           | 蓝色     | 100          |
| **1** | MacBook Air          | 8499           | 金色     | 50           |
| **2** | iPad                 | 3199           | 灰色     | 150          |
| **3** | Apple Watch Series 6 | 2699           | 粉色     | 80           |

```python
df2
```

|       | **商品名**   | **单价（元）** | **颜色** | **库存数量** |
| ----- | :----------- | :------------- | :------- | :----------- |
| **0** | AirPods Pro  | 1599           | 白色     | 120          |
| **1** | HomePod mini | 749            | 空间灰色 | 80           |
| **2** | Apple TV 4K  | 1499           | 黑色     | 60           |
| **3** | Beats Flex   | 399            | 黄色     | 200          |

```python
pd.concat([df1, df2])
```

|       | **商品名**           | **单价（元）** | **颜色** | **库存数量** |
| ----- | :------------------- | :------------- | :------- | :----------- |
| **0** | iPhone 12            | 6799           | 蓝色     | 100          |
| **1** | MacBook Air          | 8499           | 金色     | 50           |
| **2** | iPad                 | 3199           | 灰色     | 150          |
| **3** | Apple Watch Series 6 | 2699           | 粉色     | 80           |
| **0** | AirPods Pro          | 1599           | 白色     | 120          |
| **1** | HomePod mini         | 749            | 空间灰色 | 80           |
| **2** | Apple TV 4K          | 1499           | 黑色     | 60           |
| **3** | Beats Flex           | 399            | 黄色     | 200          |

```python
pd.concat([df1, df2], ignore_index=True)
```

|       | **商品名**           | **单价（元）** | **颜色** | **库存数量** |
| ----- | :------------------- | :------------- | :------- | :----------- |
| **0** | iPhone 12            | 6799           | 蓝色     | 100          |
| **1** | MacBook Air          | 8499           | 金色     | 50           |
| **2** | iPad                 | 3199           | 灰色     | 150          |
| **3** | Apple Watch Series 6 | 2699           | 粉色     | 80           |
| **4** | AirPods Pro          | 1599           | 白色     | 120          |
| **5** | HomePod mini         | 749            | 空间灰色 | 80           |
| **6** | Apple TV 4K          | 1499           | 黑色     | 60           |
| **7** | Beats Flex           | 399            | 黄色     | 200          |

1. 列名

**当参与拼接的两个DataFrame，存在列名不同时，并不会产生报错，此时DataFrame的所有列都会被进行保留，匹配不上的地方，就会自动用NaN值进行填充**

```python
df3 = pd.DataFrame({
    '商品名': ['iPhone 12', 'MacBook Air', 'iPad', 'Apple Watch Series 6'],
    '单价（元）': [6799, 8499, 3199, 2699],
    '颜色': ['蓝色', '金色', '灰色', '粉色'],
    '库存数量': [100, 50, 150, 80]
})


df4 = pd.DataFrame({
    '商品名': ['AirPods Pro', 'HomePod mini', 'Apple TV 4K', 'Beats Flex'],
    '单价': [1599, 749, 1499, 399],
    '颜色': ['白色', '空间灰色', '黑色', '黄色'],
    '库存数量': [120, 80, 60, 200]
})
```

```python
df3
```

|       | **商品名**           | **单价（元）** | **颜色** | **库存数量** |
| ----- | :------------------- | :------------- | :------- | :----------- |
| **0** | iPhone 12            | 6799           | 蓝色     | 100          |
| **1** | MacBook Air          | 8499           | 金色     | 50           |
| **2** | iPad                 | 3199           | 灰色     | 150          |
| **3** | Apple Watch Series 6 | 2699           | 粉色     | 80           |

```python
df4
```

|       | **商品名**   | **单价** | **颜色** | **库存数量** |
| ----- | :----------- | :------- | :------- | :----------- |
| **0** | AirPods Pro  | 1599     | 白色     | 120          |
| **1** | HomePod mini | 749      | 空间灰色 | 80           |
| **2** | Apple TV 4K  | 1499     | 黑色     | 60           |
| **3** | Beats Flex   | 399      | 黄色     | 200          |

```python
pd.concat([df3, df4], ignore_index=True)
```

|       | **商品名**           | **单价（元）** | **颜色** | **库存数量** | **单价** |
| ----- | :------------------- | :------------- | :------- | :----------- | :------- |
| **0** | iPhone 12            | 6799.0         | 蓝色     | 100          | NaN      |
| **1** | MacBook Air          | 8499.0         | 金色     | 50           | NaN      |
| **2** | iPad                 | 3199.0         | 灰色     | 150          | NaN      |
| **3** | Apple Watch Series 6 | 2699.0         | 粉色     | 80           | NaN      |
| **4** | AirPods Pro          | NaN            | 白色     | 120          | 1599.0   |
| **5** | HomePod mini         | NaN            | 空间灰色 | 80           | 749.0    |
| **6** | Apple TV 4K          | NaN            | 黑色     | 60           | 1499.0   |
| **7** | Beats Flex           | NaN            | 黄色     | 200          | 399.0    |

### (二)、对DataFrame进行横向拼接

仍然可以用concat函数，额外传入可选参数axis=1。因为此时是沿着列名横向操作，因此拼接的时候就是横向进行拼接。

```python
df5 = pd.DataFrame({
    '商品名': ['iPhone 12', 'MacBook Air', 'iPad', 'Apple Watch Series 6'],
    '单价（元）': [6799, 8499, 3199, 2699]
})


df6 = pd.DataFrame({
    '颜色': ['蓝色', '金色', '灰色', '粉色'],
    '库存数量': [100, 50, 150, 80]
})
```

```python
df5
```

|       | **商品名**           | **单价（元）** |
| ----- | :------------------- | :------------- |
| **0** | iPhone 12            | 6799           |
| **1** | MacBook Air          | 8499           |
| **2** | iPad                 | 3199           |
| **3** | Apple Watch Series 6 | 2699           |

```python
df6
```

|       | **颜色** | **库存数量** |
| ----- | :------- | :----------- |
| **0** | 蓝色     | 100          |
| **1** | 金色     | 50           |
| **2** | 灰色     | 150          |
| **3** | 粉色     | 80           |

```python
pd.concat([df5, df6], axis=1)
```

|       | **商品名**           | **单价（元）** | **颜色** | **库存数量** |
| ----- | :------------------- | :------------- | :------- | :----------- |
| **0** | iPhone 12            | 6799           | 蓝色     | 100          |
| **1** | MacBook Air          | 8499           | 金色     | 50           |
| **2** | iPad                 | 3199           | 灰色     | 150          |
| **3** | Apple Watch Series 6 | 2699           | 粉色     | 80           |

## 二、对DataFrame进行合并

### （一）、根据某列的值合并DataFrame

拼接相当于把两坨数据，简单粗暴地拼到一起；而合并是基于某些列的匹配连接。

#### 1、一般情况

```python
pd.merge(df1, df2, on='列名')
```

用Pandas的merge函数，传入要合并的DataFrame作为参数，可选参数on，来指定我们根据哪列的值匹配来进行合并

**给on传入的列名，要同时出现在要合并的两个DataFrame里面**

```python
customer_df = pd.DataFrame({
    '客户ID': [1, 2, 3, 4],
    '姓名': ['Amy', 'Bill', 'Cathy', 'Dave'],
    '邮箱': ['amy@xxx.com', 'bill@xxx.com', 'cat@xxx.com', 'dave@xxx.com']
})

order_df = pd.DataFrame({
    '订单ID': [1, 2, 3, 4, 5],
    '客户ID': [1, 1, 2, 4, 4],
    '销售额': [100, 50, 75, 90, 120]
})
```

```python
customer_df
```

|       | **客户ID** | **姓名** | **邮箱**     |
| ----- | :--------- | :------- | :----------- |
| **0** | 1          | Amy      | amy@xxx.com  |
| **1** | 2          | Bill     | bill@xxx.com |
| **2** | 3          | Cathy    | cat@xxx.com  |
| **3** | 4          | Dave     | dave@xxx.com |

```python
order_df
```

|       | **订单ID** | **客户ID** | **销售额** |
| ----- | :--------- | :--------- | :--------- |
| **0** | 1          | 1          | 100        |
| **1** | 2          | 1          | 50         |
| **2** | 3          | 2          | 75         |
| **3** | 4          | 4          | 90         |
| **4** | 5          | 4          | 120        |

```python
pd.merge(customer_df, order_df, on='客户ID')
```

|       | **客户ID** | **姓名** | **邮箱**     | **订单ID** | **销售额** |
| ----- | :--------- | :------- | :----------- | :--------- | :--------- |
| **0** | 1          | Amy      | amy@xxx.com  | 1          | 100        |
| **1** | 1          | Amy      | amy@xxx.com  | 2          | 50         |
| **2** | 2          | Bill     | bill@xxx.com | 3          | 75         |
| **3** | 4          | Dave     | dave@xxx.com | 4          | 90         |
| **4** | 4          | Dave     | dave@xxx.com | 5          | 120        |

如上例子，我们只有把顾客信息和订单信息分开在两个表里面，相同信息才不会多次重复出现。

假设我们只有一个表，里面又有订单数据又有客户信息的话。如果客户1下单了50次，那她的姓名和邮箱，就会在这个表里面重复50次；特别是，如果当客户1更改邮箱，那所有出现了她邮箱信息的行，都得去进行更新；但是如果我们把订单数据和客户信息放在两个表中，我们就只需要更新客户信息表中的一行。

由上可知，把不同的信息分开在不同表中，实际上是一个最佳实践。

**因此，在数据分析中，经常需要对DataFrame进行合并。**

#### 2、根据多列的值匹配来进行合并

`on=['列名1', '列名2'...]`

在合并的时候根据多列的值，要求它们得同时匹配

```python
order_df2 = pd.DataFrame({
    '订单ID': ['A001', 'A002', 'A003', 'A004'],
    '订单日期': ['2000-01-01', '2000-01-02', '2000-01-02', '2000-01-03'],
    '客户ID': ['C001', 'C002', 'C001', 'C003'],
    '销售额': [100, 200, 150, 300]
})

customer_df2 = pd.DataFrame({
    '客户ID': ['C001', 'C002', 'C003'],
    '姓名': ['张三', '李四', '王五'],
    '手机号': ['13512345678', '13612345678', '13712345678'],
    '订单日期': ['2000-01-01', '2000-01-02', '2000-01-03']
})
```

```python
order_df2
```

|       | **订单ID** | **订单日期** | **客户ID** | **销售额** |
| ----- | :--------- | :----------- | :--------- | :--------- |
| **0** | A001       | 2000-01-01   | C001       | 100        |
| **1** | A002       | 2000-01-02   | C002       | 200        |
| **2** | A003       | 2000-01-02   | C001       | 150        |
| **3** | A004       | 2000-01-03   | C003       | 300        |

```python
customer_df2
```

|       | **客户ID** | **姓名** | **手机号**  | **订单日期** |
| ----- | :--------- | :------- | :---------- | :----------- |
| **0** | C001       | 张三     | 13512345678 | 2000-01-01   |
| **1** | C002       | 李四     | 13612345678 | 2000-01-02   |
| **2** | C003       | 王五     | 13712345678 | 2000-01-03   |

```python
pd.merge(order_df2, customer_df2, on=['客户ID', '订单日期'])
```

|       | **订单ID** | **订单日期** | **客户ID** | **销售额** | **姓名** | **手机号**  |
| ----- | :--------- | :----------- | :--------- | :--------- | :------- | :---------- |
| **0** | A001       | 2000-01-01   | C001       | 100        | 张三     | 13512345678 |
| **1** | A002       | 2000-01-02   | C002       | 200        | 李四     | 13612345678 |
| **2** | A004       | 2000-01-03   | C003       | 300        | 王五     | 13712345678 |

#### 3、当某个变量，虽然在两个DataFrame里面出现，但是列名并不统一

1. 对任一DataFrame列名，进行重命名
2. 把可选参数`on`,替换成`left_on`和`right_on`，给`left_on`传入的是左边DataFrame用于合并的列名，给`right_on`传入的是右边DataFrame用于合并的列名

**合并后，原来的两个DataFrame的列都会保留，用于匹配的列的值一致**

```python
order_df3 = pd.DataFrame({
    '订单ID': ['A001', 'A002', 'A003', 'A004'],
    '订单日期': ['2000-01-01', '2000-01-02', '2000-01-02', '2000-01-03'],
    '客户编号': ['C001', 'C002', 'C001', 'C003'],
    '销售额': [100, 200, 150, 300]
})

customer_df3 = pd.DataFrame({
    '客户ID': ['C001', 'C002', 'C003'],
    '姓名': ['张三', '李四', '王五'],
    '手机号': ['13512345678', '13612345678', '13712345678'],
    '交易日期': ['2000-01-01', '2000-01-02', '2000-01-03']
})
```

```python
order_df3
```

|       | **订单ID** | **订单日期** | **客户编号** | **销售额** |
| ----- | :--------- | :----------- | :----------- | :--------- |
| **0** | A001       | 2000-01-01   | C001         | 100        |
| **1** | A002       | 2000-01-02   | C002         | 200        |
| **2** | A003       | 2000-01-02   | C001         | 150        |
| **3** | A004       | 2000-01-03   | C003         | 300        |

```python
customer_df3
```

|       | **客户ID** | **姓名** | **手机号**  | **交易日期** |
| ----- | :--------- | :------- | :---------- | :----------- |
| **0** | C001       | 张三     | 13512345678 | 2000-01-01   |
| **1** | C002       | 李四     | 13612345678 | 2000-01-02   |
| **2** | C003       | 王五     | 13712345678 | 2000-01-03   |

```python
pd.merge(order_df3, customer_df3, left_on=['客户编号', '订单日期'], right_on=['客户ID', '交易日期'])
```

|       | **订单ID** | **订单日期** | **客户编号** | **销售额** | **客户ID** | **姓名** | **手机号**  | **交易日期** |
| ----- | :--------- | :----------- | :----------- | :--------- | :--------- | :------- | :---------- | :----------- |
| **0** | A001       | 2000-01-01   | C001         | 100        | C001       | 张三     | 13512345678 | 2000-01-01   |
| **1** | A002       | 2000-01-02   | C002         | 200        | C002       | 李四     | 13612345678 | 2000-01-02   |
| **2** | A004       | 2000-01-03   | C003         | 300        | C003       | 王五     | 13712345678 | 2000-01-03   |

#### 4、除了用于匹配的列，两张表还有其它的重名列

不会报错

合并后，为了区分某个重名的列到底来自哪个表，merge函数会自动为列名的结尾加上后缀，'\_x'表示来自第一个表，'\_y'表示来自第二个表

1. 自定义后缀

传入可选参数`suffixes=['第一个表的重名列的后缀', '第二个表的重名列的后缀']`

```python
df7 = pd.DataFrame({'日期': ['2000-01-01', '2000-01-02', '2000-01-03'],
                    '店铺': ['A', 'B', 'C'],
                    '销售额': [100, 200, 300]})
df8 = pd.DataFrame({'日期': ['2000-01-02', '2000-01-03', '2000-01-04'],
                    '店铺': ['B', 'C', 'D'],
                    '销售额': [400, 500, 600]})
```

```python
df7
```

|       | **日期**   | **店铺** | **销售额** |
| ----- | :--------- | :------- | :--------- |
| **0** | 2000-01-01 | A        | 100        |
| **1** | 2000-01-02 | B        | 200        |
| **2** | 2000-01-03 | C        | 300        |

```python
df8
```

|       | **日期**   | **店铺** | **销售额** |
| ----- | :--------- | :------- | :--------- |
| **0** | 2000-01-02 | B        | 400        |
| **1** | 2000-01-03 | C        | 500        |
| **2** | 2000-01-04 | D        | 600        |

```python
pd.merge(df7, df8, on=['日期', '店铺'])
```

|       | **日期**   | **店铺** | **销售额\_x** | **销售额\_y** |
| ----- | :--------- | :------- | :----------- | :----------- |
| **0** | 2000-01-02 | B        | 200          | 400          |
| **1** | 2000-01-03 | C        | 300          | 500          |

```python
pd.merge(df7, df8, on=['日期', '店铺'], suffixes=['_df7', '_df8'])
```

|       | **日期**   | **店铺** | **销售额\_df7** | **销售额\_df8** |
| ----- | :--------- | :------- | :------------- | :------------- |
| **0** | 2000-01-02 | B        | 200            | 400            |
| **1** | 2000-01-03 | C        | 300            | 500            |

#### 5、合并类型

1）合并类型包括：inner、outer、left、right

inner表示合并结果里，只保留左右表都有匹配的值；

outer表示合并结果里，保留左右表的所有值，如果有匹配不上的，用NaN值填充，因此最终结果里所有行都出现了；

left表示合并结果里，会保留左边表的所有值，然后右边表根据左边的值去匹配，如果有匹配不上的用NaN值填充，因此最终结果里左边表的行都会在；

right表示合并结果里，会保留右边表的所有值，然后左边表根据右边的值去匹配，如果有匹配不上的用NaN值填充，因此最终结果里右边表的行都会在。

2）可以给merge函数传入可选参数`how='合并类型'`，用来指定合并类型，默认合并类型是inner

```python
customers_data = {
    '客户ID': [1, 2, 3, 4, 5],
    '姓名': ['Tom', 'Bob', 'Mary', 'Alice', 'John'],
    '年龄': [20, 35, 27, 19, 42]
}
customer_df4 = pd.DataFrame(customers_data)

orders_data = {
    '订单ID': [1001, 1002, 1003, 1004, 1005, 1006],
    '订单日期': ['2000-03-12', '2000-03-13', '2000-03-13', '2000-03-15', '2000-03-18', '2000-03-21'],
    '客户ID': [1, 1, 2, 6, 5, 3],
    '产品': ['A', 'B', 'C', 'D', 'E', 'F'],
    '数量': [2, 3, 1, 4, 5, 2]
}
order_df4 = pd.DataFrame(orders_data)
```

```python
customer_df4
```

|       | **客户ID** | **姓名** | **年龄** |
| ----- | :--------- | :------- | :------- |
| **0** | 1          | Tom      | 20       |
| **1** | 2          | Bob      | 35       |
| **2** | 3          | Mary     | 27       |
| **3** | 4          | Alice    | 19       |
| **4** | 5          | John     | 42       |

```python
order_df4
```

|       | **订单ID** | **订单日期** | **客户ID** | **产品** | **数量** |
| ----- | :--------- | :----------- | :--------- | :------- | :------- |
| **0** | 1001       | 2000-03-12   | 1          | A        | 2        |
| **1** | 1002       | 2000-03-13   | 1          | B        | 3        |
| **2** | 1003       | 2000-03-13   | 2          | C        | 1        |
| **3** | 1004       | 2000-03-15   | 6          | D        | 4        |
| **4** | 1005       | 2000-03-18   | 5          | E        | 5        |
| **5** | 1006       | 2000-03-21   | 3          | F        | 2        |

```python
pd.merge(customer_df4, order_df4, on='客户ID', how='inner')
```

|       | **客户ID** | **姓名** | **年龄** | **订单ID** | **订单日期** | **产品** | **数量** |
| ----- | :--------- | :------- | :------- | :--------- | :----------- | :------- | :------- |
| **0** | 1          | Tom      | 20       | 1001       | 2000-03-12   | A        | 2        |
| **1** | 1          | Tom      | 20       | 1002       | 2000-03-13   | B        | 3        |
| **2** | 2          | Bob      | 35       | 1003       | 2000-03-13   | C        | 1        |
| **3** | 3          | Mary     | 27       | 1006       | 2000-03-21   | F        | 2        |
| **4** | 5          | John     | 42       | 1005       | 2000-03-18   | E        | 5        |

```python
pd.merge(customer_df4, order_df4, on='客户ID', how='outer')
```

|       | **客户ID** | **姓名** | **年龄** | **订单ID** | **订单日期** | **产品** | **数量** |
| ----- | :--------- | :------- | :------- | :--------- | :----------- | :------- | :------- |
| **0** | 1          | Tom      | 20.0     | 1001.0     | 2000-03-12   | A        | 2.0      |
| **1** | 1          | Tom      | 20.0     | 1002.0     | 2000-03-13   | B        | 3.0      |
| **2** | 2          | Bob      | 35.0     | 1003.0     | 2000-03-13   | C        | 1.0      |
| **3** | 3          | Mary     | 27.0     | 1006.0     | 2000-03-21   | F        | 2.0      |
| **4** | 4          | Alice    | 19.0     | NaN        | NaN          | NaN      | NaN      |
| **5** | 5          | John     | 42.0     | 1005.0     | 2000-03-18   | E        | 5.0      |
| **6** | 6          | NaN      | NaN      | 1004.0     | 2000-03-15   | D        | 4.0      |

```python
pd.merge(customer_df4, order_df4, on='客户ID', how='left')
```

|       | **客户ID** | **姓名** | **年龄** | **订单ID** | **订单日期** | **产品** | **数量** |
| ----- | :--------- | :------- | :------- | :--------- | :----------- | :------- | :------- |
| **0** | 1          | Tom      | 20       | 1001.0     | 2000-03-12   | A        | 2.0      |
| **1** | 1          | Tom      | 20       | 1002.0     | 2000-03-13   | B        | 3.0      |
| **2** | 2          | Bob      | 35       | 1003.0     | 2000-03-13   | C        | 1.0      |
| **3** | 3          | Mary     | 27       | 1006.0     | 2000-03-21   | F        | 2.0      |
| **4** | 4          | Alice    | 19       | NaN        | NaN          | NaN      | NaN      |
| **5** | 5          | John     | 42       | 1005.0     | 2000-03-18   | E        | 5.0      |

```python
pd.merge(customer_df4, order_df4, on='客户ID', how='right')
```

|       | **客户ID** | **姓名** | **年龄** | **订单ID** | **订单日期** | **产品** | **数量** |
| ----- | :--------- | :------- | :------- | :--------- | :----------- | :------- | :------- |
| **0** | 1          | Tom      | 20.0     | 1001       | 2000-03-12   | A        | 2        |
| **1** | 1          | Tom      | 20.0     | 1002       | 2000-03-13   | B        | 3        |
| **2** | 2          | Bob      | 35.0     | 1003       | 2000-03-13   | C        | 1        |
| **3** | 6          | NaN      | NaN      | 1004       | 2000-03-15   | D        | 4        |
| **4** | 5          | John     | 42.0     | 1005       | 2000-03-18   | E        | 5        |
| **5** | 3          | Mary     | 27.0     | 1006       | 2000-03-21   | F        | 2        |

### （二）、根据索引合并DataFrame

join方法

join方法是根据索引去合并DataFrame，会保留两个DataFrame的所有的列

**假如有重名列，会报错，需要传入可选参数**`**lsuffix='左边DataFrame重名列的后缀'**`**和**`**rsuffix='右边DataFrame重名列的后缀'**`

**join是方法，不是函数，所以我们是要用某个DataFrame去调用**

**join方法也可以传入可选参数**`**how**`**，来指定去进行哪个类型的合并**

```python
customer_df4
```

|       | **客户ID** | **姓名** | **年龄** |
| ----- | :--------- | :------- | :------- |
| **0** | 1          | Tom      | 20       |
| **1** | 2          | Bob      | 35       |
| **2** | 3          | Mary     | 27       |
| **3** | 4          | Alice    | 19       |
| **4** | 5          | John     | 42       |

```python
order_df4
```

|       | **订单ID** | **订单日期** | **客户ID** | **产品** | **数量** |
| ----- | :--------- | :----------- | :--------- | :------- | :------- |
| **0** | 1001       | 2000-03-12   | 1          | A        | 2        |
| **1** | 1002       | 2000-03-13   | 1          | B        | 3        |
| **2** | 1003       | 2000-03-13   | 2          | C        | 1        |
| **3** | 1004       | 2000-03-15   | 6          | D        | 4        |
| **4** | 1005       | 2000-03-18   | 5          | E        | 5        |
| **5** | 1006       | 2000-03-21   | 3          | F        | 2        |

```python
customer_df4.join(order_df4, lsuffix='_customer', rsuffix='_order')
```

|       | **客户ID\_customer** | **姓名** | **年龄** | **订单ID** | **订单日期** | **客户ID\_order** | **产品** | **数量** |
| ----- | :------------------ | :------- | :------- | :--------- | :----------- | :--------------- | :------- | :------- |
| **0** | 1                   | Tom      | 20       | 1001       | 2000-03-12   | 1                | A        | 2        |
| **1** | 2                   | Bob      | 35       | 1002       | 2000-03-13   | 1                | B        | 3        |
| **2** | 3                   | Mary     | 27       | 1003       | 2000-03-13   | 2                | C        | 1        |
| **3** | 4                   | Alice    | 19       | 1004       | 2000-03-15   | 6                | D        | 4        |
| **4** | 5                   | John     | 42       | 1005       | 2000-03-18   | 5                | E        | 5        |

随着数据表的数量增加，和分析数据的复杂度的增加，可能会越来越多的用merge做合并数据的操作。

## 三、分组聚合

### （一）、聚合函数

定义好的聚合函数包括：

1. count()，得到元素的数量
2. first()，得到第一个元素
3. last()，得到最后一个元素
4. mean()，得到所有元素平均值
5. median()，得到所有元素中位数
6. min()，得到所有元素最小值
7. max()，得到所有元素最大值
8. std()，得到所有元素的标准差
9. var()，得到所有元素的方差
10. prod()，得到所有元素的积
11. sum()，得到所有元素的和

### （一）、对DataFrame进行分组聚合运算

#### 1、根据单个变量进行分组聚合运算

```python
import pandas as pd
import numpy as np

df = pd.DataFrame({
    '分店编号': ['001', '002', '001', '002', '001', '002', '001', '002'],
    '时间段': ['2022Q1', '2022Q1', '2022Q1', '2022Q1', '2022Q2', '2022Q2', '2022Q2', '2022Q2'],
    '商品类别': ['生鲜食品', '生鲜食品', '休闲食品', '休闲食品', '生鲜食品', '生鲜食品', '休闲食品', '休闲食品'],
    '销售额': [1500, 2000, 3000, 2500, 1800, 2200, 3200, 2700],
    '销售数量': [105,  84, 171, 162,  67, 150,  99,  57]
})
df
```

|       | **分店编号** | **时间段** | **商品类别** | **销售额** | **销售数量** |
| ----- | :----------- | :--------- | :----------- | :--------- | :----------- |
| **0** | 001          | 2022Q1     | 生鲜食品     | 1500       | 105          |
| **1** | 002          | 2022Q1     | 生鲜食品     | 2000       | 84           |
| **2** | 001          | 2022Q1     | 休闲食品     | 3000       | 171          |
| **3** | 002          | 2022Q1     | 休闲食品     | 2500       | 162          |
| **4** | 001          | 2022Q2     | 生鲜食品     | 1800       | 67           |
| **5** | 002          | 2022Q2     | 生鲜食品     | 2200       | 150          |
| **6** | 001          | 2022Q2     | 休闲食品     | 3200       | 99           |
| **7** | 002          | 2022Q2     | 休闲食品     | 2700       | 57           |

DataFrame的groupby方法

1. 根据变量进行分组，得到一个叫做DataFrameGroupBy的实例

`df.groupby('变量')`

2. 底层逻辑

groupby方法，会自动把变量值相同的行分组组合到一起，并返回一个叫做DataFrameGroupBy的实例。

分组组合，可以看成把变量值相同的许多数据组合进，以组为单位的单元格中，这时一个单元格里面有多个数据，并不是有效的表格数据，无法返回DataFrame。

因此，最后需要通过聚合函数，生成有效表格数据。

3. 提取变量，对DataFrameGroupBy的实例，提取出要进行聚合运算的单列或多列

1\)单列

`df.groupby('变量')['要进行聚合运算的单列']`

2\)多列

在方括号里传入一个列表

`df.groupby('变量')[['变量1', '变量2']]`

4. 进行聚合运算

`df.groupby('变量')['要进行聚合运算的单列'].聚合函数()`

5. 结果

返回的结果的索引，是我们给groupby传入的变量名

返回的结果，是Series还是DataFrame，取决于聚合运算前提取出的变量是单个还是多个

```python
df.groupby('分店编号') 

<pandas.core.groupby.generic.DataFrameGroupBy object at 0x000001F57F84C390> 

df.groupby('分店编号')['销售额'] 

<pandas.core.groupby.generic.SeriesGroupBy object at 0x000001F57905BAD0> 

df.groupby('分店编号')[['销售额', '销售数量']]

<pandas.core.groupby.generic.DataFrameGroupBy object at 0x000001F51882FC10>

df.groupby('分店编号')['销售额'].mean()

分店编号
001    2375.0
002    2350.0
Name: 销售额, dtype: float64
```

```python
df.groupby('分店编号')[['销售额', '销售数量']].mean()
```

|              | **销售额** | **销售数量** |
| ------------ | ---------- | ------------ |
| **分店编号** |            |              |
| **001**      | 2375.0     | 110.50       |
| **002**      | 2350.0     | 113.25       |

#### 2、根据多个变量进行分组聚合运算

给groupby方法传入一个列表，里面放上多个变量

```python
df.groupby(['分店编号', '时间段'])[['销售额', '销售数量']].mean()
```

|              |            | **销售额** | **销售数量** |
| ------------ | ---------- | ---------- | ------------ |
| **分店编号** | **时间段** |            |              |
| **001**      | **2022Q1** | 2250.0     | 138.0        |
|              | **2022Q2** | 2500.0     | 83.0         |
| **002**      | **2022Q1** | 2250.0     | 123.0        |
|              | **2022Q2** | 2450.0     | 103.5        |

#### 3、自定义聚合函数

自定义聚合函数的要求是，它能把Series里一堆数字汇总成一个数字

```python
def max_plus_10(nums):
    return nums.max() + 10
```

可以用apply方法，参数传入自定义聚合函数的函数名，详见Dataframe章节

```python
df.groupby(['分店编号', '时间段'])[['销售额', '销售数量']].apply(max_plus_10)
```

|              |            | **销售额** | **销售数量** |
| ------------ | ---------- | ---------- | ------------ |
| **分店编号** | **时间段** |            |              |
| **001**      | **2022Q1** | 3010       | 181          |
|              | **2022Q2** | 3210       | 109          |
| **002**      | **2022Q1** | 2510       | 172          |
|              | **2022Q2** | 2710       | 160          |

### （三）、透视表

Pandas的`pivot_table`函数

1. 功能

基于原始数据对表进行重塑，让表变得更加直观，方便我们阅读和理解

2. 语法

`pd.pivot_table(DataFrame, index=['列名1', '列名2'], columns='列名3', values='列名4', aggfunc='函数名')`

把包含数据的DataFrame和想设定的索引、列、值以及聚合函数的函数名，都作为参数，传入pivot\_table函数

需要两层索引，可以给`index`传入一个列表

**由于重塑得到的表格，每个单元格，相当于包含经索引和列分组后的数据，大多数情况下不是一个数据，所以需要聚合函数**

**不指定**`**aggfunc**`**时，默认是NumPy的**`**mean**`**函数**

```python
df
```

|       | **分店编号** | **时间段** | **商品类别** | **销售额** | **销售数量** |
| ----- | :----------- | :--------- | :----------- | :--------- | :----------- |
| **0** | 001          | 2022Q1     | 生鲜食品     | 1500       | 105          |
| **1** | 002          | 2022Q1     | 生鲜食品     | 2000       | 84           |
| **2** | 001          | 2022Q1     | 休闲食品     | 3000       | 171          |
| **3** | 002          | 2022Q1     | 休闲食品     | 2500       | 162          |
| **4** | 001          | 2022Q2     | 生鲜食品     | 1800       | 67           |
| **5** | 002          | 2022Q2     | 生鲜食品     | 2200       | 150          |
| **6** | 001          | 2022Q2     | 休闲食品     | 3200       | 99           |
| **7** | 002          | 2022Q2     | 休闲食品     | 2700       | 57           |

把df的分店编号和时间段作为索引，商品类别作为列，计算销售额的总和

```python
pd.pivot_table(df, index=['分店编号', '时间段'], columns='商品类别', values='销售额', aggfunc=np.sum)
```

|              | **商品类别** | **休闲食品** | **生鲜食品** |
| ------------ | :----------- | ------------ | ------------ |
| **分店编号** | **时间段**   |              |              |
| **001**      | **2022Q1**   | 3000         | 1500         |
|              | **2022Q2**   | 3200         | 1800         |
| **002**      | **2022Q1**   | 2500         | 2000         |
|              | **2022Q2**   | 2700         | 2200         |

**由于Series底层实现就是NumPy的数组，所以针对数组的函数也可以用在Pandas的Series上**

```python
pd.pivot_table(df, index=['分店编号', '时间段'], columns='商品类别', values='销售额', aggfunc=sum)
```

|              | **商品类别** | **休闲食品** | **生鲜食品** |
| ------------ | :----------- | ------------ | ------------ |
| **分店编号** | **时间段**   |              |              |
| **001**      | **2022Q1**   | 3000         | 1500         |
|              | **2022Q2**   | 3200         | 1800         |
| **002**      | **2022Q1**   | 2500         | 2000         |
|              | **2022Q2**   | 2700         | 2200         |

**因为sum是Python内置函数，所以也不一定调用NumPy数据库**

```python
# 把df的分店编号和时间段作为索引，商品类别作为列，计算销售额的平均值
# 由于这里重塑的表格每个单元格只包含原来DataFrame的一个单元格的数据，所以求和和求平均的结果一致
pd.pivot_table(df, index=['分店编号', '时间段'], columns='商品类别', values='销售额')
```

|              | **商品类别** | **休闲食品** | **生鲜食品** |
| ------------ | :----------- | ------------ | ------------ |
| **分店编号** | **时间段**   |              |              |
| **001**      | **2022Q1**   | 3000         | 1500         |
|              | **2022Q2**   | 3200         | 1800         |
| **002**      | **2022Q1**   | 2500         | 2000         |
|              | **2022Q2**   | 2700         | 2200         |

```python
# 把df的商品类别作为索引，分店编号作为列，计算销售额的平均值
pd.pivot_table(df, index='商品类别', columns='分店编号', values='销售额')
```

| **分店编号** | **001** | **002** |
| :----------- | ------- | ------- |
| **商品类别** |         |         |
| **休闲食品** | 3100    | 2600    |
| **生鲜食品** | 1650    | 2100    |

```python
# 把df的商品类别作为索引，分店编号作为列，计算销售额的总和
pd.pivot_table(df, index='商品类别', columns='分店编号', values='销售额', aggfunc=np.sum)
```

| **分店编号** | **001** | **002** |
| :----------- | ------- | ------- |
| **商品类别** |         |         |
| **休闲食品** | 6200    | 5200    |
| **生鲜食品** | 3300    | 4200    |

### （四）、groupby方法和pivot\_table函数异同

1. **groupby方法和pivot\_table函数的相同之处：**

**都可以进行分组聚合运算**

2. **groupby方法和pivot\_table函数的不同之处：**

**pivot\_table函数可以把变量作为索引分组，也可以把变量作为列进行分组；**

**groupby方法把数据分组后，用于分组的变量只能作为索引**

**当主要目的是分组聚合运算时，groupby方法会更加常用，因为根据什么分组，提取什么变量，做什么聚合操作，这个逻辑更加直接**

分组聚合操作是数据分析中的重要方法。

针对某个数据集，假如想把地区作为切入点，看地区之间是否有差异，就可以对数据用地区变量进行分组，进行聚合分析。

## 四、数据分箱

```python
import pandas as pd
df1 = pd.read_csv("residents_data.csv")
df1
```

|        | **性别** | **居住地** | **年龄** | **工资** |
| ------ | :------- | :--------- | :------- | :------- |
| **0**  | 男       | 北京       | 38       | 18053    |
| **1**  | 女       | 上海       | 42       | 9382     |
| **2**  | 男       | 广州       | 23       | 6376     |
| **3**  | 女       | 深圳       | 36       | 10746    |
| **4**  | 男       | 杭州       | 20       | 5284     |
| **5**  | 女       | 南京       | 34       | 9828     |
| **6**  | 男       | 成都       | 33       | 9366     |
| **7**  | 男       | 重庆       | 47       | 22820    |
| **8**  | 男       | 武汉       | 36       | 16927    |
| **9**  | 女       | 西安       | 42       | 11591    |
| **10** | 男       | 南昌       | 42       | 11316    |
| **11** | 男       | 合肥       | 27       | 7426     |
| **12** | 男       | 长沙       | 23       | 4705     |
| **13** | 男       | 福州       | 41       | 24117    |
| **14** | 女       | 厦门       | 50       | 69153    |
| **15** | 女       | 济南       | 32       | 5559     |
| **16** | 男       | 郑州       | 56       | 6391     |
| **17** | 男       | 长春       | 60       | 11020    |
| **18** | 男       | 哈尔滨     | 59       | 68189    |
| **19** | 女       | 南宁       | 32       | 15661    |

### （一）、进行数据分箱

**分箱，就是把数据按特定的规则进行分组，实现数据的离散化，增强数据稳定性。**

针对某个数据集，如果想把年龄作为切入点，对数据用年龄变量进行分组，进行聚合分析；会发现因为年龄存在很多可能性，出现非常多组。

```python
 df1.groupby('年龄')['工资'].mean()
```

```python
年龄
20     5284.0
23     5540.5
27     7426.0
32    10610.0
33     9366.0
34     9828.0
36    13836.5
38    18053.0
41    24117.0
42    10763.0
47    22820.0
50    69153.0
56     6391.0
59    68189.0
60    11020.0
Name: 工资, dtype: float64
```

#### 1、Pandas的cut函数

这时可以调用Pandas的cut函数

1）功能

可以帮我们对Series，根据数字范围进行划分，之后用划分后的标签进行分组

2）步骤

* 新建一个列表，用它来表示数字的分组边界，也即分箱的规则
* 新建一个列表，用来表示各个范围所对应的标签，
* 根据分箱规则对变量进行分箱，并使用分组标签
* 把将分箱结果作为新的一列添加到原数据集中，然后可以开始进行分组聚合运算

3）语法

函数中，传入要进行分箱的Series，和创建好的分箱规则，可选参数labels='分组标签列表'

**标签和区间按顺序一一对应，因此分箱规则的列表需要比标签的列表多一个元素。**

4）结果

返回一个新的Series，数据类型是category，其中的种类就是根据传入的分箱规则划分的结果，展示了原始Series里面各个数字分别是属于什么范围的。

假如不传入可选参数labels，结果中Series的值，会以区间形式呈现，圆括号和方括号分别表示开区间和闭区间；假如传入可选参数labels，结果中Series的值，就变成了我们前面指定的标签

#### 2、示例

```python
# 1. 定义年龄分组列表
# 2. 并根据以上分组对df1的年龄列进行分箱
age_bins = [0, 10, 20, 30, 40, 50, 60, 120]
pd.cut(df1.年龄, age_bins)
```

```python
0     (30, 40]
1     (40, 50]
2     (20, 30]
3     (30, 40]
4     (10, 20]
5     (30, 40]
6     (30, 40]
7     (40, 50]
8     (30, 40]
9     (40, 50]
10    (40, 50]
11    (20, 30]
12    (20, 30]
13    (40, 50]
14    (40, 50]
15    (30, 40]
16    (50, 60]
17    (50, 60]
18    (50, 60]
19    (30, 40]
Name: 年龄, dtype: category
Categories (7, interval[int64, right]): [(0, 10] < (10, 20] < (20, 30] < (30, 40] < (40, 50] < (50, 60] < (60, 120]]
```

```python
# 1. 定义年龄分组列表
# 2. 定义分组标签列表
# 3. 根据分组对df1的年龄列进行分箱，并使用以上分组标签
age_bins = [0, 10, 20, 30, 40, 50, 60, 120]
age_labels = ['儿童', '青少年', '青年', '壮年', '中年', '中老年', '老年']
pd.cut(df1.年龄, age_bins, labels=age_labels)
```

```python
0      壮年
1      中年
2      青年
3      壮年
4     青少年
5      壮年
6      壮年
7      中年
8      壮年
9      中年
10     中年
11     青年
12     青年
13     中年
14     中年
15     壮年
16    中老年
17    中老年
18    中老年
19     壮年
Name: 年龄, dtype: category
Categories (7, object): ['儿童' < '青少年' < '青年' < '壮年' < '中年' < '中老年' < '老年']
```

```python
# 4.为df1新建"年龄组"列，值为以上分组标签
df1['年龄组'] = pd.cut(df1.年龄, age_bins, labels=age_labels)
df1
```

|        | **性别** | **居住地** | **年龄** | **工资** | **年龄组** |
| ------ | :------- | :--------- | :------- | :------- | :--------- |
| **0**  | 男       | 北京       | 38       | 18053    | 壮年       |
| **1**  | 女       | 上海       | 42       | 9382     | 中年       |
| **2**  | 男       | 广州       | 23       | 6376     | 青年       |
| **3**  | 女       | 深圳       | 36       | 10746    | 壮年       |
| **4**  | 男       | 杭州       | 20       | 5284     | 青少年     |
| **5**  | 女       | 南京       | 34       | 9828     | 壮年       |
| **6**  | 男       | 成都       | 33       | 9366     | 壮年       |
| **7**  | 男       | 重庆       | 47       | 22820    | 中年       |
| **8**  | 男       | 武汉       | 36       | 16927    | 壮年       |
| **9**  | 女       | 西安       | 42       | 11591    | 中年       |
| **10** | 男       | 南昌       | 42       | 11316    | 中年       |
| **11** | 男       | 合肥       | 27       | 7426     | 青年       |
| **12** | 男       | 长沙       | 23       | 4705     | 青年       |
| **13** | 男       | 福州       | 41       | 24117    | 中年       |
| **14** | 女       | 厦门       | 50       | 69153    | 中年       |
| **15** | 女       | 济南       | 32       | 5559     | 壮年       |
| **16** | 男       | 郑州       | 56       | 6391     | 中老年     |
| **17** | 男       | 长春       | 60       | 11020    | 中老年     |
| **18** | 男       | 哈尔滨     | 59       | 68189    | 中老年     |
| **19** | 女       | 南宁       | 32       | 15661    | 壮年       |

```python
# 5. 对df1根据年龄组进行分组，计算各个年龄组的平均工资
df1.groupby('年龄组')['工资'].mean()
```

```python
年龄组
儿童              NaN
青少年     5284.000000
青年      6169.000000
壮年     12305.714286
中年     24729.833333
中老年    28533.333333
老年              NaN
Name: 工资, dtype: float64
```

### （二）、层次化索引

在用groupby方法分组的时候，利用了一个以上的变量，会发现得到的DataFrame会出现层次化索引

```python
df2 = pd.DataFrame({
    '分店编号': ['001', '002', '001', '002', '001', '002', '001', '002'],
    '时间段': ['2022Q1', '2022Q1', '2022Q1', '2022Q1', '2022Q2', '2022Q2', '2022Q2', '2022Q2'],
    '商品类别': ['生鲜食品', '生鲜食品', '休闲食品', '休闲食品', '生鲜食品', '生鲜食品', '休闲食品', '休闲食品'],
    '销售额': [1500, 2000, 3000, 2500, 1800, 2200, 3200, 2700],
    '销售数量': [105,  84, 171, 162,  67, 150,  99,  57]
})
grouped_df2 = df2.groupby(['分店编号', '时间段'])[['销售额', '销售数量']].mean()
grouped_df2
```

|              |            | **销售额** | **销售数量** |
| ------------ | ---------- | ---------- | ------------ |
| **分店编号** | **时间段** |            |              |
| **001**      | **2022Q1** | 2250.0     | 138.0        |
|              | **2022Q2** | 2500.0     | 83.0         |
| **002**      | **2022Q1** | 2250.0     | 123.0        |
|              | **2022Q2** | 2450.0     | 103.5        |

#### 1、针对层次化索引的DataFrame，提取数据行

依然可以用方括号里面放上索引，去提取数据行

1）用外层索引，会一次性提取出多行

```python
grouped_df2.loc['001']
```

|            | **销售额** | **销售数量** |
| ---------- | ---------- | ------------ |
| **时间段** |            |              |
| **2022Q1** | 2250.0     | 138.0        |
| **2022Q2** | 2500.0     | 83.0         |

2）要提取一行，可以在外层索引后，继续用内层索引继续去提取

```python
 grouped_df2.loc['001'].loc['2022Q1']
```

```python
销售额     2250.0
销售数量     138.0
Name: 2022Q1, dtype: float64
```

**不能直接用内层索引去提取数据行**

#### 2、重置索引

DataFrame的`reset_index`方法

索引会变成某列数据，详见清理数据章节

### （三）、根据条件筛选数据

```python
df1
```

|        | **性别** | **居住地** | **年龄** | **工资** | **年龄组** |
| ------ | :------- | :--------- | :------- | :------- | :--------- |
| **0**  | 男       | 北京       | 38       | 18053    | 壮年       |
| **1**  | 女       | 上海       | 42       | 9382     | 中年       |
| **2**  | 男       | 广州       | 23       | 6376     | 青年       |
| **3**  | 女       | 深圳       | 36       | 10746    | 壮年       |
| **4**  | 男       | 杭州       | 20       | 5284     | 青少年     |
| **5**  | 女       | 南京       | 34       | 9828     | 壮年       |
| **6**  | 男       | 成都       | 33       | 9366     | 壮年       |
| **7**  | 男       | 重庆       | 47       | 22820    | 中年       |
| **8**  | 男       | 武汉       | 36       | 16927    | 壮年       |
| **9**  | 女       | 西安       | 42       | 11591    | 中年       |
| **10** | 男       | 南昌       | 42       | 11316    | 中年       |
| **11** | 男       | 合肥       | 27       | 7426     | 青年       |
| **12** | 男       | 长沙       | 23       | 4705     | 青年       |
| **13** | 男       | 福州       | 41       | 24117    | 中年       |
| **14** | 女       | 厦门       | 50       | 69153    | 中年       |
| **15** | 女       | 济南       | 32       | 5559     | 壮年       |
| **16** | 男       | 郑州       | 56       | 6391     | 中老年     |
| **17** | 男       | 长春       | 60       | 11020    | 中老年     |
| **18** | 男       | 哈尔滨     | 59       | 68189    | 中老年     |
| **19** | 女       | 南宁       | 32       | 15661    | 壮年       |

#### 1、把Series作为DataFrame的索引

把条件是否符合对应的布尔值的Series，作为DataFrame的索引，来筛选出所有布尔值为True的行

```python
df1[(df1['性别'] == '男') & (df1['年龄'] <= 20)]
```

|       | **性别** | **居住地** | **年龄** | **工资** | **年龄组** |
| ----- | :------- | :--------- | :------- | :------- | :--------- |
| **4** | 男       | 杭州       | 20       | 5284     | 青少年     |

#### 2、DataFrame的query方法

1）优点

写法更加简洁直观

2）语法

给query方法传入一个字符串，字符串内容是想要筛选的条件，不同条件之间用括号包围，中间放上逻辑符号

**条件中的列名，不需要带DataFrame的名字，也不需要引号包围，直接用列名表示**

**条件里的普通字符串，需要带引号，并需要与query方法传入字符串外面，用不一样的引号；或者引号前面加上**`**\**`**，进行一个转义，让代码知道这是字符串里面的单引号**

```python
df1.query('(性别 == "男") & (年龄 <= 20)')
```

|       | **性别** | **居住地** | **年龄** | **工资** | **年龄组** |
| ----- | :------- | :--------- | :------- | :------- | :--------- |
| **4** | 男       | 杭州       | 20       | 5284     | 青少年     |

---

---
url: /Python/数据分析/4_Python数据分析—Dataframe.md
---

# **Pandas\_Dataframe**

## 一、DataFrame简介

相比Series，由多个Series组成的DataFrame，才是我们分析数据是最常打交道的数据结构。

DataFrame看起来像是一个表格，它的不同列可以是不同数据类型，而不是像NumPy二维数组那样，要求数据类型全部保持一致。

相比起Series，DataFrame每个值不止有索引，也有列名。换个角度来看，DataFrame就像是由Series组成的字典，每个Series对应一个键名，也就是列名。

## 二、创建DataFrame

**创建DataFrame时，会自动进行索引对齐**

### （一）参数传入一个字典，值是Series

既然DataFrame可以看成由Series组成的字典，那么第一个创建方法就是，参数传入一个字典。键是各个Series所对应的列名。

创建好后，Jupyter Notebook会输出一个，排版好看的DataFrame表格。索引就对应了Series的索引，而列名对应我们传入的键。

参数传入一个字典，值是Series

```python
import pandas as pd
s_id = pd.Series(["01", "02", "03", "04", "05"])
s_class = pd.Series(["二班", "一班", "二班", "三班", "一班"])
s_grade = pd.Series([92, 67, 70, 88, 76])

df1 = pd.DataFrame({"学号": s_id, "班级": s_class, "成绩": s_grade})
df1
```

运行结果

|       | **学号** | **班级** | **成绩** |
| ----- | -------- | -------- | -------- |
| **0** | 01       | 二班     | 92       |
| **1** | 02       | 一班     | 67       |
| **2** | 03       | 二班     | 70       |
| **3** | 04       | 三班     | 88       |
| **4** | 05       | 一班     | 76       |

### （二）参数传入一个字典，值是列表

```python
l_id = ["01", "02", "03", "04", "05"]
l_class = ["二班", "一班", "二班", "三班", "一班"]
l_grade = [92, 67, 70, 88, 76]

df2 = pd.DataFrame({"学号": s_id, "班级": s_class, "成绩": s_grade})
df2
```

运行结果

|       | **学号** | **班级** | **成绩** |
| ----- | -------- | -------- | -------- |
| **0** | 01       | 二班     | 92       |
| **1** | 02       | 一班     | 67       |
| **2** | 03       | 二班     | 70       |
| **3** | 04       | 三班     | 88       |
| **4** | 05       | 一班     | 76       |

Dataframe默认的索引和Series一样，都是从0开始依次递增的整数，来表示位置。但如果传入的Series有标签索引的话，DataFrame的索引也会变成相应的标签。

```python
s_id = pd.Series(["01", "02", "03", "04", "05"], index=["小明", "小红", "小杰", "小丽", "小华"])
s_class = pd.Series(["二班", "一班", "二班", "三班", "一班"], index=["小明", "小红", "小杰", "小丽", "小华"])
s_grade = pd.Series([92, 67, 70, 88, 76],
                    index=["小明", "小红", "小杰", "小丽", "小华"])

df3 = pd.DataFrame({"学号": s_id, "班级": s_class, "成绩": s_grade})
df3
```

运行结果

|          | **学号** | **班级** | **成绩** |
| -------- | -------- | -------- | -------- |
| **小明** | 01       | 二班     | 92       |
| **小红** | 02       | 一班     | 67       |
| **小杰** | 03       | 二班     | 70       |
| **小丽** | 04       | 三班     | 88       |
| **小华** | 05       | 一班     | 76       |

### （三）参数传入一个嵌套字典

参数传入一个嵌套字典，可以一次性创建出既有标签索引，也有列名的DataFrame

最外层的键仍然对应各个列名，而值则对应每列的Series；里层字典的键对应Series的标签索引

```python
df4 = pd.DataFrame({"学号": {"小明": "01", "小红": "02", "小杰": "03", "小丽": "04", "小华": "05"}, 
                    "班级": {"小明": "二班", "小红": "一班", "小杰": "二班", "小丽": "三班", "小华": "一班"}, 
                    "成绩": {"小明": 92, "小红": 67, "小杰": 70, "小丽": 88, "小华": 76}})
df4
```

运行结果

|          | **学号** | **班级** | **成绩** |
| -------- | -------- | -------- | -------- |
| **小明** | 01       | 二班     | 92       |
| **小红** | 02       | 一班     | 67       |
| **小杰** | 03       | 二班     | 70       |
| **小丽** | 04       | 三班     | 88       |
| **小华** | 05       | 一班     | 76       |

## 三、DataFrame的常用属性

### （一）index属性与columns属性

要获得DataFrame的索引，可以用index属性；要获取所有列名，可以用columns属性。

**返回的索引和列名的数据类型，都是pandas库的Index类**

```python
df4.index

Index(['小明', '小红', '小杰', '小丽', '小华'], dtype='object')

df4.columns

Index(['学号', '班级', '成绩'], dtype='object')
```

### （二）values属性

要获取所有的值，可以用values属性

返回的类型是NumPy数组，**那所有针对NumPy数组的操作，都可以用在values属性上了**

```python
df4.values

array([['01', '二班', 92],
       ['02', '一班', 67],
       ['03', '二班', 70],
       ['04', '三班', 88],
       ['05', '一班', 76]], dtype=object)
```

### （三）T属性

对DataFrame进行转置，可以用T属性，**大写的T。**

返回的结果会把行和列进行转置。

```python
df4.T
```

运行结果

|          | **小明** | **小红** | **小杰** | **小丽** | **小华** |
| -------- | -------- | -------- | -------- | -------- | -------- |
| **学号** | 01       | 02       | 03       | 04       | 05       |
| **班级** | 二班     | 一班     | 二班     | 三班     | 一班     |
| **成绩** | 92       | 67       | 70       | 88       | 76       |

获取转置后的属性

```python
df5 = df4.T
print(df5.index)
print(df5.columns)
print(df5.values)
```

运行结果

```python
Index(['学号', '班级', '成绩'], dtype='object')
Index(['小明', '小红', '小杰', '小丽', '小华'], dtype='object')
[['01' '02' '03' '04' '05']
 ['二班' '一班' '二班' '三班' '一班']
 [92 67 70 88 76]]
```

**转置后，索引、列名、值都相应发生了变化**

## 四、从DataFrame中提取数据

### （一）提取DataFrame的列

#### 1、提取某个列

1\)在DataFrame后面跟上方括号，里面放上列名，就能把列名对应的列提取出来。

列的类型是Series，索引对应DataFrame原本的索引。

这就跟我们在Python字典后面跟上方括号，里面放上键名，就能把键对应的值提取出来是类似的

```python
df4['成绩']

小明    92
小红    67
小杰    70
小丽    88
小华    76
Name: 成绩, dtype: int64
```

```python
df4['班级']

小明    二班
小红    一班
小杰    二班
小丽    三班
小华    一班
Name: 班级, dtype: object
```

2\)也可以通过".列名" ，来获取对应的列，因为每列Series其实也是DataFrame的属性

**如果列名里面有空格或特殊符号的话，就不能通过属性名来获取了，只能通过方括号**

**Pandas不允许通过属性来添加/更新列**

```python
df4.成绩

小明    92
小红    67
小杰    70
小丽    88
小华    76
Name: 成绩, dtype: int64
```

```python
df4.班级

小明    二班
小红    一班
小杰    二班
小丽    三班
小华    一班
Name: 班级, dtype: object
```

#### 2、提取任意多列

可以在方括号里放入列表，再在列表里面传入多个列名。

**这样做返回的就不是Series，而是DataFrame了**

```python
df4[["成绩", "班级"]]
```

运行结果

|          | **成绩** | **班级** |
| -------- | -------- | -------- |
| **小明** | 92       | 二班     |
| **小红** | 67       | 一班     |
| **小杰** | 70       | 二班     |
| **小丽** | 88       | 三班     |
| **小华** | 76       | 一班     |

### （二）提取DataFrame的行

提取列用列名，提取行就应该用索引

#### 1、提取某个行

每行数据是以Series类型进行返回的

1）loc 按照标签索引提取行

2）iloc 按照位置索引提取行

```python
df4.loc["小丽"]

学号    04
班级    三班
成绩    88
Name: 小丽, dtype: object
```

```python
df4.iloc[3]

学号    04
班级    三班
成绩    88
Name: 小丽, dtype: object
```

#### 2、提取部分行

与Series切片类似，给loc标签索引范围，或者给iloc位置索引范围，可以获得多行数据

**标签索引做切片是包含结束值的**

```python
df4.loc["小红": "小丽"]
```

运行结果

|          | **学号** | **班级** | **成绩** |
| -------- | -------- | -------- | -------- |
| **小红** | 02       | 一班     | 67       |
| **小杰** | 03       | 二班     | 70       |
| **小丽** | 04       | 三班     | 88       |

```plain
 df4.iloc[1: 3]
```

运行结果

|          | **学号** | **班级** | **成绩** |
| -------- | -------- | -------- | -------- |
| **小红** | 02       | 一班     | 67       |
| **小杰** | 03       | 二班     | 70       |

#### 3、提取任意多行

可以给loc或iloc后面的方括号里，放入一个列表，里面是想提取出的行的标签或位置索引。与之前提取任意列类似。

```python
df4.loc[["小丽", "小红"]]
```

运行结果

|          | **学号** | **班级** | **成绩** |
| -------- | -------- | -------- | -------- |
| **小丽** | 04       | 三班     | 88       |
| **小红** | 02       | 一班     | 67       |

```python
df4.iloc[[3, 1]]
```

运行结果

|          | **学号** | **班级** | **成绩** |
| -------- | -------- | -------- | -------- |
| **小丽** | 04       | 三班     | 88       |
| **小红** | 02       | 一班     | 67       |

### （三）提取DataFrame的值

#### 1、提取某个DataFrame的元素

可以在loc或iloc后面的方括号里，放上2个参数，第一个表示行，第二个表示列，就能提取出表格某个位置上的值

```python
 df4.loc["小杰", "学号"]

'03'

df4.iloc[2, 0]

'03'
```

#### 2、提取部分DataFrame

提取部分表格数据类似，只需要在loc或iloc后面的方括号里放上2个参数，第一个表示行的切片，第二个表示列的切片，就可以把表格的一部分给切出来

**标签索引做切片会包含结束值**

```python
df4.loc["小红": "小丽", "学号": "成绩"]
```

运行结果

|          | **学号** | **班级** | **成绩** |
| -------- | -------- | -------- | -------- |
| **小红** | 02       | 一班     | 67       |
| **小杰** | 03       | 二班     | 70       |
| **小丽** | 04       | 三班     | 88       |

```python
df4.iloc[1: 3, 0: 2]
```

运行结果

|          | **学号** | **班级** |
| -------- | -------- | -------- |
| **小红** | 02       | 一班     |
| **小杰** | 03       | 二班     |

#### 3、提取部分列，同时保留所有行;提取部分行，同时保留所有列

一个省事的方法是，省略希望保留所有的切片里冒号前后的值，直接放上一个冒号，这能默认表示全部范围的索引

```python
df4.loc[:, "班级": "成绩"]
```

运行结果

|          | **班级** | **成绩** |
| -------- | -------- | -------- |
| **小明** | 二班     | 92       |
| **小红** | 一班     | 67       |
| **小杰** | 二班     | 70       |
| **小丽** | 三班     | 88       |
| **小华** | 一班     | 76       |

```python
df4.iloc[0: 3, :]
```

运行结果

|          | **学号** | **班级** | **成绩** |
| -------- | -------- | -------- | -------- |
| **小明** | 01       | 二班     | 92       |
| **小红** | 02       | 一班     | 67       |
| **小杰** | 03       | 二班     | 70       |

#### 4、提取不相邻的行或列

通过往loc或iloc后面的方括号里放入列表，通过标签或位置指明提取哪些

```python
df4.loc[["小红", "小丽"], "学号": "班级"]
```

运行结果

|          | **学号** | **班级** |
| -------- | -------- | -------- |
| **小红** | 02       | 一班     |
| **小丽** | 04       | 三班     |

```python
df4.iloc[[1, 3], 0: 2]
```

运行结果

|          | **学号** | **班级** |
| -------- | -------- | -------- |
| **小红** | 02       | 一班     |
| **小丽** | 04       | 三班     |

## 五、根据条件筛选DataFrame中的行

### （一）根据条件筛选行，相比于根据条件筛选列来说，是更加常见和合理的。

因为一般每一行代表一个实例，比如一个城市、一个学生，而每一列代表数据实例的属性，比如说城市的人口、学生的身高。那我们筛选符合条件的行，就相当于从已有数据里，提取符合条件的实例，比如人口在1000万以上的城市，身高在1.6米以上的学生。

### （二）语法

和NumPy的数组以及Pandas的Series是很类似的，在DataFrame后面跟一个方括号，里面放上针对列的条件。列的条件包括：数据类型是Series的列和条件。返回列的两种方法：通过"\["列名"]"，或者通过".属性名"，都可以应用在列的条件中。

### （三）原理

DataFrame的列是Series类型，而Series和条件结合起来，会返回一个布尔值组成的Series,它的长度和DataFrame的行数相对应。DataFrame会用布尔值的Series进行索引，保留True所对应的索引的行。

```python
df4['成绩'] > 80

小明     True
小红    False
小杰    False
小丽     True
小华    False
Name: 成绩, dtype: bool
```

```python
df4[df4["成绩"] > 80]
```

运行结果

|          | **学号** | **班级** | **成绩** |
| -------- | -------- | -------- | -------- |
| **小明** | 01       | 二班     | 92       |
| **小丽** | 04       | 三班     | 88       |

```python
df4[df4.成绩 > 80]
```

运行结果

|          | **学号** | **班级** | **成绩** |
| -------- | -------- | -------- | -------- |
| **小明** | 01       | 二班     | 92       |
| **小丽** | 04       | 三班     | 88       |

### （四）结合逻辑运算

条件也可以结合逻辑运算，因为DataFrame有不同列，所以不同条件里，可以根据不同列的变量进行筛选。

```python
df4[(df4.成绩 > 80) & (df4["班级"] == "三班")]
```

运行结果

|          | **学号** | **班级** | **成绩** |
| -------- | -------- | -------- | -------- |
| **小丽** | 04       | 三班     | 88       |

## 六、head方法和tail方法

**基本上，对DataFrame的操作方法，都是默认不改变原始DataFrame的，而是返回一个新的DataFrame**

**要操作生效的话，要么就得进行重新赋值，要么指定可选参数inplace=True**

### （一）head方法

1、DataFrame.head(num)会返回给我们DataFrame前num行的内容，num是可选参数，默认是前5行内容。

2、用处：当我们和实际数据打交道的时候，可能动辄几千、几万甚至几十万数据，这种时候，这个方法就很实用了。可以看一眼开头几行，快速了解数据包含的信息，以及各列里面变量的特点。

```python
df4.head()
```

运行结果

|          | **学号** | **班级** | **成绩** |
| -------- | -------- | -------- | -------- |
| **小明** | 01       | 二班     | 92       |
| **小红** | 02       | 一班     | 67       |
| **小杰** | 03       | 二班     | 70       |
| **小丽** | 04       | 三班     | 88       |
| **小华** | 05       | 一班     | 76       |

增加指定行数

```python
df4.head(2)
```

运行结果

|          | **学号** | **班级** | **成绩** |
| -------- | -------- | -------- | -------- |
| **小明** | 01       | 二班     | 92       |
| **小红** | 02       | 一班     | 67       |

### （二）tail方法

DataFrame.tail(num)会返回给我们DataFrame后num行的内容，num是可选参数，默认是后5行内容

```python
df4.tail(2)
```

运行结果

|          | **学号** | **班级** | **成绩** |
| -------- | -------- | -------- | -------- |
| **小丽** | 04       | 三班     | 88       |
| **小华** | 05       | 一班     | 76       |

### （三）sample方法

DataFrame.sample(num)会返回给我们DataFrame随机num行的内容，num是可选参数，默认是随机1行内容

```python
df4.sample()
```

运行结果

|          | **学号** | **班级** | **成绩** |
| -------- | -------- | -------- | -------- |
| **小华** | 05       | 一班     | 76       |

## 七、更新/增加DataFrame的一列值

**更新或者增加，取决于列名是否已经存在**

```python
import pandas as pd
name = pd.Series(["小陈", "小李", "小王", "小张", "小赵", "小周"], index=["001", "002", "003", "004", "005", "006"])
gender = pd.Series(["女", "女", "男", "男", "女", "男"], index=["006", "005", "004", "003", "002", "001"])
height = pd.Series([172.5, 168.0, 178.2, 181.3, 161.7, 159.8], index=["001", "002", "003", "004", "005", "006"])
grade = pd.Series([89, 92, 82, 96, 93, 84], index=["001", "002", "003", "004", "005", "006"])
df1 = pd.DataFrame({"姓名": name, "性别": gender, "身高": height, "成绩": grade})
df1
```

|         | **姓名** | **性别** | **身高** | **成绩** |
| ------- | :------- | :------- | :------- | :------- |
| **001** | 小陈     | 男       | 172.5    | 89       |
| **002** | 小李     | 女       | 168.0    | 92       |
| **003** | 小王     | 男       | 178.2    | 82       |
| **004** | 小张     | 男       | 181.3    | 96       |
| **005** | 小赵     | 女       | 161.7    | 93       |
| **006** | 小周     | 女       | 159.8    | 84       |

### （一）、Series处理列值

```python
dataframe['列名'] = Series
```

**如果DataFrame有标签索引，那被赋值的Series也要有index属性，才能成功和DataFrame的索引对齐，否则所有标签无法对齐的地方，都会产生缺失值**

```python
df1['成绩'] = pd.Series([90, 91, 83, 95, 94, 85])
df1
```

|         | **姓名** | **性别** | **身高** | **成绩** |
| ------- | :------- | :------- | :------- | :------- |
| **001** | 小陈     | 男       | 172.5    | NaN      |
| **002** | 小李     | 女       | 168.0    | NaN      |
| **003** | 小王     | 男       | 178.2    | NaN      |
| **004** | 小张     | 男       | 181.3    | NaN      |
| **005** | 小赵     | 女       | 161.7    | NaN      |
| **006** | 小周     | 女       | 159.8    | NaN      |

```python
df1['成绩'] = pd.Series([90, 91, 83, 95, 94, 85], index=['001', '002', '003', '004', '005', '006'])
df1
```

|         | **姓名** | **性别** | **身高** | **成绩** |
| ------- | :------- | :------- | :------- | :------- |
| **001** | 小陈     | 男       | 172.5    | 90       |
| **002** | 小李     | 女       | 168.0    | 91       |
| **003** | 小王     | 男       | 178.2    | 83       |
| **004** | 小张     | 男       | 181.3    | 95       |
| **005** | 小赵     | 女       | 161.7    | 94       |
| **006** | 小周     | 女       | 159.8    | 85       |

### （二）、列表处理列值

```python
dataframe['列名'] = 列表
```

**用列表的好处是不需要指明标签，会自动按顺序对齐，列表的长度和DataFrame本身的行数温和就可以**

```python
df1['成绩'] = [90, 91, 83, 95, 94, 85]
df1
```

|         | **姓名** | **性别** | **身高** | **成绩** |
| ------- | :------- | :------- | :------- | :------- |
| **001** | 小陈     | 男       | 172.5    | 90       |
| **002** | 小李     | 女       | 168.0    | 91       |
| **003** | 小王     | 男       | 178.2    | 83       |
| **004** | 小张     | 男       | 181.3    | 95       |
| **005** | 小赵     | 女       | 161.7    | 94       |
| **006** | 小周     | 女       | 159.8    | 85       |

```python
# 增加DataFrame的一列值
df1["班级"] = ["一班", "三班", "二班", "三班", "一班", "二班"]
df1
```

|         | **姓名** | **性别** | **身高** | **成绩** | **班级** |
| ------- | :------- | :------- | :------- | :------- | :------- |
| **001** | 小陈     | 男       | 172.5    | 90       | 一班     |
| **002** | 小李     | 女       | 168.0    | 91       | 三班     |
| **003** | 小王     | 男       | 178.2    | 83       | 二班     |
| **004** | 小张     | 男       | 181.3    | 95       | 三班     |
| **005** | 小赵     | 女       | 161.7    | 94       | 一班     |
| **006** | 小周     | 女       | 159.8    | 85       | 二班     |

## 八、更新/增加DataFrame的一行值

**更新或者增加，取决于列名是否已经存在**

既然可以通过loc或iloc提取出一行数据，那就可以以同样的方式更新那行

### （一）、Series处理行值

```python
dataframe.loc/iloc['索引'] = Series
```

需要把Series的标签与列名进行对应，否则会产生缺失值

```python
df1.loc["005"] = pd.Series(["小赵", "女", 162.7, 95, "一班"],
                           index=["姓名", "性别", "身高", 
                                  "成绩", "班级"])
df1
```

|         | **姓名** | **性别** | **身高** | **成绩** | **班级** |
| ------- | :------- | :------- | :------- | :------- | :------- |
| **001** | 小陈     | 男       | 172.5    | 90       | 一班     |
| **002** | 小李     | 女       | 168.0    | 91       | 三班     |
| **003** | 小王     | 男       | 178.2    | 83       | 二班     |
| **004** | 小张     | 男       | 181.3    | 95       | 三班     |
| **005** | 小赵     | 女       | 162.7    | 95       | 一班     |
| **006** | 小周     | 女       | 159.8    | 85       | 二班     |

### （二）、列表处理行值

```python
dataframe.loc/iloc['索引'] = 列表
```

不需要指明标签，会自动按顺序对齐

```python
df1.loc["005"] = ["小赵", "女", 162.7, 95, "一班"]
df1
```

|         | **姓名** | **性别** | **身高** | **成绩** | **班级** |
| ------- | :------- | :------- | :------- | :------- | :------- |
| **001** | 小陈     | 男       | 172.5    | 90       | 一班     |
| **002** | 小李     | 女       | 168.0    | 91       | 三班     |
| **003** | 小王     | 男       | 178.2    | 83       | 二班     |
| **004** | 小张     | 男       | 181.3    | 95       | 三班     |
| **005** | 小赵     | 女       | 162.7    | 95       | 一班     |
| **006** | 小周     | 女       | 159.8    | 85       | 二班     |

**添加行，只能用loc。用iloc，会报错，因为给iloc里面的位置索引得是存在的**

```python
# 增加DataFrame的一行值
df1.loc["007"] = ["小孙", "男", 182.7, 71, "一班"]
df1
```

|         | **姓名** | **性别** | **身高** | **成绩** | **班级** |
| ------- | :------- | :------- | :------- | :------- | :------- |
| **001** | 小陈     | 男       | 172.5    | 90       | 一班     |
| **002** | 小李     | 女       | 168.0    | 91       | 三班     |
| **003** | 小王     | 男       | 178.2    | 83       | 二班     |
| **004** | 小张     | 男       | 181.3    | 95       | 三班     |
| **005** | 小赵     | 女       | 162.7    | 95       | 一班     |
| **006** | 小周     | 女       | 159.8    | 85       | 二班     |
| **007** | 小孙     | 男       | 182.7    | 71       | 一班     |

## 九、删除DataFrame的行或列

用drop函数

**drop函数只会返回删除后的DataFrame，但不会改变原始的DataFrame。假如让原本DataFrame的行被删除，可以把drop返回的结果赋值给原本的DataFrame，来实现更新。**

### （一）、删除DataFrame的行

#### 1、删除DataFrame的单行

```python
dataframe.drop('标签索引')
df1.drop('003')
```

|         | **姓名** | **性别** | **身高** | **成绩** | **班级** |
| ------- | :------- | :------- | :------- | :------- | :------- |
| **001** | 小陈     | 男       | 172.5    | 90       | 一班     |
| **002** | 小李     | 女       | 168.0    | 91       | 三班     |
| **004** | 小张     | 男       | 181.3    | 95       | 三班     |
| **005** | 小赵     | 女       | 162.7    | 95       | 一班     |
| **006** | 小周     | 女       | 159.8    | 85       | 二班     |
| **007** | 小孙     | 男       | 182.7    | 71       | 一班     |

#### 2、删除DataFrame的任意多行

```python
dataframe.drop(['标签索引1', '标签索引2'...])
df1.drop(['003', '007'])
```

|         | **姓名** | **性别** | **身高** | **成绩** | **班级** |
| ------- | :------- | :------- | :------- | :------- | :------- |
| **001** | 小陈     | 男       | 172.5    | 90       | 一班     |
| **002** | 小李     | 女       | 168.0    | 91       | 三班     |
| **004** | 小张     | 男       | 181.3    | 95       | 三班     |
| **005** | 小赵     | 女       | 162.7    | 95       | 一班     |
| **006** | 小周     | 女       | 159.8    | 85       | 二班     |

### (二)、删除DataFrame的列

#### 1、删除DataFrame的单列

```python
dataframe.drop('列名', axis=1)
```

传入可选参数'axis=1'

**因为DataFrame是二维的，可以看成是有两个轴线，axis=0表示沿着索引纵向进行操作，axis=1，表示沿着列名横向进行操作。一般针对DataFrame的方法，默认axis为0，因为沿着索引纵向进行操作的时候，数据的变量含义和类型一般是相同的，比如都是表示身高的浮点数，所以适合进行统计操作。drop，默认axis也是0，那沿着索引纵向操作，删除的就是行，当我们指明axis为1时，是沿着列名横向进行操作，所以删除的就是列。正是因为DataFrame二维的特征，针对很多操作我们都可以指明axis到底是0还是1。**

```python
df1.drop('身高', axis=1)
```

|         | **姓名** | **性别** | **成绩** | **班级** |
| ------- | :------- | :------- | :------- | :------- |
| **001** | 小陈     | 男       | 90       | 一班     |
| **002** | 小李     | 女       | 91       | 三班     |
| **003** | 小王     | 男       | 83       | 二班     |
| **004** | 小张     | 男       | 95       | 三班     |
| **005** | 小赵     | 女       | 95       | 一班     |
| **006** | 小周     | 女       | 85       | 二班     |
| **007** | 小孙     | 男       | 71       | 一班     |

#### 2、删除DataFrame的任意多列

```python
dataframe.drop(['列名1', '列名2'...], axis=1)
df1.drop(['身高', '性别'], axis=1)
```

|         | **姓名** | **成绩** | **班级** |
| ------- | :------- | :------- | :------- |
| **001** | 小陈     | 90       | 一班     |
| **002** | 小李     | 91       | 三班     |
| **003** | 小王     | 83       | 二班     |
| **004** | 小张     | 95       | 三班     |
| **005** | 小赵     | 95       | 一班     |
| **006** | 小周     | 85       | 二班     |
| **007** | 小孙     | 71       | 一班     |

### (三)、根据条件删除行或列

1. 可以反向操作，把不符合条件的行或列筛选出来，重新赋值给原始DataFrame

```python
# 删除成绩小于75的同学的档案
df1 = df1[df1.成绩 > 75]
df1
```

|         | **姓名** | **性别** | **身高** | **成绩** | **班级** |
| ------- | :------- | :------- | :------- | :------- | :------- |
| **001** | 小陈     | 男       | 172.5    | 90       | 一班     |
| **002** | 小李     | 女       | 168.0    | 91       | 三班     |
| **003** | 小王     | 男       | 178.2    | 83       | 二班     |
| **004** | 小张     | 男       | 181.3    | 95       | 三班     |
| **005** | 小赵     | 女       | 162.7    | 95       | 一班     |
| **006** | 小周     | 女       | 159.8    | 85       | 二班     |

1. 正向操作。由于drop函数中传入`'标签索引'`作为参数。所以我们先根据条件筛选出要删除的行，再用`.index`获取作为属性的标签索引，最后作为参数放入DataFrame的drop函数中。

```plain
# 删除身高大于180的同学的档案
 df1.drop(df1[df1.身高 > 180].index, inplace=True)
 df1
```

```plain
C:\Users\stube\AppData\Local\Temp\ipykernel_26868\3609063738.py:2: SettingWithCopyWarning: 
 A value is trying to be set on a copy of a slice from a DataFrame
 
 See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
   df1.drop(df1[df1.身高 > 180].index, inplace=True)
```

```plain
.dataframe tbody tr th {
     vertical-align: top;
 }
 
 .dataframe thead th {
     text-align: right;
 }
```

|         | **姓名** | **性别** | **身高** | **成绩** | **班级** |
| ------- | :------- | :------- | :------- | :------- | :------- |
| **001** | 小陈     | 男       | 172.5    | 90       | 一班     |
| **002** | 小李     | 女       | 168.0    | 91       | 三班     |
| **003** | 小王     | 男       | 178.2    | 83       | 二班     |
| **005** | 小赵     | 女       | 162.7    | 95       | 一班     |
| **006** | 小周     | 女       | 159.8    | 85       | 二班     |

## 十、DataFrame和DataFrame之间的运算

与Series与Series之间的运算类似。

DataFrame和DataFrame之间的运算，索引和索引、列名和列名会自动对齐。

### 1、使用符号运算

准备df1数据

```python
df1 = pd.DataFrame({"a": {"001": 0, "003": 4, "005": 8, "007": 12},
                    "b": {"001": 1, "003": 5, "005": 9, "007": 13},
                    "c": {"001": 2, "003": 6, "005": 10, "007": 14},
                    "d": {"001": 3, "003": 7, "005": 11, "007": 15}})
df1
```

|         | **a** | **b** | **c** | **d** |
| ------- | :---- | :---- | :---- | :---- |
| **001** | 0     | 1     | 2     | 3     |
| **003** | 4     | 5     | 6     | 7     |
| **005** | 8     | 9     | 10    | 11    |
| **007** | 12    | 13    | 14    | 15    |

准备df2数据

```python
df2 = pd.DataFrame({"a": [0, 3, 6],
                    "b": [1, 4, 7],
                    "c": [2, 5, 8]})
df2
```

|       | **a** | **b** | **c** |
| ----- | :---- | :---- | :---- |
| **0** | 0     | 1     | 2     |
| **1** | 3     | 4     | 5     |
| **2** | 6     | 7     | 8     |

相加

```python
df1 + df2
```

|         | **a** | **b** | **c** | **d** |
| ------- | :---- | :---- | :---- | :---- |
| **001** | NaN   | NaN   | NaN   | NaN   |
| **003** | NaN   | NaN   | NaN   | NaN   |
| **005** | NaN   | NaN   | NaN   | NaN   |
| **007** | NaN   | NaN   | NaN   | NaN   |
| **0**   | NaN   | NaN   | NaN   | NaN   |
| **1**   | NaN   | NaN   | NaN   | NaN   |
| **2**   | NaN   | NaN   | NaN   | NaN   |

分析：df1与df2是有重合的列名的，a, b, c这三列是可以对齐上，而d列会全是缺失值；但除了列，行也要对齐，df1的标签是001、003、005、007，而df2的标签是0、1、2，没有一个对得上。可以看到，计算结果会保留双方全部索引和列名，所以每一个计算结果都是NaN

**假如一个DataFrame没有标签，另一个DataFrame有标签，两个DataFrame之间运算是会拿标签索引和位置索引对齐，不一样则会保留全部索引**

```python
df2 = pd.DataFrame({"a": {"001": 0, "003": 3, "006": 6},
                    "b": {"001": 1, "003": 4, "006": 7},
                    "c": {"001": 2, "003": 5, "006": 8}})
df2
```

|         | **a** | **b** | **c** |
| ------- | :---- | :---- | :---- |
| **001** | 0     | 1     | 2     |
| **003** | 3     | 4     | 5     |
| **006** | 6     | 7     | 8     |

```python
df1 + df2
```

|         | **a** | **b** | **c** | **d** |
| ------- | :---- | :---- | :---- | :---- |
| **001** | 0.0   | 2.0   | 4.0   | NaN   |
| **003** | 7.0   | 9.0   | 11.0  | NaN   |
| **005** | NaN   | NaN   | NaN   | NaN   |
| **006** | NaN   | NaN   | NaN   | NaN   |
| **007** | NaN   | NaN   | NaN   | NaN   |

分析：有一部分数据对齐成功，不全是缺失值了

### 2、用方法而不是符号运算

如果你希望给缺失的值一个默认值的话，可以给fill\_value这个参数传一个值

```python
df1.add(df2, fill_value=0)
```

|         | **a** | **b** | **c** | **d** |
| ------- | :---- | :---- | :---- | :---- |
| **001** | 0.0   | 2.0   | 4.0   | 3.0   |
| **003** | 7.0   | 9.0   | 11.0  | 7.0   |
| **005** | 8.0   | 9.0   | 10.0  | 11.0  |
| **006** | 6.0   | 7.0   | 8.0   | NaN   |
| **007** | 12.0  | 13.0  | 14.0  | 15.0  |

分析：索引006、列名d的值不存在，是由于这个位置的值，在两个DataFrame中都不存在。因此没有相加操作，也不存在替换。但因为结果会保留所有索引和列名，导致那个位置会有个占位的NaN值。

```python
df1.sub(df2, fill_value=0)
```

|         | **a** | **b** | **c** | **d** |
| ------- | :---- | :---- | :---- | :---- |
| **001** | 0.0   | 0.0   | 0.0   | 3.0   |
| **003** | 1.0   | 1.0   | 1.0   | 7.0   |
| **005** | 8.0   | 9.0   | 10.0  | 11.0  |
| **006** | -6.0  | -7.0  | -8.0  | NaN   |
| **007** | 12.0  | 13.0  | 14.0  | 15.0  |

```python
df1.mul(df2, fill_value=0)
```

|         | **a** | **b** | **c** | **d** |
| ------- | :---- | :---- | :---- | :---- |
| **001** | 0.0   | 1.0   | 4.0   | 0.0   |
| **003** | 12.0  | 20.0  | 30.0  | 0.0   |
| **005** | 0.0   | 0.0   | 0.0   | 0.0   |
| **006** | 0.0   | 0.0   | 0.0   | NaN   |
| **007** | 0.0   | 0.0   | 0.0   | 0.0   |

```python
df1.div(df2, fill_value=0)
```

|         | **a**    | **b** | **c** | **d** |
| ------- | :------- | :---- | :---- | :---- |
| **001** | NaN      | 1.00  | 1.0   | inf   |
| **003** | 1.333333 | 1.25  | 1.2   | inf   |
| **005** | inf      | inf   | inf   | inf   |
| **006** | 0.000000 | 0.00  | 0.0   | NaN   |
| **007** | inf      | inf   | inf   | inf   |

## 十一、DataFrame和Series之间运算

DataFrame和Series之间运算同样会自动对齐，是用Series的索引和DataFrame的列名对齐，然后把操作运用在DataFrame的每一行上，对齐不了的就是NaN。

比如：

对齐的情况

```python
s1 = pd.Series([0.1, 0.2, 0.3, 0.4, 0.5], index=["a", "b", "c", "d", "e"])
s1
```

运行结果

```python
a    0.1
b    0.2
c    0.3
d    0.4
e    0.5
dtype: float64
```

不对齐的情况

```python
s1 * df1
```

运行结果

|         | **a** | **b** | **c** | **d** | **e** |
| ------- | :---- | :---- | :---- | :---- | :---- |
| **001** | 0.0   | 0.2   | 0.6   | 1.2   | NaN   |
| **003** | 0.4   | 1.0   | 1.8   | 2.8   | NaN   |
| **005** | 0.8   | 1.8   | 3.0   | 4.4   | NaN   |
| **007** | 1.2   | 2.6   | 4.2   | 6.0   | NaN   |

这里s1里索引a对应的值，会与df1里a列下面的所有值进行相乘；索引b对应的值，会与df1里b列下面的所有值进行相乘。

广播机制，Series和DataFrame之间的操作也是广播机制的体现，因为广播机制值得就是不同维度数据之间的运算机制。

## 十二、DataFrame和单个数字之间运算

```python
name = pd.Series(["小陈", "小李", "小王", "小张", "小赵", "小周"], index=["001", "002", "003", "004", "005", "006"])
gender = pd.Series(["女", "女", "男", "男", "女", "男"], index=["006", "005", "004", "003", "002", "001"])
height = pd.Series([172.5, 168.0, 178.2, 181.3, 161.7], index=["001", "002", "003", "004", "005"])
students = pd.DataFrame({"姓名": name, "性别": gender, "身高": height})
students
```

|         | **姓名** | **性别** | **身高** |
| ------- | :------- | :------- | :------- |
| **001** | 小陈     | 男       | 172.5    |
| **002** | 小李     | 女       | 168.0    |
| **003** | 小王     | 男       | 178.2    |
| **004** | 小张     | 男       | 181.3    |
| **005** | 小赵     | 女       | 161.7    |
| **006** | 小周     | 女       | NaN      |

### 1、运算运用到DataFrame的每一项数据上

运算会被运用到DataFrame的每一项数据上，所以要确保操作确实能被运用到里面的所有数据类型上。因为DataFrame包含多种数据类型，同一个操作放到不同类型上可能会有不同效果，也可能会报错

```python
students * 5
```

|         | **姓名**             | **性别**   | **身高** |
| ------- | :------------------- | :--------- | :------- |
| **001** | 小陈小陈小陈小陈小陈 | 男男男男男 | 862.5    |
| **002** | 小李小李小李小李小李 | 女女女女女 | 840.0    |
| **003** | 小王小王小王小王小王 | 男男男男男 | 891.0    |
| **004** | 小张小张小张小张小张 | 男男男男男 | 906.5    |
| **005** | 小赵小赵小赵小赵小赵 | 女女女女女 | 808.5    |
| **006** | 小周小周小周小周小周 | 女女女女女 | NaN      |

### 2、先提取出需要的DataFrame数据，再进行操作

也可以先提取出需要的DataFrame数据，用数据类型一致的那一部分，再拿去进行操作

```python
students[['姓名', '性别']] * 5
```

|         | **姓名**             | **性别**   |
| ------- | :------------------- | :--------- |
| **001** | 小陈小陈小陈小陈小陈 | 男男男男男 |
| **002** | 小李小李小李小李小李 | 女女女女女 |
| **003** | 小王小王小王小王小王 | 男男男男男 |
| **004** | 小张小张小张小张小张 | 男男男男男 |
| **005** | 小赵小赵小赵小赵小赵 | 女女女女女 |
| **006** | 小周小周小周小周小周 | 女女女女女 |

## 十三、聚合运算

#### (一)、统计方法

NumPy数组和Pandas的Series的统计方法，包括max, min, sum, mean，在DataFrame中也有相同名字的方法

不同之处在于，由于DataFrame是二维的，我们可以指定是沿着索引纵向操作，还是沿着列名横向操作。

```python
import pandas as pd
player1 = pd.Series([8.5, 7.9, 8.2, 7.6, 8.8, 7.4], index=["001", "002", "003", "004", "005", "006"])
player2 = pd.Series([9.0, 8.3, 8.6, 7.7, 8.9, 7.8], index=["001", "002", "003", "004", "005", "006"])
player3 = pd.Series([8.7, 8.1, 8.4, 7.9, 8.6, 7.3], index=["001", "002", "003", "004", "005", "006"])
df1 = pd.DataFrame({"选手1": player1, "选手2": player2, "选手3": player3})
df1
```

|         | **选手1** | **选手2** | **选手3** |
| ------- | :-------- | :-------- | :-------- |
| **001** | 8.5       | 9.0       | 8.7       |
| **002** | 7.9       | 8.3       | 8.1       |
| **003** | 8.2       | 8.6       | 8.4       |
| **004** | 7.6       | 7.7       | 7.9       |
| **005** | 8.8       | 8.9       | 8.6       |
| **006** | 7.4       | 7.8       | 7.3       |

```python
df1.mean()

选手1    8.066667
选手2    8.383333
选手3    8.166667
dtype: float64
```

默认沿着索引纵向操作，计算各列的统计值，因此返回结果的Series索引和列名对应

```python
df1.mean(axis=1)

001    8.733333
002    8.100000
003    8.400000
004    7.733333
005    8.766667
006    7.500000
dtype: float64
```

如果用可选参数指定axis=1的话，就变成横向操作，计算各行的统计值，因此返回结果的Series索引和DataFrame索引对应。

#### (二)、describe方法

与Series的describe方法类似。用在DataFrame上，能得到各列的统计信息

**如果DataFrame既有数字列也有非数字列，我们不需要吧数字列提取出来再调用这个方法，describe方法会自动忽略掉所有非数字列，只计算数字列的统计信息**

**如果DataFrame数据类型全都都是object，那么describe方法会展现针对object类型数据的信息**

```python
df1["姓名"] = ["小陈", "小李", "小王", "小张", "小赵", "小周"]
df1
```

|         | **选手1** | **选手2** | **选手3** | **姓名** |
| ------- | :-------- | :-------- | :-------- | :------- |
| **001** | 8.5       | 9.0       | 8.7       | 小陈     |
| **002** | 7.9       | 8.3       | 8.1       | 小李     |
| **003** | 8.2       | 8.6       | 8.4       | 小王     |
| **004** | 7.6       | 7.7       | 7.9       | 小张     |
| **005** | 8.8       | 8.9       | 8.6       | 小赵     |
| **006** | 7.4       | 7.8       | 7.3       | 小周     |

```python
df1.describe()
```

|           | **选手1** | **选手2** | **选手3** |
| --------- | :-------- | :-------- | :-------- |
| **count** | 6.000000  | 6.000000  | 6.000000  |
| **mean**  | 8.066667  | 8.383333  | 8.166667  |
| **std**   | 0.535413  | 0.549242  | 0.520256  |
| **min**   | 7.400000  | 7.700000  | 7.300000  |
| **25%**   | 7.675000  | 7.925000  | 7.950000  |
| **50%**   | 8.050000  | 8.450000  | 8.250000  |
| **75%**   | 8.425000  | 8.825000  | 8.550000  |
| **max**   | 8.800000  | 9.000000  | 8.700000  |

## 十四、apply方法

与Series的apply方法类似，DataFrame中也有相同名字的方法。

参数接受一个函数，调用后把DataFrame的每列或每行的Series数据，分别作为那个函数的参数，默认对列进行操作。返回的Series里的元素，就是那个函数对原始Series里各列或各行调用后的结果。

**apply方法不改变原始DataFrame，而是会返回一个新的DataFrame**

```python
# 写出一个函数，去掉选手最高分和最低分后，求平均分
def trim_mean(data):
    data_len = len(data)
    data_sum = data.sum()
    max_num = data.max()
    min_num = data.min()
    return (data_sum - max_num - min_num) / data_len
```

```python
df1.apply(trim_mean)

选手1    5.366667
选手2    5.600000
选手3    5.500000
dtype: float64
```

```python
df1.apply(trim_mean, axis=1)

001    2.900000
002    2.700000
003    2.800000
004    2.566667
005    2.933333
006    2.466667
dtype: float64
```

## 十五、applymap方法

**与apply方法区别在于，传给apply的函数，会运用在每列或每行上；而传给applymap的函数，会运用在每一个元素上**

**applymap方法不改变原始DataFrame，而是会返回一个新的DataFrame**

```python
df1.applymap(lambda x: x + 5)
```

|         | **选手1** | **选手2** | **选手3** |
| ------- | :-------- | :-------- | :-------- |
| **001** | 13.5      | 14.0      | 13.7      |
| **002** | 12.9      | 13.3      | 13.1      |
| **003** | 13.2      | 13.6      | 13.4      |
| **004** | 12.6      | 12.7      | 12.9      |
| **005** | 13.8      | 13.9      | 13.6      |
| **006** | 12.4      | 12.8      | 12.3      |

---

---
url: /Python/数据分析/2_Python数据分析—NumPy.md
---

# Python数据分析—NumPy

## NumPy入门

用Python做数据分析离不开两个最常用的库，NumPy和Pandas。

NumPy的全程叫Numerical Python,  是专门针对计算用的，NumPy是很多数据或科学相关Python包的基础。

## 一、安装与导入

安装命令：`pip install numpy`

导入命令：`import numpy as np`

> "as np" 的意思是，给这个库一个别名叫np，这样使用这个库的任何东西时，都可以用np，而不是numpy来调用。目的是打字更少、更简洁。

## 二、N维数组(ND array)

NumPy里最核心的数据结构叫ND array。

NumPy的数组与Python的内置列表的异同

### 1、相似之处

```python
arr = np.array([5, 17, 3, 26, 31])
```

1）都可以通过索引去获得某个元素

```python
print(arr[0])
```

2）都可以通过切片获得某范围的多个元素

```python
# 打印第1个到第4个之前元素
print(arr[0:3])
```

3）可以去迭代各个元素

```python
for element in arr:
    print(element)
```

### 2、不同之处

NumPy数组里的数据类型需要统一，而列表里的数据类型不需要统一

```python
list = [5, "a", True, 12.2, "!"]
```

### 3、优点

1）在对NumPy数组进行大规模数学运算或其它操作时，执行速度远高于Python内置列表，因此效率是数据处理方面选择NumPy的首要原因。

2）NumPy提供了很多专门做运算的函数，为操作数据提供了很多便利。

## 三、创建数组

### (一)、array方法

1、创建一个数组，最直观的方法就是通过NumPy的array方法，把列表转换成数组。

2、如果传入的是"\[1, 2, 3]"这样一个简单的列表，它会被转换成一维数组。

如果传入的是"\[\[1, 2, 3], \[4, 5, 6]]"这样一个嵌套列表，也就是一个列表里面有另一个列表的情况，它会被转换成二维数组。一个简单的判断方法是，最左边有几个方括号，它就会被转换成几维的数组。

3、这里也可以试一下传入不同类型的元素，虽然不报错，但array方法会强制把它们转换成同一类型。

### (二)、其他方法

1、zeros方法

给zeros方法传入一个数字，会返回一个全部是0的，长度为那个数字的数组

```python
np.zeros(3)
# 会返回
array([0., 0., 0.,])
```

由于数字类型是浮点数，所以每个数字后面有个小数点

2、ones方法

给ones方法传入一个数字，会返回一个全部是1的，长度为那个数字的数组

```python
np.ones(3)
# 会返回
array([1., 1., 1.,])
```

3、arange方法

表示是针对array的range方法，里面传入的参数和range是一样的，第一个表示起始值，第二个表示结束值，第三个表示步长，\*和range方法一样，结束值不会被包括在范围内

```python
np.arange(5, 11, 2)
# 会返回
array([5, 7, 9])
```

## 四、数组的属性

### 1、ndim会返回给我们数组的维度

```python
print(arr.ndim)
```

### 2、shape会返回一个元组，表示各个维度的元素的个数

```python
print(arr.shape)
```

因为arr是一维数组，第一个维度有5个元素，所以会打印"(5,)"

\*如果元组里面只有一个元素，它会用元素后面的逗号，来强调这是由一个元素组成的元组

```python
arr2 = np.array([[1, 2, 3], [4, 5, 6]])
print(arr2.shape)  
```

因为arr2是二维数组，第一个维度有2个元素，第二个维度有3个元素，所以会打印"(2, 3)"的元组

### 3、size会返回数组里面元素的总个数

```python
print(arr.size)
```

### 4、dtype会返回数组元素的类型

dtype表达的是data type的意思

```python
print(arr.dtype)
```

会打印"int32"，int开头说明类型是整数，32表示的是比特长度

## 五、针对NumPy数组的常用操作

### (一)、用concatenate函数连接数组

1. concatenate函数接收的参数是列表，所以可以把两个数组，用中括号包围起来，作为一个列表传进去
2. concatenate函数输出的结果，是两个数组里所有元素拼接起来后组成的新数组

```python
import numpy as np

arr1 = np.array([5, 17, 3, 26, 31])
arr2 = np.zeros(2)
np.concatenate([arr1, arr2])
# 输出
array([ 5., 17.,  3., 26., 31.,  0.,  0.])
```

**可以看到数组里的数字后面都有个小数点，这是因为zeroes方法产生的是浮点数类型的数组，然后数组里数据类型又必须统一，所以拼接后的结果也是浮点数数组**

由于传入列表的长度是不限的，所以也可以一次性拼接多个数组

```python
np.concatenate([arr1, arr2, arr3])
```

### (二)、对内容进行排序

#### 1、针对列表来说

1）sorted函数

传入列表后，就会返回一个新的排序好的列表

**只返回排序好的新列表，不改变原始列表**

**也可以传入Series作为参数**

2）sort方法

调用后，该列表里面的元素就都会被排序好

**什么都不返回，但原始列表会被排序好**

```python
list1 = [5, 17, 3, 26, 31]
sorted_list1 = sorted(list1)
print(sorted_list1)
print(list1)

print(list1.sort())
print(list1)
```

结果

```python
[3, 5, 17, 26, 31]
[5, 17, 3, 26, 31]
None
[3, 5, 17, 26, 31]
```

#### 2、针对NumPy数组来说

1）NumPy的sort函数

np.sort会返回排序好的新的数组，但是传入的原始数组不会被改变

**也可以传入Series作为参数**

2）NumPy的sort方法

而数组的sort方法，会直接在该数组上进行改动，把元素排序好

**你可以根据是否要直接更改数组，决定使用**

```python
arr1 = np.array([5, 17, 3, 26, 31])
print(np.sort(arr1))
print(arr1)

print(arr1.sort())
print(arr1)
```

结果

```python
[ 3  5 17 26 31]
[ 5 17  3 26 31]
None
[ 3  5 17 26 31]
```

### (三)、用索引获得元素

#### 1、获得某个元素

1）正着数，第一个元素的索引为0，后面依次+1

2）倒着数，最后一个元素的索引是-1，倒数第二个的索引是-2，以此类推

#### 2、获得某范围的多个元素

切片会返回开头索引到结束索引前一个的所有元素

```python
arr1[1: 4]

array([ 5, 17, 26])
```

### (四)、数组和数组进行运算

**NumPy数组的强项之一是运算**

以加减乘除为例

#### 1、数组与数组之间

如果把形状相同的两个一维数组进行运算，会返回一个相同位置元素加减乘除后得到的数组。而列表做不到这么便捷了

```python
arr4 = np.array([6, 7, 8, 9, 10])
arr5 = np.array([2, 2, 2, 2, 2])
print(arr4 + arr5)
print(arr4 - arr5)
print(arr4 * arr5)
print(arr4 / arr5)
```

```python
[ 8  9 10 11 12]
[4 5 6 7 8]
[12 14 16 18 20]
[3.  3.5 4.  4.5 5. ]
```

#### 2、数组和单个数字之间

如果学过线性代数，可以看成是向量和标量之间的运算

```python
arr4 * 3

array([18, 21, 24, 27, 30])
```

这个例子里的乘3会运用在数组的每一个数字上，这种操作机制，叫做广播机制

#### 3、聚合运算

a、聚合运算是指，通过一组值，来得到一个值

b、包括max求最大值，min求最小值，sum求和，mean求平均值等待

c、数组.操作名()，返回相应的运算结果

```python
print(arr4.max())
print(arr4.min())
print(arr4.sum())
print(arr4.mean())
```

结果

```python
10
6
40
8.0
```

### (五)、根据条件筛选数组元素

```python
arr = np.array([-22, 3, 65, 9, 11, 7])
arr > 6

array([False, False,  True,  True,  True,  True])
```

#### 1、数组与数字之间的操作

一个数组和一个数字之间的操作，根据广播机制，`> 6`会被运用到每一个数字上，那产生的结果就是由True和False组成的布尔值数组，每个布尔值都代表了这个数字是否大于6

```python
arr[arr > 6]

array([65,  9, 11,  7])
```

布尔值数组可以用来对形状相同的数组进行索引，在方括号里放上这个布尔值数组，相应位置为True的元素就会被筛选出来，那作为结果的数组里就会有那个元素

#### 2、结合逻辑运算，让筛选逻辑更加复杂

与：Python里用and，在数组上用&

或：Python里用or，在数组上用|

非：Python里用not，在数组上用~

```python
arr[(arr > 6) & (arr < 10)]

array([9, 7])
```

比如要筛选出所有大于6且小于10的数字，就把 > 6和 < 10这两个条件用括号括住，表示这两个条件要先于&计算

## 六、其他

np.nan表示缺失值，将np.nan赋值给其他变量，可以使变量为NaN值

```python
np.nan

nan 
```

---

---
url: /Python/数据分析/3_Python数据分析—Pandas.md
---

# Python数据分析—Pandas

## Pandas

**Pandas优势在于：由于它是构建在NumPy之上的，所以继承了NumPy高性能的数组计算功能，同时它还提供了更多复杂精细的数据处理功能。**

## 一、安装与导入Pandas

安装Pandas，在CMD或终端，输入`pip install pandas`

导入Pandas，`**import** pandas **as** pd`

## 二、Series

Series和NumPy的一维数组很相似。

### 1、创建Series

创建Series对象，用Series类，参数传入一个列表。

Series第一个字母S要大写，说明调用的是Series类的构造函数，会返回给我们一个Series类的对象。

```python
s1 = pd.Series([5, 17, 3, 26, 31])
a1 = np.array([5, 17, 3, 26, 31])
print(s1)
print()
print(a1)
```

运行结果

```python
0     5
1    17
2     3
3    26
4    31
dtype: int64

[ 5 17  3 26 31]
```

**Series和NumPy数组的区别在于：不止展示了有什么元素，元素左边还专门展示对应的index，最后一行的dtype还展示了Series里元素的类型。**

### 2、获得Series的元素和索引

如果想单独获得Series的所有元素值，可以用values属性

```python
s1.values

array([ 5, 17,  3, 26, 31], dtype=int64)
```

如果想单独获得Series的所有索引值，可以用index属性

```python
s1.index

RangeIndex(start=0, stop=5, step=1)
```

这里index属性返回的值表示说，index从0开始，到5结束且不包括5，范围内每个index增加1。

### 3、索引和切片操作

对Series同样可以进行索引、切片操作，这个和Python列表、NumPy一维数组都是非常相似。

索引

```python
s1[2]

3
```

切片

```python
s1[1: 3]

1    17
2     3
dtype: int64
```

## 三、Series的特别之处

数组的索引和Python列表一样，都是表示位置的整数，从0开始逐步加1。

Series的索引，默认也是表示位置的整数，但是也可以自己指定。

创建带标签索引的Series对象，后面跟一个index参数，放入标签索引组成的列表。比如：

```python
s1 = pd.Series([5, 17, 3, 26, 31],
              index=["a", "d", "b", "c", "e"])
s1
```

运行结果

```python
a     5
d    17
b     3
c    26
e    31
dtype: int64
```

元素对应的索引变成了a, d, b, c, e

有了标签索引以后，相当于多了一种索引方法。一般单独说索引，指的是标签索引。

### 1、用两种索引取值

这时对Series元素取值的时候，可以用位置索引，也可以用标签索引。

```python
print(s1[2])
print(s1["b"])

3
3
```

### 2、用两种索引切片

可以用位置索引切片，也可以用标签索引切片

```python
s1[1: 3]

d    17
b     3
dtype: int64
```

**用标签索引切片的时候，结束值是包含的**

```python
s1["d": "c"]

d    17
b     3
c    26
dtype: int64
```

### 3、用索引获得任意元素

要得到多个元素组成的Series，可以在方括号里放一个列表，里面包含想选出的元素的索引，这样顺序就不受限制了。

用位置索引获得任意元素

```python
s1[[2, 3, 1]]

d    17
b     3
dtype: int64
```

用标签索引获得任意元素

```python
s1[["b", "c", "d"]]

b     3
c    26
d    17
dtype: int64
```

### 4、loc和iloc

当用整数作为标签索引时，索引取值时按照标签索引，切片时按照位置索引。

```python
s2 = pd.Series([5, 17, 3, 26 ,31], 
              index=[1, 3, 5, 7, 9])

s2[5]
3

s2[1: 3]
 FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.
  s2[1: 3]
3    17
5     3
dtype: int64
```

***这*****种不一致让人困惑，也很容易造成错误，所以Pandas提供了两个更好的索引取值或切片方法，叫loc和iloc**

**但未来版本，切片时按照标签索引。**

loc表示用**标签索引**去取值或切片

```python
s2.iloc[1: 3]

3    17
5     3
dtype: int64
```

iloc表示用**坐标位置索引**去取值或切片

```python
s2.loc[1: 3]

1     5
3    17
dtype: int64
```

### 5、创建自定义索引Series的另一种方式

除了可以通过index这个可选参数指定标签索引，另一种创建自定义索引Series的方法是：给Series这个构造函数直接传入一个字典。

字典的键，就会自动变成值所对应的标签索引。

```python
s3 = pd.Series({"青菜": 4.1, "白萝卜": 2.2, "西红柿":5.3, "土豆":3.7, "黄瓜": 6.8})
s3

青菜     4.1
白萝卜    2.2
西红柿    5.3
土豆     3.7
黄瓜     6.8
dtype: float64
```

### 6、查看标签是否存在

要想知道某个标签是否在Series里，可以用in,它会返回一个布尔值，来告知我们是否存在

```python
"青菜" in s3

True
```

### 7、修改Series里的值

如果你想更改某个标签对应的值，可以像字典那样，方括号里面放上标签去更新。

**如果标签也是整数，最好不要直接放方括号，因为可能一时区分不出来方括号里的整数指的是标签索引还是位置索引，更好的方法还是用loc和iloc。**

**更好的习惯是，不管标签是不是整数，都用loc和iloc。**

```python
s3.loc["青菜"] = 4.5
s3.iloc[0] = 4.5
```

## 四、根据条件筛选Series元素

```python
s3 > 5

青菜     False
白萝卜    False
西红柿     True
土豆     False
黄瓜      True
dtype: bool
```

### 1、与NumPy数组类似

比如: s3 > 5，会返回一个有标签索引和布尔值组成的Series

通过布尔值数组，可以筛选出符合这个条件的元素所组成的Series

```python
s3[s3 > 5]

西红柿    5.3
黄瓜     6.8
dtype: float64
```

### 2、结合逻辑运算，让筛选逻辑更复杂

要增加筛选条件复杂度，也可以利用&, |, ~这些逻辑符号，实现与、或、非运算

```python
s3[(s3 > 5) | (s3 < 3)]

白萝卜    2.2
西红柿    5.3
黄瓜     6.8
dtype: float64
```

## 五、Series的运算

### 1、Series和Series之间

在Series和Series之间,可以做加减乘除等各种运算，Pandas会自动根据索引去排序并对齐。

如果某个索引只在其中一个Series出现的话，结果就会是NaN，表示not a number，说明无法得到计算值。

也就是说，由于Series之间的计算会自动进行索引对齐，只有当某个索引同时出现在两个Series里时，结果里才会有对应的值。

按照什么进行排序? 数字按照大小，英文按照字母顺序，数字英文中文可以混在一起排序对齐，数字在英文前，英文在中文前。中文排序方式或许是ASCII。

```python
import pandas as pd
s1 = pd.Series([1, 4, 2, 3, 5], index=[1, 3, 5, 7, 9])
s2 = pd.Series([8, 1, 7 ,3 ,9], index=[1, 2, 3, 5, 10])
s1 + s2
```

运行结果

```python
1      9.0
2      NaN
3     11.0
5      5.0
7      NaN
9      NaN
10     NaN
dtype: float64
```

中英文数字

```python
s0 = pd.Series({1: 1, "只": 1, "住": 1, "num": 1, 3: 1})
s0 + s1
```

运行结果

```python
1      2.0
3      5.0
5      NaN
7      NaN
9      NaN
num    NaN
住      NaN
只      NaN
dtype: float64
```

### 2、默认值

如果希望给缺失的值一个默认值的话，可以用方法而不是运算符号进行运算，然后给`fill_value`这个参数传入一个值。

用符号的话我们没法额外传参，但用方法的话就可以。

两个Series相加，给个默认值，等同于s1 + s2，并同时给两边缺失的值一个默认值0

```python
s1.add(s2, fill_value=0)
```

运行结果

```python
1      9.0
2      1.0
3     11.0
5      5.0
7      3.0
9      5.0
10     9.0
dtype: float64
```

两个Series相减，给个默认值

```python
s1.sub(s2, fill_value=0)
```

运行结果

```python
1    -7.0
2    -1.0
3    -3.0
5    -1.0
7     3.0
9     5.0
10   -9.0
dtype: float64
```

两个Series相乘，给个默认值

```python
 s1.mul(s2, fill_value=0)
```

运行结果

```python
1      8.0
2      0.0
3     28.0
5      6.0
7      0.0
9      0.0
10     0.0
dtype: float64
```

两个Series相除，给个默认值

```python
 s1.div(s2, fill_value=0)
```

运行结果

```python
1     0.125000
2     0.000000
3     0.571429
5     0.666667
7          inf
9          inf
10    0.000000
dtype: float64
```

### 3、优势

Series之间的操作会根据索引自动对齐的好处是，由于一般我们会利用标签索引表示不同对象的数据,那即使不同Series里数据顺序不一样，计算时也会根据索引自动对齐

### 4、聚合运算

#### 4.1、统计方法

NumPy数组的统计方法，包括max, min, sum, mean，Pandas的Series对象也有相同名字的方法。

```python
print(s1.max())
print(s1.min())
print(s1.sum())
print(s1.mean())
```

运行结果

```python
5
1
15
3.0
```

#### 4.2、describe方法

describe方法，是Series特有的一个强大的方法，describe方法能直接告诉我们很多这个Series的统计信息，包括：元素个数、平均数、标准差、最小值、四分位数、最大值

```python
 s1.describe()
```

运行结果

```python
count    5.000000
mean     3.000000
std      1.581139
min      1.000000
25%      2.000000
50%      3.000000
75%      4.000000
max      5.000000
dtype: float64
```

### 5、Series和单个数字之间

与NumPy数组的广播机制一样，在Pandas Series里，单个数字和Series之间进行操作的时候，操作会被自动运用到Series里每个元素上

```python
 s1 * 3
```

运行结果

```python
1     3
3    12
5     6
7     9
9    15
dtype: int64
```

## 六、对元素分别执行相同操作

### 1、apply方法接收函数

apply方法，接收函数作为参数，然后调用时把Series里各个元素，分别作为那个函数的参数，返回的Series里的元素，就是那个函数对原始Series里各个元素调用后的结果。

**apply方法不改变原始Series，而是会返回一个新的Series**

**apply相当于是高阶函数注意传入的函数后面不要跟括号，因为不是要把函数调用后的结果，去作为apply的参数，而是把函数本身给applyapply的定义语句里肯定包括：**

1. **让每一个Series里面的元素作为参数，调用函数。**
2. **将每一个元素调用函数得到的结果，组成的新Series。**

***

优势：apply方法大大增加了我们操作Series的灵活性，能定义出来的函数，我们都可以作用在Series的各个元素上，帮我们得到新的Series。

应用场景：当前有5名学生的成绩所组成的Series,索引为学生名字，我们希望能得到每个成绩对应的等级：90及以上是A，80到90是B，70到80是C，70以下是D。我们知道怎么根据分数数字得到对应等级，只需要get\_grade\_from\_score函数即可。现在问题在于，如何对Series里每个元素，都运用这个函数，得到对应结果组成的新Series。新方法，apply方法可以实现这一步。

```python
scores = pd.Series({"小明": 92, "小红": 67, "小杰": 70, "小丽": 88, "小华": 76})
def get_grade_from_score(score):
    if score >= 90:
        return "A"
    elif score >= 80:
        return "B"
    elif score >= 70:
        return "C"
    else:
        return "D"


grades = scores.apply(get_grade_from_score)
grades
```

运行结果

```python
小明    A
小红    D
小杰    C
小丽    B
小华    C
dtype: object
```

### 2、apply方法接收匿名函数

除了传入定义好的函数名，在函数逻辑比较简单的时候，匿名函数也可以应用在这里。

```python
half_scores = scores.apply(lambda x: 0.5*x)
half_scores
```

运行结果

```python
小明    46.0
小红    33.5
小杰    35.0
小丽    44.0
小华    38.0
dtype: float64
```

## 七、转换数据类型

astype方法：转换Series的数据类型

```python
scores = scores.astype(str)
scores
```

运行结果

```python
小明    92
小红    67
小杰    70
小丽    88
小华    76
dtype: object
```

## 八、针对字符串Series，保留Series每个元素的某一部分

str.slice方法

str是Series类自带的一个属性，会返回一个包含了很多字符串相关操作方法的,StringMethods类的实例(返回实例才可以调用方法)，对这个StringMethods实例调用slice方法，就会分别保留Series里每个元素选定的部分

第一个参数传入，要保留的起始位置的索引；第二个参数传入，要保留的结束位置的下一索引

```python
 scores.str.slice(0, 1)
```

运行结果

```python
小明    9
小红    6
小杰    7
小丽    8
小华    7
dtype: object
```

---

---
url: /daily/开发文档/QT开发.md
---

# QT开发

## QT学习教程

https://www.bilibili.com/video/BV1GW42197ff

## QT界面框架

### ElaWidgetTools

> 视频介绍：https://www.bilibili.com/video/BV1xrvZe9EJW
>
> 项目地址：https://github.com/Liniyous/ElaWidgetTools

## QT开发项目

### QT开发网易云音乐

> 视频教程：https://www.bilibili.com/video/BV1ZR4y1x7Qd
>
> 文档资料：https://blog.csdn.net/weixin\_42126427/category\_10092752.html

### Github上一些Qt开源项目

[Github上的一些高分Qt开源项目【多图】\_github上好的qt项目-CSDN博客](https://blog.csdn.net/ccf19881030/article/details/113007901)

---

---
url: /Java/微服务专栏/07.Quarkus/Quarkus.md
---

# Quarkus

Quarkus 是为 OpenJDK HotSpot 和 GraalVM 量身定制的 Kubernetes Native Java 框架，基于同类最佳的 Java 库和标准制作而成。Quarkus 的到来为开发 Linux 容器和 kubernetes 原生 Java 微服务带来了一个创新平台。Quarkus的目标是使Java成为Kubernetes和无服务器环境中的领先平台，同时为开发人员提供统一的反应式和命令式编程模型，以优化地满足更广泛的分布式应用程序架构。

针对云、无服务器和容器化环境进行了优化。

---

---
url: /Java/架构设计/分布式/03.分布式消息队列/02.RabbitMQ/1_RabbitMQ安装.md
---

# RabbitMQ安装

## 一、单机部署

### 1.1.Linux原生安装

Linux下安装 erlang（RabbitMQ的安装依赖于erlang所以先安装）

tar安装方式：https://blog.csdn.net/Amber\_1/article/details/124474036

rpm安装方式：https://blog.csdn.net/laterstage/article/details/131513793

Linux安装RabbitMQ

https://blog.csdn.net/laterstage/article/details/131522924

### 1.2.Docker安装

#### 1.2.1.普通镜像安装

##### 拉取镜像

方式一：在线拉取

```sh
docker pull rabbitmq:3.8-management
```

方式二：从本地加载

下载镜像包

上传到虚拟机中后，使用命令加载镜像即可：

```sh
docker load -i mq.tar
```

##### 安装MQ

执行下面的命令来运行MQ容器：

```sh
docker run \
 -e RABBITMQ_DEFAULT_USER=xxl \
 -e RABBITMQ_DEFAULT_PASS=123456 \
 -v mq-plugins:/plugins \
 --name mq \
 --hostname mq1 \
 -p 15672:15672 \
 -p 5672:5672 \
 -d \
 rabbitmq:3.8-management
```

#### 1.2.2.docker-compose安装

https://gitee.com/xu\_xiaolong/docker-compose/blob/master/compose/rabbitmq/run.md

docker-compose-rabbitmq.yml

```yaml
# 环境变量可参考： https://www.rabbitmq.com/configure.html
#               https://github.com/rabbitmq/rabbitmq-server/blob/master/deps/rabbit/docs/rabbitmq.conf.example
version: '3'
services:
  rabbitmq:
    image: rabbitmq:management          # 镜像`rabbitmq:3.9.1-management` 【 注：该版本包含了web控制页面 】
    container_name: rabbitmq            # 容器名为'rabbitmq'
    hostname: my-rabbit
    restart: unless-stopped             # 指定容器退出后的重启策略为始终重启，但是不考虑在Docker守护进程启动时就已经停止了的容器
    environment:                        # 设置环境变量,相当于docker run命令中的-e
      TZ: Asia/Shanghai
      LANG: en_US.UTF-8
    volumes:                            # 数据卷挂载路径设置,将本机目录映射到容器目录
      - "./rabbitmq/config/rabbitmq.conf:/etc/rabbitmq/rabbitmq.conf"
      - "./rabbitmq/config/10-default-guest-user.conf:/etc/rabbitmq/conf.d/10-default-guest-user.conf"
      - "./rabbitmq/data:/var/lib/rabbitmq"
      - "./rabbitmq/plugins/rabbitmq_delayed_message_exchange-3.9.0.ez:/opt/rabbitmq/plugins/rabbitmq_delayed_message_exchange-3.9.0.ez"
#      - "./rabbitmq/log:/var/log/rabbitmq"
    ports:                              # 映射端口
      - "5672:5672"
      - "15672:15672"
```

## 二、安装DelayExchange插件

官方的安装指南地址为：https://blog.rabbitmq.com/posts/2015/04/scheduling-messages-with-rabbitmq

上述文档是基于linux原生安装RabbitMQ，然后安装插件。

下面是基于Docker来安装RabbitMQ插件步骤

### 2.1.下载插件

RabbitMQ有一个官方的插件社区，地址为：https://www.rabbitmq.com/community-plugins.html

其中包含各种各样的插件，包括我们要使用的DelayExchange插件：

![image-20210713104511055](/assets/image-20210713104511055.7-FzeBxF.png)

大家可以去对应的GitHub页面下载3.8.9版本的插件，地址为https://github.com/rabbitmq/rabbitmq-delayed-message-exchange/releases/tag/3.8.9这个对应RabbitMQ的3.8.5以上版本。

### 2.2.上传插件

因为我们是基于Docker安装，所以需要先查看RabbitMQ的插件目录对应的数据卷。如果不是基于Docker的同学，请参考第一章部分，重新创建Docker容器。

我们之前设定的RabbitMQ的数据卷名称为`mq-plugins`，所以我们使用下面命令查看数据卷：

```sh
docker volume inspect mq-plugins
```

可以得到下面结果：

![image-20210713105135701](/assets/image-20210713105135701.agQPbIn6.png)

接下来，将插件上传到这个目录即可：

![image-20210713105339785](/assets/image-20210713105339785.TpzIB3YL.png)

### 2.3.安装插件

最后就是安装了，需要进入MQ容器内部来执行安装。我的容器名为`mq`，所以执行下面命令：

```sh
docker exec -it mq bash
```

执行时，请将其中的 `-it` 后面的`mq`替换为你自己的容器名.

进入容器内部后，执行下面命令开启插件：

```sh
rabbitmq-plugins enable rabbitmq_delayed_message_exchange
```

结果如下：

![image-20210713105829435](/assets/image-20210713105829435.CS44PpKB.png)

## 三、集群部署

### 3.1.集群分类

在RabbitMQ的官方文档中，讲述了两种集群的配置方式：

* 普通模式：普通模式集群不进行数据同步，每个MQ都有自己的队列、数据信息（其它元数据信息如交换机等会同步）。例如我们有2个MQ：mq1，和mq2，如果你的消息在mq1，而你连接到了mq2，那么mq2会去mq1拉取消息，然后返回给你。如果mq1宕机，消息就会丢失。
* 镜像模式：与普通模式不同，队列会在各个mq的镜像节点之间同步，因此你连接到任何一个镜像节点，均可获取到消息。而且如果一个节点宕机，并不会导致数据丢失。不过，这种方式增加了数据同步的带宽消耗。

我们先来看普通模式集群，我们的计划部署3节点的mq集群：

| 主机名 | 控制台端口      | amqp通信端口    |
| ------ | --------------- | --------------- |
| mq1    | 8081 ---> 15672 | 8071 ---> 5672  |
| mq2    | 8082 ---> 15672 | 8072 ---> 5672  |
| mq3    | 8083 ---> 15672 | 8073  ---> 5672 |

集群中的节点标示默认都是：`rabbit@[hostname]`，因此以上三个节点的名称分别为：

* rabbit@mq1
* rabbit@mq2
* rabbit@mq3

### 3.2.获取cookie

RabbitMQ底层依赖于Erlang，而Erlang虚拟机就是一个面向分布式的语言，默认就支持集群模式。集群模式中的每个RabbitMQ 节点使用 cookie 来确定它们是否被允许相互通信。

要使两个节点能够通信，它们必须具有相同的共享秘密，称为**Erlang cookie**。cookie 只是一串最多 255 个字符的字母数字字符。

每个集群节点必须具有**相同的 cookie**。实例之间也需要它来相互通信。

我们先在之前启动的mq容器中获取一个cookie值，作为集群的cookie。执行下面的命令：

```sh
docker exec -it mq cat /var/lib/rabbitmq/.erlang.cookie
```

可以看到cookie值如下：

```sh
FXZMCVGLBIXZCDEMMVZQ
```

接下来，停止并删除当前的mq容器，我们重新搭建集群。

```sh
docker rm -f mq
```

![image-20210717212345165](/assets/image-20210717212345165.MmJHZjOM.png)

### 3.3.准备集群配置

在/tmp目录新建一个配置文件 rabbitmq.conf：

```sh
cd /tmp
# 创建文件
touch rabbitmq.conf
```

文件内容如下：

```nginx
loopback_users.guest = false
listeners.tcp.default = 5672
cluster_formation.peer_discovery_backend = rabbit_peer_discovery_classic_config
cluster_formation.classic_config.nodes.1 = rabbit@mq1
cluster_formation.classic_config.nodes.2 = rabbit@mq2
cluster_formation.classic_config.nodes.3 = rabbit@mq3
```

再创建一个文件，记录cookie

```sh
cd /tmp
# 创建cookie文件
touch .erlang.cookie
# 写入cookie
echo "FXZMCVGLBIXZCDEMMVZQ" > .erlang.cookie
# 修改cookie文件的权限
chmod 600 .erlang.cookie
```

准备三个目录,mq1、mq2、mq3：

```sh
cd /tmp
# 创建目录
mkdir mq1 mq2 mq3
```

然后拷贝rabbitmq.conf、cookie文件到mq1、mq2、mq3：

```sh
# 进入/tmp
cd /tmp
# 拷贝
cp rabbitmq.conf mq1
cp rabbitmq.conf mq2
cp rabbitmq.conf mq3
cp .erlang.cookie mq1
cp .erlang.cookie mq2
cp .erlang.cookie mq3
```

### 3.4.启动集群

创建一个网络：

```sh
docker network create mq-net
```

创建数据卷

```sh
docker volume create 数据卷名称
// 默认存放在宿主机的/var/lib/docker/volumes/数据卷名称/_data
```

运行命令

```sh
docker run -d --net mq-net \
-v ${PWD}/mq1/rabbitmq.conf:/etc/rabbitmq/rabbitmq.conf \
-v ${PWD}/.erlang.cookie:/var/lib/rabbitmq/.erlang.cookie \
-e RABBITMQ_DEFAULT_USER=xxl \
-e RABBITMQ_DEFAULT_PASS=123456 \
--name mq1 \
--hostname mq1 \
-p 8071:5672 \
-p 8081:15672 \
rabbitmq:3.8-management
```

```sh
docker run -d --net mq-net \
-v ${PWD}/mq2/rabbitmq.conf:/etc/rabbitmq/rabbitmq.conf \
-v ${PWD}/.erlang.cookie:/var/lib/rabbitmq/.erlang.cookie \
-e RABBITMQ_DEFAULT_USER=xxl \
-e RABBITMQ_DEFAULT_PASS=123456 \
--name mq2 \
--hostname mq2 \
-p 8072:5672 \
-p 8082:15672 \
rabbitmq:3.8-management
```

```sh
docker run -d --net mq-net \
-v ${PWD}/mq3/rabbitmq.conf:/etc/rabbitmq/rabbitmq.conf \
-v ${PWD}/.erlang.cookie:/var/lib/rabbitmq/.erlang.cookie \
-e RABBITMQ_DEFAULT_USER=xxl \
-e RABBITMQ_DEFAULT_PASS=123456 \
--name mq3 \
--hostname mq3 \
-p 8073:5672 \
-p 8083:15672 \
rabbitmq:3.8-management
```

### 3.5.测试

在mq1这个节点上添加一个队列：

![image-20210717222833196](/assets/image-20210717222833196.BBoGyiBN.png)

如图，在mq2和mq3两个控制台也都能看到：

![image-20210717223057902](/assets/image-20210717223057902.SMSouy5z.png)

#### 3.5.1.数据共享测试

点击这个队列，进入管理页面：

![image-20210717223421750](/assets/image-20210717223421750.Bgipe2nX.png)

然后利用控制台发送一条消息到这个队列：

![image-20210717223320238](/assets/image-20210717223320238.VF8UkYzi.png)

结果在mq2、mq3上都能看到这条消息：

![image-20210717223603628](/assets/image-20210717223603628.BJFT-hm3.png)

#### 3.5.2.可用性测试

我们让其中一台节点mq1宕机：

```sh
docker stop mq1
```

然后登录mq2或mq3的控制台，发现simple.queue也不可用了：

![image-20210717223800203](/assets/image-20210717223800203.BFVRyHib.png)

说明数据并没有拷贝到mq2和mq3。

## 四.镜像模式

一旦创建队列的主机宕机，队列就会不可用。不具备高可用能力。如果要解决这个问题，必须使用官方提供的镜像集群方案。

官方文档地址：https://www.rabbitmq.com/ha.html

### 4.1.镜像模式的特征

默认情况下，队列只保存在创建该队列的节点上。而镜像模式下，创建队列的节点被称为该队列的**主节点**，队列还会拷贝到集群中的其它节点，也叫做该队列的**镜像**节点。

但是，不同队列可以在集群中的任意节点上创建，因此不同队列的主节点可以不同。甚至，**一个队列的主节点可能是另一个队列的镜像节点**。

用户发送给队列的一切请求，例如发送消息、消息回执默认都会在主节点完成，如果是从节点接收到请求，也会路由到主节点去完成。**镜像节点仅仅起到备份数据作用**。

当主节点接收到消费者的ACK时，所有镜像都会删除节点中的数据。

总结如下：

* 镜像队列结构是一主多从（从就是镜像）
* 所有操作都是主节点完成，然后同步给镜像节点
* 主宕机后，镜像节点会替代成新的主（如果在主从同步完成前，主就已经宕机，可能出现数据丢失）
* 不具备负载均衡功能，因为所有操作都会有主节点完成（但是不同队列，其主节点可以不同，可以利用这个提高吞吐量）

### 4.2.镜像模式的配置

镜像模式的配置有3种模式：

| ha-mode         | ha-params         | 效果                                                         |
| :-------------- | :---------------- | :----------------------------------------------------------- |
| 准确模式exactly | 队列的副本量count | 集群中队列副本（主服务器和镜像服务器之和）的数量。count如果为1意味着单个副本：即队列主节点。count值为2表示2个副本：1个队列主和1个队列镜像。换句话说：count = 镜像数量 + 1。如果群集中的节点数少于count，则该队列将镜像到所有节点。如果有集群总数大于count+1，并且包含镜像的节点出现故障，则将在另一个节点上创建一个新的镜像。 |
| all             | (none)            | 队列在群集中的所有节点之间进行镜像。队列将镜像到任何新加入的节点。镜像到所有节点将对所有群集节点施加额外的压力，包括网络I / O，磁盘I / O和磁盘空间使用情况。推荐使用exactly，设置副本数为（N / 2 +1）。 |
| nodes           | *node names*      | 指定队列创建到哪些节点，如果指定的节点全部不存在，则会出现异常。如果指定的节点在集群中存在，但是暂时不可用，会创建节点到当前客户端连接到的节点。 |

这里我们以rabbitmqctl命令作为案例来讲解配置语法。

语法示例：

### 4.2.1.exactly模式

```
rabbitmqctl set_policy ha-two "^two\." '{"ha-mode":"exactly","ha-params":2,"ha-sync-mode":"automatic"}'
```

* `rabbitmqctl set_policy`：固定写法
* `ha-two`：策略名称，自定义
* `"^two\."`：匹配队列的正则表达式，符合命名规则的队列才生效，这里是任何以`two.`开头的队列名称
* `'{"ha-mode":"exactly","ha-params":2,"ha-sync-mode":"automatic"}'`: 策略内容
  * `"ha-mode":"exactly"`：策略模式，此处是exactly模式，指定副本数量
  * `"ha-params":2`：策略参数，这里是2，就是副本数量为2，1主1镜像
  * `"ha-sync-mode":"automatic"`：同步策略，默认是manual，即新加入的镜像节点不会同步旧的消息。如果设置为automatic，则新加入的镜像节点会把主节点中所有消息都同步，会带来额外的网络开销

### 4.2.2.all模式

```
rabbitmqctl set_policy ha-all "^all\." '{"ha-mode":"all"}'
```

* `ha-all`：策略名称，自定义
* `"^all\."`：匹配所有以`all.`开头的队列名
* `'{"ha-mode":"all"}'`：策略内容
  * `"ha-mode":"all"`：策略模式，此处是all模式，即所有节点都会称为镜像节点

### 4.2.3.nodes模式

```
rabbitmqctl set_policy ha-nodes "^nodes\." '{"ha-mode":"nodes","ha-params":["rabbit@nodeA", "rabbit@nodeB"]}'
```

* `rabbitmqctl set_policy`：固定写法
* `ha-nodes`：策略名称，自定义
* `"^nodes\."`：匹配队列的正则表达式，符合命名规则的队列才生效，这里是任何以`nodes.`开头的队列名称
* `'{"ha-mode":"nodes","ha-params":["rabbit@nodeA", "rabbit@nodeB"]}'`: 策略内容
  * `"ha-mode":"nodes"`：策略模式，此处是nodes模式
  * `"ha-params":["rabbit@mq1", "rabbit@mq2"]`：策略参数，这里指定副本所在节点名称

### 4.3.测试

我们使用exactly模式的镜像，因为集群节点数量为3，因此镜像数量就设置为2.

运行下面的命令：

```sh
docker exec -it mq1 rabbitmqctl set_policy ha-two "^two\." '{"ha-mode":"exactly","ha-params":2,"ha-sync-mode":"automatic"}'
```

下面，我们创建一个新的队列：

![image-20210717231751411](/assets/image-20210717231751411.CgPMl2_Y.png)

在任意一个mq控制台查看队列：

![image-20210717231829505](/assets/image-20210717231829505.mTVs_hbX.png)

### 4.3.1.测试数据共享

给two.queue发送一条消息：

![image-20210717231958996](/assets/image-20210717231958996.gEioaJe0.png)

然后在mq1、mq2、mq3的任意控制台查看消息：

![image-20210717232108584](/assets/image-20210717232108584.BlSc-Ar8.png)

### 4.3.2.测试高可用

现在，我们让two.queue的主节点mq1宕机：

```sh
docker stop mq1
```

查看集群状态：

![image-20210717232257420](/assets/image-20210717232257420.D-81p3Ok.png)

查看队列状态：

![image-20210717232322646](/assets/image-20210717232322646.BsIkuDYe.png)

发现依然是健康的！并且其主节点切换到了rabbit@mq2上

## 五.仲裁队列

从RabbitMQ 3.8版本开始，引入了新的仲裁队列，他具备与镜像队里类似的功能，但使用更加方便。

### 5.1.添加仲裁队列

在任意控制台添加一个队列，一定要选择队列类型为Quorum类型。

![image-20210717234329640](/assets/image-20210717234329640.DWKhcqzN.png)

在任意控制台查看队列：

![image-20210717234426209](/assets/image-20210717234426209.BQLHMoBI.png)

可以看到，仲裁队列的 + 2字样。代表这个队列有2个镜像节点。

因为仲裁队列默认的镜像数为5。如果你的集群有7个节点，那么镜像数肯定是5；而我们集群只有3个节点，因此镜像数量就是3.

### 5.2.测试

可以参考对镜像集群的测试，效果是一样的。

### 5.3.集群扩容

#### 5.3.1.加入集群

1）启动一个新的MQ容器：

```sh
docker run -d --net mq-net \
-v ${PWD}/.erlang.cookie:/var/lib/rabbitmq/.erlang.cookie \
-e RABBITMQ_DEFAULT_USER=xxl \
-e RABBITMQ_DEFAULT_PASS=123456 \
--name mq4 \
--hostname mq5 \
-p 8074:15672 \
-p 8084:15672 \
rabbitmq:3.8-management
```

2）进入容器控制台：

```sh
docker exec -it mq4 bash
```

3）停止mq进程

```sh
rabbitmqctl stop_app
```

4）重置RabbitMQ中的数据：

```sh
rabbitmqctl reset
```

5）加入mq1：

```sh
rabbitmqctl join_cluster rabbit@mq1
```

6）再次启动mq进程

```sh
rabbitmqctl start_app
```

![image-20210718001909492](/assets/image-20210718001909492.D5TxaPIB.png)

#### 5.3.2.增加仲裁队列副本

我们先查看下quorum.queue这个队列目前的副本情况，进入mq1容器：

```sh
docker exec -it mq1 bash
```

执行命令：

```sh
rabbitmq-queues quorum_status "quorum.queue"
```

结果：

![image-20210718002118357](/assets/image-20210718002118357.BtX5vURR.png)

现在，我们让mq4也加入进来：

```sh
rabbitmq-queues add_member "quorum.queue" "rabbit@mq4"
```

结果：

![image-20210718002253226](/assets/image-20210718002253226.Dqu-DTPX.png)

再次查看：

```sh
rabbitmq-queues quorum_status "quorum.queue"
```

![image-20210718002342603](/assets/image-20210718002342603.DPIrembQ.png)

查看控制台，发现quorum.queue的镜像数量也从原来的 +2 变成了 +3：

![image-20210718002422365](/assets/image-20210718002422365.fPpA7f5f.png)

---

---
url: /Java/架构设计/分布式/03.分布式消息队列/02.RabbitMQ/3_RabbitMQ高级.md
---

# 服务异步通信-高级篇

消息队列在使用过程中，面临着很多实际问题需要思考：

![image-20210718155003157](/assets/image-20210718155003157.DSClDi8U.png)

# 1.消息可靠性

消息从发送，到消费者接收，会经理多个过程：

![image-20210718155059371](/assets/image-20210718155059371.zZNGlLnv.png)

其中的每一步都可能导致消息丢失，常见的丢失原因包括：

* 发送时丢失：
  * 生产者发送的消息未送达exchange
  * 消息到达exchange后未到达queue
* MQ宕机，queue将消息丢失
* consumer接收到消息后未消费就宕机

针对这些问题，RabbitMQ分别给出了解决方案：

* 生产者确认机制
* mq持久化
* 消费者确认机制
* 失败重试机制

下面我们就通过案例来演示每一个步骤。

首先，导入课前资料提供的demo工程：

![image-20210718155328927](/assets/image-20210718155328927.BjcM96ZO.png)

项目结构如下：

![image-20210718155448734](/assets/image-20210718155448734.BOM4z4qG.png)

## 1.1.生产者消息确认

RabbitMQ提供了publisher confirm机制来避免消息发送到MQ过程中丢失。这种机制必须给每个消息指定一个唯一ID。消息发送到MQ以后，会返回一个结果给发送者，表示消息是否处理成功。

返回结果有两种方式：

* publisher-confirm，发送者确认
  * 消息成功投递到交换机，返回ack
  * 消息未投递到交换机，返回nack
* publisher-return，发送者回执
  * 消息投递到交换机了，但是没有路由到队列。返回ACK，及路由失败原因。

![image-20210718160907166](/assets/image-20210718160907166.DsGGR-vS.png)

注意：

![image-20210718161707992](/assets/image-20210718161707992.BEpNbRQ4.png)

### 1.1.1.修改配置

首先，修改publisher服务中的application.yml文件，添加下面的内容：

```yaml
spring:
  rabbitmq:
    publisher-confirm-type: correlated
    publisher-returns: true
    template:
      mandatory: true
   
```

说明：

* `publish-confirm-type`：开启publisher-confirm，这里支持两种类型：
  * `simple`：同步等待confirm结果，直到超时
  * `correlated`：异步回调，定义ConfirmCallback，MQ返回结果时会回调这个ConfirmCallback
* `publish-returns`：开启publish-return功能，同样是基于callback机制，不过是定义ReturnCallback
* `template.mandatory`：定义消息路由失败时的策略。true，则调用ReturnCallback；false：则直接丢弃消息

### 1.1.2.定义Return回调

每个RabbitTemplate只能配置一个ReturnCallback，因此需要在项目加载时配置：

修改publisher服务，添加一个：

```java
package cn.itcast.mq.config;

import lombok.extern.slf4j.Slf4j;
import org.springframework.amqp.rabbit.core.RabbitTemplate;
import org.springframework.beans.BeansException;
import org.springframework.context.ApplicationContext;
import org.springframework.context.ApplicationContextAware;
import org.springframework.context.annotation.Configuration;

@Slf4j
@Configuration
public class CommonConfig implements ApplicationContextAware {
    @Override
    public void setApplicationContext(ApplicationContext applicationContext) throws BeansException {
        // 获取RabbitTemplate
        RabbitTemplate rabbitTemplate = applicationContext.getBean(RabbitTemplate.class);
        // 设置ReturnCallback
        rabbitTemplate.setReturnCallback((message, replyCode, replyText, exchange, routingKey) -> {
            // 投递失败，记录日志
            log.info("消息发送失败，应答码{}，原因{}，交换机{}，路由键{},消息{}",
                     replyCode, replyText, exchange, routingKey, message.toString());
            // 如果有业务需要，可以重发消息
        });
    }
}
```

### 1.1.3.定义ConfirmCallback

ConfirmCallback可以在发送消息时指定，因为每个业务处理confirm成功或失败的逻辑不一定相同。

在publisher服务的cn.itcast.mq.spring.SpringAmqpTest类中，定义一个单元测试方法：

```java
public void testSendMessage2SimpleQueue() throws InterruptedException {
    // 1.消息体
    String message = "hello, spring amqp!";
    // 2.全局唯一的消息ID，需要封装到CorrelationData中
    CorrelationData correlationData = new CorrelationData(UUID.randomUUID().toString());
    // 3.添加callback
    correlationData.getFuture().addCallback(
        result -> {
            if(result.isAck()){
                // 3.1.ack，消息成功
                log.debug("消息发送成功, ID:{}", correlationData.getId());
            }else{
                // 3.2.nack，消息失败
                log.error("消息发送失败, ID:{}, 原因{}",correlationData.getId(), result.getReason());
            }
        },
        ex -> log.error("消息发送异常, ID:{}, 原因{}",correlationData.getId(),ex.getMessage())
    );
    // 4.发送消息
    rabbitTemplate.convertAndSend("task.direct", "task", message, correlationData);

    // 休眠一会儿，等待ack回执
    Thread.sleep(2000);
}
```

## 1.2.消息持久化

生产者确认可以确保消息投递到RabbitMQ的队列中，但是消息发送到RabbitMQ以后，如果突然宕机，也可能导致消息丢失。

要想确保消息在RabbitMQ中安全保存，必须开启消息持久化机制。

* 交换机持久化
* 队列持久化
* 消息持久化

### 1.2.1.交换机持久化

RabbitMQ中交换机默认是非持久化的，mq重启后就丢失。

SpringAMQP中可以通过代码指定交换机持久化：

```java
@Bean
public DirectExchange simpleExchange(){
    // 三个参数：交换机名称、是否持久化、当没有queue与其绑定时是否自动删除
    return new DirectExchange("simple.direct", true, false);
}
```

事实上，默认情况下，由SpringAMQP声明的交换机都是持久化的。

可以在RabbitMQ控制台看到持久化的交换机都会带上`D`的标示：

![image-20210718164412450](/assets/image-20210718164412450.BAlbCjuF.png)

### 1.2.2.队列持久化

RabbitMQ中队列默认是非持久化的，mq重启后就丢失。

SpringAMQP中可以通过代码指定交换机持久化：

```java
@Bean
public Queue simpleQueue(){
    // 使用QueueBuilder构建队列，durable就是持久化的
    return QueueBuilder.durable("simple.queue").build();
}
```

事实上，默认情况下，由SpringAMQP声明的队列都是持久化的。

可以在RabbitMQ控制台看到持久化的队列都会带上`D`的标示：

![image-20210718164729543](/assets/image-20210718164729543.DOsDIp1C.png)

### 1.2.3.消息持久化

利用SpringAMQP发送消息时，可以设置消息的属性（MessageProperties），指定delivery-mode：

* 1：非持久化
* 2：持久化

用java代码指定：

![image-20210718165100016](/assets/image-20210718165100016.C1sd9hcX.png)

默认情况下，SpringAMQP发出的任何消息都是持久化的，不用特意指定。

## 1.3.消费者消息确认

RabbitMQ是**阅后即焚**机制，RabbitMQ确认消息被消费者消费后会立刻删除。

而RabbitMQ是通过消费者回执来确认消费者是否成功处理消息的：消费者获取消息后，应该向RabbitMQ发送ACK回执，表明自己已经处理消息。

设想这样的场景：

* 1）RabbitMQ投递消息给消费者
* 2）消费者获取消息后，返回ACK给RabbitMQ
* 3）RabbitMQ删除消息
* 4）消费者宕机，消息尚未处理

这样，消息就丢失了。因此消费者返回ACK的时机非常重要。

而SpringAMQP则允许配置三种确认模式：

•manual：手动ack，需要在业务代码结束后，调用api发送ack。

•auto：自动ack，由spring监测listener代码是否出现异常，没有异常则返回ack；抛出异常则返回nack

•none：关闭ack，MQ假定消费者获取消息后会成功处理，因此消息投递后立即被删除

由此可知：

* none模式下，消息投递是不可靠的，可能丢失
* auto模式类似事务机制，出现异常时返回nack，消息回滚到mq；没有异常，返回ack
* manual：自己根据业务情况，判断什么时候该ack

一般，我们都是使用默认的auto即可。

### 1.3.1.演示none模式

修改consumer服务的application.yml文件，添加下面内容：

```yaml
spring:
  rabbitmq:
    listener:
      simple:
        acknowledge-mode: none # 关闭ack
```

修改consumer服务的SpringRabbitListener类中的方法，模拟一个消息处理异常：

```java
@RabbitListener(queues = "simple.queue")
public void listenSimpleQueue(String msg) {
    log.info("消费者接收到simple.queue的消息：【{}】", msg);
    // 模拟异常
    System.out.println(1 / 0);
    log.debug("消息处理完成！");
}
```

测试可以发现，当消息处理抛异常时，消息依然被RabbitMQ删除了。

### 1.3.2.演示auto模式

再次把确认机制修改为auto:

```yaml
spring:
  rabbitmq:
    listener:
      simple:
        acknowledge-mode: auto # 关闭ack
```

在异常位置打断点，再次发送消息，程序卡在断点时，可以发现此时消息状态为unack（未确定状态）：

![image-20210718171705383](/assets/image-20210718171705383.DNGUcNFn.png)

抛出异常后，因为Spring会自动返回nack，所以消息恢复至Ready状态，并且没有被RabbitMQ删除：

![image-20210718171759179](/assets/image-20210718171759179.BUh_ZL1X.png)

## 1.4.消费失败重试机制

当消费者出现异常后，消息会不断requeue（重入队）到队列，再重新发送给消费者，然后再次异常，再次requeue，无限循环，导致mq的消息处理飙升，带来不必要的压力：

![image-20210718172746378](/assets/image-20210718172746378.H5elu-Cu.png)

怎么办呢？

### 1.4.1.本地重试

我们可以利用Spring的retry机制，在消费者出现异常时利用本地重试，而不是无限制的requeue到mq队列。

修改consumer服务的application.yml文件，添加内容：

```yaml
spring:
  rabbitmq:
    listener:
      simple:
        retry:
          enabled: true # 开启消费者失败重试
          initial-interval: 1000 # 初识的失败等待时长为1秒
          multiplier: 1 # 失败的等待时长倍数，下次等待时长 = multiplier * last-interval
          max-attempts: 3 # 最大重试次数
          stateless: true # true无状态；false有状态。如果业务中包含事务，这里改为false
```

重启consumer服务，重复之前的测试。可以发现：

* 在重试3次后，SpringAMQP会抛出异常AmqpRejectAndDontRequeueException，说明本地重试触发了
* 查看RabbitMQ控制台，发现消息被删除了，说明最后SpringAMQP返回的是ack，mq删除消息了

结论：

* 开启本地重试时，消息处理过程中抛出异常，不会requeue到队列，而是在消费者本地重试
* 重试达到最大次数后，Spring会返回ack，消息会被丢弃

### 1.4.2.失败策略

在之前的测试中，达到最大重试次数后，消息会被丢弃，这是由Spring内部机制决定的。

在开启重试模式后，重试次数耗尽，如果消息依然失败，则需要有MessageRecovery接口来处理，它包含三种不同的实现：

* RejectAndDontRequeueRecoverer：重试耗尽后，直接reject，丢弃消息。默认就是这种方式

* ImmediateRequeueMessageRecoverer：重试耗尽后，返回nack，消息重新入队

* RepublishMessageRecoverer：重试耗尽后，将失败消息投递到指定的交换机

比较优雅的一种处理方案是RepublishMessageRecoverer，失败后将消息投递到一个指定的，专门存放异常消息的队列，后续由人工集中处理。

1）在consumer服务中定义处理失败消息的交换机和队列

```java
@Bean
public DirectExchange errorMessageExchange(){
    return new DirectExchange("error.direct");
}
@Bean
public Queue errorQueue(){
    return new Queue("error.queue", true);
}
@Bean
public Binding errorBinding(Queue errorQueue, DirectExchange errorMessageExchange){
    return BindingBuilder.bind(errorQueue).to(errorMessageExchange).with("error");
}
```

2）定义一个RepublishMessageRecoverer，关联队列和交换机

```java
@Bean
public MessageRecoverer republishMessageRecoverer(RabbitTemplate rabbitTemplate){
    return new RepublishMessageRecoverer(rabbitTemplate, "error.direct", "error");
}
```

完整代码：

```java
package cn.itcast.mq.config;

import org.springframework.amqp.core.Binding;
import org.springframework.amqp.core.BindingBuilder;
import org.springframework.amqp.core.DirectExchange;
import org.springframework.amqp.core.Queue;
import org.springframework.amqp.rabbit.core.RabbitTemplate;
import org.springframework.amqp.rabbit.retry.MessageRecoverer;
import org.springframework.amqp.rabbit.retry.RepublishMessageRecoverer;
import org.springframework.context.annotation.Bean;

@Configuration
public class ErrorMessageConfig {
    @Bean
    public DirectExchange errorMessageExchange(){
        return new DirectExchange("error.direct");
    }
    @Bean
    public Queue errorQueue(){
        return new Queue("error.queue", true);
    }
    @Bean
    public Binding errorBinding(Queue errorQueue, DirectExchange errorMessageExchange){
        return BindingBuilder.bind(errorQueue).to(errorMessageExchange).with("error");
    }

    @Bean
    public MessageRecoverer republishMessageRecoverer(RabbitTemplate rabbitTemplate){
        return new RepublishMessageRecoverer(rabbitTemplate, "error.direct", "error");
    }
}
```

## 1.5.总结

如何确保RabbitMQ消息的可靠性？

* 开启生产者确认机制，确保生产者的消息能到达队列
* 开启持久化功能，确保消息未消费前在队列中不会丢失
* 开启消费者确认机制为auto，由spring确认消息处理成功后完成ack
* 开启消费者失败重试机制，并设置MessageRecoverer，多次重试失败后将消息投递到异常交换机，交由人工处理

# 2.死信交换机

## 2.1.初识死信交换机

### 2.1.1.什么是死信交换机

什么是死信？

当一个队列中的消息满足下列情况之一时，可以成为死信（dead letter）：

* 消费者使用basic.reject或 basic.nack声明消费失败，并且消息的requeue参数设置为false
* 消息是一个过期消息，超时无人消费
* 要投递的队列消息满了，无法投递

如果这个包含死信的队列配置了`dead-letter-exchange`属性，指定了一个交换机，那么队列中的死信就会投递到这个交换机中，而这个交换机称为**死信交换机**（Dead Letter Exchange，检查DLX）。

如图，一个消息被消费者拒绝了，变成了死信：

![image-20210718174328383](/assets/image-20210718174328383.C6G8vzJ-.png)

因为simple.queue绑定了死信交换机 dl.direct，因此死信会投递给这个交换机：

![image-20210718174416160](/assets/image-20210718174416160.DN-G5thy.png)

如果这个死信交换机也绑定了一个队列，则消息最终会进入这个存放死信的队列：

![image-20210718174506856](/assets/image-20210718174506856.CmXNSEgC.png)

另外，队列将死信投递给死信交换机时，必须知道两个信息：

* 死信交换机名称
* 死信交换机与死信队列绑定的RoutingKey

这样才能确保投递的消息能到达死信交换机，并且正确的路由到死信队列。

![image-20210821073801398](/assets/image-20210821073801398.BXebnm_u.png)

### 2.1.2.利用死信交换机接收死信（拓展）

在失败重试策略中，默认的RejectAndDontRequeueRecoverer会在本地重试次数耗尽后，发送reject给RabbitMQ，消息变成死信，被丢弃。

我们可以给simple.queue添加一个死信交换机，给死信交换机绑定一个队列。这样消息变成死信后也不会丢弃，而是最终投递到死信交换机，路由到与死信交换机绑定的队列。

![image-20210718174506856](/assets/image-20210718174506856.CmXNSEgC.png)

我们在consumer服务中，定义一组死信交换机、死信队列：

```java
// 声明普通的 simple.queue队列，并且为其指定死信交换机：dl.direct
@Bean
public Queue simpleQueue2(){
    return QueueBuilder.durable("simple.queue") // 指定队列名称，并持久化
        .deadLetterExchange("dl.direct") // 指定死信交换机
        .build();
}
// 声明死信交换机 dl.direct
@Bean
public DirectExchange dlExchange(){
    return new DirectExchange("dl.direct", true, false);
}
// 声明存储死信的队列 dl.queue
@Bean
public Queue dlQueue(){
    return new Queue("dl.queue", true);
}
// 将死信队列 与 死信交换机绑定
@Bean
public Binding dlBinding(){
    return BindingBuilder.bind(dlQueue()).to(dlExchange()).with("simple");
}
```

### 2.1.3.总结

什么样的消息会成为死信？

* 消息被消费者reject或者返回nack
* 消息超时未消费
* 队列满了

死信交换机的使用场景是什么？

* 如果队列绑定了死信交换机，死信会投递到死信交换机；
* 可以利用死信交换机收集所有消费者处理失败的消息（死信），交由人工处理，进一步提高消息队列的可靠性。

## 2.2.TTL

一个队列中的消息如果超时未消费，则会变为死信，超时分为两种情况：

* 消息所在的队列设置了超时时间
* 消息本身设置了超时时间

![image-20210718182643311](/assets/image-20210718182643311.BpWTxnlq.png)

### 2.2.1.接收超时死信的死信交换机

在consumer服务的SpringRabbitListener中，定义一个新的消费者，并且声明 死信交换机、死信队列：

```java
@RabbitListener(bindings = @QueueBinding(
    value = @Queue(name = "dl.ttl.queue", durable = "true"),
    exchange = @Exchange(name = "dl.ttl.direct"),
    key = "ttl"
))
public void listenDlQueue(String msg){
    log.info("接收到 dl.ttl.queue的延迟消息：{}", msg);
}
```

### 2.2.2.声明一个队列，并且指定TTL

要给队列设置超时时间，需要在声明队列时配置x-message-ttl属性：

```java
@Bean
public Queue ttlQueue(){
    return QueueBuilder.durable("ttl.queue") // 指定队列名称，并持久化
        .ttl(10000) // 设置队列的超时时间，10秒
        .deadLetterExchange("dl.ttl.direct") // 指定死信交换机
        .build();
}
```

注意，这个队列设定了死信交换机为`dl.ttl.direct`

声明交换机，将ttl与交换机绑定：

```java
@Bean
public DirectExchange ttlExchange(){
    return new DirectExchange("ttl.direct");
}
@Bean
public Binding ttlBinding(){
    return BindingBuilder.bind(ttlQueue()).to(ttlExchange()).with("ttl");
}
```

发送消息，但是不要指定TTL：

```java
@Test
public void testTTLQueue() {
    // 创建消息
    String message = "hello, ttl queue";
    // 消息ID，需要封装到CorrelationData中
    CorrelationData correlationData = new CorrelationData(UUID.randomUUID().toString());
    // 发送消息
    rabbitTemplate.convertAndSend("ttl.direct", "ttl", message, correlationData);
    // 记录日志
    log.debug("发送消息成功");
}
```

发送消息的日志：

![image-20210718191657478](/assets/image-20210718191657478.qthz3PhS.png)

查看下接收消息的日志：

![image-20210718191738706](/assets/image-20210718191738706.CPn0VI8Y.png)

因为队列的TTL值是10000ms，也就是10秒。可以看到消息发送与接收之间的时差刚好是10秒。

### 2.2.3.发送消息时，设定TTL

在发送消息时，也可以指定TTL：

```java
@Test
public void testTTLMsg() {
    // 创建消息
    Message message = MessageBuilder
        .withBody("hello, ttl message".getBytes(StandardCharsets.UTF_8))
        .setExpiration("5000")
        .build();
    // 消息ID，需要封装到CorrelationData中
    CorrelationData correlationData = new CorrelationData(UUID.randomUUID().toString());
    // 发送消息
    rabbitTemplate.convertAndSend("ttl.direct", "ttl", message, correlationData);
    log.debug("发送消息成功");
}
```

查看发送消息日志：

![image-20210718191939140](/assets/image-20210718191939140.D_KNaixn.png)

接收消息日志：

![image-20210718192004662](/assets/image-20210718192004662.CPib7exQ.png)

这次，发送与接收的延迟只有5秒。说明当队列、消息都设置了TTL时，任意一个到期就会成为死信。

### 2.2.4.总结

消息超时的两种方式是？

* 给队列设置ttl属性，进入队列后超过ttl时间的消息变为死信
* 给消息设置ttl属性，队列接收到消息超过ttl时间后变为死信

如何实现发送一个消息20秒后消费者才收到消息？

* 给消息的目标队列指定死信交换机
* 将消费者监听的队列绑定到死信交换机
* 发送消息时给消息设置超时时间为20秒

## 2.3.延迟队列

利用TTL结合死信交换机，我们实现了消息发出后，消费者延迟收到消息的效果。这种消息模式就称为延迟队列（Delay Queue）模式。

延迟队列的使用场景包括：

* 延迟发送短信
* 用户下单，如果用户在15 分钟内未支付，则自动取消
* 预约工作会议，20分钟后自动通知所有参会人员

因为延迟队列的需求非常多，所以RabbitMQ的官方也推出了一个插件，原生支持延迟队列效果。

这个插件就是DelayExchange插件。参考RabbitMQ的插件列表页面：https://www.rabbitmq.com/community-plugins.html

![image-20210718192529342](/assets/image-20210718192529342.CMpLnQfW.png)

使用方式可以参考官网地址：https://blog.rabbitmq.com/posts/2015/04/scheduling-messages-with-rabbitmq

### 2.3.1.安装DelayExchange插件

参考课前资料：

![image-20210718193409812](/assets/image-20210718193409812.D9qvS4MZ.png)

### 2.3.2.DelayExchange原理

DelayExchange需要将一个交换机声明为delayed类型。当我们发送消息到delayExchange时，流程如下：

* 接收消息
* 判断消息是否具备x-delay属性
* 如果有x-delay属性，说明是延迟消息，持久化到硬盘，读取x-delay值，作为延迟时间
* 返回routing not found结果给消息发送者
* x-delay时间到期后，重新投递消息到指定队列

### 2.3.3.使用DelayExchange

插件的使用也非常简单：声明一个交换机，交换机的类型可以是任意类型，只需要设定delayed属性为true即可，然后声明队列与其绑定即可。

#### 1）声明DelayExchange交换机

基于注解方式（推荐）：

![image-20210718193747649](/assets/image-20210718193747649.B8_rIFlW.png)

也可以基于@Bean的方式：

![image-20210718193831076](/assets/image-20210718193831076.AcCRAhmw.png)

#### 2）发送消息

发送消息时，一定要携带x-delay属性，指定延迟的时间：

![image-20210718193917009](/assets/image-20210718193917009.ZR7aWpCz.png)

### 2.3.4.总结

延迟队列插件的使用步骤包括哪些？

•声明一个交换机，添加delayed属性为true

•发送消息时，添加x-delay头，值为超时时间

# 3.惰性队列

## 3.1.消息堆积问题

当生产者发送消息的速度超过了消费者处理消息的速度，就会导致队列中的消息堆积，直到队列存储消息达到上限。之后发送的消息就会成为死信，可能会被丢弃，这就是消息堆积问题。

![image-20210718194040498](/assets/image-20210718194040498.CRwsc403.png)

解决消息堆积有两种思路：

* 增加更多消费者，提高消费速度。也就是我们之前说的work queue模式
* 扩大队列容积，提高堆积上限

要提升队列容积，把消息保存在内存中显然是不行的。

## 3.2.惰性队列

从RabbitMQ的3.6.0版本开始，就增加了Lazy Queues的概念，也就是惰性队列。惰性队列的特征如下：

* 接收到消息后直接存入磁盘而非内存
* 消费者要消费消息时才会从磁盘中读取并加载到内存
* 支持数百万条的消息存储

### 3.2.1.基于命令行设置lazy-queue

而要设置一个队列为惰性队列，只需要在声明队列时，指定x-queue-mode属性为lazy即可。可以通过命令行将一个运行中的队列修改为惰性队列：

```sh
rabbitmqctl set_policy Lazy "^lazy-queue$" '{"queue-mode":"lazy"}' --apply-to queues  
```

命令解读：

* `rabbitmqctl` ：RabbitMQ的命令行工具
* `set_policy` ：添加一个策略
* `Lazy` ：策略名称，可以自定义
* `"^lazy-queue$"` ：用正则表达式匹配队列的名字
* `'{"queue-mode":"lazy"}'` ：设置队列模式为lazy模式
* `--apply-to queues  `：策略的作用对象，是所有的队列

### 3.2.2.基于@Bean声明lazy-queue

![image-20210718194522223](/assets/image-20210718194522223.sksSQj08.png)

### 3.2.3.基于@RabbitListener声明LazyQueue

![image-20210718194539054](/assets/image-20210718194539054.m4rod0rg.png)

### 3.3.总结

消息堆积问题的解决方案？

* 队列上绑定多个消费者，提高消费速度
* 使用惰性队列，可以再mq中保存更多消息

惰性队列的优点有哪些？

* 基于磁盘存储，消息上限高
* 没有间歇性的page-out，性能比较稳定

惰性队列的缺点有哪些？

* 基于磁盘存储，消息时效性会降低
* 性能受限于磁盘的IO

# 4.MQ集群

## 4.1.集群分类

RabbitMQ的是基于Erlang语言编写，而Erlang又是一个面向并发的语言，天然支持集群模式。RabbitMQ的集群有两种模式：

•**普通集群**：是一种分布式集群，将队列分散到集群的各个节点，从而提高整个集群的并发能力。

•**镜像集群**：是一种主从集群，普通集群的基础上，添加了主从备份功能，提高集群的数据可用性。

镜像集群虽然支持主从，但主从同步并不是强一致的，某些情况下可能有数据丢失的风险。因此在RabbitMQ的3.8版本以后，推出了新的功能：**仲裁队列**来代替镜像集群，底层采用Raft协议确保主从的数据一致性。

## 4.2.普通集群

### 4.2.1.集群结构和特征

普通集群，或者叫标准集群（classic cluster），具备下列特征：

* 会在集群的各个节点间共享部分数据，包括：交换机、队列元信息。不包含队列中的消息。
* 当访问集群某节点时，如果队列不在该节点，会从数据所在节点传递到当前节点并返回
* 队列所在节点宕机，队列中的消息就会丢失

结构如图：

![image-20210718220843323](/assets/image-20210718220843323.D8oEwhvg.png)

### 4.2.2.部署

参考课前资料：《RabbitMQ部署指南.md》

## 4.3.镜像集群

### 4.3.1.集群结构和特征

镜像集群：本质是主从模式，具备下面的特征：

* 交换机、队列、队列中的消息会在各个mq的镜像节点之间同步备份。
* 创建队列的节点被称为该队列的**主节点，**备份到的其它节点叫做该队列的**镜像**节点。
* 一个队列的主节点可能是另一个队列的镜像节点
* 所有操作都是主节点完成，然后同步给镜像节点
* 主宕机后，镜像节点会替代成新的主

结构如图：

![image-20210718221039542](/assets/image-20210718221039542.DU2MfKd8.png)

### 4.3.2.部署

参考课前资料：《RabbitMQ部署指南.md》

## 4.4.仲裁队列

### 4.4.1.集群特征

仲裁队列：仲裁队列是3.8版本以后才有的新功能，用来替代镜像队列，具备下列特征：

* 与镜像队列一样，都是主从模式，支持主从数据同步
* 使用非常简单，没有复杂的配置
* 主从同步基于Raft协议，强一致

### 4.4.2.部署

参考课前资料：《RabbitMQ部署指南.md》

### 4.4.3.Java代码创建仲裁队列

```java
@Bean
public Queue quorumQueue() {
    return QueueBuilder
        .durable("quorum.queue") // 持久化
        .quorum() // 仲裁队列
        .build();
}
```

### 4.4.4.SpringAMQP连接MQ集群

注意，这里用address来代替host、port方式

```java
spring:
  rabbitmq:
    addresses: 192.168.150.105:8071, 192.168.150.105:8072, 192.168.150.105:8073
    username: itcast
    password: 123321
    virtual-host: /
```

---

---
url: /Java/架构设计/分布式/03.分布式消息队列/02.RabbitMQ/5_RabbitMQ日志追踪.md
---

# RabbitMQ日志追踪

Trace 是Rabbitmq用于记录每一次发送的消息，方便使用Rabbitmq的开发者调试、排错。

## 1、启动Tracing插件

```sh
## 进入rabbitMq中
docker exec -it rabbitmq1 bash
## 启动日志插件
rabbitmq-plugins enable rabbitmq_tracing
## 开启rabbitmq的tracing插件
rabbitmqctl trace_on
```

开启了插件后，无需重启，rabbitMq管理界面就会出现Tracing项，可新建追踪。

## 2、新建trace

新建trace时，JSON模式的数据会被Base64加密，不好观察，所以选择Text模式，同时可在Pattern中配置过滤条件

![image-20241113103421842](/assets/image-20241113103421842.CbCcDVhO.png)

## 其他命令

```sh
# 查看打开的插件 
rabbitmq-plugins list
# 关闭trace功能
rabbitmqctl trace_off
# 停止tracing
rabbitmq-plugins disable rabbitmq_tracing
```

## trace消息跟踪显示乱码（谷歌如何修改字符编码）

从github上下载最新的crx：https://[github](https://so.csdn.net/so/search?q=github\&spm=1001.2101.3001.7020).com/jinliming2/Chrome-Charset/releases

选择谷歌设置–扩展程序，将crx拖入谷歌浏览器，确认即可。

选择插件修改编码格式为utf-8,问题解决。

## 参考资料

https://blog.51cto.com/u\_15002821/8984730

https://blog.csdn.net/theconqueror/article/details/109177781

https://blog.csdn.net/m0\_59281987/article/details/131876259

---

---
url: /Java/架构设计/分布式/03.分布式消息队列/02.RabbitMQ/2_RabbitMQ.md
---

# RabbitMQ

# 1.初识MQ

## 1.1.同步和异步通讯

微服务间通讯有同步和异步两种方式：

同步通讯：就像打电话，需要实时响应。

异步通讯：就像发邮件，不需要马上回复。

![image-20210717161939695](/assets/image-20210717161939695.BtDYoG8J.png)

两种方式各有优劣，打电话可以立即得到响应，但是你却不能跟多个人同时通话。发送邮件可以同时与多个人收发邮件，但是往往响应会有延迟。

### 1.1.1.同步通讯

我们之前学习的Feign调用就属于同步方式，虽然调用可以实时得到结果，但存在下面的问题：

![image-20210717162004285](/assets/image-20210717162004285.GV9Zixxn.png)

总结：

同步调用的优点：

* 时效性较强，可以立即得到结果

同步调用的问题：

* 耦合度高
* 性能和吞吐能力下降
* 有额外的资源消耗
* 有级联失败问题

### 1.1.2.异步通讯

异步调用则可以避免上述问题：

我们以购买商品为例，用户支付后需要调用订单服务完成订单状态修改，调用物流服务，从仓库分配响应的库存并准备发货。

在事件模式中，支付服务是事件发布者（publisher），在支付完成后只需要发布一个支付成功的事件（event），事件中带上订单id。

订单服务和物流服务是事件订阅者（Consumer），订阅支付成功的事件，监听到事件后完成自己业务即可。

为了解除事件发布者与订阅者之间的耦合，两者并不是直接通信，而是有一个中间人（Broker）。发布者发布事件到Broker，不关心谁来订阅事件。订阅者从Broker订阅事件，不关心谁发来的消息。

![image-20210422095356088](/assets/image-20210422095356088.Du3R1hNY.png)

Broker 是一个像数据总线一样的东西，所有的服务要接收数据和发送数据都发到这个总线上，这个总线就像协议一样，让服务间的通讯变得标准和可控。

好处：

* 吞吐量提升：无需等待订阅者处理完成，响应更快速

* 故障隔离：服务没有直接调用，不存在级联失败问题

* 调用间没有阻塞，不会造成无效的资源占用

* 耦合度极低，每个服务都可以灵活插拔，可替换

* 流量削峰：不管发布事件的流量波动多大，都由Broker接收，订阅者可以按照自己的速度去处理事件

缺点：

* 架构复杂了，业务没有明显的流程线，不好管理
* 需要依赖于Broker的可靠、安全、性能

好在现在开源软件或云平台上 Broker 的软件是非常成熟的，比较常见的一种就是我们今天要学习的MQ技术。

## 1.2.技术对比：

MQ，中文是消息队列（MessageQueue），字面来看就是存放消息的队列。也就是事件驱动架构中的Broker。

比较常见的MQ实现：

* ActiveMQ
* RabbitMQ
* RocketMQ
* Kafka

几种常见MQ的对比：

|            | **RabbitMQ**            | **ActiveMQ**                   | **RocketMQ** | **Kafka**  |
| ---------- | ----------------------- | ------------------------------ | ------------ | ---------- |
| 公司/社区  | Rabbit                  | Apache                         | 阿里         | Apache     |
| 开发语言   | Erlang                  | Java                           | Java         | Scala\&Java |
| 协议支持   | AMQP，XMPP，SMTP，STOMP | OpenWire,STOMP，REST,XMPP,AMQP | 自定义协议   | 自定义协议 |
| 可用性     | 高                      | 一般                           | 高           | 高         |
| 单机吞吐量 | 一般                    | 差                             | 高           | 非常高     |
| 消息延迟   | 微秒级                  | 毫秒级                         | 毫秒级       | 毫秒以内   |
| 消息可靠性 | 高                      | 一般                           | 高           | 一般       |

追求可用性：Kafka、 RocketMQ 、RabbitMQ

追求可靠性：RabbitMQ、RocketMQ

追求吞吐能力：RocketMQ、Kafka

追求消息低延迟：RabbitMQ、Kafka

# 2.快速入门

## 2.1.安装RabbitMQ

安装RabbitMQ，参考课前资料：

![image-20210717162628635](/assets/image-20210717162628635.JSit75fR.png)

MQ的基本结构：

![image-20210717162752376](/assets/image-20210717162752376.Dh0uwsJW.png)

RabbitMQ中的一些角色：

* publisher：生产者
* consumer：消费者
* exchange个：交换机，负责消息路由
* queue：队列，存储消息
* virtualHost：虚拟主机，隔离不同租户的exchange、queue、消息的隔离

## 2.2.RabbitMQ消息模型

RabbitMQ官方提供了5个不同的Demo示例，对应了不同的消息模型：

![image-20210717163332646](/assets/image-20210717163332646.Cy8Sw06r.png)

## 2.3.导入Demo工程

课前资料提供了一个Demo工程，mq-demo:

![image-20210717163253264](/assets/image-20210717163253264.En9Wp1_z.png)

导入后可以看到结构如下：

![image-20210717163604330](/assets/image-20210717163604330.Cm7Ptbrj.png)

包括三部分：

* mq-demo：父工程，管理项目依赖
* publisher：消息的发送者
* consumer：消息的消费者

## 2.4.入门案例

简单队列模式的模型图：

![image-20210717163434647](/assets/image-20210717163434647.DCqwff_z.png)

官方的HelloWorld是基于最基础的消息队列模型来实现的，只包括三个角色：

* publisher：消息发布者，将消息发送到队列queue
* queue：消息队列，负责接受并缓存消息
* consumer：订阅队列，处理队列中的消息

### 2.4.1.publisher实现

思路：

* 建立连接
* 创建Channel
* 声明队列
* 发送消息
* 关闭连接和channel

代码实现：

```java
package cn.xxl.mq.helloworld;

import com.rabbitmq.client.Channel;
import com.rabbitmq.client.Connection;
import com.rabbitmq.client.ConnectionFactory;
import org.junit.Test;

import java.io.IOException;
import java.util.concurrent.TimeoutException;

public class PublisherTest {
    @Test
    public void testSendMessage() throws IOException, TimeoutException {
        // 1.建立连接
        ConnectionFactory factory = new ConnectionFactory();
        // 1.1.设置连接参数，分别是：主机名、端口号、vhost、用户名、密码
        factory.setHost("192.168.150.101");
        factory.setPort(5672);
        factory.setVirtualHost("/");
        factory.setUsername("xxl");
        factory.setPassword("123456");
        // 1.2.建立连接
        Connection connection = factory.newConnection();

        // 2.创建通道Channel
        Channel channel = connection.createChannel();

        // 3.创建队列
        String queueName = "simple.queue";
        channel.queueDeclare(queueName, false, false, false, null);

        // 4.发送消息
        String message = "hello, rabbitmq!";
        channel.basicPublish("", queueName, null, message.getBytes());
        System.out.println("发送消息成功：【" + message + "】");

        // 5.关闭通道和连接
        channel.close();
        connection.close();

    }
}
```

### 2.4.2.consumer实现

代码思路：

* 建立连接
* 创建Channel
* 声明队列
* 订阅消息

代码实现：

```java
package cn.xxl.mq.helloworld;

import com.rabbitmq.client.*;

import java.io.IOException;
import java.util.concurrent.TimeoutException;

public class ConsumerTest {

    public static void main(String[] args) throws IOException, TimeoutException {
        // 1.建立连接
        ConnectionFactory factory = new ConnectionFactory();
        // 1.1.设置连接参数，分别是：主机名、端口号、vhost、用户名、密码
        factory.setHost("192.168.150.101");
        factory.setPort(5672);
        factory.setVirtualHost("/");
        factory.setUsername("xxl");
        factory.setPassword("123456");
        // 1.2.建立连接
        Connection connection = factory.newConnection();

        // 2.创建通道Channel
        Channel channel = connection.createChannel();

        // 3.创建队列
        String queueName = "simple.queue";
        channel.queueDeclare(queueName, false, false, false, null);

        // 4.订阅消息
        channel.basicConsume(queueName, true, new DefaultConsumer(channel){
            @Override
            public void handleDelivery(String consumerTag, Envelope envelope,
                                       AMQP.BasicProperties properties, byte[] body) throws IOException {
                // 5.处理消息
                String message = new String(body);
                System.out.println("接收到消息：【" + message + "】");
            }
        });
        System.out.println("等待接收消息。。。。");
    }
}
```

## 2.5.总结

基本消息队列的消息发送流程：

1. 建立connection

2. 创建channel

3. 利用channel声明队列

4. 利用channel向队列发送消息

基本消息队列的消息接收流程：

1. 建立connection

2. 创建channel

3. 利用channel声明队列

4. 定义consumer的消费行为handleDelivery()

5. 利用channel将消费者与队列绑定

# 3.SpringAMQP

SpringAMQP是基于RabbitMQ封装的一套模板，并且还利用SpringBoot对其实现了自动装配，使用起来非常方便。

SpringAmqp的官方地址：https://spring.io/projects/spring-amqp

![image-20210717164024967](/assets/image-20210717164024967.7iIr3nBq.png)

![image-20210717164038678](/assets/image-20210717164038678.DKUwak98.png)

SpringAMQP提供了三个功能：

* 自动声明队列、交换机及其绑定关系
* 基于注解的监听器模式，异步接收消息
* 封装了RabbitTemplate工具，用于发送消息

## 3.1.Basic Queue 简单队列模型

在父工程mq-demo中引入依赖

```xml
<!--AMQP依赖，包含RabbitMQ-->
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-amqp</artifactId>
</dependency>
```

### 3.1.1.消息发送

首先配置MQ地址，在publisher服务的application.yml中添加配置：

```yaml
spring:
  rabbitmq:
    host: 192.168.150.101 # 主机名
    port: 5672 # 端口
    virtual-host: / # 虚拟主机
    username: xxl # 用户名
    password: 123456 # 密码
```

然后在publisher服务中编写测试类SpringAmqpTest，并利用RabbitTemplate实现消息发送：

```java
package cn.xxl.mq.spring;

import org.junit.Test;
import org.junit.runner.RunWith;
import org.springframework.amqp.rabbit.core.RabbitTemplate;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.test.context.SpringBootTest;
import org.springframework.test.context.junit4.SpringRunner;

@RunWith(SpringRunner.class)
@SpringBootTest
public class SpringAmqpTest {

    @Autowired
    private RabbitTemplate rabbitTemplate;

    @Test
    public void testSimpleQueue() {
        // 队列名称
        String queueName = "simple.queue";
        // 消息
        String message = "hello, spring amqp!";
        // 发送消息
        rabbitTemplate.convertAndSend(queueName, message);
    }
}
```

### 3.1.2.消息接收

首先配置MQ地址，在consumer服务的application.yml中添加配置：

```yaml
spring:
  rabbitmq:
    host: 192.168.150.101 # 主机名
    port: 5672 # 端口
    virtual-host: / # 虚拟主机
    username: xxl # 用户名
    password: 123456 # 密码
```

然后在consumer服务的`cn.xxl.mq.listener`包中新建一个类SpringRabbitListener，代码如下：

```java
package cn.xxl.mq.listener;

import org.springframework.amqp.rabbit.annotation.RabbitListener;
import org.springframework.stereotype.Component;

@Component
public class SpringRabbitListener {

    @RabbitListener(queues = "simple.queue")
    public void listenSimpleQueueMessage(String msg) throws InterruptedException {
        System.out.println("spring 消费者接收到消息：【" + msg + "】");
    }
}
```

### 3.1.3.测试

启动consumer服务，然后在publisher服务中运行测试代码，发送MQ消息

## 3.2.WorkQueue

Work queues，也被称为（Task queues），任务模型。简单来说就是**让多个消费者绑定到一个队列，共同消费队列中的消息**。

![image-20210717164238910](/assets/image-20210717164238910.DDuk0-2u.png)

当消息处理比较耗时的时候，可能生产消息的速度会远远大于消息的消费速度。长此以往，消息就会堆积越来越多，无法及时处理。

此时就可以使用work 模型，多个消费者共同处理消息处理，速度就能大大提高了。

### 3.2.1.消息发送

这次我们循环发送，模拟大量消息堆积现象。

在publisher服务中的SpringAmqpTest类中添加一个测试方法：

```java
/**
     * workQueue
     * 向队列中不停发送消息，模拟消息堆积。
     */
@Test
public void testWorkQueue() throws InterruptedException {
    // 队列名称
    String queueName = "simple.queue";
    // 消息
    String message = "hello, message_";
    for (int i = 0; i < 50; i++) {
        // 发送消息
        rabbitTemplate.convertAndSend(queueName, message + i);
        Thread.sleep(20);
    }
}
```

### 3.2.2.消息接收

要模拟多个消费者绑定同一个队列，我们在consumer服务的SpringRabbitListener中添加2个新的方法：

```java
@RabbitListener(queues = "simple.queue")
public void listenWorkQueue1(String msg) throws InterruptedException {
    System.out.println("消费者1接收到消息：【" + msg + "】" + LocalTime.now());
    Thread.sleep(20);
}

@RabbitListener(queues = "simple.queue")
public void listenWorkQueue2(String msg) throws InterruptedException {
    System.err.println("消费者2........接收到消息：【" + msg + "】" + LocalTime.now());
    Thread.sleep(200);
}
```

注意到这个消费者sleep了1000秒，模拟任务耗时。

### 3.2.3.测试

启动ConsumerApplication后，在执行publisher服务中刚刚编写的发送测试方法testWorkQueue。

可以看到消费者1很快完成了自己的25条消息。消费者2却在缓慢的处理自己的25条消息。

也就是说消息是平均分配给每个消费者，并没有考虑到消费者的处理能力。这样显然是有问题的。

### 3.2.4.能者多劳

在spring中有一个简单的配置，可以解决这个问题。我们修改consumer服务的application.yml文件，添加配置：

```yaml
spring:
  rabbitmq:
    listener:
      simple:
        prefetch: 1 # 每次只能获取一条消息，处理完成才能获取下一个消息
```

### 3.2.5.总结

Work模型的使用：

* 多个消费者绑定到一个队列，同一条消息只会被一个消费者处理
* 通过设置prefetch来控制消费者预取的消息数量

## 3.3.发布/订阅

发布订阅的模型如图：

![image-20210717165309625](/assets/image-20210717165309625.D2I6iYJr.png)

可以看到，在订阅模型中，多了一个exchange角色，而且过程略有变化：

* Publisher：生产者，也就是要发送消息的程序，但是不再发送到队列中，而是发给X（交换机）
* Exchange：交换机，图中的X。一方面，接收生产者发送的消息。另一方面，知道如何处理消息，例如递交给某个特别队列、递交给所有队列、或是将消息丢弃。到底如何操作，取决于Exchange的类型。Exchange有以下3种类型：
  * Fanout：广播，将消息交给所有绑定到交换机的队列
  * Direct：定向，把消息交给符合指定routing key 的队列
  * Topic：通配符，把消息交给符合routing pattern（路由模式） 的队列
* Consumer：消费者，与以前一样，订阅队列，没有变化
* Queue：消息队列也与以前一样，接收消息、缓存消息。

**Exchange（交换机）只负责转发消息，不具备存储消息的能力**，因此如果没有任何队列与Exchange绑定，或者没有符合路由规则的队列，那么消息会丢失！

## 3.4.Fanout

Fanout，英文翻译是扇出，我觉得在MQ中叫广播更合适。

![image-20210717165438225](/assets/image-20210717165438225.D6N55aXt.png)

在广播模式下，消息发送流程是这样的：

* 1）  可以有多个队列
* 2）  每个队列都要绑定到Exchange（交换机）
* 3）  生产者发送的消息，只能发送到交换机，交换机来决定要发给哪个队列，生产者无法决定
* 4）  交换机把消息发送给绑定过的所有队列
* 5）  订阅队列的消费者都能拿到消息

我们的计划是这样的：

* 创建一个交换机 xxl.fanout，类型是Fanout
* 创建两个队列fanout.queue1和fanout.queue2，绑定到交换机xxl.fanout

![image-20210717165509466](/assets/image-20210717165509466.D4ZEQqdb.png)

### 3.4.1.声明队列和交换机

Spring提供了一个接口Exchange，来表示所有不同类型的交换机：

![image-20210717165552676](/assets/image-20210717165552676.CCi686sb.png)

在consumer中创建一个类，声明队列和交换机：

```java
package cn.xxl.mq.config;

import org.springframework.amqp.core.Binding;
import org.springframework.amqp.core.BindingBuilder;
import org.springframework.amqp.core.FanoutExchange;
import org.springframework.amqp.core.Queue;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

@Configuration
public class FanoutConfig {
    /**
     * 声明交换机
     * @return Fanout类型交换机
     */
    @Bean
    public FanoutExchange fanoutExchange(){
        return new FanoutExchange("xxl.fanout");
    }

    /**
     * 第1个队列
     */
    @Bean
    public Queue fanoutQueue1(){
        return new Queue("fanout.queue1");
    }

    /**
     * 绑定队列和交换机
     */
    @Bean
    public Binding bindingQueue1(Queue fanoutQueue1, FanoutExchange fanoutExchange){
        return BindingBuilder.bind(fanoutQueue1).to(fanoutExchange);
    }

    /**
     * 第2个队列
     */
    @Bean
    public Queue fanoutQueue2(){
        return new Queue("fanout.queue2");
    }

    /**
     * 绑定队列和交换机
     */
    @Bean
    public Binding bindingQueue2(Queue fanoutQueue2, FanoutExchange fanoutExchange){
        return BindingBuilder.bind(fanoutQueue2).to(fanoutExchange);
    }
}
```

### 3.4.2.消息发送

在publisher服务的SpringAmqpTest类中添加测试方法：

```java
@Test
public void testFanoutExchange() {
    // 队列名称
    String exchangeName = "xxl.fanout";
    // 消息
    String message = "hello, everyone!";
    rabbitTemplate.convertAndSend(exchangeName, "", message);
}
```

### 3.4.3.消息接收

在consumer服务的SpringRabbitListener中添加两个方法，作为消费者：

```java
@RabbitListener(queues = "fanout.queue1")
public void listenFanoutQueue1(String msg) {
    System.out.println("消费者1接收到Fanout消息：【" + msg + "】");
}

@RabbitListener(queues = "fanout.queue2")
public void listenFanoutQueue2(String msg) {
    System.out.println("消费者2接收到Fanout消息：【" + msg + "】");
}
```

### 3.4.4.总结

交换机的作用是什么？

* 接收publisher发送的消息
* 将消息按照规则路由到与之绑定的队列
* 不能缓存消息，路由失败，消息丢失
* FanoutExchange的会将消息路由到每个绑定的队列

声明队列、交换机、绑定关系的Bean是什么？

* Queue
* FanoutExchange
* Binding

## 3.5.Direct

在Fanout模式中，一条消息，会被所有订阅的队列都消费。但是，在某些场景下，我们希望不同的消息被不同的队列消费。这时就要用到Direct类型的Exchange。

![image-20210717170041447](/assets/image-20210717170041447.BsW1SeJy.png)

在Direct模型下：

* 队列与交换机的绑定，不能是任意绑定了，而是要指定一个`RoutingKey`（路由key）
* 消息的发送方在 向 Exchange发送消息时，也必须指定消息的 `RoutingKey`。
* Exchange不再把消息交给每一个绑定的队列，而是根据消息的`Routing Key`进行判断，只有队列的`Routingkey`与消息的 `Routing key`完全一致，才会接收到消息

**案例需求如下**：

1. 利用@RabbitListener声明Exchange、Queue、RoutingKey

2. 在consumer服务中，编写两个消费者方法，分别监听direct.queue1和direct.queue2

3. 在publisher中编写测试方法，向xxl. direct发送消息

![image-20210717170223317](/assets/image-20210717170223317.D2zTrz5l.png)

### 3.5.1.基于注解声明队列和交换机

基于@Bean的方式声明队列和交换机比较麻烦，Spring还提供了基于注解方式来声明。

在consumer的SpringRabbitListener中添加两个消费者，同时基于注解来声明队列和交换机：

```java
@RabbitListener(bindings = @QueueBinding(
    value = @Queue(name = "direct.queue1"),
    exchange = @Exchange(name = "xxl.direct", type = ExchangeTypes.DIRECT),
    key = {"red", "blue"}
))
public void listenDirectQueue1(String msg){
    System.out.println("消费者接收到direct.queue1的消息：【" + msg + "】");
}

@RabbitListener(bindings = @QueueBinding(
    value = @Queue(name = "direct.queue2"),
    exchange = @Exchange(name = "xxl.direct", type = ExchangeTypes.DIRECT),
    key = {"red", "yellow"}
))
public void listenDirectQueue2(String msg){
    System.out.println("消费者接收到direct.queue2的消息：【" + msg + "】");
}
```

### 3.5.2.消息发送

在publisher服务的SpringAmqpTest类中添加测试方法：

```java
@Test
public void testSendDirectExchange() {
    // 交换机名称
    String exchangeName = "xxl.direct";
    // 消息
    String message = "红色警报！日本乱排核废水，导致海洋生物变异，惊现哥斯拉！";
    // 发送消息
    rabbitTemplate.convertAndSend(exchangeName, "red", message);
}
```

### 3.5.3.总结

描述下Direct交换机与Fanout交换机的差异？

* Fanout交换机将消息路由给每一个与之绑定的队列
* Direct交换机根据RoutingKey判断路由给哪个队列
* 如果多个队列具有相同的RoutingKey，则与Fanout功能类似

基于@RabbitListener注解声明队列和交换机有哪些常见注解？

* @Queue
* @Exchange

## 3.6.Topic

### 3.6.1.说明

`Topic`类型的`Exchange`与`Direct`相比，都是可以根据`RoutingKey`把消息路由到不同的队列。只不过`Topic`类型`Exchange`可以让队列在绑定`Routing key` 的时候使用通配符！

`Routingkey` 一般都是有一个或多个单词组成，多个单词之间以”.”分割，例如： `item.insert`

通配符规则：

`#`：匹配一个或多个词

`*`：匹配不多不少恰好1个词

举例：

`item.#`：能够匹配`item.spu.insert` 或者 `item.spu`

`item.*`：只能匹配`item.spu`

​

图示：

![image-20210717170705380](/assets/image-20210717170705380.CDVw5Mou.png)

解释：

* Queue1：绑定的是`china.#` ，因此凡是以 `china.`开头的`routing key` 都会被匹配到。包括china.news和china.weather
* Queue2：绑定的是`#.news` ，因此凡是以 `.news`结尾的 `routing key` 都会被匹配。包括china.news和japan.news

案例需求：

实现思路如下：

1. 并利用@RabbitListener声明Exchange、Queue、RoutingKey

2. 在consumer服务中，编写两个消费者方法，分别监听topic.queue1和topic.queue2

3. 在publisher中编写测试方法，向xxl. topic发送消息

![image-20210717170829229](/assets/image-20210717170829229.BDyiPzwN.png)

### 3.6.2.消息发送

在publisher服务的SpringAmqpTest类中添加测试方法：

```java
/**
     * topicExchange
     */
@Test
public void testSendTopicExchange() {
    // 交换机名称
    String exchangeName = "xxl.topic";
    // 消息
    String message = "喜报！孙悟空大战哥斯拉，胜!";
    // 发送消息
    rabbitTemplate.convertAndSend(exchangeName, "china.news", message);
}
```

### 3.6.3.消息接收

在consumer服务的SpringRabbitListener中添加方法：

```java
@RabbitListener(bindings = @QueueBinding(
    value = @Queue(name = "topic.queue1"),
    exchange = @Exchange(name = "xxl.topic", type = ExchangeTypes.TOPIC),
    key = "china.#"
))
public void listenTopicQueue1(String msg){
    System.out.println("消费者接收到topic.queue1的消息：【" + msg + "】");
}

@RabbitListener(bindings = @QueueBinding(
    value = @Queue(name = "topic.queue2"),
    exchange = @Exchange(name = "xxl.topic", type = ExchangeTypes.TOPIC),
    key = "#.news"
))
public void listenTopicQueue2(String msg){
    System.out.println("消费者接收到topic.queue2的消息：【" + msg + "】");
}
```

### 3.6.4.总结

描述下Direct交换机与Topic交换机的差异？

* Topic交换机接收的消息RoutingKey必须是多个单词，以 `**.**` 分割
* Topic交换机与队列绑定时的bindingKey可以指定通配符
* `#`：代表0个或多个词
* `*`：代表1个词

## 3.7.消息转换器

之前说过，Spring会把你发送的消息序列化为字节发送给MQ，接收消息的时候，还会把字节反序列化为Java对象。

![image-20200525170410401](/assets/image-20200525170410401.iSYy0psM.png)

只不过，默认情况下Spring采用的序列化方式是JDK序列化。众所周知，JDK序列化存在下列问题：

* 数据体积过大
* 有安全漏洞
* 可读性差

我们来测试一下。

### 3.7.1.测试默认转换器

我们修改消息发送的代码，发送一个Map对象：

```java
@Test
public void testSendMap() throws InterruptedException {
    // 准备消息
    Map<String,Object> msg = new HashMap<>();
    msg.put("name", "Jack");
    msg.put("age", 21);
    // 发送消息
    rabbitTemplate.convertAndSend("simple.queue","", msg);
}
```

停止consumer服务

发送消息后查看控制台：

![image-20210422232835363](/assets/image-20210422232835363.C1ZCwqf-.png)

### 3.7.2.配置JSON转换器

显然，JDK序列化方式并不合适。我们希望消息体的体积更小、可读性更高，因此可以使用JSON方式来做序列化和反序列化。

在publisher和consumer两个服务中都引入依赖：

```xml
<dependency>
    <groupId>com.fasterxml.jackson.dataformat</groupId>
    <artifactId>jackson-dataformat-xml</artifactId>
    <version>2.9.10</version>
</dependency>
```

配置消息转换器。

在启动类中添加一个Bean即可：

```java
@Bean
public MessageConverter jsonMessageConverter(){
    return new Jackson2JsonMessageConverter();
}
```

---

---
url: /Java/架构设计/分布式/03.分布式消息队列/02.RabbitMQ/4_RabbitMQ延时队列.md
---

# RabbitMQ延时队列

一、什么是延时队列

三、RabbitMQ中的TTL

## 基本使用

### 依赖

```xml
<dependencies>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-web</artifactId>
    </dependency>
    <!-- mq -->
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-amqp</artifactId>
    </dependency>
    <!-- fastjson -->
    <dependency>
        <groupId>com.alibaba</groupId>
        <artifactId>fastjson</artifactId>
        <version>1.2.49</version>
    </dependency>
    <!-- lombok -->
    <dependency>
        <groupId>org.projectlombok</groupId>
        <artifactId>lombok</artifactId>
        <version>1.18.30</version>
    </dependency>
</dependencies>
```

### 配置文件

```yaml
server:
  port: 8056
spring:
  rabbitmq:
    host: 192.168.100.105 # 主机名
    port: 5672 # 端口
    virtual-host: / # 虚拟主机
    username: admin # 用户名
    password: admin # 密码
    listener:
      simple:
        concurrency: 1
        max-concurrency: 1
        acknowledge-mode: manual
        prefetch: 1 # 每次只能获取一条消息，处理完成才能获取下一个消息
```

### 延迟队列配置类

```java
@Configuration
public class DelayedRabbitMQConfig {

    public static final String DELAYED_QUEUE_NAME = "delay.queue.demo.delay.queue";
    public static final String DELAYED_EXCHANGE_NAME = "delay.queue.demo.delay.exchange";
    public static final String DELAYED_ROUTING_KEY = "delay.queue.demo.delay.routingkey";

    @Bean


    public Queue immediateQueue() {
        return new Queue(DELAYED_QUEUE_NAME);
    }

    @Bean
    public CustomExchange customExchange() {
        Map<String, Object> args = new HashMap<>();
        args.put("x-delayed-type", "direct");
        return new CustomExchange(DELAYED_EXCHANGE_NAME, "x-delayed-message", true, false, args);
    }

    @Bean
    public Binding bindingNotify(@Qualifier("immediateQueue") Queue queue,
                                 @Qualifier("customExchange") CustomExchange customExchange) {
        return BindingBuilder.bind(queue).to(customExchange).with(DELAYED_ROUTING_KEY).noargs();
    }
}
```

### 监听器

```java
@Slf4j
@Component
public class Listener {

    @RabbitListener(queues = DELAYED_QUEUE_NAME)
    public void receiveD(Message message, Channel channel) throws IOException {
        String msg = new String(message.getBody());
        log.info("当前时间：{},延时队列收到消息：{}", new Date().toString(), msg);
        channel.basicAck(message.getMessageProperties().getDeliveryTag(), false);

        Object object = JSON.parseObject(message.getBody(), Map.class);
        System.out.println("延时队列收到消息: " + object);
    }
}
```

### 发送消息

```java
@Slf4j
@RestController
public class TestController {

    @Autowired
    private RabbitTemplate rabbitTemplate;

    @RequestMapping("delayMsg")
    public Object delayMsg(String msg, Integer delayTime) {
        log.info("当前时间：{},收到请求，msg:{},delayTime:{}", new Date(), msg, delayTime);

        // 准备消息
        for (int i = 1; i <= 10; i++) {
            Map<String, Object> msgMap = new HashMap<>();
            msgMap.put("name", "Jack" + i);
            msgMap.put("age", i);
            sendDelayMsg(msgMap, delayTime);
        }

        return "发送成功";
    }

    public void sendDelayMsg(Object msg, Integer delayTime) {
        rabbitTemplate.convertAndSend(DELAYED_EXCHANGE_NAME, DELAYED_ROUTING_KEY, msg, a -> {
            a.getMessageProperties().setDelay(delayTime);
            return a;
        });
    }
}
```

### 测试消息

http://127.0.0.1:8056/delayMsg?msg=content\&delayTime=2000

## 报错

Caused by: com.rabbitmq.client.ShutdownSignalException: connection error; protocol method: #method\<connection.close>(reply-code=530, reply-text=NOT\_ALLOWED - vhost my-rabbit not found, class-id=10, method-id=40)

https://blog.csdn.net/m0\_46114643/article/details/122543014

确认消息报错：Channel shutdown: channel error; protocol method

```
@RabbitListener(queues = DELAYED_QUEUE_NAME)
public void receiveD(Message message, Channel channel) throws IOException {
    String msg = new String(message.getBody());
    log.info("当前时间：{},延时队列收到消息：{}", new Date().toString(), msg);
    channel.basicAck(message.getMessageProperties().getDeliveryTag(), false);
}
```

https://blog.csdn.net/hantanxin/article/details/103871321

修改配置为手动签收

```
    listener:
      simple:
        concurrency: 1
        max-concurrency: 1
        acknowledge-mode: manual
        prefetch: 1 
```

## 参考资料

https://cloud.tencent.com/developer/article/1659393

https://blog.csdn.net/Mou\_O/article/details/106093749

https://cloud.tencent.com/developer/article/1475254

https://www.cnblogs.com/chengxy-nds/p/13217828.html

深入浅出RabbitMQ：顺序消费、死信队列和延时队列：https://cloud.tencent.com/developer/article/2355643：

---

---
url: /daily/开发文档/RabbitMQAssistant使用.md
---

# RabbitMQAssistant使用

RabbitMQ Assistant是RabbitMQ可视化管理与监控工具

> 官网：<https://www.redisant.cn/rta>

## 主要功能

主要功能包括：

* GPU渲染：RabbitMQ Assistant 在渲染界面时充分利用您的 GPU。这带来流畅的的用户体验，同时使用比以前更少的功率。
* 生成拓扑图：为指定的vhost一键生成拓扑图，支持导出png、svg、pdf、tk等多种格式，让您的消息流动一目了然。
* 测试生产者和消费者：您可以测试解决方案的各个部分，模拟进出的第三方消息。您可以创建和编辑测试消息，一个接一个或批量发送。
* 数据格式化：RabbitMQ Assistant 会自动识别并格式化不同的消息格式，包括Text、JSON、XML、HEX、MessagePack，以及各种整数、浮点类型。
* 查看系统状态：您可以一目了然地查看系统状态，检查某些队列是否变得太大，死信队列中是否有消息等。通过自动刷新功能，可以近乎实时地查看整个系统的工作方式。
* 创建消息：创建消息时，可以指定消息头、属性和载荷；或者一次发送数千条消息进行性能测试，以了解系统如何处理负载。

## 连接远程MQ

由于 RabbitMQ Assistant 仅在连接本地时提供免费服务，而连接远程地址则需收费，因此使用 Nginx 将本地端口反向代理到远程地址。

### 1、下载 Nginx

访问 [Nginx 官方网站](http://nginx.org/en/download.html) 或 Nginx 中文网，选择适合的版本并下载。将下载的压缩包解压到目标目录（如 `D:\Tools\Nginx`）。 **注意：** 路径中不要包含中文字符或空格。

### 2、启动 Nginx

打开命令提示符，进入 Nginx 解压目录：

```sh
cd D:\Tools\Nginx
start nginx
```

如果防火墙弹出提示，请允许访问网络。

在浏览器中访问 `http://localhost`，若显示欢迎页面，则启动成功。

### 3、修改配置文件

配置文件位于 `conf/nginx.conf`。打开该文件，找到 *server* 节点，根据需要修改或添加配置。例如：

**反向代理示例：**

```conf
server {
    listen 80;
    server_name example.com;
    location / {
        proxy_pass http://127.0.0.1:8080;
    }
}
```

**负载均衡示例：**

```conf
upstream backend {
    server 127.0.0.1:8080;
    server 127.0.0.1:8081;
}

server {
    listen 80;
    server_name example.com;
    location / {
        proxy_pass http://backend;
    }
}
```

我们的需求只需要反向代理，因此增加server代理本地端口5557到远程http://192.168.100.105:15672

`nginx.conf`如下

```conf
#user  nobody;
worker_processes  1;

#error_log  logs/error.log;
#error_log  logs/error.log  notice;
#error_log  logs/error.log  info;

#pid        logs/nginx.pid;


events {
    worker_connections  1024;
}


http {
    include       mime.types;
    default_type  application/octet-stream;

    #log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
    #                  '$status $body_bytes_sent "$http_referer" '
    #                  '"$http_user_agent" "$http_x_forwarded_for"';

    #access_log  logs/access.log  main;

    sendfile        on;
    #tcp_nopush     on;

    #keepalive_timeout  0;
    keepalive_timeout  65;

    #gzip  on;

    #server {
    #    listen       80;
    #    server_name  localhost;
	#
    #    #charset koi8-r;
	#
    #    #access_log  logs/host.access.log  main;
	#
    #    location / {
    #        root   html;
    #        index  index.html index.htm;
    #    }
	#
    #    #error_page  404              /404.html;
	#
    #    # redirect server error pages to the static page /50x.html
    #    #
    #    error_page   500 502 503 504  /50x.html;
    #    location = /50x.html {
    #        root   html;
    #    }
	#
    #    # proxy the PHP scripts to Apache listening on 127.0.0.1:80
    #    #
    #    #location ~ \.php$ {
    #    #    proxy_pass   http://127.0.0.1;
    #    #}
	#
    #    # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000
    #    #
    #    #location ~ \.php$ {
    #    #    root           html;
    #    #    fastcgi_pass   127.0.0.1:9000;
    #    #    fastcgi_index  index.php;
    #    #    fastcgi_param  SCRIPT_FILENAME  /scripts$fastcgi_script_name;
    #    #    include        fastcgi_params;
    #    #}
	#
    #    # deny access to .htaccess files, if Apache's document root
    #    # concurs with nginx's one
    #    #
    #    #location ~ /\.ht {
    #    #    deny  all;
    #    #}
    #}


    # another virtual host using mix of IP-, name-, and port-based configuration
    #
    #server {
    #    listen       8000;
    #    listen       somename:8080;
    #    server_name  somename  alias  another.alias;

    #    location / {
    #        root   html;
    #        index  index.html index.htm;
    #    }
    #}


    # HTTPS server
    #
    #server {
    #    listen       443 ssl;
    #    server_name  localhost;

    #    ssl_certificate      cert.pem;
    #    ssl_certificate_key  cert.key;

    #    ssl_session_cache    shared:SSL:1m;
    #    ssl_session_timeout  5m;

    #    ssl_ciphers  HIGH:!aNULL:!MD5;
    #    ssl_prefer_server_ciphers  on;

    #    location / {
    #        root   html;
    #        index  index.html index.htm;
    #    }
    #}
	
	server {
		listen 5557;
		server_name localhost;

		location / {
			proxy_pass http://192.168.100.105:15672;
			proxy_set_header Host $host;
			proxy_set_header X-Real-IP $remote_addr;
			proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
			proxy_set_header X-Forwarded-Proto $scheme;
			proxy_redirect off;
			proxy_buffering off;
			proxy_request_buffering off;
			proxy_http_version 1.1;
			proxy_set_header Connection "";
		}
	}

}
```

### 4、重载配置

修改完成后，运行以下命令重载配置：

```
nginx -s reload
```

常用命令

* 查看版本：nginx -v
* 停止服务：nginx -s stop
* 优雅停止：nginx -s quit
* 重启服务：nginx -s reload

### 5、查看效果

浏览器访问地址localhost:5557会直接访问到远程mq地址

![image-20250728113243920](/assets/image-20250728113243920.DnDln8Wa.png)

使用RabbitMQ Assistant连接

![image-20250728113320937](/assets/image-20250728113320937.GNJfqXaB.png)

![image-20250728111954696](/assets/image-20250728111954696.qsg1tc4h.png)

---

---
url: /常用框架/SpringAIAlibaba/Agent Framework/14_RAG检索增强生成.md
---

# RAG 检索增强生成

---

---
url: /Java/架构设计/高可用/降级熔断/README.md
---
# README

---

---
url: /Java/架构设计/高并发/数据库/README.md
---
# README

---

---
url: /Java/架构设计/高并发/缓存/README.md
---
# README

---

---
url: /Java/架构设计/高并发/负载均衡/README.md
---
# README

---

---
url: /Redis/Redis基础/1_RedisStack介绍.md
---

# Redis Stack介绍

#### Redis Stack介绍

> [Redis Stack - Redis 中文文档 (tkcnn.com)](https://www.tkcnn.com/redis/Redis-Stack.html)

官方关于Redis Stack的说明：用现代数据模型和处理引擎扩展Redis，提供完整的开发者体验，简单来说Redis Stack是针对redis的扩展

##### Redis Stack都扩展了哪些功能？

###### 可搜索Redis

对Redis数据结构和数据模型进行索引和查询；对Redis数据运行复杂的聚合和全文搜索。

> 感觉像ElasticSearch，支持全文搜索

###### 文档数据库

利用JSON及其灵活的数据模型将Redis用作文档数据库。高效地建模、搜索和查询数据，而不需要额外的缓存。

> 感觉像MongoDB

###### 遥测

从现场设备中获取连续数据，将其存储为时间序列数据，或使用概率数据结构进行分析和重复数据消除。

###### 身份和资源管理

将数字资源和ACL定义为图形，并使用单个Cypher查询实时计算权限。

###### 矢量搜索

通过查询矢量嵌入来开发人工智能驱动的应用程序。您可以使用Redis作为矢量数据库来实现推荐系统、检索增强生成（RAG）、LLM语义缓存等。有关更多信息，请参阅快速入门指南。

> 向量数据库吗？适用于每个组织的矢量数据库解决方案

###### 欺诈检测

实时检测欺诈的所有工具，概率查询，矢量搜索，甚至使用流进行处理。

> 提供的有：HyperLogLog、Cuckoo filter、Bloom filter，最后一个看着很眼熟啊，这不布隆过滤器吗？

##### Redis Stack套件

* Redis Stack Server
* RedisInsight
* Redis Stack 客户端 SDK

###### Redis Stack Server

由 Redis，RedisSearch，RedisJSON，RedisGraph，RedisTimeSeries 和 RedisBloom 组成。

###### RedisInsight

亲儿子，官方的客户端可视化工具。

原来QML的RESP已经下架。

> 下载地址：https://redis.com/redis-enterprise/redis-insight

###### Redis Stack 客户端 SDK

针对常用Python、.Net、Java、JS等等都提供了客户端SDK。

###### redis-om-spring

redis-om-spring组件，使用方式和访问MySQL基本相同，开发门槛降低了很多。

> 官网：https://github.com/redis/redis-om-spring
>
> RedisOM是Redis官方推出的ORM框架，是对Spring Data Redis的扩展。由于Redis目前已经支持原生JSON对象的存储，之前使用RedisTemplate直接用字符串来存储JOSN对象的方式明显不够优雅。通过RedisOM我们不仅能够以对象的形式来操作Redis中的数据，而且可以实现搜索功能！

---

---
url: /Redis/Redis基础/0_Redis概述及使用.md
---

# Redis概述及使用

## 一、Nosql概述

### 为什么使用Nosql

> 1、单机Mysql时代

![在这里插入图片描述](/assets/2020082010365930._dHIx1AT.png)

90年代,一个网站的访问量一般不会太大，单个数据库完全够用。随着用户增多，网站出现以下问题

1. 数据量增加到一定程度，单机数据库就放不下了
2. 数据的索引（B+ Tree）,一个机器内存也存放不下
3. 访问量变大后（读写混合），一台服务器承受不住。

> 2、Memcached(缓存) + Mysql + 垂直拆分（读写分离）

网站80%的情况都是在读，每次都要去查询数据库的话就十分的麻烦！所以说我们希望减轻数据库的压力，我们可以使用缓存来保证效率！

![在这里插入图片描述](/assets/20200820103713734.D3neszxy.png)

优化过程经历了以下几个过程：

1. 优化数据库的数据结构和索引(难度大)
2. 文件缓存，通过IO流获取比每次都访问数据库效率略高，但是流量爆炸式增长时候，IO流也承受不了
3. MemCache,当时最热门的技术，通过在数据库和数据库访问层之间加上一层缓存，第一次访问时查询数据库，将结果保存到缓存，后续的查询先检查缓存，若有直接拿去使用，效率显著提升。

> 3、分库分表 + 水平拆分 + Mysql集群

![在这里插入图片描述](/assets/20200820103739584.CzC2vTFI.png)

> 4、如今最近的年代

如今信息量井喷式增长，各种各样的数据出现（用户定位数据，图片数据等），大数据的背景下关系型数据库（RDBMS）无法满足大量数据要求。Nosql数据库就能轻松解决这些问题。

> 目前一个基本的互联网项目

![在这里插入图片描述](/assets/20200820103804572.CWrAo94G.png)

> 为什么要用NoSQL ？

用户的个人信息，社交网络，地理位置。用户自己产生的数据，用户日志等等爆发式增长！
这时候我们就需要使用NoSQL数据库的，Nosql可以很好的处理以上的情况！

### 什么是Nosql

**NoSQL = Not Only SQL（不仅仅是SQL）**

Not Only Structured Query Language

关系型数据库：列+行，同一个表下数据的结构是一样的。

非关系型数据库：数据存储没有固定的格式，并且可以进行横向扩展。

NoSQL泛指非关系型数据库，随着web2.0互联网的诞生，传统的关系型数据库很难对付web2.0时代！尤其是超大规模的高并发的社区，暴露出来很多难以克服的问题，NoSQL在当今大数据环境下发展的十分迅速，Redis是发展最快的。

### Nosql特点

1. 方便扩展（数据之间没有关系，很好扩展！）

2. 大数据量高性能（Redis一秒可以写8万次，读11万次，NoSQL的缓存记录级，是一种细粒度的缓存，性能会比较高！）

3. 数据类型是多样型的！（不需要事先设计数据库，随取随用）

4. 传统的 RDBMS 和 NoSQL

   ```
   传统的 RDBMS(关系型数据库)
   - 结构化组织
   - SQL
   - 数据和关系都存在单独的表中 row col
   - 操作，数据定义语言
   - 严格的一致性
   - 基础的事务
   - ...
   ```

   ```
   Nosql
   - 不仅仅是数据
   - 没有固定的查询语言
   - 键值对存储，列存储，文档存储，图形数据库（社交关系）
   - 最终一致性
   - CAP定理和BASE
   - 高性能，高可用，高扩展
   - ...
   ```

> 了解：3V + 3高

大数据时代的3V ：主要是**描述问题**的

1. 海量Velume
2. 多样Variety
3. 实时Velocity

大数据时代的3高 ： 主要是**对程序的要求**

1. 高并发
2. 高可扩
3. 高性能

真正在公司中的实践：NoSQL + RDBMS 一起使用才是最强的。

### 阿里巴巴演进分析

推荐阅读：阿里云的这群疯子https://yq.aliyun.com/articles/653511

![1](/assets/20200820103829446.BrpljIQr.png)

![在这里插入图片描述](/assets/20200820103851613.B8jssSPl.png)

```bash
# 商品信息
- 一般存放在关系型数据库：Mysql,阿里巴巴使用的Mysql都是经过内部改动的。

# 商品描述、评论(文字居多)
- 文档型数据库：MongoDB

# 图片
- 分布式文件系统 FastDFS
- 淘宝：TFS
- Google: GFS
- Hadoop: HDFS
- 阿里云: oss

# 商品关键字 用于搜索
- 搜索引擎：solr,elasticsearch
- 阿里：Isearch 多隆

# 商品热门的波段信息
- 内存数据库：Redis，Memcache

# 商品交易，外部支付接口
- 第三方应用
```

### Nosql的四大分类

> **KV键值对**

* 新浪：**Redis**
* 美团：Redis + Tair
* 阿里、百度：Redis + Memcache

> **文档型数据库（bson数据格式）：**

* **MongoDB**(掌握)
  * 基于分布式文件存储的数据库。C++编写，用于处理大量文档。
  * MongoDB是RDBMS和NoSQL的中间产品。MongoDB是非关系型数据库中功能最丰富的，NoSQL中最像关系型数据库的数据库。
* ConthDB

> **列存储数据库**

* **HBase**(大数据必学)
* 分布式文件系统

> **图关系数据库**

用于广告推荐，社交网络

* **Neo4j**、InfoGrid

| **分类**                | **Examples举例**                                   | **典型应用场景**                                             | **数据模型**                                    | **优点**                                                     | **缺点**                                                     |
| ----------------------- | -------------------------------------------------- | ------------------------------------------------------------ | ----------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **键值对（key-value）** | Tokyo Cabinet/Tyrant, Redis, Voldemort, Oracle BDB | 内容缓存，主要用于处理大量数据的高访问负载，也用于一些日志系统等等。 | Key 指向 Value 的键值对，通常用hash table来实现 | 查找速度快                                                   | 数据无结构化，通常只被当作字符串或者二进制数据               |
| **列存储数据库**        | Cassandra, HBase, Riak                             | 分布式的文件系统                                             | 以列簇式存储，将同一列数据存在一起              | 查找速度快，可扩展性强，更容易进行分布式扩展                 | 功能相对局限                                                 |
| **文档型数据库**        | CouchDB, MongoDb                                   | Web应用（与Key-Value类似，Value是结构化的，不同的是数据库能够了解Value的内容） | Key-Value对应的键值对，Value为结构化数据        | 数据结构要求不严格，表结构可变，不需要像关系型数据库一样需要预先定义表结构 | 查询性能不高，而且缺乏统一的查询语法。                       |
| **图形(Graph)数据库**   | Neo4J, InfoGrid, Infinite Graph                    | 社交网络，推荐系统等。专注于构建关系图谱                     | 图结构                                          | 利用图结构相关算法。比如最短路径寻址，N度关系查找等          | 很多时候需要对整个图做计算才能得出需要的信息，而且这种结构不太好做分布式的集群 |

## 二、Redis入门

### 概述

> Redis是什么？

Redis（Remote Dictionary Server )，即远程字典服务。

是一个开源的使用ANSI C语言编写、支持网络、可基于内存亦可持久化的日志型、**Key-Value数据库**，并提供多种语言的API。

与memcached一样，为了保证效率，**数据都是缓存在内存中**。区别的是redis会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件，并且在此基础上实现了master-slave(主从)同步。

> Redis能该干什么？

1. 内存存储、持久化，内存是断电即失的，所以需要持久化（RDB、AOF）
2. 高效率、用于高速缓冲
3. 发布订阅系统
4. 地图信息分析
5. 计时器、计数器(eg：浏览量)
6. 。。。

> 特性

1. 多样的数据类型

2. 持久化

3. 集群

4. 事务

   …

### 环境搭建

官网：https://redis.io/

推荐使用Linux服务器学习。

windows版本的Redis已经停更很久了…

### Windows安装

https://github.com/dmajkic/redis

1. 解压安装包
   ![在这里插入图片描述](/assets/20200820103922318.CHWL3e8Q.png)
2. 开启redis-server.exe
3. 启动redis-cli.exe测试![在这里插入图片描述](/assets/20200820103950934.DfImH2rc.png)

### Linux安装

1. 下载安装包！`redis-5.0.8.tar.gz`

2. 解压Redis的安装包！程序一般放在 `/opt` 目录下

   ![在这里插入图片描述](/assets/20200820104016426.BGBkOQG_.png)

3. 基本环境安装

   ```bash
   yum install gcc-c++
   # 然后进入redis目录下执行
   make
   # 然后执行
   make install
   ```

![在这里插入图片描述](/assets/20200820104048327.CJ378tZf.png)

1. redis默认安装路径 `/usr/local/bin`![在这里插入图片描述](/assets/20200820104140692.CT1-UGv3.png)

2. 将redis的配置文件复制到 程序安装目录 `/usr/local/bin/kconfig`下

   ![\[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(0\_Redis概述及使用.assets/20200820104157817.png)(狂神说 0\_Redis概述及使用.assets/image-20200813114000868.png)\]](/assets/20200820104157817.DS4cpjnn.png)

3. redis默认不是后台启动的，需要修改配置文件！

   ![\[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(0\_Redis概述及使用.assets/20200820104213706.png)(狂神说 0\_Redis概述及使用.assets/image-20200813114019063.png)\]](/assets/20200820104213706.i5G8LOlG.png)

4. 通过制定的配置文件启动redis服务

   ![\[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(0\_Redis概述及使用.assets/20200820104228556.png)(狂神说 0\_Redis概述及使用.assets/image-20200813114030597.png)\]](/assets/20200820104228556.DIjm_cKs.png)

5. 使用redis-cli连接指定的端口号测试，Redis的默认端口6379

   ![\[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(0\_Redis概述及使用.assets/20200820104243223.png)(狂神说 0\_Redis概述及使用.assets/image-20200813114045299.png)\]](/assets/20200820104243223.C5q3iEtw.png)

6. 查看redis进程是否开启

   ![\[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(0\_Redis概述及使用.assets/20200820104300532.png)(狂神说 0\_Redis概述及使用.assets/image-20200813114103769.png)\]](/assets/20200820104300532.OkWaZX7z.png)

7. 关闭Redis服务 `shutdown`

   ![\[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(0\_Redis概述及使用.assets/20200820104314297.png)(狂神说 0\_Redis概述及使用.assets/image-20200813114116691.png)\]](/assets/20200820104314297.C4nwKsGE.png)

8. 再次查看进程是否存在

9. 后面我们会使用单机多Redis启动集群测试

### 测试性能

\*\*redis-benchmark：\*\*Redis官方提供的性能测试工具，参数选项如下：

![img](/assets/20200513214125892.OJaLGY1k.png)

**简单测试：**

```bash
# 测试：100个并发连接 100000请求
redis-benchmark -h localhost -p 6379 -c 100 -n 100000
12
```

![\[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(0\_Redis概述及使用.assets/20200820104343472.png)(狂神说 0\_Redis概述及使用.assets/image-20200813114143365.png)\]](/assets/20200820104343472.CZ84bXNH.png)

### 基础知识

> redis默认有16个数据库

![\[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(0\_Redis概述及使用.assets/20200820104357466.png)(狂神说 0\_Redis概述及使用.assets/image-20200813114158322.png)\]](/assets/20200820104357466.JuBRQPuW.png)

默认使用的第0个;

16个数据库为：DB 0~DB 15
默认使用DB 0 ，可以使用`select n`切换到DB n，`dbsize`可以查看当前数据库的大小，与key数量相关。

```bash
127.0.0.1:6379> config get databases # 命令行查看数据库数量databases
1) "databases"
2) "16"

127.0.0.1:6379> select 8 # 切换数据库 DB 8
OK
127.0.0.1:6379[8]> dbsize # 查看数据库大小
(integer) 0

# 不同数据库之间 数据是不能互通的，并且dbsize 是根据库中key的个数。
127.0.0.1:6379> set name sakura 
OK
127.0.0.1:6379> SELECT 8
OK
127.0.0.1:6379[8]> get name # db8中并不能获取db0中的键值对。
(nil)
127.0.0.1:6379[8]> DBSIZE
(integer) 0
127.0.0.1:6379[8]> SELECT 0
OK
127.0.0.1:6379> keys *
1) "counter:__rand_int__"
2) "mylist"
3) "name"
4) "key:__rand_int__"
5) "myset:__rand_int__"
127.0.0.1:6379> DBSIZE # size和key个数相关
(integer) 5
```

`keys *` ：查看当前数据库中所有的key。

`flushdb`：清空当前数据库中的键值对。

`flushall`：清空所有数据库的键值对。

> **Redis是单线程的，Redis是基于内存操作的。**

所以Redis的性能瓶颈不是CPU,而是机器内存和网络带宽。

那么为什么Redis的速度如此快呢，性能这么高呢？QPS达到10W+

> **Redis为什么单线程还这么快？**

* 误区1：高性能的服务器一定是多线程的？
* 误区2：多线程（CPU上下文会切换！）一定比单线程效率高！

核心：Redis是将所有的数据放在内存中的，所以说使用单线程去操作效率就是最高的，多线程（CPU上下文会切换：耗时的操作！），对于内存系统来说，如果没有上下文切换效率就是最高的，多次读写都是在一个CPU上的，在内存存储数据情况下，单线程就是最佳的方案。

## 三、五大数据类型

Redis是一个开源（BSD许可），内存存储的数据结构服务器，可用作**数据库**，**高速缓存**和**消息队列代理**。它支持[字符串](https://www.redis.net.cn/tutorial/3508.html)、[哈希表](https://www.redis.net.cn/tutorial/3509.html)、[列表](https://www.redis.net.cn/tutorial/3510.html)、[集合](https://www.redis.net.cn/tutorial/3511.html)、[有序集合](https://www.redis.net.cn/tutorial/3512.html)，[位图](https://www.redis.net.cn/tutorial/3508.html)，[hyperloglogs](https://www.redis.net.cn/tutorial/3513.html)等数据类型。内置复制、[Lua脚本](https://www.redis.net.cn/tutorial/3516.html)、LRU收回、[事务](https://www.redis.net.cn/tutorial/3515.html)以及不同级别磁盘持久化功能，同时通过Redis Sentinel提供高可用，通过Redis Cluster提供自动[分区](https://www.redis.net.cn/tutorial/3524.html)。

### Redis-key

> 在redis中无论什么数据类型，在数据库中都是以key-value形式保存，通过进行对Redis-key的操作，来完成对数据库中数据的操作。

下面学习的命令：

* `exists key`：判断键是否存在
* `del key`：删除键值对
* `move key db`：将键值对移动到指定数据库
* `expire key second`：设置键值对的过期时间
* `type key`：查看value的数据类型

```bash
127.0.0.1:6379> keys * # 查看当前数据库所有key
(empty list or set)
127.0.0.1:6379> set name qinjiang # set key
OK
127.0.0.1:6379> set age 20
OK
127.0.0.1:6379> keys *
1) "age"
2) "name"
127.0.0.1:6379> move age 1 # 将键值对移动到指定数据库
(integer) 1
127.0.0.1:6379> EXISTS age # 判断键是否存在
(integer) 0 # 不存在
127.0.0.1:6379> EXISTS name
(integer) 1 # 存在
127.0.0.1:6379> SELECT 1
OK
127.0.0.1:6379[1]> keys *
1) "age"
127.0.0.1:6379[1]> del age # 删除键值对
(integer) 1 # 删除个数


127.0.0.1:6379> set age 20
OK
127.0.0.1:6379> EXPIRE age 15 # 设置键值对的过期时间

(integer) 1 # 设置成功 开始计数
127.0.0.1:6379> ttl age # 查看key的过期剩余时间
(integer) 13
127.0.0.1:6379> ttl age
(integer) 11
127.0.0.1:6379> ttl age
(integer) 9
127.0.0.1:6379> ttl age
(integer) -2 # -2 表示key过期，-1表示key未设置过期时间

127.0.0.1:6379> get age # 过期的key 会被自动delete
(nil)
127.0.0.1:6379> keys *
1) "name"

127.0.0.1:6379> type name # 查看value的数据类型
string
```

关于`TTL`命令

Redis的key，通过TTL命令返回key的过期时间，一般来说有3种：

1. 当前key没有设置过期时间，所以会返回-1.
2. 当前key有设置过期时间，而且key已经过期，所以会返回-2.
3. 当前key有设置过期时间，且key还没有过期，故会返回key的正常剩余时间.

关于重命名`RENAME`和`RENAMENX`

* `RENAME key newkey`修改 key 的名称
* `RENAMENX key newkey`仅当 newkey 不存在时，将 key 改名为 newkey 。

更多命令学习：https://www.redis.net.cn/order/

\[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-wBVZtGVm-1597890996517)(狂神说 0\_Redis概述及使用.assets/image-20200813114228439.png)]

### String(字符串)

普通的set、get直接略过。

| 命令                                 | 描述                                                         | 示例                                                         |
| ------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| `APPEND key value`                   | 向指定的key的value后追加字符串                               | 127.0.0.1:6379> set msg hello OK 127.0.0.1:6379> append msg " world" (integer) 11 127.0.0.1:6379> get msg “hello world” |
| `DECR/INCR key`                      | 将指定key的value数值进行+1/-1(仅对于数字)                    | 127.0.0.1:6379> set age 20 OK 127.0.0.1:6379> incr age (integer) 21 127.0.0.1:6379> decr age (integer) 20 |
| `INCRBY/DECRBY key n`                | 按指定的步长对数值进行加减                                   | 127.0.0.1:6379> INCRBY age 5 (integer) 25 127.0.0.1:6379> DECRBY age 10 (integer) 15 |
| `INCRBYFLOAT key n`                  | 为数值加上浮点型数值                                         | 127.0.0.1:6379> INCRBYFLOAT age 5.2 “20.2”                   |
| `STRLEN key`                         | 获取key保存值的字符串长度                                    | 127.0.0.1:6379> get msg “hello world” 127.0.0.1:6379> STRLEN msg (integer) 11 |
| `GETRANGE key start end`             | 按起止位置获取字符串（闭区间，起止位置都取）                 | 127.0.0.1:6379> get msg “hello world” 127.0.0.1:6379> GETRANGE msg 3 9 “lo worl” |
| `SETRANGE key offset value`          | 用指定的value 替换key中 offset开始的值                       | 127.0.0.1:6379> SETRANGE msg 2 hello (integer) 7 127.0.0.1:6379> get msg “tehello” |
| `GETSET key value`                   | 将给定 key 的值设为 value ，并返回 key 的旧值(old value)。   | 127.0.0.1:6379> GETSET msg test “hello world”                |
| `SETNX key value`                    | 仅当key不存在时进行set                                       | 127.0.0.1:6379> SETNX msg test (integer) 0 127.0.0.1:6379> SETNX name sakura (integer) 1 |
| `SETEX key seconds value`            | set 键值对并设置过期时间                                     | 127.0.0.1:6379> setex name 10 root OK 127.0.0.1:6379> get name (nil) |
| `MSET key1 value1 [key2 value2..]`   | 批量set键值对                                                | 127.0.0.1:6379> MSET k1 v1 k2 v2 k3 v3 OK                    |
| `MSETNX key1 value1 [key2 value2..]` | 批量设置键值对，仅当参数中所有的key都不存在时执行            | 127.0.0.1:6379> MSETNX k1 v1 k4 v4 (integer) 0               |
| `MGET key1 [key2..]`                 | 批量获取多个key保存的值                                      | 127.0.0.1:6379> MGET k1 k2 k3 1) “v1” 2) “v2” 3) “v3”        |
| `PSETEX key milliseconds value`      | 和 SETEX 命令相似，但它以毫秒为单位设置 key 的生存时间，     |                                                              |
| `getset key value`                   | 如果不存在值，则返回nil，如果存在值，获取原来的值，并设置新的值 |                                                              |

String类似的使用场景：value除了是字符串还可以是数字，用途举例：

* 计数器
* 统计多单位的数量：uid:123666：follow 0
* 粉丝数
* 对象存储缓存

### List(列表)

> Redis列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边）
>
> 一个列表最多可以包含 232 - 1 个元素 (4294967295, 每个列表超过40亿个元素)。

首先我们列表，可以经过规则定义将其变为队列、栈、双端队列等

![\[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(0\_Redis概述及使用.assets/20200820104440398.png)(狂神说 0\_Redis概述及使用.assets/image-20200813114255459.png)\]](/assets/20200820104440398.DYMDelVp.png)

正如图Redis中List是可以进行双端操作的，所以命令也就分为了LXXX和RLLL两类，有时候L也表示List例如LLEN

| 命令                                    | 描述                                                         |
| --------------------------------------- | ------------------------------------------------------------ |
| `LPUSH/RPUSH key value1[value2..]`      | 从左边/右边向列表中PUSH值(一个或者多个)。                    |
| `LRANGE key start end`                  | 获取list 起止元素==（索引从左往右 递增）==                   |
| `LPUSHX/RPUSHX key value`               | 向已存在的列名中push值（一个或者多个）                       |
| `LINSERT key BEFORE|AFTER pivot value`  | 在指定列表元素的前/后 插入value                              |
| `LLEN key`                              | 查看列表长度                                                 |
| `LINDEX key index`                      | 通过索引获取列表元素                                         |
| `LSET key index value`                  | 通过索引为元素设值                                           |
| `LPOP/RPOP key`                         | 从最左边/最右边移除值 并返回                                 |
| `RPOPLPUSH source destination`          | 将列表的尾部(右)最后一个值弹出，并返回，然后加到另一个列表的头部 |
| `LTRIM key start end`                   | 通过下标截取指定范围内的列表                                 |
| `LREM key count value`                  | List中是允许value重复的 `count > 0`：从头部开始搜索 然后删除指定的value 至多删除count个 `count < 0`：从尾部开始搜索… `count = 0`：删除列表中所有的指定value。 |
| `BLPOP/BRPOP key1[key2] timout`         | 移出并获取列表的第一个/最后一个元素， 如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止。 |
| `BRPOPLPUSH source destination timeout` | 和`RPOPLPUSH`功能相同，如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止。 |

```bash
---------------------------LPUSH---RPUSH---LRANGE--------------------------------

127.0.0.1:6379> LPUSH mylist k1 # LPUSH mylist=>{1}
(integer) 1
127.0.0.1:6379> LPUSH mylist k2 # LPUSH mylist=>{2,1}
(integer) 2
127.0.0.1:6379> RPUSH mylist k3 # RPUSH mylist=>{2,1,3}
(integer) 3
127.0.0.1:6379> get mylist # 普通的get是无法获取list值的
(error) WRONGTYPE Operation against a key holding the wrong kind of value
127.0.0.1:6379> LRANGE mylist 0 4 # LRANGE 获取起止位置范围内的元素
1) "k2"
2) "k1"
3) "k3"
127.0.0.1:6379> LRANGE mylist 0 2
1) "k2"
2) "k1"
3) "k3"
127.0.0.1:6379> LRANGE mylist 0 1
1) "k2"
2) "k1"
127.0.0.1:6379> LRANGE mylist 0 -1 # 获取全部元素
1) "k2"
2) "k1"
3) "k3"

---------------------------LPUSHX---RPUSHX-----------------------------------

127.0.0.1:6379> LPUSHX list v1 # list不存在 LPUSHX失败
(integer) 0
127.0.0.1:6379> LPUSHX list v1 v2  
(integer) 0
127.0.0.1:6379> LPUSHX mylist k4 k5 # 向mylist中 左边 PUSH k4 k5
(integer) 5
127.0.0.1:6379> LRANGE mylist 0 -1
1) "k5"
2) "k4"
3) "k2"
4) "k1"
5) "k3"

---------------------------LINSERT--LLEN--LINDEX--LSET----------------------------

127.0.0.1:6379> LINSERT mylist after k2 ins_key1 # 在k2元素后 插入ins_key1
(integer) 6
127.0.0.1:6379> LRANGE mylist 0 -1
1) "k5"
2) "k4"
3) "k2"
4) "ins_key1"
5) "k1"
6) "k3"
127.0.0.1:6379> LLEN mylist # 查看mylist的长度
(integer) 6
127.0.0.1:6379> LINDEX mylist 3 # 获取下标为3的元素
"ins_key1"
127.0.0.1:6379> LINDEX mylist 0
"k5"
127.0.0.1:6379> LSET mylist 3 k6 # 将下标3的元素 set值为k6
OK
127.0.0.1:6379> LRANGE mylist 0 -1
1) "k5"
2) "k4"
3) "k2"
4) "k6"
5) "k1"
6) "k3"

---------------------------LPOP--RPOP--------------------------

127.0.0.1:6379> LPOP mylist # 左侧(头部)弹出
"k5"
127.0.0.1:6379> RPOP mylist # 右侧(尾部)弹出
"k3"

---------------------------RPOPLPUSH--------------------------

127.0.0.1:6379> LRANGE mylist 0 -1
1) "k4"
2) "k2"
3) "k6"
4) "k1"
127.0.0.1:6379> RPOPLPUSH mylist newlist # 将mylist的最后一个值(k1)弹出，加入到newlist的头部
"k1"
127.0.0.1:6379> LRANGE newlist 0 -1
1) "k1"
127.0.0.1:6379> LRANGE mylist 0 -1
1) "k4"
2) "k2"
3) "k6"

---------------------------LTRIM--------------------------

127.0.0.1:6379> LTRIM mylist 0 1 # 截取mylist中的 0~1部分
OK
127.0.0.1:6379> LRANGE mylist 0 -1
1) "k4"
2) "k2"

# 初始 mylist: k2,k2,k2,k2,k2,k2,k4,k2,k2,k2,k2
---------------------------LREM--------------------------

127.0.0.1:6379> LREM mylist 3 k2 # 从头部开始搜索 至多删除3个 k2
(integer) 3
# 删除后：mylist: k2,k2,k2,k4,k2,k2,k2,k2

127.0.0.1:6379> LREM mylist -2 k2 #从尾部开始搜索 至多删除2个 k2
(integer) 2
# 删除后：mylist: k2,k2,k2,k4,k2,k2


---------------------------BLPOP--BRPOP--------------------------

mylist: k2,k2,k2,k4,k2,k2
newlist: k1

127.0.0.1:6379> BLPOP newlist mylist 30 # 从newlist中弹出第一个值，mylist作为候选
1) "newlist" # 弹出
2) "k1"
127.0.0.1:6379> BLPOP newlist mylist 30
1) "mylist" # 由于newlist空了 从mylist中弹出
2) "k2"
127.0.0.1:6379> BLPOP newlist 30
(30.10s) # 超时了

127.0.0.1:6379> BLPOP newlist 30 # 我们连接另一个客户端向newlist中push了test, 阻塞被解决。
1) "newlist"
2) "test"
(12.54s)
```

> 小结

* list实际上是一个链表，before Node after , left, right 都可以插入值
* **如果key不存在，则创建新的链表**
* 如果key存在，新增内容
* 如果移除了所有值，空链表，也代表不存在
* 在两边插入或者改动值，效率最高！修改中间元素，效率相对较低

**应用：**

**消息排队！消息队列（Lpush Rpop）,栈（Lpush Lpop）**

### Set(集合)

> Redis的Set是**string类型**的无序集合。集合成员是唯一的，这就意味着集合中不能出现重复的数据。
>
> Redis 中 集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是O(1)。
>
> 集合中最大的成员数为 232 - 1 (4294967295, 每个集合可存储40多亿个成员)。

| 命令                                      | 描述                                                         |
| ----------------------------------------- | ------------------------------------------------------------ |
| `SADD key member1[member2..]`             | 向集合中无序增加一个/多个成员                                |
| `SCARD key`                               | 获取集合的成员数                                             |
| `SMEMBERS key`                            | 返回集合中所有的成员                                         |
| `SISMEMBER key member`                    | 查询member元素是否是集合的成员,结果是无序的                  |
| `SRANDMEMBER key [count]`                 | 随机返回集合中count个成员，count缺省值为1                    |
| `SPOP key [count]`                        | 随机移除并返回集合中count个成员，count缺省值为1              |
| `SMOVE source destination member`         | 将source集合的成员member移动到destination集合                |
| `SREM key member1[member2..]`             | 移除集合中一个/多个成员                                      |
| `SDIFF key1[key2..]`                      | 返回所有集合的差集 key1- key2 - …                            |
| `SDIFFSTORE destination key1[key2..]`     | 在SDIFF的基础上，将结果保存到集合中==(覆盖)==。不能保存到其他类型key噢！ |
| `SINTER key1 [key2..]`                    | 返回所有集合的交集                                           |
| `SINTERSTORE destination key1[key2..]`    | 在SINTER的基础上，存储结果到集合中。覆盖                     |
| `SUNION key1 [key2..]`                    | 返回所有集合的并集                                           |
| `SUNIONSTORE destination key1 [key2..]`   | 在SUNION的基础上，存储结果到及和张。覆盖                     |
| `SSCAN KEY [MATCH pattern] [COUNT count]` | 在大量数据环境下，使用此命令遍历集合中元素，每次遍历部分     |

```bash
---------------SADD--SCARD--SMEMBERS--SISMEMBER--------------------

127.0.0.1:6379> SADD myset m1 m2 m3 m4 # 向myset中增加成员 m1~m4
(integer) 4
127.0.0.1:6379> SCARD myset # 获取集合的成员数目
(integer) 4
127.0.0.1:6379> smembers myset # 获取集合中所有成员
1) "m4"
2) "m3"
3) "m2"
4) "m1"
127.0.0.1:6379> SISMEMBER myset m5 # 查询m5是否是myset的成员
(integer) 0 # 不是，返回0
127.0.0.1:6379> SISMEMBER myset m2
(integer) 1 # 是，返回1
127.0.0.1:6379> SISMEMBER myset m3
(integer) 1

---------------------SRANDMEMBER--SPOP----------------------------------

127.0.0.1:6379> SRANDMEMBER myset 3 # 随机返回3个成员
1) "m2"
2) "m3"
3) "m4"
127.0.0.1:6379> SRANDMEMBER myset # 随机返回1个成员
"m3"
127.0.0.1:6379> SPOP myset 2 # 随机移除并返回2个成员
1) "m1"
2) "m4"
# 将set还原到{m1,m2,m3,m4}

---------------------SMOVE--SREM----------------------------------------

127.0.0.1:6379> SMOVE myset newset m3 # 将myset中m3成员移动到newset集合
(integer) 1
127.0.0.1:6379> SMEMBERS myset
1) "m4"
2) "m2"
3) "m1"
127.0.0.1:6379> SMEMBERS newset
1) "m3"
127.0.0.1:6379> SREM newset m3 # 从newset中移除m3元素
(integer) 1
127.0.0.1:6379> SMEMBERS newset
(empty list or set)

# 下面开始是多集合操作,多集合操作中若只有一个参数默认和自身进行运算
# setx=>{m1,m2,m4,m6}, sety=>{m2,m5,m6}, setz=>{m1,m3,m6}

-----------------------------SDIFF------------------------------------

127.0.0.1:6379> SDIFF setx sety setz # 等价于setx-sety-setz
1) "m4"
127.0.0.1:6379> SDIFF setx sety # setx - sety
1) "m4"
2) "m1"
127.0.0.1:6379> SDIFF sety setx # sety - setx
1) "m5"


-------------------------SINTER---------------------------------------
# 共同关注（交集）

127.0.0.1:6379> SINTER setx sety setz # 求 setx、sety、setx的交集
1) "m6"
127.0.0.1:6379> SINTER setx sety # 求setx sety的交集
1) "m2"
2) "m6"

-------------------------SUNION---------------------------------------

127.0.0.1:6379> SUNION setx sety setz # setx sety setz的并集
1) "m4"
2) "m6"
3) "m3"
4) "m2"
5) "m1"
6) "m5"
127.0.0.1:6379> SUNION setx sety # setx sety 并集
1) "m4"
2) "m6"
3) "m2"
4) "m1"
5) "m5"
```

### Hash（哈希）

> Redis hash 是一个string类型的field和value的映射表，hash特别适合用于存储对象。
>
> Set就是一种简化的Hash,只变动key,而value使用默认值填充。可以将一个Hash表作为一个对象进行存储，表中存放对象的信息。

| 命令                                             | 描述                                                         |
| ------------------------------------------------ | ------------------------------------------------------------ |
| `HSET key field value`                           | 将哈希表 key 中的字段 field 的值设为 value 。重复设置同一个field会覆盖,返回0 |
| `HMSET key field1 value1 [field2 value2..]`      | 同时将多个 field-value (域-值)对设置到哈希表 key 中。        |
| `HSETNX key field value`                         | 只有在字段 field 不存在时，设置哈希表字段的值。              |
| `HEXISTS key field`                              | 查看哈希表 key 中，指定的字段是否存在。                      |
| `HGET key field value`                           | 获取存储在哈希表中指定字段的值                               |
| `HMGET key field1 [field2..]`                    | 获取所有给定字段的值                                         |
| `HGETALL key`                                    | 获取在哈希表key 的所有字段和值                               |
| `HKEYS key`                                      | 获取哈希表key中所有的字段                                    |
| `HLEN key`                                       | 获取哈希表中字段的数量                                       |
| `HVALS key`                                      | 获取哈希表中所有值                                           |
| `HDEL key field1 [field2..]`                     | 删除哈希表key中一个/多个field字段                            |
| `HINCRBY key field n`                            | 为哈希表 key 中的指定字段的整数值加上增量n，并返回增量后结果 一样只适用于整数型字段 |
| `HINCRBYFLOAT key field n`                       | 为哈希表 key 中的指定字段的浮点数值加上增量 n。              |
| `HSCAN key cursor [MATCH pattern] [COUNT count]` | 迭代哈希表中的键值对。                                       |

```bash
------------------------HSET--HMSET--HSETNX----------------
127.0.0.1:6379> HSET studentx name sakura # 将studentx哈希表作为一个对象，设置name为sakura
(integer) 1
127.0.0.1:6379> HSET studentx name gyc # 重复设置field进行覆盖，并返回0
(integer) 0
127.0.0.1:6379> HSET studentx age 20 # 设置studentx的age为20
(integer) 1
127.0.0.1:6379> HMSET studentx sex 1 tel 15623667886 # 设置sex为1，tel为15623667886
OK
127.0.0.1:6379> HSETNX studentx name gyc # HSETNX 设置已存在的field
(integer) 0 # 失败
127.0.0.1:6379> HSETNX studentx email 12345@qq.com
(integer) 1 # 成功

----------------------HEXISTS--------------------------------
127.0.0.1:6379> HEXISTS studentx name # name字段在studentx中是否存在
(integer) 1 # 存在
127.0.0.1:6379> HEXISTS studentx addr
(integer) 0 # 不存在

-------------------HGET--HMGET--HGETALL-----------
127.0.0.1:6379> HGET studentx name # 获取studentx中name字段的value
"gyc"
127.0.0.1:6379> HMGET studentx name age tel # 获取studentx中name、age、tel字段的value
1) "gyc"
2) "20"
3) "15623667886"
127.0.0.1:6379> HGETALL studentx # 获取studentx中所有的field及其value
 1) "name"
 2) "gyc"
 3) "age"
 4) "20"
 5) "sex"
 6) "1"
 7) "tel"
 8) "15623667886"
 9) "email"
10) "12345@qq.com"


--------------------HKEYS--HLEN--HVALS--------------
127.0.0.1:6379> HKEYS studentx # 查看studentx中所有的field
1) "name"
2) "age"
3) "sex"
4) "tel"
5) "email"
127.0.0.1:6379> HLEN studentx # 查看studentx中的字段数量
(integer) 5
127.0.0.1:6379> HVALS studentx # 查看studentx中所有的value
1) "gyc"
2) "20"
3) "1"
4) "15623667886"
5) "12345@qq.com"

-------------------------HDEL--------------------------
127.0.0.1:6379> HDEL studentx sex tel # 删除studentx 中的sex、tel字段
(integer) 2
127.0.0.1:6379> HKEYS studentx
1) "name"
2) "age"
3) "email"

-------------HINCRBY--HINCRBYFLOAT------------------------
127.0.0.1:6379> HINCRBY studentx age 1 # studentx的age字段数值+1
(integer) 21
127.0.0.1:6379> HINCRBY studentx name 1 # 非整数字型字段不可用
(error) ERR hash value is not an integer
127.0.0.1:6379> HINCRBYFLOAT studentx weight 0.6 # weight字段增加0.6
"90.8"
```

Hash变更的数据user name age，尤其是用户信息之类的，经常变动的信息！**Hash更适合于对象的存储，Sring更加适合字符串存储！**

### Zset（有序集合）

> 不同的是每个元素都会关联一个double类型的分数（score）。redis正是通过分数来为集合中的成员进行从小到大的排序。
>
> score相同：按字典顺序排序
>
> 有序集合的成员是唯一的,但分数(score)却可以重复。

| 命令                                              | 描述                                                         |
| ------------------------------------------------- | ------------------------------------------------------------ |
| `ZADD key score member1 [score2 member2]`         | 向有序集合添加一个或多个成员，或者更新已存在成员的分数       |
| `ZCARD key`                                       | 获取有序集合的成员数                                         |
| `ZCOUNT key min max`                              | 计算在有序集合中指定区间score的成员数                        |
| `ZINCRBY key n member`                            | 有序集合中对指定成员的分数加上增量 n                         |
| `ZSCORE key member`                               | 返回有序集中，成员的分数值                                   |
| `ZRANK key member`                                | 返回有序集合中指定成员的索引                                 |
| `ZRANGE key start end`                            | 通过索引区间返回有序集合成指定区间内的成员                   |
| `ZRANGEBYLEX key min max`                         | 通过字典区间返回有序集合的成员                               |
| `ZRANGEBYSCORE key min max`                       | 通过分数返回有序集合指定区间内的成员==-inf 和 +inf分别表示最小最大值，只支持开区间()== |
| `ZLEXCOUNT key min max`                           | 在有序集合中计算指定字典区间内成员数量                       |
| `ZREM key member1 [member2..]`                    | 移除有序集合中一个/多个成员                                  |
| `ZREMRANGEBYLEX key min max`                      | 移除有序集合中给定的字典区间的所有成员                       |
| `ZREMRANGEBYRANK key start stop`                  | 移除有序集合中给定的排名区间的所有成员                       |
| `ZREMRANGEBYSCORE key min max`                    | 移除有序集合中给定的分数区间的所有成员                       |
| `ZREVRANGE key start end`                         | 返回有序集中指定区间内的成员，通过索引，分数从高到底         |
| `ZREVRANGEBYSCORRE key max min`                   | 返回有序集中指定分数区间内的成员，分数从高到低排序           |
| `ZREVRANGEBYLEX key max min`                      | 返回有序集中指定字典区间内的成员，按字典顺序倒序             |
| `ZREVRANK key member`                             | 返回有序集合中指定成员的排名，有序集成员按分数值递减(从大到小)排序 |
| `ZINTERSTORE destination numkeys key1 [key2 ..]`  | 计算给定的一个或多个有序集的交集并将结果集存储在新的有序集合 key 中，numkeys：表示参与运算的集合数，将score相加作为结果的score |
| `ZUNIONSTORE destination numkeys key1 [key2..]`   | 计算给定的一个或多个有序集的交集并将结果集存储在新的有序集合 key 中 |
| `ZSCAN key cursor [MATCH pattern\] [COUNT count]` | 迭代有序集合中的元素（包括元素成员和元素分值）               |

```bash
-------------------ZADD--ZCARD--ZCOUNT--------------
127.0.0.1:6379> ZADD myzset 1 m1 2 m2 3 m3 # 向有序集合myzset中添加成员m1 score=1 以及成员m2 score=2..
(integer) 2
127.0.0.1:6379> ZCARD myzset # 获取有序集合的成员数
(integer) 2
127.0.0.1:6379> ZCOUNT myzset 0 1 # 获取score在 [0,1]区间的成员数量
(integer) 1
127.0.0.1:6379> ZCOUNT myzset 0 2
(integer) 2

----------------ZINCRBY--ZSCORE--------------------------
127.0.0.1:6379> ZINCRBY myzset 5 m2 # 将成员m2的score +5
"7"
127.0.0.1:6379> ZSCORE myzset m1 # 获取成员m1的score
"1"
127.0.0.1:6379> ZSCORE myzset m2
"7"

--------------ZRANK--ZRANGE-----------------------------------
127.0.0.1:6379> ZRANK myzset m1 # 获取成员m1的索引，索引按照score排序，score相同索引值按字典顺序顺序增加
(integer) 0
127.0.0.1:6379> ZRANK myzset m2
(integer) 2
127.0.0.1:6379> ZRANGE myzset 0 1 # 获取索引在 0~1的成员
1) "m1"
2) "m3"
127.0.0.1:6379> ZRANGE myzset 0 -1 # 获取全部成员
1) "m1"
2) "m3"
3) "m2"

#testset=>{abc,add,amaze,apple,back,java,redis} score均为0
------------------ZRANGEBYLEX---------------------------------
127.0.0.1:6379> ZRANGEBYLEX testset - + # 返回所有成员
1) "abc"
2) "add"
3) "amaze"
4) "apple"
5) "back"
6) "java"
7) "redis"
127.0.0.1:6379> ZRANGEBYLEX testset - + LIMIT 0 3 # 分页 按索引显示查询结果的 0,1,2条记录
1) "abc"
2) "add"
3) "amaze"
127.0.0.1:6379> ZRANGEBYLEX testset - + LIMIT 3 3 # 显示 3,4,5条记录
1) "apple"
2) "back"
3) "java"
127.0.0.1:6379> ZRANGEBYLEX testset (- [apple # 显示 (-,apple] 区间内的成员
1) "abc"
2) "add"
3) "amaze"
4) "apple"
127.0.0.1:6379> ZRANGEBYLEX testset [apple [java # 显示 [apple,java]字典区间的成员
1) "apple"
2) "back"
3) "java"

-----------------------ZRANGEBYSCORE---------------------
127.0.0.1:6379> ZRANGEBYSCORE myzset 1 10 # 返回score在 [1,10]之间的的成员
1) "m1"
2) "m3"
3) "m2"
127.0.0.1:6379> ZRANGEBYSCORE myzset 1 5
1) "m1"
2) "m3"

--------------------ZLEXCOUNT-----------------------------
127.0.0.1:6379> ZLEXCOUNT testset - +
(integer) 7
127.0.0.1:6379> ZLEXCOUNT testset [apple [java
(integer) 3

------------------ZREM--ZREMRANGEBYLEX--ZREMRANGBYRANK--ZREMRANGEBYSCORE--------------------------------
127.0.0.1:6379> ZREM testset abc # 移除成员abc
(integer) 1
127.0.0.1:6379> ZREMRANGEBYLEX testset [apple [java # 移除字典区间[apple,java]中的所有成员
(integer) 3
127.0.0.1:6379> ZREMRANGEBYRANK testset 0 1 # 移除排名0~1的所有成员
(integer) 2
127.0.0.1:6379> ZREMRANGEBYSCORE myzset 0 3 # 移除score在 [0,3]的成员
(integer) 2


# testset=> {abc,add,apple,amaze,back,java,redis} score均为0
# myzset=> {(m1,1),(m2,2),(m3,3),(m4,4),(m7,7),(m9,9)}
----------------ZREVRANGE--ZREVRANGEBYSCORE--ZREVRANGEBYLEX-----------
127.0.0.1:6379> ZREVRANGE myzset 0 3 # 按score递减排序，然后按索引，返回结果的 0~3
1) "m9"
2) "m7"
3) "m4"
4) "m3"
127.0.0.1:6379> ZREVRANGE myzset 2 4 # 返回排序结果的 索引的2~4
1) "m4"
2) "m3"
3) "m2"
127.0.0.1:6379> ZREVRANGEBYSCORE myzset 6 2 # 按score递减顺序 返回集合中分数在[2,6]之间的成员
1) "m4"
2) "m3"
3) "m2"
127.0.0.1:6379> ZREVRANGEBYLEX testset [java (add # 按字典倒序 返回集合中(add,java]字典区间的成员
1) "java"
2) "back"
3) "apple"
4) "amaze"

-------------------------ZREVRANK------------------------------
127.0.0.1:6379> ZREVRANK myzset m7 # 按score递减顺序，返回成员m7索引
(integer) 1
127.0.0.1:6379> ZREVRANK myzset m2
(integer) 4


# mathscore=>{(xm,90),(xh,95),(xg,87)} 小明、小红、小刚的数学成绩
# enscore=>{(xm,70),(xh,93),(xg,90)} 小明、小红、小刚的英语成绩
-------------------ZINTERSTORE--ZUNIONSTORE-----------------------------------
127.0.0.1:6379> ZINTERSTORE sumscore 2 mathscore enscore # 将mathscore enscore进行合并 结果存放到sumscore
(integer) 3
127.0.0.1:6379> ZRANGE sumscore 0 -1 withscores # 合并后的score是之前集合中所有score的和
1) "xm"
2) "160"
3) "xg"
4) "177"
5) "xh"
6) "188"

127.0.0.1:6379> ZUNIONSTORE lowestscore 2 mathscore enscore AGGREGATE MIN # 取两个集合的成员score最小值作为结果的
(integer) 3
127.0.0.1:6379> ZRANGE lowestscore 0 -1 withscores
1) "xm"
2) "70"
3) "xg"
4) "87"
5) "xh"
6) "93"
```

应用案例：

* set排序 存储班级成绩表 工资表排序！
* 普通消息，1.重要消息 2.带权重进行判断
* 排行榜应用实现，取Top N测试

## 四、三种特殊数据类型

### Geospatial(地理位置)

> 使用经纬度定位地理坐标并用一个**有序集合zset保存**，所以zset命令也可以使用

| 命令                                                         | 描述                                                         |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| `geoadd key longitud(经度) latitude(纬度) member [..]`       | 将具体经纬度的坐标存入一个有序集合                           |
| `geopos key member [member..]`                               | 获取集合中的一个/多个成员坐标                                |
| `geodist key member1 member2 [unit]`                         | 返回两个给定位置之间的距离。默认以米作为单位。               |
| `georadius key longitude latitude radius m|km|mi|ft [WITHCOORD][WITHDIST] [WITHHASH] [COUNT count]` | 以给定的经纬度为中心， 返回集合包含的位置元素当中， 与中心的距离不超过给定最大距离的所有位置元素。 |
| `GEORADIUSBYMEMBER key member radius...`                     | 功能与GEORADIUS相同，只是中心位置不是具体的经纬度，而是使用结合中已有的成员作为中心点。 |
| `geohash key member1 [member2..]`                            | 返回一个或多个位置元素的Geohash表示。使用Geohash位置52点整数编码。 |

**有效经纬度**

> * 有效的经度从-180度到180度。
> * 有效的纬度从-85.05112878度到85.05112878度。

指定单位的参数 **unit** 必须是以下单位的其中一个：

* **m** 表示单位为米。
* **km** 表示单位为千米。
* **mi** 表示单位为英里。
* **ft** 表示单位为英尺。

**关于GEORADIUS的参数**

> 通过`georadius`就可以完成 **附近的人**功能
>
> withcoord:带上坐标
>
> withdist:带上距离，单位与半径单位相同
>
> COUNT n : 只显示前n个(按距离递增排序)

```bash
----------------georadius---------------------
127.0.0.1:6379> GEORADIUS china:city 120 30 500 km withcoord withdist # 查询经纬度(120,30)坐标500km半径内的成员
1) 1) "hangzhou"
   2) "29.4151"
   3) 1) "120.20000249147415"
      2) "30.199999888333501"
2) 1) "shanghai"
   2) "205.3611"
   3) 1) "121.40000134706497"
      2) "31.400000253193539"
     
------------geohash---------------------------
127.0.0.1:6379> geohash china:city yichang shanghai # 获取成员经纬坐标的geohash表示
1) "wmrjwbr5250"
2) "wtw6ds0y300"
```

### Hyperloglog(基数统计)

> Redis HyperLogLog 是用来做基数统计的算法，HyperLogLog 的优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定的、并且是很小的。
>
> 花费 12 KB 内存，就可以计算接近 2^64 个不同元素的基数。
>
> 因为 HyperLogLog 只会根据输入元素来计算基数，而不会储存输入元素本身，所以 HyperLogLog 不能像集合那样，返回输入的各个元素。
>
> 其底层使用string数据类型

**什么是基数？**

> 数据集中不重复的元素的个数。

**应用场景：**

网页的访问量（UV）：一个用户多次访问，也只能算作一个人。

> 传统实现，存储用户的id,然后每次进行比较。当用户变多之后这种方式及其浪费空间，而我们的目的只是**计数**，Hyperloglog就能帮助我们利用最小的空间完成。

| 命令                                      | 描述                                      |
| ----------------------------------------- | ----------------------------------------- |
| `PFADD key element1 [elememt2..]`         | 添加指定元素到 HyperLogLog 中             |
| `PFCOUNT key [key]`                       | 返回给定 HyperLogLog 的基数估算值。       |
| `PFMERGE destkey sourcekey [sourcekey..]` | 将多个 HyperLogLog 合并为一个 HyperLogLog |

```bash
----------PFADD--PFCOUNT---------------------
127.0.0.1:6379> PFADD myelemx a b c d e f g h i j k # 添加元素
(integer) 1
127.0.0.1:6379> type myelemx # hyperloglog底层使用String
string
127.0.0.1:6379> PFCOUNT myelemx # 估算myelemx的基数
(integer) 11
127.0.0.1:6379> PFADD myelemy i j k z m c b v p q s
(integer) 1
127.0.0.1:6379> PFCOUNT myelemy
(integer) 11

----------------PFMERGE-----------------------
127.0.0.1:6379> PFMERGE myelemz myelemx myelemy # 合并myelemx和myelemy 成为myelemz
OK
127.0.0.1:6379> PFCOUNT myelemz # 估算基数
(integer) 17
```

如果允许容错，那么一定可以使用Hyperloglog !

如果不允许容错，就使用set或者自己的数据类型即可 ！

### BitMaps(位图)

> 使用位存储，信息状态只有 0 和 1
>
> Bitmap是一串连续的2进制数字（0或1），每一位所在的位置为偏移(offset)，在bitmap上可执行AND,OR,XOR,NOT以及其它位操作。

**应用场景**

签到统计、状态统计

| 命令                                  | 描述                                                         |
| ------------------------------------- | ------------------------------------------------------------ |
| `setbit key offset value`             | 为指定key的offset位设置值                                    |
| `getbit key offset`                   | 获取offset位的值                                             |
| `bitcount key [start end]`            | 统计字符串被设置为1的bit数，也可以指定统计范围按字节         |
| `bitop operration destkey key[key..]` | 对一个或多个保存二进制位的字符串 key 进行位元操作，并将结果保存到 destkey 上。 |
| `BITPOS key bit [start] [end]`        | 返回字符串里面第一个被设置为1或者0的bit位。start和end只能按字节,不能按位 |

```bash
------------setbit--getbit--------------
127.0.0.1:6379> setbit sign 0 1 # 设置sign的第0位为 1 
(integer) 0
127.0.0.1:6379> setbit sign 2 1 # 设置sign的第2位为 1  不设置默认 是0
(integer) 0
127.0.0.1:6379> setbit sign 3 1
(integer) 0
127.0.0.1:6379> setbit sign 5 1
(integer) 0
127.0.0.1:6379> type sign
string

127.0.0.1:6379> getbit sign 2 # 获取第2位的数值
(integer) 1
127.0.0.1:6379> getbit sign 3
(integer) 1
127.0.0.1:6379> getbit sign 4 # 未设置默认是0
(integer) 0

-----------bitcount----------------------------
127.0.0.1:6379> BITCOUNT sign # 统计sign中为1的位数
(integer) 4
```

**bitmaps的底层**

\[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-9PlszjhS-1597890996519)(D:\我\MyBlog\狂神说 0\_Redis概述及使用.assets\image-20200803234336175.png)]

这样设置以后你能get到的值是：**\xA2\x80**，所以bitmaps是一串从左到右的二进制串

## 五、事务

Redis的单条命令是保证原子性的，但是redis事务不能保证原子性

> Redis事务本质：一组命令的集合。
>
> \----------------- 队列 set set set 执行 -------------------
>
> 事务中每条命令都会被序列化，执行过程中按顺序执行，不允许其他命令进行干扰。
>
> * 一次性
> * 顺序性
> * 排他性
>
> ***
>
> 1. Redis事务没有隔离级别的概念
> 2. Redis单条命令是保证原子性的，但是事务不保证原子性！

### Redis事务操作过程

* 开启事务（`multi`）
* 命令入队
* 执行事务（`exec`）

所以事务中的命令在加入时都没有被执行，直到提交时才会开始执行(Exec)一次性完成。

```bash
127.0.0.1:6379> multi # 开启事务
OK
127.0.0.1:6379> set k1 v1 # 命令入队
QUEUED
127.0.0.1:6379> set k2 v2 # ..
QUEUED
127.0.0.1:6379> get k1
QUEUED
127.0.0.1:6379> set k3 v3
QUEUED
127.0.0.1:6379> keys *
QUEUED
127.0.0.1:6379> exec # 事务执行
1) OK
2) OK
3) "v1"
4) OK
5) 1) "k3"
   2) "k2"
   3) "k1"
```

**取消事务(`discurd`)**

```bash
127.0.0.1:6379> multi
OK
127.0.0.1:6379> set k1 v1
QUEUED
127.0.0.1:6379> set k2 v2
QUEUED
127.0.0.1:6379> DISCARD # 放弃事务
OK
127.0.0.1:6379> EXEC 
(error) ERR EXEC without MULTI # 当前未开启事务
127.0.0.1:6379> get k1 # 被放弃事务中命令并未执行
(nil)
```

### 事务错误

> 代码语法错误（编译时异常）所有的命令都不执行

```bash
127.0.0.1:6379> multi
OK
127.0.0.1:6379> set k1 v1
QUEUED
127.0.0.1:6379> set k2 v2
QUEUED
127.0.0.1:6379> error k1 # 这是一条语法错误命令
(error) ERR unknown command `error`, with args beginning with: `k1`, # 会报错但是不影响后续命令入队 
127.0.0.1:6379> get k2
QUEUED
127.0.0.1:6379> EXEC
(error) EXECABORT Transaction discarded because of previous errors. # 执行报错
127.0.0.1:6379> get k1 
(nil) # 其他命令并没有被执行
```

> 代码逻辑错误 (运行时异常) \*\*其他命令可以正常执行 \*\* >>> 所以不保证事务原子性

```bash
127.0.0.1:6379> multi
OK
127.0.0.1:6379> set k1 v1
QUEUED
127.0.0.1:6379> set k2 v2
QUEUED
127.0.0.1:6379> INCR k1 # 这条命令逻辑错误（对字符串进行增量）
QUEUED
127.0.0.1:6379> get k2
QUEUED
127.0.0.1:6379> exec
1) OK
2) OK
3) (error) ERR value is not an integer or out of range # 运行时报错
4) "v2" # 其他命令正常执行

# 虽然中间有一条命令报错了，但是后面的指令依旧正常执行成功了。
# 所以说Redis单条指令保证原子性，但是Redis事务不能保证原子性。
```

### 监控

**悲观锁：**

* 很悲观，认为什么时候都会出现问题，无论做什么都会加锁

**乐观锁：**

* 很乐观，认为什么时候都不会出现问题，所以不会上锁！更新数据的时候去判断一下，在此期间是否有人修改过这个数据
* 获取version
* 更新的时候比较version

使用`watch key`监控指定数据，相当于乐观锁加锁。

> 正常执行

```bash
127.0.0.1:6379> set money 100 # 设置余额:100
OK
127.0.0.1:6379> set use 0 # 支出使用:0
OK
127.0.0.1:6379> watch money # 监视money (上锁)
OK
127.0.0.1:6379> multi
OK
127.0.0.1:6379> DECRBY money 20
QUEUED
127.0.0.1:6379> INCRBY use 20
QUEUED
127.0.0.1:6379> exec # 监视值没有被中途修改，事务正常执行
1) (integer) 80
2) (integer) 20
```

> 测试多线程修改值，使用watch可以当做redis的乐观锁操作（相当于getversion）

我们启动另外一个客户端模拟插队线程。

线程1：

```bash
127.0.0.1:6379> watch money # money上锁
OK
127.0.0.1:6379> multi
OK
127.0.0.1:6379> DECRBY money 20
QUEUED
127.0.0.1:6379> INCRBY use 20
QUEUED
127.0.0.1:6379> 	# 此时事务并没有执行
```

模拟线程插队，线程2：

```bash
127.0.0.1:6379> INCRBY money 500 # 修改了线程一中监视的money
(integer) 600
12
```

回到线程1，执行事务：

```bash
127.0.0.1:6379> EXEC # 执行之前，另一个线程修改了我们的值，这个时候就会导致事务执行失败
(nil) # 没有结果，说明事务执行失败

127.0.0.1:6379> get money # 线程2 修改生效
"600"
127.0.0.1:6379> get use # 线程1事务执行失败，数值没有被修改
"0"
```

> 解锁获取最新值，然后再加锁进行事务。
>
> `unwatch`进行解锁。

注意：每次提交执行exec后都会自动释放锁，不管是否成功

## 六、Jedis

使用Java来操作Redis，Jedis是Redis官方推荐使用的Java连接redis的客户端。

1. 导入依赖

   ```xml
   <!--导入jredis的包-->
   <dependency>
       <groupId>redis.clients</groupId>
       <artifactId>jedis</artifactId>
       <version>3.2.0</version>
   </dependency>
   <!--fastjson-->
   <dependency>
       <groupId>com.alibaba</groupId>
       <artifactId>fastjson</artifactId>
       <version>1.2.70</version>
   </dependency>
   ```

2. 编码测试

   * 连接数据库

     1. 修改redis的配置文件

        ```bash
        vim /usr/local/bin/myconfig/redis.conf
        1
        ```

        1. 将只绑定本地注释

           \[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-4IRUFJ95-1597890996520)(狂神说 0\_Redis概述及使用.assets/image-20200813161921480.png)]

        2. 保护模式改为 no

           \[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-oKjIVapw-1597890996521)(狂神说 0\_Redis概述及使用.assets/image-20200813161939847.png)]

        3. 允许后台运行

           \[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-c2IMvpZL-1597890996522)(狂神说 0\_Redis概述及使用.assets/image-20200813161954567.png)]

3. 开放端口6379

   ```bash
   firewall-cmd --zone=public --add-port=6379/tcp --permanet
   1
   ```

   重启防火墙服务

   ```bash
   systemctl restart firewalld.service
   1
   ```

   1. 阿里云服务器控制台配置安全组

   2. 重启redis-server

      ```bash
      [root@AlibabaECS bin]# redis-server myconfig/redis.conf 
      1
      ```

* 操作命令

  **TestPing.java**

  ```java
  public class TestPing {
      public static void main(String[] args) {
          Jedis jedis = new Jedis("192.168.xx.xxx", 6379);
          String response = jedis.ping();
          System.out.println(response); // PONG
      }
  }
  ```

* 断开连接

1. **事务**

   ```java
   public class TestTX {
       public static void main(String[] args) {
           Jedis jedis = new Jedis("39.99.xxx.xx", 6379);

           JSONObject jsonObject = new JSONObject();
           jsonObject.put("hello", "world");
           jsonObject.put("name", "kuangshen");
           // 开启事务
           Transaction multi = jedis.multi();
           String result = jsonObject.toJSONString();
           // jedis.watch(result)
           try {
               multi.set("user1", result);
               multi.set("user2", result);
               // 执行事务
               multi.exec();
           }catch (Exception e){
               // 放弃事务
               multi.discard();
           } finally {
               // 关闭连接
               System.out.println(jedis.get("user1"));
               System.out.println(jedis.get("user2"));
               jedis.close();
           }
       }
   }
   ```

## 七、SpringBoot整合

1. 导入依赖

```xml
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-data-redis</artifactId>
</dependency>
```

springboot 2.x后 ，原来使用的 Jedis 被 lettuce 替换。

> jedis：采用的直连，多个线程操作的话，是不安全的。如果要避免不安全，使用jedis pool连接池！更像BIO模式
>
> lettuce：采用netty，实例可以在多个线程中共享，不存在线程不安全的情况！可以减少线程数据了，更像NIO模式

我们在学习SpringBoot自动配置的原理时，整合一个组件并进行配置一定会有一个自动配置类xxxAutoConfiguration,并且在spring.factories中也一定能找到这个类的完全限定名。Redis也不例外。

![在这里插入图片描述](/assets/20200513214531573.CA9eg_RU.png)

那么就一定还存在一个RedisProperties类

![在这里插入图片描述](/assets/20200513214554661.BHpe8GiA.png)

之前我们说SpringBoot2.x后默认使用Lettuce来替换Jedis，现在我们就能来验证了。

先看Jedis:

![在这里插入图片描述](/assets/20200513214607475.BKnhnk7a.png)

@ConditionalOnClass注解中有两个类是默认不存在的，所以Jedis是无法生效的

然后再看Lettuce：

![在这里插入图片描述](/assets/20200513214618179.GdQEOwyU.png)

完美生效。

现在我们回到RedisAutoConfiguratio

![img](/assets/2020051321462777.CrRYMOJ4.png)

只有两个简单的Bean

* **RedisTemplate**
* **StringRedisTemplate**

当看到xxTemplate时可以对比RestTemplat、SqlSessionTemplate,通过使用这些Template来间接操作组件。那么这俩也不会例外。分别用于操作Redis和Redis中的String数据类型。

在RedisTemplate上也有一个条件注解，说明我们是可以对其进行定制化的

说完这些，我们需要知道如何编写配置文件然后连接Redis，就需要阅读RedisProperties

![在这里插入图片描述](/assets/20200513214638238.DsGhuNRJ.png)

这是一些基本的配置属性。

![在这里插入图片描述](/assets/20200513214649380.FGwPs--Z.png)

还有一些连接池相关的配置。注意使用时一定使用Lettuce的连接池。

![在这里插入图片描述](/assets/20200513214700372.Boh2iWJr.png)

1. 编写配置文件

   ```properties
   # 配置redis
   spring.redis.host=39.99.xxx.xx
   spring.redis.port=6379
   ```

2. 使用RedisTemplate

   ```java
   @SpringBootTest
   class Redis02SpringbootApplicationTests {

       @Autowired
       private RedisTemplate redisTemplate;

       @Test
       void contextLoads() {

           // redisTemplate 操作不同的数据类型，api和我们的指令是一样的
           // opsForValue 操作字符串 类似String
           // opsForList 操作List 类似List
           // opsForHah

           // 除了基本的操作，我们常用的方法都可以直接通过redisTemplate操作，比如事务和基本的CRUD

           // 获取连接对象
           //RedisConnection connection = redisTemplate.getConnectionFactory().getConnection();
           //connection.flushDb();
           //connection.flushAll();

           redisTemplate.opsForValue().set("mykey","kuangshen");
           System.out.println(redisTemplate.opsForValue().get("mykey"));
       }
   }
   ```

3. 测试结果

   **此时我们回到Redis查看数据时候，惊奇发现全是乱码，可是程序中可以正常输出：**

   ![在这里插入图片描述](/assets/20200513214734520.BUGRtm-q.png)

   这时候就关系到存储对象的序列化问题，在网络中传输的对象也是一样需要序列化，否者就全是乱码。

   我们转到看那个默认的RedisTemplate内部什么样子：

   ![在这里插入图片描述](/assets/20200513214746506.B8dyh59z.png)

   在最开始就能看到几个关于序列化的参数。

   默认的序列化器是采用JDK序列化器

   ![在这里插入图片描述](/assets/20200513214757247.BsUeulRU.png)

   而默认的RedisTemplate中的所有序列化器都是使用这个序列化器：

   ![在这里插入图片描述](/assets/20200513214809494.Cl1U2rOT.png)

   后续我们定制RedisTemplate就可以对其进行修改。

   `RedisSerializer`提供了多种序列化方案：

   * 直接调用RedisSerializer的静态方法来返回序列化器，然后set

     ![在这里插入图片描述](/assets/20200513214818682.BGEDXJTm.png)

   * 自己new 相应的实现类，然后set

     ![在这里插入图片描述](/assets/20200513214827233.Cslag359.png)

4. **定制RedisTemplate的模板：**

   我们创建一个Bean加入容器，就会触发RedisTemplate上的条件注解使默认的RedisTemplate失效。

   ```java
   @Configuration
   public class RedisConfig {

      @Bean
       public RedisTemplate<String, Object> redisTemplate(RedisConnectionFactory redisConnectionFactory) throws UnknownHostException {
           // 将template 泛型设置为 <String, Object>
           RedisTemplate<String, Object> template = new RedisTemplate();
           // 连接工厂，不必修改
           template.setConnectionFactory(redisConnectionFactory);
           /*
            * 序列化设置
            */
           // key、hash的key 采用 String序列化方式
           template.setKeySerializer(RedisSerializer.string());
           template.setHashKeySerializer(RedisSerializer.string());
           // value、hash的value 采用 Jackson 序列化方式
           template.setValueSerializer(RedisSerializer.json());
           template.setHashValueSerializer(RedisSerializer.json());
           template.afterPropertiesSet();
           
           return template;
       }
   }
   ```

   这样一来，只要实体类进行了序列化，我们存什么都不会有乱码的担忧了。

   \[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-oc8kJP08-1597890996523)(狂神说 0\_Redis概述及使用.assets/image-20200817175638086.png)]

## 八、自定义Redis工具类

使用RedisTemplate需要频繁调用`.opForxxx`然后才能进行对应的操作，这样使用起来代码效率低下，工作中一般不会这样使用，而是将这些常用的公共API抽取出来封装成为一个工具类，然后直接使用工具类来间接操作Redis,不但效率高并且易用。

工具类参考博客：

https://www.cnblogs.com/zeng1994/p/03303c805731afc9aa9c60dbbd32a323.html

https://www.cnblogs.com/zhzhlong/p/11434284.html

## 九、Redis.conf

> 容量单位不区分大小写，G和GB有区别

![img](/assets/2020051321485460.lUCkEOlk.png)

> 可以使用 include 组合多个配置问题

![在这里插入图片描述](/assets/20200513214902552.LakiL0UI.png)

> 网络配置

![在这里插入图片描述](/assets/20200513214912813.CzmZZAPW.png)

> 日志输出级别

![img](/assets/20200513214923678.Dg8BE_vA.png)

> 日志输出文件

![在这里插入图片描述](/assets/20200513214933713.6sOX8pP7.png)

> 持久化规则

由于Redis是基于内存的数据库，需要将数据由内存持久化到文件中

持久化方式：

* RDB
* AOF

![在这里插入图片描述](/assets/20200513214944964.DWWTFOyX.png)

> RDB文件相关

![在这里插入图片描述](/assets/20200513214955679.OzcvBsvL.png)

![在这里插入图片描述](/assets/20200513215006207.DmVtf4qs.png)

> 主从复制

![在这里插入图片描述](/assets/20200513215016371.DbuGrsSW.png)

> Security模块中进行密码设置

![在这里插入图片描述](/assets/20200513215026143.DlWMLYSY.png)

> 客户端连接相关

```bash
maxclients 10000  最大客户端数量
maxmemory <bytes> 最大内存限制
maxmemory-policy noeviction # 内存达到限制值的处理策略
```

redis 中的**默认**的过期策略是 **volatile-lru** 。

**设置方式**

```bash
config set maxmemory-policy volatile-lru 
1
```

#### **maxmemory-policy 六种方式**

\*\*1、volatile-lru：\*\*只对设置了过期时间的key进行LRU（默认值）

**2、allkeys-lru ：** 删除lru算法的key

\*\*3、volatile-random：\*\*随机删除即将过期key

\*\*4、allkeys-random：\*\*随机删除

**5、volatile-ttl ：** 删除即将过期的

**6、noeviction ：** 永不过期，返回错误

> AOF相关部分

![在这里插入图片描述](/assets/20200513215037918.BO7r8TqF.png)

![在这里插入图片描述](/assets/20200513215047999.D8hTquxx.png)

## 十、持久化—RDB

RDB：Redis Databases

\[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-C0mm1D4A-1597890996524)(狂神说 0\_Redis概述及使用.assets/image-20200818122236614.png)]

### 什么是RDB

***

在指定时间间隔后，将内存中的数据集快照写入数据库 ；在恢复时候，直接读取快照文件，进行数据的恢复 ；

![在这里插入图片描述](/assets/20200513215126515.DGzuHfdM.jpg)

默认情况下， Redis 将数据库快照保存在名字为 dump.rdb的二进制文件中。文件名可以在配置文件中进行自定义。

### 工作原理

***

在进行 **`RDB`** 的时候，**`redis`** 的主线程是不会做 **`io`** 操作的，主线程会 **`fork`** 一个子线程来完成该操作；

1. Redis 调用forks。同时拥有父进程和子进程。
2. 子进程将数据集写入到一个临时 RDB 文件中。
3. 当子进程完成对新 RDB 文件的写入时，Redis 用新 RDB 文件替换原来的 RDB 文件，并删除旧的 RDB 文件。

这种工作方式使得 Redis 可以从写时复制（copy-on-write）机制中获益(因为是使用子进程进行写操作，而父进程依然可以接收来自客户端的请求。)

![在这里插入图片描述](0_Redis概述及使用.assets/20200513215141519.jpg)

### 触发机制

***

1. save的规则满足的情况下，会自动触发rdb原则
2. 执行flushall命令，也会触发我们的rdb原则
3. 退出redis，也会自动产生rdb文件

#### save

使用 `save` 命令，会立刻对当前内存中的数据进行持久化 ,但是会阻塞，也就是不接受其他操作了；

> 由于 `save` 命令是同步命令，会占用Redis的主进程。若Redis数据非常多时，`save`命令执行速度会非常慢，阻塞所有客户端的请求。

![在这里插入图片描述](/assets/20200513215150892.FfjxVGJ0.jpg)

#### flushall命令

`flushall` 命令也会触发持久化 ；

#### 触发持久化规则

满足配置条件中的触发条件 ；

> 可以通过配置文件对 Redis 进行设置， 让它在“ N 秒内数据集至少有 M 个改动”这一条件被满足时， 自动进行数据集保存操作。
>
> ![在这里插入图片描述](0_Redis概述及使用.assets/20200513215205970.png)

![在这里插入图片描述](/assets/20200513215220858.Dn9cw_-n.jpg)

#### bgsave

`bgsave` 是异步进行，进行持久化的时候，`redis` 还可以将继续响应客户端请求 ；

![在这里插入图片描述](/assets/2020051321523151.CXoUYFJT.jpg)

**bgsave和save对比**

| 命令   | save               | bgsave                             |
| ------ | ------------------ | ---------------------------------- |
| IO类型 | 同步               | 异步                               |
| 阻塞？ | 是                 | 是（阻塞发生在fock()，通常非常快） |
| 复杂度 | O(n)               | O(n)                               |
| 优点   | 不会消耗额外的内存 | 不阻塞客户端命令                   |
| 缺点   | 阻塞客户端命令     | 需要fock子进程，消耗内存           |

### 优缺点

**优点：**

1. 适合大规模的数据恢复
2. 对数据的完整性要求不高

**缺点：**

1. 需要一定的时间间隔进行操作，如果redis意外宕机了，这个最后一次修改的数据就没有了。
2. fork进程的时候，会占用一定的内容空间。

## 十一、持久化AOF

**Append Only File**

将我们所有的命令都记录下来，history，恢复的时候就把这个文件全部再执行一遍

\[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-Z8wr9lBW-1597890996525)(狂神说 0\_Redis概述及使用.assets/image-20200818123711375.png)]

> 以日志的形式来记录每个写的操作，将Redis执行过的所有指令记录下来（读操作不记录），只许追加文件但不可以改写文件，redis启动之初会读取该文件重新构建数据，换言之，redis重启的话就根据日志文件的内容将写指令从前到后执行一次以完成数据的恢复工作。

### 什么是AOF

快照功能（RDB）并不是非常耐久（durable）： 如果 Redis 因为某些原因而造成故障停机， 那么服务器将丢失最近写入、以及未保存到快照中的那些数据。 从 1.1 版本开始， Redis 增加了一种完全耐久的持久化方式： AOF 持久化。

如果要使用AOF，需要修改配置文件：

![在这里插入图片描述](0_Redis概述及使用.assets/20200513215247113.png)

`appendonly no yes`则表示启用AOF

默认是不开启的，我们需要手动配置，然后重启redis，就可以生效了！

如果这个aof文件有错位，这时候redis是启动不起来的，我需要修改这个aof文件

redis给我们提供了一个工具`redis-check-aof --fix`

> 优点和缺点

```bash
appendonly yes  # 默认是不开启aof模式的，默认是使用rdb方式持久化的，在大部分的情况下，rdb完全够用
appendfilename "appendonly.aof"

# appendfsync always # 每次修改都会sync 消耗性能
appendfsync everysec # 每秒执行一次 sync 可能会丢失这一秒的数据
# appendfsync no # 不执行 sync ,这时候操作系统自己同步数据，速度最快
```

**优点**

1. 每一次修改都会同步，文件的完整性会更加好
2. 没秒同步一次，可能会丢失一秒的数据
3. 从不同步，效率最高

**缺点**

1. 相对于数据文件来说，aof远远大于rdb，修复速度比rdb慢！
2. Aof运行效率也要比rdb慢，所以我们redis默认的配置就是rdb持久化

## 十二、RDB和AOP选择

### RDB 和 AOF 对比

| RDB        | AOF    |              |
| ---------- | ------ | ------------ |
| 启动优先级 | 低     | 高           |
| 体积       | 小     | 大           |
| 恢复速度   | 快     | 慢           |
| 数据安全性 | 丢数据 | 根据策略决定 |

### 如何选择使用哪种持久化方式？

一般来说， 如果想达到足以媲美 PostgreSQL 的数据安全性， 你应该同时使用两种持久化功能。

如果你非常关心你的数据， 但仍然可以承受数分钟以内的数据丢失， 那么你可以只使用 RDB 持久化。

有很多用户都只使用 AOF 持久化， 但并不推荐这种方式： 因为定时生成 RDB 快照（snapshot）非常便于进行数据库备份， 并且 RDB 恢复数据集的速度也要比 AOF 恢复的速度要快。

## 十三、Redis发布与订阅

Redis 发布订阅(pub/sub)是一种消息通信模式：发送者(pub)发送消息，订阅者(sub)接收消息。

\[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-IBT2pjCa-1597890996526)(狂神说 0\_Redis概述及使用.assets/image-20200818162849693.png)]

下图展示了频道 channel1 ， 以及订阅这个频道的三个客户端 —— client2 、 client5 和 client1 之间的关系：

![在这里插入图片描述](/assets/20200513215523258.DehS7v8m.png)

当有新消息通过 PUBLISH 命令发送给频道 channel1 时， 这个消息就会被发送给订阅它的三个客户端：

![在这里插入图片描述](/assets/2020051321553483.D2Z4oiH8.png)

### 命令

| 命令                                     | 描述                               |
| ---------------------------------------- | ---------------------------------- |
| `PSUBSCRIBE pattern [pattern..]`         | 订阅一个或多个符合给定模式的频道。 |
| `PUNSUBSCRIBE pattern [pattern..]`       | 退订一个或多个符合给定模式的频道。 |
| `PUBSUB subcommand [argument[argument]]` | 查看订阅与发布系统状态。           |
| `PUBLISH channel message`                | 向指定频道发布消息                 |
| `SUBSCRIBE channel [channel..]`          | 订阅给定的一个或多个频道。         |
| `SUBSCRIBE channel [channel..]`          | 退订一个或多个频道                 |

### 示例

```bash
------------订阅端----------------------
127.0.0.1:6379> SUBSCRIBE sakura # 订阅sakura频道
Reading messages... (press Ctrl-C to quit) # 等待接收消息
1) "subscribe" # 订阅成功的消息
2) "sakura"
3) (integer) 1
1) "message" # 接收到来自sakura频道的消息 "hello world"
2) "sakura"
3) "hello world"
1) "message" # 接收到来自sakura频道的消息 "hello i am sakura"
2) "sakura"
3) "hello i am sakura"

--------------消息发布端-------------------
127.0.0.1:6379> PUBLISH sakura "hello world" # 发布消息到sakura频道
(integer) 1
127.0.0.1:6379> PUBLISH sakura "hello i am sakura" # 发布消息
(integer) 1

-----------------查看活跃的频道------------
127.0.0.1:6379> PUBSUB channels
1) "sakura"
```

### 原理

每个 Redis 服务器进程都维持着一个表示服务器状态的 redis.h/redisServer 结构， 结构的 pubsub\_channels 属性是一个字典， 这个字典就用于保存订阅频道的信息，其中，字典的键为正在被订阅的频道， 而字典的值则是一个链表， 链表中保存了所有订阅这个频道的客户端。

![在这里插入图片描述](/assets/2020051321554964.D5yjuPSa.png)

客户端订阅，就被链接到对应频道的链表的尾部，退订则就是将客户端节点从链表中移除。

### 缺点

1. 如果一个客户端订阅了频道，但自己读取消息的速度却不够快的话，那么不断积压的消息会使redis输出缓冲区的体积变得越来越大，这可能使得redis本身的速度变慢，甚至直接崩溃。
2. 这和数据传输可靠性有关，如果在订阅方断线，那么他将会丢失所有在短线期间发布者发布的消息。

### 应用

1. 消息订阅：公众号订阅，微博关注等等（起始更多是使用消息队列来进行实现）
2. 多人在线聊天室。

稍微复杂的场景，我们就会使用消息中间件MQ处理。

## 十四、Redis主从复制

### 概念

主从复制，是指将一台Redis服务器的数据，复制到其他的Redis服务器。前者称为主节点（Master/Leader）,后者称为从节点（Slave/Follower）， 数据的复制是单向的！只能由主节点复制到从节点（主节点以写为主、从节点以读为主）。

默认情况下，每台Redis服务器都是主节点，一个主节点可以有0个或者多个从节点，但每个从节点只能由一个主节点。

### 作用

1. 数据冗余：主从复制实现了数据的热备份，是持久化之外的一种数据冗余的方式。
2. 故障恢复：当主节点故障时，从节点可以暂时替代主节点提供服务，是一种服务冗余的方式
3. 负载均衡：在主从复制的基础上，配合读写分离，由主节点进行写操作，从节点进行读操作，分担服务器的负载；尤其是在多读少写的场景下，通过多个从节点分担负载，提高并发量。
4. 高可用基石：主从复制还是哨兵和集群能够实施的基础。

### 为什么使用集群

1. 单台服务器难以负载大量的请求
2. 单台服务器故障率高，系统崩坏概率大
3. 单台服务器内存容量有限。

### 环境配置

我们在讲解配置文件的时候，注意到有一个`replication`模块 (见Redis.conf中第8条)

查看当前库的信息：`info replication`

```bash
127.0.0.1:6379> info replication
# Replication
role:master # 角色
connected_slaves:0 # 从机数量
master_replid:3b54deef5b7b7b7f7dd8acefa23be48879b4fcff
master_replid2:0000000000000000000000000000000000000000
master_repl_offset:0
second_repl_offset:-1
repl_backlog_active:0
repl_backlog_size:1048576
repl_backlog_first_byte_offset:0
repl_backlog_histlen:0
```

既然需要启动多个服务，就需要多个配置文件。每个配置文件对应修改以下信息：

* 端口号
* pid文件名
* 日志文件名
* rdb文件名

启动单机多服务集群：

![在这里插入图片描述](/assets/20200513215610163.Dn4OchRo.png)

### 一主二从配置

\==默认情况下，每台Redis服务器都是主节点；==我们一般情况下只用配置从机就好了！

认老大！一主（79）二从（80，81）

使用`SLAVEOF host port`就可以为从机配置主机了。

![在这里插入图片描述](/assets/20200513215637483.Dx9lkgMH.png)

然后主机上也能看到从机的状态：

![在这里插入图片描述](/assets/20200513215645778.BMkGkZk4.png)

我们这里是使用命令搭建，是暂时的，==真实开发中应该在从机的配置文件中进行配置，==这样的话是永久的。

![在这里插入图片描述](/assets/20200513215654634.DxPUGsLg.png)

### 使用规则

1. 从机只能读，不能写，主机可读可写但是多用于写。

   ```bash
    127.0.0.1:6381> set name sakura # 从机6381写入失败
   (error) READONLY You can't write against a read only replica.

   127.0.0.1:6380> set name sakura # 从机6380写入失败
   (error) READONLY You can't write against a read only replica.

   127.0.0.1:6379> set name sakura
   OK
   127.0.0.1:6379> get name
   "sakura"
   ```

2. 当主机断电宕机后，默认情况下从机的角色不会发生变化 ，集群中只是失去了写操作，当主机恢复以后，又会连接上从机恢复原状。

3. 当从机断电宕机后，若不是使用配置文件配置的从机，再次启动后作为主机是无法获取之前主机的数据的，若此时重新配置称为从机，又可以获取到主机的所有数据。这里就要提到一个同步原理。

4. 第二条中提到，默认情况下，主机故障后，不会出现新的主机，有两种方式可以产生新的主机：

   * 从机手动执行命令`slaveof no one`,这样执行以后从机会独立出来成为一个主机
   * 使用哨兵模式（自动选举）

> 如果没有老大了，这个时候能不能选择出来一个老大呢？手动！

如果主机断开了连接，我们可以使用`SLAVEOF no one`让自己变成主机！其他的节点就可以手动连接到最新的主节点（手动）！如果这个时候老大修复了，那么久重新连接！

## 十五、哨兵模式

更多信息参考博客：https://www.jianshu.com/p/06ab9daf921d

**主从切换技术的方法是：当主服务器宕机后，需要手动把一台从服务器切换为主服务器，这就需要人工干预，费事费力，还会造成一段时间内服务不可用。**这不是一种推荐的方式，更多时候，我们优先考虑**哨兵模式**。

单机单个哨兵

\[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-2ENYVAPp-1597890996527)(狂神说 0\_Redis概述及使用.assets/image-20200818233231154.png)]

哨兵的作用：

* 通过发送命令，让Redis服务器返回监控其运行状态，包括主服务器和从服务器。
* 当哨兵监测到master宕机，会自动将slave切换成master，然后通过**发布订阅模式**通知其他的从服务器，修改配置文件，让它们切换主机。

多哨兵模式

\[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-Ga1RyfVc-1597890996528)(狂神说 0\_Redis概述及使用.assets/image-20200818233316478.png)]

哨兵的核心配置

```
sentinel monitor mymaster 127.0.0.1 6379 1
```

* 数字1表示 ：当一个哨兵主观认为主机断开，就可以客观认为主机故障，然后开始选举新的主机。

> 测试

```
redis-sentinel xxx/sentinel.conf
```

成功启动哨兵模式

![在这里插入图片描述](/assets/20200513215752444.raoWzpEx.png)

此时哨兵监视着我们的主机6379，当我们断开主机后：

![在这里插入图片描述](/assets/20200513215806972.DX-WPEw0.png)

> 哨兵模式优缺点

**优点：**

1. 哨兵集群，基于主从复制模式，所有主从复制的优点，它都有
2. 主从可以切换，故障可以转移，系统的可用性更好
3. 哨兵模式是主从模式的升级，手动到自动，更加健壮

**缺点：**

1. Redis不好在线扩容，集群容量一旦达到上限，在线扩容就十分麻烦
2. 实现哨兵模式的配置其实是很麻烦的，里面有很多配置项

> 哨兵模式的全部配置

完整的哨兵模式配置文件 sentinel.conf

```bash
# Example sentinel.conf
 
# 哨兵sentinel实例运行的端口 默认26379
port 26379
 
# 哨兵sentinel的工作目录
dir /tmp
 
# 哨兵sentinel监控的redis主节点的 ip port 
# master-name  可以自己命名的主节点名字 只能由字母A-z、数字0-9 、这三个字符".-_"组成。
# quorum 当这些quorum个数sentinel哨兵认为master主节点失联 那么这时 客观上认为主节点失联了
# sentinel monitor <master-name> <ip> <redis-port> <quorum>
sentinel monitor mymaster 127.0.0.1 6379 1
 
# 当在Redis实例中开启了requirepass foobared 授权密码 这样所有连接Redis实例的客户端都要提供密码
# 设置哨兵sentinel 连接主从的密码 注意必须为主从设置一样的验证密码
# sentinel auth-pass <master-name> <password>
sentinel auth-pass mymaster MySUPER--secret-0123passw0rd
 
 
# 指定多少毫秒之后 主节点没有应答哨兵sentinel 此时 哨兵主观上认为主节点下线 默认30秒
# sentinel down-after-milliseconds <master-name> <milliseconds>
sentinel down-after-milliseconds mymaster 30000
 
# 这个配置项指定了在发生failover主备切换时最多可以有多少个slave同时对新的master进行 同步，
这个数字越小，完成failover所需的时间就越长，
但是如果这个数字越大，就意味着越 多的slave因为replication而不可用。
可以通过将这个值设为 1 来保证每次只有一个slave 处于不能处理命令请求的状态。
# sentinel parallel-syncs <master-name> <numslaves>
sentinel parallel-syncs mymaster 1
 
 
 
# 故障转移的超时时间 failover-timeout 可以用在以下这些方面： 
#1. 同一个sentinel对同一个master两次failover之间的间隔时间。
#2. 当一个slave从一个错误的master那里同步数据开始计算时间。直到slave被纠正为向正确的master那里同步数据时。
#3.当想要取消一个正在进行的failover所需要的时间。  
#4.当进行failover时，配置所有slaves指向新的master所需的最大时间。不过，即使过了这个超时，slaves依然会被正确配置为指向master，但是就不按parallel-syncs所配置的规则来了
# 默认三分钟
# sentinel failover-timeout <master-name> <milliseconds>
sentinel failover-timeout mymaster 180000
 
# SCRIPTS EXECUTION
 
#配置当某一事件发生时所需要执行的脚本，可以通过脚本来通知管理员，例如当系统运行不正常时发邮件通知相关人员。
#对于脚本的运行结果有以下规则：
#若脚本执行后返回1，那么该脚本稍后将会被再次执行，重复次数目前默认为10
#若脚本执行后返回2，或者比2更高的一个返回值，脚本将不会重复执行。
#如果脚本在执行过程中由于收到系统中断信号被终止了，则同返回值为1时的行为相同。
#一个脚本的最大执行时间为60s，如果超过这个时间，脚本将会被一个SIGKILL信号终止，之后重新执行。
 
#通知型脚本:当sentinel有任何警告级别的事件发生时（比如说redis实例的主观失效和客观失效等等），将会去调用这个脚本，
#这时这个脚本应该通过邮件，SMS等方式去通知系统管理员关于系统不正常运行的信息。调用该脚本时，将传给脚本两个参数，
#一个是事件的类型，
#一个是事件的描述。
#如果sentinel.conf配置文件中配置了这个脚本路径，那么必须保证这个脚本存在于这个路径，并且是可执行的，否则sentinel无法正常启动成功。
#通知脚本
# sentinel notification-script <master-name> <script-path>
  sentinel notification-script mymaster /var/redis/notify.sh
 
# 客户端重新配置主节点参数脚本
# 当一个master由于failover而发生改变时，这个脚本将会被调用，通知相关的客户端关于master地址已经发生改变的信息。
# 以下参数将会在调用脚本时传给脚本:
# <master-name> <role> <state> <from-ip> <from-port> <to-ip> <to-port>
# 目前<state>总是“failover”,
# <role>是“leader”或者“observer”中的一个。 
# 参数 from-ip, from-port, to-ip, to-port是用来和旧的master和新的master(即旧的slave)通信的
# 这个脚本应该是通用的，能被多次调用，不是针对性的。
# sentinel client-reconfig-script <master-name> <script-path>
sentinel client-reconfig-script mymaster /var/redis/reconfig.sh
```

## 十六、缓存穿透与雪崩

### 缓存穿透（查不到）

> 概念

在默认情况下，用户请求数据时，会先在缓存(Redis)中查找，若没找到即缓存未命中，再在数据库中进行查找，数量少可能问题不大，可是一旦大量的请求数据（例如秒杀场景）缓存都没有命中的话，就会全部转移到数据库上，造成数据库极大的压力，就有可能导致数据库崩溃。网络安全中也有人恶意使用这种手段进行攻击被称为洪水攻击。

> 解决方案

**布隆过滤器**

对所有可能查询的参数以Hash的形式存储，以便快速确定是否存在这个值，在控制层先进行拦截校验，校验不通过直接打回，减轻了存储系统的压力。

![在这里插入图片描述](/assets/20200513215824722.C3FwWtEr.jpg)

**缓存空对象**

一次请求若在缓存和数据库中都没找到，就在缓存中方一个空对象用于处理后续这个请求。

![在这里插入图片描述](/assets/20200513215836317.Bcj-d263.jpg)

这样做有一个缺陷：存储空对象也需要空间，大量的空对象会耗费一定的空间，存储效率并不高。解决这个缺陷的方式就是设置较短过期时间

即使对空值设置了过期时间，还是会存在缓存层和存储层的数据会有一段时间窗口的不一致，这对于需要保持一致性的业务会有影响。

### 缓存击穿（量太大，缓存过期）

> 概念

相较于缓存穿透，缓存击穿的目的性更强，一个存在的key，在缓存过期的一刻，同时有大量的请求，这些请求都会击穿到DB，造成瞬时DB请求量大、压力骤增。这就是缓存被击穿，只是针对其中某个key的缓存不可用而导致击穿，但是其他的key依然可以使用缓存响应。

比如热搜排行上，一个热点新闻被同时大量访问就可能导致缓存击穿。

> 解决方案

1. **设置热点数据永不过期**

   这样就不会出现热点数据过期的情况，但是当Redis内存空间满的时候也会清理部分数据，而且此种方案会占用空间，一旦热点数据多了起来，就会占用部分空间。

2. **加互斥锁(分布式锁)**

   在访问key之前，采用SETNX（set if not exists）来设置另一个短期key来锁住当前key的访问，访问结束再删除该短期key。保证同时刻只有一个线程访问。这样对锁的要求就十分高。

### 缓存雪崩

> 概念

大量的key设置了相同的过期时间，导致在缓存在同一时刻全部失效，造成瞬时DB请求量大、压力骤增，引起雪崩。

![在这里插入图片描述](/assets/20200513215850428.DoRxTOgh.jpeg)

> 解决方案

* redis高可用

  这个思想的含义是，既然redis有可能挂掉，那我多增设几台redis，这样一台挂掉之后其他的还可以继续工作，其实就是搭建的集群

* 限流降级

  这个解决方案的思想是，在缓存失效后，通过加锁或者队列来控制读数据库写缓存的线程数量。比如对某个key只允许一个线程查询数据和写缓存，其他线程等待。

* 数据预热

  数据加热的含义就是在正式部署之前，我先把可能的数据先预先访问一遍，这样部分可能大量访问的数据就会加载到缓存中。在即将发生大并发访问前手动触发加载缓存不同的key，设置不同的过期时间，让缓存失效的时间点尽量均匀。

---

---
url: /Redis/Redis基础/1_Redis缓存三大问题.md
---

# Redis缓存三大问题

Redis是我们日常在工作中使用非常多的缓存解决手段，使用缓存，能够提升我们应用程序的性能，同时极大程度的降低数据库的压力。但如果使用不当，同样会造成许多问题，其中三大经典问题就包括了缓存穿透、缓存击穿和缓存雪崩。

### 缓存穿透

缓存穿透是指用户在查找一个数据时查找了一个根本不存在的数据。按照缓存设计流程，首先查询redis缓存，发现并没有这条数据，于是直接查询数据库，发现也没有，于是本次查询结果以失败告终。

当存在大量的这种请求或恶意使用不存在的数据进行访问攻击时，大量的请求将直接访问数据库，造成数据库压力甚至可能直接瘫痪。以电商商城为例，以商品id进行商品查询，这时如果使用一个不存在的id进行攻击，每次的攻击都将访问在数据库上。

来看一下应对方案：

#### 1、缓存空对象

修改数据库写回缓存逻辑，对于缓存中不存在，数据库中也不存在的数据，我们仍然将其缓存起来，并且设置一个缓存过期时间。

![img](/assets/20200407195145851.6v-CIiFz.png)

如上图所示，查询数据库失败时，仍以查询的key值缓存一个空对象（key，null）。

但是这么做仍然存在不少问题：

a、这时在缓存中查找这个key值时，会返回一个null的空对象。需要注意的是这个空对象可能并不是客户端需要的，所以需要对结果为空进行处理后，再返回给客户端。

b、占用redis中大量内存。因为空对象能够被缓存，redis会使用大量的内存来存储这些值为空的key。

c、如果在写缓存后数据库中存入的这个key的数据，由于缓存没有过期，取到的仍为空值，所以可能出现短暂的数据不一致问题。

#### 2、布隆过滤器

布隆过滤器是一个二进制向量，或者说二进制的数组，或者说是位（bit）数组。

![img](Redis缓存三大问题.assets/20200407195241902.png)

因为是二进制的向量，它的每一位只能存放0或者1。当需要向布隆过滤器中添加一个数据映射时，添加的并不是原始的数据，而是使用多个不同的哈希函数生成多个哈希值，并将每个生成哈希值指向的下标位置置为1。所以，别再说从布隆过滤器中取数据啦，我们根本就没有存原始数据。

![img](/assets/20200407195259261.cVrobwtT.png)

例如"Hydra"的三个哈希函数生成的下标分别为1，3，6，那么将这三位置为1，其他数据以此类推。那么这样的数据结构能够起到什么效果呢？我们可以根据这个位向量，来判断数据是否存在。

具体流程：

a、计算数据的多个哈希值；

b、判断这些bit是否为1，全部为1，则数据可能存在；

c、若其中一个或多个bit不为1，则判断数据不存在。

需要注意，布隆过滤器是存在误判的，因为随着数据存储量的增加，被置为1的bit数量也会增加，因此，有可能在查询一个并不存在的数据时，碰巧所有bit都已经被其他数据置为了1，也就是发生了哈希碰撞。因此，布隆过滤器只能做到判断数据是否可能存在，不能做到百分百的确定。

Google的`guava`包为我们提供了单机版的布隆过滤器实现，来看一下具体使用

首先引入maven依赖：

```xml
<dependency>
    <groupId>com.google.guava</groupId>
    <artifactId>guava</artifactId>
    <version>27.1-jre</version>
</dependency>
```

向布隆过滤器中模拟传入1000000条数据，给定误判率，再使用不存在的数据进行判断：

```java
public class BloomTest {
    public static void test(int dataSize,double errorRate){
        BloomFilter<Integer> bloomFilter=
                BloomFilter.create(Funnels.integerFunnel(), dataSize, errorRate);

        for(int i = 0; i< dataSize; i++){
            bloomFilter.put(i);
        }

        int errorCount=0;
        for(int i = dataSize; i<2* dataSize; i++){
            if(bloomFilter.mightContain(i)){
                errorCount++;
            }
        }
        System.out.println("Total error count: "+errorCount);
    }

    public static void main(String[] args) {
        BloomTest.test(1000000,0.01);
        BloomTest.test(1000000,0.001);
    }
}
```

测试结果：

```text
Total error count: 10314
Total error count: 994
```

可以看出，在给定误判率为0.01时误判了10314次，在误判率为0.001时误判了994次，大体符合我们的期望。

但是因为guava的布隆过滤器是运行在的jvm内存中，所以仅支持单体应用，并不支持微服务分布式。那么有没有支持分布式的布隆过滤器呢，这时Redis站了出来，自己造成的问题自己来解决！

Redis的BitMap（位图）支持了对位的操作，通过一个bit位来表示某个元素对应的值或者状态。

```bash
//对key所存储的字符串值，设置或清除指定偏移量上的位（bit）
setbit key offset value
//对key所存储的字符串值，获取指定偏移量上的位（bit）
getbit key offset
```

既然布隆过滤器是对位进行赋值，我们就可以使用BitMap提供的setbit和getbit命令非常简单的对其进行实现，并且setbit操作可以实现自动数组扩容，所以不用担心在使用过程中数组位数不够的情况。

```java
//源码参考https://www.cnblogs.com/CodeBear/p/10911177.html
public class RedisBloomTest {
    private static int dataSize = 1000;
    private static double errorRate = 0.01;

    //bit数组长度
    private static long numBits;
    //hash函数数量
    private static int numHashFunctions;

    public static void main(String[] args) {
        numBits = optimalNumOfBits(dataSize, errorRate);
        numHashFunctions = optimalNumOfHashFunctions(dataSize, numBits);

        System.out.println("Bits length: "+numBits);
        System.out.println("Hash nums: "+numHashFunctions);

        Jedis jedis = new Jedis("127.0.0.1", 6379);
        for (int i = 0; i <= 1000; i++) {
            long[] indexs = getIndexs(String.valueOf(i));
            for (long index : indexs) {
                jedis.setbit("bloom", index, true);
            }
        }

        num:
        for (int i = 1000; i < 1100; i++) {
            long[] indexs = getIndexs(String.valueOf(i));
            for (long index : indexs) {
                Boolean isContain = jedis.getbit("bloom", index);
                if (!isContain) {
                    System.out.println(i + "不存在");
                    continue  num;
                }
            }
            System.out.println(i + "可能存在");
        }
    }

    //根据key获取bitmap下标
    private static long[] getIndexs(String key) {
        long hash1 = hash(key);
        long hash2 = hash1 >>> 16;
        long[] result = new long[numHashFunctions];
        for (int i = 0; i < numHashFunctions; i++) {
            long combinedHash = hash1 + i * hash2;
            if (combinedHash < 0) {
                combinedHash = ~combinedHash;
            }
            result[i] = combinedHash % numBits;
        }
        return result;
    }

    private static long hash(String key) {
        Charset charset = Charset.forName("UTF-8");
        return Hashing.murmur3_128().hashObject(key, Funnels.stringFunnel(charset)).asLong();
    }

    //计算hash函数个数
    private static int optimalNumOfHashFunctions(long n, long m) {
        return Math.max(1, (int) Math.round((double) m / n * Math.log(2)));
    }

    //计算bit数组长度
    private static long optimalNumOfBits(long n, double p) {
        if (p == 0) {
            p = Double.MIN_VALUE;
        }
        return (long) (-n * Math.log(p) / (Math.log(2) * Math.log(2)));
    }
}
```

基于BitMap实现分布式布隆过滤器的过程中，哈希函数的数量以及位数组的长度都是动态计算的。可以说，给定的容错率越低，哈希函数的个数则越多，数组长度越长，使用的redis内存开销越大。

guava中布隆过滤器的数组最大长度是由int值的上限决定的，大概为21亿，而redis的位数组为512MB，也就是2^32位，所以最大长度能够达到42亿，容量为guava的两倍。

### 缓存击穿

缓存击穿是指缓存中没有但数据库中有的数据，由于出现大量的并发请求，同时读缓存没读到数据，又同时去数据库去取数据，引起数据库压力瞬间增大，造成过大压力。

造成这种情况大致有两种情况：

* 第一次查询数据时，没有进行缓存预热，数据并没有加入缓存当中。
* 缓存由于到达过期时间导致失效。

解决思路：

* 当缓存不命中时，在查询数据库前使用redis分布式锁，使用查询的key值作为锁条件；
* 获取锁的线程在查询数据库前，再查询一次缓存。这样做是因为高并发请求获取锁的时候造成排队，但第一次进来的线程在查询完数据库后会写入缓存，之后再获得锁的线程直接查询缓存就可以获得数据；
* 读取完数据后释放分布式锁。

代码思路：

```text
public String queryData(String key) throws Exception {
    String data;
    data = queryDataFromRedis(key);// 查询缓存数据
    if (data == null) {
        if(redisLock.tryLock()){//获取分布式锁
            data = queryDataFromRedis(key); // 再次查询缓存
            if (data == null) {
                data = queryDataFromDB(key); // 查询数据库
                writeDataToRedis(data); // 将查询到的数据写入缓存
            }
            redisLock.unlock();//释放分布式锁
        }
    }
    return data;
}
```

具体分布式锁的实现可以使用redis中强大的setnx命令：

```text
/*
* 加锁
* key-键;value-值
* nxxx-nx(只在key不存在时才可以set)|xx(只在key存在的时候set)
* expx--ex代表秒，px代表毫秒;time-过期时间，单位是expx所代表的单位。
* */
jedis.set(key, value, nxxx, expx, time);

//解锁
jedis.del(key);
```

通过在加锁的同时设置过期时间，还可以防止线程挂掉仍然占用锁的情况。

### 缓存雪崩

缓存雪崩是指缓存中数据大批量到过期时间，引发的大部分缓存突然同时不可用，而查询数据量巨大，引起数据库压力过大甚至宕机的情况。 需要注意缓存击穿和缓存雪崩的不同之处缓存击穿指的是大量的并发请求去查询同一条数据；而缓存雪崩是大量缓存同时过期，导致很多查询请求都查不到缓存数据从而查数据库。

解决方案：

* 错开缓存的过期时间，可通过设置缓存数据的过期时间为默认值基础上加上一个随机值，防止同一时间大量数据过期现象发生。
* 搭建高可用的redis集群，避免出现缓存服务器宕机引起的雪崩问题。
* 参照hystrix，进行熔断降级。

### 总结：

随着Redis的使用日渐普及，越来越多的系统开始使用缓存技术，但伴随着便利的同时也因为使用不当造成了很多问题。只有在系统设计时期考虑到这些问题并加以克服，系统才能够更加健壮。

---

---
url: /Redis/Redis实战/2_Redis实战之解决超卖问题.md
---

# Redis 解决超卖问题

结合Redis对高并发环境下，商品超卖问题的解决思路

### **基础代码**

> 两个接口，创建一个`stock` 商品设置200个库存
> 另一个接口，获取 redis 的库存数，判断是否有库存，如果有，就取出来-1再放回去。

```java
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.data.redis.core.StringRedisTemplate;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RestController;

import java.util.Objects;

/**
 * 超卖问题模拟
 *
 * @author xxl
 * @date 2023/10/09
 */
@RestController
public class OversoldController {

    /**
     * 引入String类型redis操作模板
     */
    @Autowired
    private StringRedisTemplate stringRedisTemplate;

    /**
     * 测试数据设置接口
     *
     * @return
     */
    @RequestMapping("/setStock")
    public String setStock() {
        stringRedisTemplate.opsForValue().set("stock", 200 + "");
        return "ok";
    }

    /**
     * 模拟商品超卖代码
     *
     * @return
     */
    @RequestMapping("/deductStock")
    public String deductStock() {
        // 获取Redis数据库中的商品数量
        int stock = Integer.parseInt(stringRedisTemplate.opsForValue().get("stock"));
        // 减库存
        if (stock > 0) {
            int realStock = stock - 1;
            stringRedisTemplate.opsForValue().set("stock", String.valueOf(realStock));
            System.out.println("商品扣减成功，剩余商品：" + realStock);
        } else {
            System.out.println("库存不足.....");
        }
        return "end";
    }

}
```

启动服务。

先执行第一个接口，创建一个stock 商品，

```http
### 测试数据设置接口
GET http://localhost:8080/setStock
```

然后创建 jmeter 测试案例

![image-20231009103440883](/assets/image-20231009103440883.DwIBdojb.png)

线程数：设置200，每次200请求

Ramp-Up时间：设置0，指定时间内发完，0表示一次性

循环次数：设置4，轮询4次，每次200请求

![image-20231009093820199](/assets/image-20231009093820199.BMgkqWFH.png)

测试结果：

![image-20231009093520953](/assets/image-20231009093520953.CFi8A9d7.png)

每个请求相当于一个线程，当几个线程同时拿到数据时，做了变更，但最先处理的数据，由于某些原因(延迟等)，造成数据最后的更新，覆盖了之前的卖出数据。导致超卖。

### 方案一：设置synchronized

既然因为并发太高导致的问题，那肯定和线程有关，尝试加个锁呗

先调用第一个接口，重置库存数

![image-20231009095930572](Redis解决超卖问题.assets/image-20231009095930572.png)

修改代码，增加synchronized：

```java
    /**
     * 模拟商品超卖代码 设置synchronized
     *
     * @return
     */
    @RequestMapping("/deductStock2")
    public String deductStock2() {
        synchronized (this) {
            // 获取Redis数据库中的商品数量
            int stock = Integer.parseInt(stringRedisTemplate.opsForValue().get("stock"));
            // 减库存
            if (stock > 0) {
                int realStock = stock - 1;
                stringRedisTemplate.opsForValue().set("stock", String.valueOf(realStock));
                System.out.println("商品扣减成功，剩余商品：" + realStock);
            } else {
                System.out.println("库存不足.....");
            }
        }
        return "end";
    }
```

重新压测

![image-20231009100030637](/assets/image-20231009100030637.sLlKtXJn.png)

![image-20231009101148459](/assets/image-20231009101148459.CFwVLec2.png)

在单机模式下，添加synchronized关键字，的确能够避免商品的超卖现象！

但是在分布式微服务中，针对该服务，如果设置了集群，synchronized依旧还能保证数据的正确性吗？

假设多个请求，被注册中心负载均衡，每个微服务中的该处理接口，都添加有synchronized，依然会出现类似的`超卖`

问题：

> `synchronized`只是针对`单一服务器`的`JVM`进行`加锁`，但是分布式是很多个不同的服务器，导致两个线程或多个在不同服务器上共同对商品数量信息做了操作！

### 方案二：Redis实现分布式锁

在Redis中存在一条命令`setnx (set if not exists)`

```shell
setnx key value
```

如果不存在key，则可以设置成功；否则设置失败。

```shell
setnx 这个命令是一个上锁的命令

127.0.0.1:6379[10]> exists lock
(integer) 0
127.0.0.1:6379[10]> setnx lock "hi"
(integer) 1
127.0.0.1:6379[10]> setnx lock "good"
(integer) 0
127.0.0.1:6379[10]> get lock
"hi"
```

修改处理接口，增加key

```java
/**
 * 模拟商品超卖代码 设置setnx
 * 
 * @return
 */
@RequestMapping("/deductStock3")
public String deductStock3() {
	// 创建一个key，保存至redis
	String key = "lock";
	// setnx
	// 由于redis是一个单线程，执行命令采取“队列”形式排队！
	// 优先进入队列的命令先执行，由于是setnx，第一个执行后，其他操作执行失败。
	boolean result = stringRedisTemplate.opsForValue().setIfAbsent(key, "this is lock");
	// 当不存在key时，可以设置成功，回执true；如果存在key，则无法设置，返回false
	if (!result) {
		// 前端监测，redis中存在，则不能让这个抢购操作执行，予以提示！
		return "err";
	}
	// 获取Redis数据库中的商品数量
	Integer stock = Integer.parseInt(stringRedisTemplate.opsForValue().get("stock"));
	// 减库存
	if (stock > 0) {
		int realStock = stock - 1;
		stringRedisTemplate.opsForValue().set("stock", String.valueOf(realStock));
		System.out.println("商品扣减成功，剩余商品：" + realStock);
	} else {
		System.out.println("库存不足.....");
	}
	
	// 程序执行完成，则删除这个key
	stringRedisTemplate.delete(key);

	return "end";
}
```

1、请求进入接口中，如果redis中不存在key，则会新建一个setnx；如果存在，则不会新建，同时返回错误编码，不会继续执行抢购逻辑。

2、当创建成功后，执行抢购逻辑。

3、抢购逻辑执行完成后，删除数据库中对应的setnx的key。让其他请求能够设置并操作。

这种逻辑来说比之前单一使用syn合理的多，但是如果执行抢购操作中出现了异常，导致这个key无法被删除。以至于其他处理请求，一直无法拿到key，程序逻辑死锁！

#### **try…finally解决Redis分布式锁问题**

如何解决上述的死锁问题？

可以采取try … finally进行操作

修改业务代码逻辑，如下所示：

```java
/**
 * 模拟商品超卖代码 设置setnx
 * try…finally解决Redis分布式锁问题
 * 
 * @return
 */
@RequestMapping("/deductStock4")
public String deductStock4() {
	// 创建一个key，保存至redis
	String key = "lock";
	// setnx
	// 由于redis是一个单线程，执行命令采取队列形式排队！优先进入队列的命令先执行，由于是setnx，第一个执行后，其他操作执行失败
	boolean result = stringRedisTemplate.opsForValue().setIfAbsent(key, "this is lock");
	// 当不存在key时，可以设置成功，回执true；如果存在key，则无法设置，返回false
	if (!result) {
		// 前端监测，redis中存在，则不能让这个抢购操作执行，予以提示！
		return "err";
	}
	
	try {
		// 获取Redis数据库中的商品数量
		Integer stock = Integer.parseInt(stringRedisTemplate.opsForValue().get("stock"));
		// 减库存
		if (stock > 0) {
			int realStock = stock - 1;
			stringRedisTemplate.opsForValue().set("stock", String.valueOf(realStock));
			System.out.println("商品扣减成功，剩余商品：" + realStock);
		} else {
			System.out.println("库存不足.....");
		}
	} finally {
		// 程序执行完成，则删除这个key
		// 放置于finally中，保证即使上述逻辑出问题，也能del掉
		stringRedisTemplate.delete(key);
	}

	return "end";
}
```

这个逻辑相比上面其他的逻辑来说，显得更加的严谨。

但是，如果一套服务器，因为断电、系统崩溃等原因出现宕机，导致本该执行finally中的语句未成功执行完成！！同样出现key一直存在，导致死锁！

#### **通过超时间解决上述问题**

在设置成功setnx后，以及抢购代码逻辑执行前，增加key的限时。

```java
/**
 * 模拟商品超卖代码
 * 设置setnx保证分布式环境下，数据处理安全行问题；<br>
 * 但如果某个代码段执行异常，导致key无法清理，出现死锁，添加try...finally;<br>
 * 如果某个服务因某些问题导致释放key不能执行，导致死锁，此时解决思路为：增加key的有效时间
 * 
 * @return
 */
@RequestMapping("/deductStock5")
public String deductStock5() {
	// 创建一个key，保存至redis
	String key = "lock";
	// setnx
	// 由于redis是一个单线程，执行命令采取队列形式排队！优先进入队列的命令先执行，由于是setnx，第一个执行后，其他操作执行失败
	boolean result = stringRedisTemplate.opsForValue().setIfAbsent(key, "this is lock");
	// 当不存在key时，可以设置成功，回执true；如果存在key，则无法设置，返回false
	if (!result) {
		// 前端监测，redis中存在，则不能让这个抢购操作执行，予以提示！
		return "err";
	}
	// 设置key有效时间
	stringRedisTemplate.expire(key, 10, TimeUnit.SECONDS);
	
	try {
		// 获取Redis数据库中的商品数量
		Integer stock = Integer.parseInt(stringRedisTemplate.opsForValue().get("stock"));
		// 减库存
		if (stock > 0) {
			int realStock = stock - 1;
			stringRedisTemplate.opsForValue().set("stock", String.valueOf(realStock));
			System.out.println("商品扣减成功，剩余商品：" + realStock);
		} else {
			System.out.println("库存不足.....");
		}
	} finally {
		// 程序执行完成，则删除这个key
		// 放置于finally中，保证即使上述逻辑出问题，也能del掉
		stringRedisTemplate.delete(key);
	}

	return "end";
}
```

但是在上面的代码中，依旧会出现问题：

假设执行stringRedisTemplate.opsForValue().setIfAbsent(key, "this is lock");代码逻辑后，因为服务器原因，服务器宕机。

导致下列设置的key有效时间代码逻辑未执行。依旧会出现之前说的死锁问题。

能否保证设置key和设置时间代码能同时执行？

在Redis中，开发者考虑到这种情况，新增了一项方法可以使用，如下所示：

```java
/**
 * 模拟商品超卖代码
 * 设置setnx保证分布式环境下，数据处理安全行问题；<br>
 * 但如果某个代码段执行异常，导致key无法清理，出现死锁，添加try...finally;<br>
 * 如果某个服务因某些问题导致释放key不能执行，导致死锁，此时解决思路为：增加key的有效时间;<br>
 * 为了保证设置key的值和设置key的有效时间，两条命令构成同一条原子命令，将下列逻辑换成其他代码。
 * 
 * @return
 */
@RequestMapping("/deductStock6")
public String deductStock6() {
    // 创建一个key，保存至redis
    String key = "lock";
    // setnx
    // 由于redis是一个单线程，执行命令采取队列形式排队！优先进入队列的命令先执行，由于是setnx，第一个执行后，其他操作执行失败
    //boolean result = stringRedisTemplate.opsForValue().setIfAbsent(key, "this is lock");
    //让设置key和设置key的有效时间都可以同时执行
	boolean result = stringRedisTemplate.opsForValue().setIfAbsent(key, "this is lock", 10, TimeUnit.SECONDS);
	
	// 当不存在key时，可以设置成功，回执true；如果存在key，则无法设置，返回false
	if (!result) {
		// 前端监测，redis中存在，则不能让这个抢购操作执行，予以提示！
		return "err";
	}
	// 设置key有效时间
	//stringRedisTemplate.expire(key, 10, TimeUnit.SECONDS);
	
	try {
		// 获取Redis数据库中的商品数量
		Integer stock = Integer.parseInt(stringRedisTemplate.opsForValue().get("stock"));
		// 减库存
		if (stock > 0) {
			int realStock = stock - 1;
			stringRedisTemplate.opsForValue().set("stock", String.valueOf(realStock));
			System.out.println("商品扣减成功，剩余商品：" + realStock);
		} else {
			System.out.println("库存不足.....");
		}
	} finally {
		// 程序执行完成，则删除这个key
		// 放置于finally中，保证即使上述逻辑出问题，也能del掉
		stringRedisTemplate.delete(key);
	}

	return "end";
}
```

> // 让设置key和设置key的有效时间都可以同时执行
>
> boolean result = stringRedisTemplate.opsForValue().setIfAbsent(key, “this is lock”, 10, TimeUnit.SECONDS);

将setIfAbsent和expire两条命令合并成一条原子命令。

但是上述代码的逻辑中依旧会有问题：

> 如果处理逻辑中，出现超时问题。
>
> 当逻辑执行时，时间超过设定key有效时间，此时会出现什么问题？
>
> 如果一个请求执行时间超过了key的有效时间。
>
> 新的请求执行过来时，必然可以拿到key并设置时间；
>
> 此时的redis中保存的key并不是请求1的key，而是别的请求设置的。
>
> 当请求1执行完成后，此处删除key，删除的是别的请求设置的key！

依然出现了key形同虚设的问题！如果失效一直存在，超卖问题依旧不会解决。

#### **通过key设置值匹配的方式解决形同虚设问题**

既然出现key形同虚设的现象，是否可以增加条件，当finally中需要执行删除操作时，获取数据判断值是否是该请求中对应的，如果是则删除，不是则不管！

修改上述代码如下所示：

```java
/**
 * 模拟商品超卖代码 <br>
 * 解决`deductStock6`中，key形同虚设的问题。
 * 
 * @return
 */
@RequestMapping("/deductStock7")
public String deductStock7() {
    // 创建一个key，保存至redis
    String key = "lock";
    String lock_value = UUID.randomUUID().toString();
    // setnx
    //让设置key和设置key的有效时间都可以同时执行
    boolean result = stringRedisTemplate.opsForValue().setIfAbsent(key, lock_value, 10, TimeUnit.SECONDS);
    // 当不存在key时，可以设置成功，回执true；如果存在key，则无法设置，返回false
    if (!result) {
        // 前端监测，redis中存在，则不能让这个抢购操作执行，予以提示！
        return "err";
    }
    try {
        // 获取Redis数据库中的商品数量
        Integer stock = Integer.parseInt(stringRedisTemplate.opsForValue().get("stock"));
        // 减库存
        if (stock > 0) {
            int realStock = stock - 1;
            stringRedisTemplate.opsForValue().set("stock", String.valueOf(realStock));
            System.out.println("商品扣减成功，剩余商品：" + realStock);
        } else {
            System.out.println("库存不足.....");
        }
    } finally {
        // 程序执行完成，则删除这个key
        // 放置于finally中，保证即使上述逻辑出问题，也能del掉
        // 判断redis中该数据是否是这个接口处理时的设置的，如果是则删除
		if(lock_value.equalsIgnoreCase(stringRedisTemplate.opsForValue().get(key))) {
			stringRedisTemplate.delete(key);
		}
	}
	return "end";
}
```

但是此处也有一个很极限的问题：

1、在finally流程中，由于是先判断在处理。

2、如果判断条件结束后，获取到的结果为true。

3、但是在执行del操作前，此时jvm在执行GC操作(为了保证GC操作获取GC roots根完全，会暂停java程序)，导致程序暂停。

4、GC操作执行完成后(暂停恢复后)，执行del操作，但是此时的key还在当前加锁的key么？

必须保证del操作的判断和执行能够同时执行，此处代码才合理。

### **方案三：Redisson API**

setnx 的方式会出现无法准确判断业务操作时长，而无法保证安全，设置时间太长，性能不好，设置时间短，容易出现卖超问题。

难道就没有其他办法了吗？

Redisson 和 jedis 都是 Java 实现 Redis 的客户端，但是 Redisson 比 jedis 具有更多功能

引入新的 pom 文件

```xml
        <dependency>
            <groupId>org.redisson</groupId>
            <artifactId>redisson</artifactId>
            <version>3.17.6</version>
        </dependency>
```

创建 bean 配置类

```java
import org.redisson.Redisson;
import org.redisson.config.Config;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

/**
 * 配置类
 *
 * @author xxl
 * @date 2023/10/09
 */
@Configuration
public class RedissonConfig {

    @Value("${spring.redis.host}")
    private String redisHost;
    
    @Value("${spring.redis.port}")
    private String redisPort;
    
    @Value("${spring.redis.password}")
    private String password;
    
    @Value("${spring.redis.database}")
    private Integer dataBase;


    @Bean
    public Redisson createRedisson(){
        Config config = new Config();
        config.useSingleServer().setAddress("redis://" + redisHost + ":" + redisPort).setDatabase(dataBase).setPassword(null);
        return (Redisson) Redisson.create(config);
    }
}
```

这里要注意一下，如果 redis 没有密码，在 setPassword(null) 这里给了空。否则会报 ==Unable to connect to Redis server: 127.0.0.1/127.0.0.1:6379== 错误

消费的接口方法

```java
@GetMapping("/redissonLockStock")
public String redissonLockStock() throws InterruptedException {
    String key = "lock";
    RLock lock = redisson.getLock(key);
    try {
        lock.lock();
        int stock = Integer.parseInt(stringRedisTemplate.opsForValue().get("stock"));
        if (stock % 100 == 0){
            System.out.println("A服务 延迟10秒");
            Thread.sleep(10000);
            System.out.println("A服务 延迟10秒结束 继续后续操作");
        }
        if (stock>0){
            int resultStock = stock - 1;
            stringRedisTemplate.opsForValue().set("stock",String.valueOf(resultStock));
            System.out.println("A服务 商品扣除成功，剩余商品:"+resultStock);
        }else {
            System.out.println("A服务 库存不足...");
        }
    }finally {
        lock.unlock();
    }
    return "end";
}
```

这里我还特地添加了 10秒延迟，可以在这个时间段看到 lock 的值

我们看到他也是通过 TTL 延时过期来实现的。那到底是咋做的呢？

我们来看一下 Redisson 源码

```java
String key = "lock";
RLock lock = redisson.getLock(key);
```

在`org.redisson.RedissonLock.RedissonLock(CommandAsyncExecutor, String)`中，我们看到 redisson 给 key 设置的属性中有超时时间

```java
public RedissonLock(CommandAsyncExecutor commandExecutor, String name) {
  super(commandExecutor, name);
  this.commandExecutor = commandExecutor;
  this.internalLockLeaseTime = commandExecutor.getConnectionManager().getCfg().getLockWatchdogTimeout();
  this.pubSub = commandExecutor.getConnectionManager().getSubscribeService().getLockPubSub();
}
```

超时间数为(`org.redisson.config.Config.lockWatchdogTimeout`)：默认30s

```text
private long lockWatchdogTimeout = 30 * 1000;
```

其中加锁、续命锁在以下代码中实现

```text
lock.lock();
```

查看源码(`org.redisson.RedissonLock.lock()`)，逐个判断分析得到核心逻辑代码如下所示：

```java
<T> RFuture<T> tryLockInnerAsync(long leaseTime, TimeUnit unit, long threadId, RedisStrictCommand<T> command) {
    internalLockLeaseTime = unit.toMillis(leaseTime);

    return commandExecutor.evalWriteAsync(getName(), LongCodec.INSTANCE, command,
                                          // 如果存在 KEYS[1]，这个KEYS[1]就是最初设置的redisson.getLock(key)
                                          "if (redis.call('exists', KEYS[1]) == 0) then " +
                                          //上述代码执行逻辑为0，表示不存在
                                          // 不存在则将 锁key+线程id设置为hash类型数据保存redis(ARGV[2]为当前执行线程id)
                                          "redis.call('hset', KEYS[1], ARGV[2], 1); " +
                                          // 设置这个 hash数据类型 的有效时间
                                          "redis.call('pexpire', KEYS[1], ARGV[1]); " +
                                          "return nil; " +
                                          "end; " +
                                          "if (redis.call('hexists', KEYS[1], ARGV[2]) == 1) then " +
                                          // 如果这个 锁key 在redis中存在，返回1表示数据存在
                                          //hincrby 自增1 
                                          "redis.call('hincrby', KEYS[1], ARGV[2], 1); " +
                                          // 重新设定有效时间
                                          "redis.call('pexpire', KEYS[1], ARGV[1]); " +
                                          "return nil; " +
                                          "end; " +
                                          "return redis.call('pttl', KEYS[1]);",
                                          Collections.<Object>singletonList(getName()), internalLockLeaseTime, getLockName(threadId));
}
```

> “我们可以看到一打开 redis.call 命令，其实这是 lua 语言，其中的指令有
> exists 存在、pexpire 设置有效时间
> ”

根据上述源码中，存在设置超时时间默认为30秒，但是我们知道，真正的业务执行过程不见得就是30秒，拿着一块 redisson 怎么处理呢？

在源码`org.redisson.RedissonLock.tryAcquireAsync(long, TimeUnit, long)`中，针对时间处理参数做了如下操作：

```java
private <T> RFuture<Long> tryAcquireAsync(long leaseTime, TimeUnit unit, final long threadId) {
    if (leaseTime != -1) {
        return tryLockInnerAsync(leaseTime, unit, threadId, RedisCommands.EVAL_LONG);
    }
    RFuture<Long> ttlRemainingFuture = tryLockInnerAsync(commandExecutor.getConnectionManager().getCfg().getLockWatchdogTimeout(), TimeUnit.MILLISECONDS, threadId, RedisCommands.EVAL_LONG);
    // 设置监听线程，当异步方法tryLockInnerAsync执行完触发
    ttlRemainingFuture.addListener(new FutureListener<Long>() {
        // 重写 operationComplete 方法
        @Override
        public void operationComplete(Future<Long> future) throws Exception {
            if (!future.isSuccess()) {
                return;
            }

            Long ttlRemaining = future.getNow();
            // lock acquired
            if (ttlRemaining == null) {
                // 开启定时任务
                scheduleExpirationRenewal(threadId);
            }
        }
    });
    return ttlRemainingFuture;
}
```

查看定时任务源码(`org.redisson.RedissonLock.scheduleExpirationRenewal(long)`)：

```java
private void scheduleExpirationRenewal(final long threadId) {
    if (expirationRenewalMap.containsKey(getEntryName())) {
        return;
    }
    // 定时任务的创建
    Timeout task = commandExecutor.getConnectionManager().newTimeout(new TimerTask() {
        @Override
        public void run(Timeout timeout) throws Exception {
            //又是一个lua脚本，重新设置锁
            RFuture<Boolean> future = commandExecutor.evalWriteAsync(getName(), LongCodec.INSTANCE, RedisCommands.EVAL_BOOLEAN,
                                                                     "if (redis.call('hexists', KEYS[1], ARGV[2]) == 1) then " +
                                                                     // 获取redis的hash数据类型中，指定的key-线程id 信息。
                                                                     // 如果 == 1 表示存在这个锁
                                                                     // 重新设置key的失效时间
                                                                     "redis.call('pexpire', KEYS[1], ARGV[1]); " +
                                                                     "return 1; " +
                                                                     "end; " +
                                                                     "return 0;",
                                                                     Collections.<Object>singletonList(getName()), internalLockLeaseTime, getLockName(threadId));

            // 设置失效时间后(evalWriteAsync执行后)，开启监听
            future.addListener(new FutureListener<Boolean>() {
                @Override
                public void operationComplete(Future<Boolean> future) throws Exception {
                    expirationRenewalMap.remove(getEntryName());
                    // 如果future 未执行成功
                    if (!future.isSuccess()) {
                        log.error("Can't update lock " + getName() + " expiration", future.cause());
                        return;
                    }
                    // future 执行完成
                    if (future.getNow()) {
                        // 调取自身，此时并不会造成死循环
                        // 调用自身，继续执行 TimerTask中的逻辑，包括定时操作
                        // reschedule itself
                        scheduleExpirationRenewal(threadId);
                    }
                }
            });
        }
        // 每 30/3 也就是10秒
    }, internalLockLeaseTime / 3, TimeUnit.MILLISECONDS);

    if (expirationRenewalMap.putIfAbsent(getEntryName(), task) != null) {
        task.cancel();
    }
}
```

通过这个定时和设置延迟时间，我们就可以清楚的知道， redisson 是如何做延时处理的

redisson 并不是等30秒都执行完了，再去续命，而是每过10秒就续10秒

我每隔一秒执行 ttl lock 发现

```shell
127.0.0.1:6379[10]> ttl lock
(integer) 23
127.0.0.1:6379[10]> ttl lock
(integer) 22
127.0.0.1:6379[10]> ttl lock
(integer) 21
127.0.0.1:6379[10]> ttl lock
(integer) 20
127.0.0.1:6379[10]> ttl lock
(integer) 29
127.0.0.1:6379[10]> ttl lock
(integer) 28
```

那我们知道了 redisson 通过上锁加续命的方式解决分布式锁。还有其他的办法吗？

### 方案四：RedLock 高可用并发锁

用之前，我先说一下大致原理，RedLock 思想为了保证高可用性，在设置key 的时候，会创建多个节点，单个节点设置成功不会告诉程序获得了锁，只有超过半数的节点设置成功，才会告诉程序锁上了

所以我们要创建多个 key

```java
@GetMapping("/redLockStock")
public String redLockStock(){
    // 创建多个key，
    String key1 = "lock:1";
    String key2 = "lock:2";
    String key3 = "lock:3";
    RLock lock1 = redisson.getLock(key1);
    RLock lock2 = redisson.getLock(key2);
    RLock lock3 = redisson.getLock(key3);

    RedissonRedLock redLock = new RedissonRedLock(lock1, lock2, lock3);
    try {
        boolean tryLock = redLock.tryLock(10, 30, TimeUnit.SECONDS);
        if (tryLock){
            int stock = Integer.parseInt(stringRedisTemplate.opsForValue().get("stock"));
            if (stock>0){
                int resultStock = stock - 1;
                stringRedisTemplate.opsForValue().set("stock",String.valueOf(resultStock));
                System.out.println("A服务 商品扣除成功，剩余商品:"+resultStock);
            }else {
                System.out.println("A服务 库存不足...");
            }
        }
    }catch (InterruptedException e){
        e.printStackTrace();
    }finally {
        redLock.unlock();
    }
    return "end";
}
```

多节点的Redis实现的分布式锁 RedLock 可以有效防止单点故障。

我们再来细说他为什么可以实现这样的功能

1. 获取当前时间戳
2. client 尝试按顺序使用相同的 key、value 获取所有 redis 服务的锁，在获取锁的过程中，获取时间比锁过期时间短的多，这是为了不要过长时间等待已经关闭的 Redis 服务，并且试着获取下一个 Redis 实例
   比如 TTL为5秒，设置获取锁的时间最多用1秒，如果1秒都没有获取到锁，那就放弃这个锁，立刻获取下一个锁
3. client通过获取所有能获取的时间减去第一步的时间，这个时间差小于TTL时间并且少于有3个redis实例成功获取锁，才算正在的获取锁成功
4. 如果成功拿到锁，锁的真正有效时间是 TTL 减去第三步的时间差，假如TTL是5秒，获取锁用了2秒，真正有效的就是3秒。
5. 如果客户端由于某些情况获取锁失败，便会开始解锁所有redis，因为可能也就获取了小于3个锁，必须释放，否则影响其他client获取锁

开始时间是T1是 0:00 ，获取锁时所有 key-value 都是一样的，TTL 是5min，假设漂移时间 1min，最后结束时间是 T2是 0:02 ，所以此锁最小有效时间为：TTL-(T2-T1)-漂移时间 = 5min - (0:02 - 0:00) -1min = 2min

* RedLock 算法是否是异步算法？

可以看成是同步算法，因为即使进程间（多个电脑间）没有同步时间，但是每个进程时间流速大致相同，并且时钟漂移相对于 TTL 较小，可以忽略，所以可以看成同步算法。

* RedLock 失败重试

当client 不能获取锁时，应该在随机时间后重试获取锁，并且最好在同一时刻并发把set命令发给所有redis实例，而且对于已经获取锁的client在完成任务后及时释放锁

* RedLock 释放锁

由于释放锁会判断这个锁value是不是自己设置的，如果是才删除，所以释放的时候很简单，只要向所有实例发出释放锁的命令，不用考虑是否成功释放

参考资料：

https://zhuanlan.zhihu.com/p/565811400

https://blog.csdn.net/qq\_38322527/article/details/112603213

---

---
url: /Redis/Redis实战/1_Redis实战之解决限流问题.md
---

# Redis解决限流问题

## 1、概述

在突发情况下（最常见的场景就是秒杀、抢购），瞬时大流量会直接将系统打垮，无法对外提供服务。为了防止出现这种情况最常见的解决方案之一就是限流，当请求达到一定的并发数或速率，就进行等待、排队、降级、拒绝服务等。

限流是对某一时间窗口内的请求数进行限制，保持系统的可用性和稳定性，防止因流量暴增而导致的系统运行缓慢或宕机。

常见的限流算法有三种：

1） 计数器限流

计数器限流算法是最为简单粗暴的解决方案，主要用来限制总并发数，比如数据库连接池大小、线程池大小、接口访问并发数等都是使用计数器算法。

如：使用 AomicInteger 来进行统计当前正在并发执行的次数，如果超过域值就直接拒绝请求，提示系统繁忙。

2） 漏桶算法

![img](/assets/1746338-20230509161132738-2105902408.KGApG104.png)

漏桶算法思路很简单，我们把水比作是`请求`，漏桶比作是`系统处理能力极限`，水先进入到漏桶里，漏桶里的水按一定速率流出，当流出的速率小于流入的速率时，由于漏桶容量有限，后续进入的水直接溢出（拒绝请求），以此实现限流。

3）令牌桶算法

![img](/assets/1746338-20230509161201059-1078819893.2UO_-g1I.png)

令牌桶算法的原理也比较简单，我们可以理解成医院的挂号看病，只有拿到号以后才可以进行诊病。

系统会维护一个令牌（`token`）桶，以一个恒定的速度往桶里放入令牌（`token`），这时如果有请求进来想要被处理，则需要先从桶里获取一个令牌（`token`），当桶里没有令牌（`token`）可取时，则该请求将被拒绝服务。令牌桶算法通过控制桶的容量、发放令牌的速率，来达到对请求的限制。

## 2、具体实现

对于单机版，直接使用Guava即可。

Google开源工具包Guava提供了限流工具类RateLimiter，该类基于令牌桶算法实现流量限制，使用十分方便，而且十分高效。

对于分布式版

可以 **通过`请求唯一ID`+`redis分布式锁`来防止接口重复提交**

本次方案的核心流程图

![image-20231009113807127](/assets/image-20231009113807127.Dj9U9Xt7.png)

实现的逻辑，流程如下：

* 1.用户点击提交按钮，服务端接受到请求后，通过规则计算出本次请求唯一ID值
* 2.使用`redis`的分布式锁服务，对请求 ID 在限定的时间内尝试进行加锁，如果加锁成功，继续后续流程；如果加锁失败，说明服务正在处理，请勿重复提交
* 3.最后一步，如果加锁成功后，需要将锁手动释放掉，以免再次请求时，提示同样的信息

引入缓存服务后，防止重复提交的大体思路如上，实践代码如下！

#### 2.1、引入 redis 组件

本次 demo 项目是基于`SpringBoot`版本进行构建，添加相关的`redis`依赖环境如下：

```xml
<!-- 引入springboot -->
<parent>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-parent</artifactId>
    <version>2.1.0.RELEASE</version>
</parent>

......

<!-- Redis相关依赖包，采用jedis作为客户端 -->
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-data-redis</artifactId>
    <exclusions>
        <exclusion>
            <groupId>redis.clients</groupId>
            <artifactId>jedis</artifactId>
        </exclusion>
        <exclusion>
            <artifactId>lettuce-core</artifactId>
            <groupId>io.lettuce</groupId>
        </exclusion>
    </exclusions>
</dependency>
<dependency>
    <groupId>redis.clients</groupId>
    <artifactId>jedis</artifactId>
</dependency>
<dependency>
    <groupId>org.apache.commons</groupId>
    <artifactId>commons-pool2</artifactId>
</dependency>
```

#### 2.2、添加 redis 环境配置

在全局配置`application.properties`文件中，添加`redis`相关服务配置如下

```properties
# 项目名
spring.application.name=springboot-example-submit

# Redis数据库索引（默认为0）
spring.redis.database=1
# Redis服务器地址
spring.redis.host=127.0.0.1
# Redis服务器连接端口
spring.redis.port=6379
# Redis服务器连接密码（默认为空）
spring.redis.password=
# Redis服务器连接超时配置
spring.redis.timeout=1000

# 连接池配置
spring.redis.jedis.pool.max-active=8
spring.redis.jedis.pool.max-wait=1000
spring.redis.jedis.pool.max-idle=8
spring.redis.jedis.pool.min-idle=0
spring.redis.jedis.pool.time-between-eviction-runs=100
```

#### 2.3、编写服务验证逻辑，通过 aop 代理方式实现

首先创建一个`@SubmitLimit`注解，通过这个注解来进行方法代理拦截！

```java
@Retention(RetentionPolicy.RUNTIME)
@Target({ElementType.METHOD})
@Documented
public @interface SubmitLimit {

    /**
     * 指定时间内不可重复提交（仅相对上一次发起请求时间差）,单位毫秒
     * @return
     */
    int waitTime() default 1000;

    /**
     * 指定请求头部key，可以组合生成签名
     * @return
     */
    String[] customerHeaders() default {};


    /**
     * 自定义重复提交提示语
     * @return
     */
    String customerTipMsg() default "";
}
```

编写方法代理服务，增加防止重复提交的验证，实现了逻辑如下！

```java
@Order(1)
@Aspect
@Component
public class SubmitLimitAspect {

    private static final Logger LOGGER = LoggerFactory.getLogger(SubmitLimitAspect.class);

    /**
     * redis分割符
     */
    private static final String REDIS_SEPARATOR = ":";

    /**
     * 默认锁对应的值
     */
    private static final String DEFAULT_LOCK_VALUE = "DEFAULT_SUBMIT_LOCK_VALUE";

    /**
     * 默认重复提交提示语
     */
    private static final String DEFAULT_TIP_MSG = "服务正在处理，请勿重复提交！";


    @Value("${spring.application.name}")
    private String applicationName;

    @Autowired
    private RedisLockService redisLockService;


    /**
     * 方法调用环绕拦截
     */
    @Around(value = "@annotation(com.example.submittoken.config.annotation.SubmitLimit)")
    public Object doAround(ProceedingJoinPoint joinPoint){
        HttpServletRequest request = getHttpServletRequest();
        if(Objects.isNull(request)){
            return ResResult.getSysError("请求参数不能为空！");
        }
        //获取注解配置的参数
        SubmitLimit submitLimit = getSubmitLimit(joinPoint);
        //组合生成key，通过key实现加锁和解锁
        String lockKey = buildSubmitLimitKey(joinPoint, request, submitLimit.customerHeaders());
        //尝试在指定的时间内加锁
        boolean lock = redisLockService.tryLock(lockKey, DEFAULT_LOCK_VALUE, Duration.ofMillis(submitLimit.waitTime()));
        if(!lock){
            String tipMsg = StringUtils.isEmpty(submitLimit.customerTipMsg()) ? DEFAULT_TIP_MSG : submitLimit.customerTipMsg();
            return ResResult.getSysError(tipMsg);
        }
        try {
            //继续执行后续流程
            return execute(joinPoint);
        } finally {
            //执行完毕之后，手动将锁释放
            redisLockService.releaseLock(lockKey, DEFAULT_LOCK_VALUE);
        }
    }

    /**
     * 执行任务
     * @param joinPoint
     * @return
     */
    private Object execute(ProceedingJoinPoint joinPoint){
        try {
            return joinPoint.proceed();
        } catch (CommonException e) {
            return ResResult.getSysError(e.getMessage());
        } catch (Throwable e) {
            LOGGER.error("业务处理发生异常，错误信息：",e);
            return ResResult.getSysError(ResResultEnum.DEFAULT_ERROR_MESSAGE);
        }
    }


    /**
     * 获取请求对象
     * @return
     */
    private HttpServletRequest getHttpServletRequest(){
        RequestAttributes ra = RequestContextHolder.getRequestAttributes();
        ServletRequestAttributes sra = (ServletRequestAttributes)ra;
        HttpServletRequest request = sra.getRequest();
        return request;
    }

    /**
     * 获取注解值
     * @param joinPoint
     * @return
     */
    private SubmitLimit getSubmitLimit(JoinPoint joinPoint){
        MethodSignature methodSignature = (MethodSignature) joinPoint.getSignature();
        Method method = methodSignature.getMethod();
        SubmitLimit submitLimit = method.getAnnotation(SubmitLimit.class);
        return submitLimit;
    }

    /**
     * 组合生成lockKey
     * 生成规则：项目名+接口名+方法名+请求参数签名（对请求头部参数+请求body参数，取SHA1值）
     * @param joinPoint
     * @param request
     * @param customerHeaders
     * @return
     */
    private String buildSubmitLimitKey(JoinPoint joinPoint, HttpServletRequest request, String[] customerHeaders){
        //请求参数=请求头部+请求body
        String requestHeader = getRequestHeader(request, customerHeaders);
        String requestBody = getRequestBody(joinPoint.getArgs());
        String requestParamSign = DigestUtils.sha1Hex(requestHeader + requestBody);
        String submitLimitKey = new StringBuilder()
                .append(applicationName)
                .append(REDIS_SEPARATOR)
                .append(joinPoint.getSignature().getDeclaringType().getSimpleName())
                .append(REDIS_SEPARATOR)
                .append(joinPoint.getSignature().getName())
                .append(REDIS_SEPARATOR)
                .append(requestParamSign)
                .toString();
        return submitLimitKey;
    }


    /**
     * 获取指定请求头部参数
     * @param request
     * @param customerHeaders
     * @return
     */
    private String getRequestHeader(HttpServletRequest request, String[] customerHeaders){
        if (Objects.isNull(customerHeaders)) {
            return "";
        }
        StringBuilder sb = new StringBuilder();
        for (String headerKey : customerHeaders) {
            sb.append(request.getHeader(headerKey));
        }
        return sb.toString();
    }


    /**
     * 获取请求body参数
     * @param args
     * @return
     */
    private String getRequestBody(Object[] args){
        if (Objects.isNull(args)) {
            return "";
        }
        StringBuilder sb = new StringBuilder();
        for (Object arg : args) {
            if (arg instanceof HttpServletRequest
                    || arg instanceof HttpServletResponse
                    || arg instanceof MultipartFile
                    || arg instanceof BindResult
                    || arg instanceof MultipartFile[]
                    || arg instanceof ModelMap
                    || arg instanceof Model
                    || arg instanceof ExtendedServletRequestDataBinder
                    || arg instanceof byte[]) {
                continue;
            }
            sb.append(JacksonUtils.toJson(arg));
        }
        return sb.toString();
    }
}
```

部分校验逻辑用到了`redis`分布式锁，具体实现逻辑如下：

```java
/**
 * redis分布式锁服务类
 * 采用LUA脚本实现，保证加锁、解锁操作原子性
 *
 */
@Component
public class RedisLockService {

    /**
     * 分布式锁过期时间，单位秒
     */
    private static final Long DEFAULT_LOCK_EXPIRE_TIME = 60L;

    @Autowired
    private StringRedisTemplate stringRedisTemplate;

    /**
     * 尝试在指定时间内加锁
     * @param key
     * @param value
     * @param timeout 锁等待时间
     * @return
     */
    public boolean tryLock(String key,String value, Duration timeout){
        long waitMills = timeout.toMillis();
        long currentTimeMillis = System.currentTimeMillis();
        do {
            boolean lock = lock(key, value, DEFAULT_LOCK_EXPIRE_TIME);
            if (lock) {
                return true;
            }
            try {
                Thread.sleep(1L);
            } catch (InterruptedException e) {
                Thread.interrupted();
            }
        } while (System.currentTimeMillis() < currentTimeMillis + waitMills);
        return false;
    }

    /**
     * 直接加锁
     * @param key
     * @param value
     * @param expire
     * @return
     */
    public boolean lock(String key,String value, Long expire){
        String luaScript = "if redis.call('setnx', KEYS[1], ARGV[1]) == 1 then return redis.call('expire', KEYS[1], ARGV[2]) else return 0 end";
        RedisScript<Long> redisScript = new DefaultRedisScript<>(luaScript, Long.class);
        Long result = stringRedisTemplate.execute(redisScript, Collections.singletonList(key), value, String.valueOf(expire));
        return result.equals(Long.valueOf(1));
    }


    /**
     * 释放锁
     * @param key
     * @param value
     * @return
     */
    public boolean releaseLock(String key,String value){
        String luaScript = "if redis.call('get', KEYS[1]) == ARGV[1] then return redis.call('del', KEYS[1]) else return 0 end";
        RedisScript<Long> redisScript = new DefaultRedisScript<>(luaScript, Long.class);
        Long result = stringRedisTemplate.execute(redisScript, Collections.singletonList(key),value);
        return result.equals(Long.valueOf(1));
    }
}
```

部分代码使用到了序列化相关类`JacksonUtils`，源码如下：

```java
public class JacksonUtils {

    private static final Logger LOGGER = LoggerFactory.getLogger(JacksonUtils.class);


    private static final ObjectMapper objectMapper = new ObjectMapper();

    static {
        // 对象的所有字段全部列入
        objectMapper.setSerializationInclusion(JsonInclude.Include.ALWAYS);
        // 忽略未知的字段
        objectMapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false);
        // 读取不认识的枚举时，当null值处理
        objectMapper.configure(DeserializationFeature.READ_UNKNOWN_ENUM_VALUES_AS_NULL, true);
//        序列化忽略未知属性
        objectMapper.configure(SerializationFeature.FAIL_ON_EMPTY_BEANS, false);
        //忽略字段大小写
        objectMapper.configure(MapperFeature.ACCEPT_CASE_INSENSITIVE_PROPERTIES, true);

        objectMapper.configure(JsonParser.Feature.AUTO_CLOSE_SOURCE, true);
        SimpleModule module = new SimpleModule();
        module.addSerializer(Long.class, ToStringSerializer.instance);
        module.addSerializer(Long.TYPE, ToStringSerializer.instance);
        objectMapper.registerModule(module);
    }

    public static String toJson(Object object) {
        if (object == null) {
            return null;
        }
        try {
            return objectMapper.writeValueAsString(object);
        } catch (Exception e) {
            LOGGER.error("序列化失败",e);
        }
        return null;
    }

    public static <T> T fromJson(String json, Class<T> classOfT) {
        if (json == null) {
            return null;
        }
        try {
            return objectMapper.readValue(json, classOfT);
        } catch (Exception e) {
            LOGGER.error("反序列化失败",e);
        }
        return null;
    }

    public static <T> T fromJson(String json, Type typeOfT) {
        if (json == null) {
            return null;
        }
        try {
            return objectMapper.readValue(json, objectMapper.constructType(typeOfT));
        } catch (Exception e) {
            LOGGER.error("反序列化失败",e);
        }
        return null;
    }
}
```

#### 2.4、在相关的业务接口上，增加SubmitLimit注解即可

```java
@RestController
@RequestMapping("order")
public class OrderController {

    @Autowired
    private OrderService orderService;

    /**
     * 下单，指定请求头部参与请求唯一值计算
     * @param request
     * @return
     */
    @SubmitLimit(customerHeaders = {"appId", "token"}, customerTipMsg = "正在加紧为您处理，请勿重复下单！")
    @PostMapping(value = "confirm")
    public ResResult confirmOrder(@RequestBody OrderConfirmRequest request){
        //调用订单下单相关逻辑
        orderService.confirm(request);
        return ResResult.getSuccess();
    }
}
```

**其中最关键的一个步就是将唯一请求 ID  的生成，放在服务端通过组合来实现，在保证防止接口重复提交的效果同时，也可以显著的降低接口测试复杂度**！

源码地址：

https://github.com/Daneliya/springboot\_chowder/tree/main/springboot\_redis\_repeat\_submit

参考资料：

https://cloud.tencent.com/developer/article/2185751

https://www.cnblogs.com/zys2019/p/17385568.html

https://blog.csdn.net/Jason009/article/details/127896906

[redis分布式锁的实现（setNx命令和Lua脚本）](https://blog.csdn.net/qq_26222859/article/details/51516011)

[Redis分布式锁的一点小理解](https://www.cnblogs.com/zhangweicheng/p/11495971.html)

[为什么使用LUA脚本](https://blog.csdn.net/weixin_44259720/article/details/121968837)

https://www.cnblogs.com/wangyingshuo/p/14510524.html

Redis实现分布式锁的7种方案 https://www.cnblogs.com/wangyingshuo/p/14510524.html

https://www.cnblogs.com/zys2019/p/17385568.html

---

---
url: /Redis/Redis数据结构/Redis数据结构.md
---

# Redis数据结构

## 一、Redis[数据结构](https://so.csdn.net/so/search?spm=a2c6h.13046898.publish-article.41.3eaf6ffaRQROo8\&q=数据结构)介绍

Redis是一个key-value的数据库，key一般是String类型，value的类型可以是多种。

可以在Redis的官方网站上查看一些命令，<https://redis.io/commands>

![image-20240603221115734](/assets/image-20240603221115734.BoETw0M-.png)

也可以在控制台上通过help命令查看

```sh
# 登录redis后台
redis-cli
# 查看帮助
127.0.0.1:6379> help @generic
```

Redis 主要支持以下几种数据类型：

* **string（字符串）:** 基本的数据存储单元，可以存储字符串、整数或者浮点数。
* **hash（哈希）:** 一个键值对集合，可以存储多个字段。
* **list（列表）:** 一个简单的列表，可以存储一系列的字符串元素。
* **set（集合）:** 一个无序集合，可以存储不重复的字符串元素。
* **zset（sorted set：有序集合）：**  类似于集合，但是每个元素都有一个分数（score）与之关联。
* **位图（Bitmaps）：** 基于字符串类型，可以对每个位进行操作。
* **超日志（HyperLogLogs）：** 用于基数统计，可以估算集合中的唯一元素数量。
* **地理空间（Geospatial）：** 用于存储地理位置信息。
* **发布/订阅（Pub/Sub）：** 一种消息通信模式，允许客户端订阅消息通道，并接收发布到该通道的消息。
* **流（Streams）：** 用于消息队列和日志存储，支持消息的持久化和时间排序。
* **模块（Modules）：** Redis 支持动态加载模块，可以扩展 Redis 的功能。

## 二、Redis通用命令

通用命令是部分数据类型的，都可以使用的类型，常见的有：

### KEYS

`KEYS`：查看符合模板的所以key ,**不建议在生产环境设备上使用**

```sh
127.0.0.1:6379> help keys

  KEYS pattern
  summary: Find all keys matching the given pattern
  since: 1.0.0
  group: generic

127.0.0.1:6379> keys *
 1) "item:stock:id:10003"
 2) "item:id:10004"
 3) "item:id:10003"
 4) "item:stock:id:10005"
 5) "item:stock:id:10001"
 6) "item:stock:id:10002"
 7) "item:id:10005"
 8) "item:id:10002"
 9) "item:stock:id:10004"
10) "item:id:10001"
11) "login:3"
127.0.0.1:6379>
```

### DEL

`DEL`：删除一个指定的key

```sh
127.0.0.1:6379> help del

  DEL key [key ...]
  summary: Delete a key
  since: 1.0.0
  group: generic

127.0.0.1:6379> MSET k1 v1 k2 v3 k3 v3
OK
127.0.0.1:6379> DEL k1 k2
(integer) 2
127.0.0.1:6379> KEYS *
 1) "k3"
127.0.0.1:6379>
```

### EXISTS

`EXISTS`:判断一个key是否存在

```sh
127.0.0.1:6379> help exists

  EXISTS key [key ...]
  summary: Determine if a key exists
  since: 1.0.0
  group: generic

127.0.0.1:6379> exists k3
(integer) 1
127.0.0.1:6379>
```

### EXPIRE

`EXPIRE`:给一个key设置一个有效期，有效期到期时key自动被删除

### TTL

`TTL`：查看一个key的剩余有效期

```sh
127.0.0.1:6379> help expire

  EXPIRE key seconds
  summary: Set a key's time to live in seconds
  since: 1.0.0
  group: generic

127.0.0.1:6379> expire k3 20
(integer) 1

127.0.0.1:6379> help ttl

  TTL key
  summary: Get the time to live for a key
  since: 1.0.0
  group: generic

127.0.0.1:6379> ttl k3
(integer) 16
127.0.0.1:6379>
127.0.0.1:6379> ttl k3
(integer) 9
127.0.0.1:6379> ttl k3
(integer) 3
127.0.0.1:6379> ttl k3
(integer) -2 # 有效期结束
```

## 三、String类型

### 特点

* String类型，也就是字符串类型，是Redis中最简单的存储类型

* 其value是字符串，不过根据字符串的格式不同，又可以分为3类：

* * String：普通字符串
  * int：整数类型，可以做自增、自减操作
  * float：浮点类型，可以做自增、自减操作

* 不管是哪种格式，底层都是字节数组形式存储，只不过是编码方式不同，字符串类型的最大空间不能超过512M

* 可以存储字符串或整数值，支持原子性的增减操作（incr/decr）。

| KEY   | VALUE       |
| ----- | ----------- |
| msg   | hello world |
| num   | 10          |
| score | 99.5        |

### 适用场景

单个值的缓存，计数器（如网页访问次数），简单的KV存储。

### String常见的命令

* * `SET`：添加或者修改已经存在的一个String类型的键值对
  * `GET`：根据key获得String类型的value
  * `MSET`：批量添加多个String类型的键值对
  * `MGET`：根据多个key获取多个String类型的value
  * `INCR`：让一个整型的key自增1
  * `INCRBY`：让一个整型的key自增并指定步长，例如：`incrby num 2` 表示让num值自增2
  * `INCRBYFLOAT`：让一个浮点类型的数字自增并指定步长
  * `SETNX`：添加一个String类型的键值对，前提是这个key不存在，否则不执行
  * `SETEX`：添加一个String类型的键值对，并且指定有效期

## 四、Hash类型

### 特点

* Hash类型，也叫散列，其value是一个无序字典，类似于Java中的HashMap结构
* String结构是将对象序列化为JSON字符串后存储，当需要修改对象某个字段时很不方便，Hash结构可以将对象中的**每个字段独立存储**，可以针对单个字段做CRUD

### 适用场景

当一个实体拥有多个属性且这些属性都需要存储时，如用户信息、商品详情等。

### Hash类型的常见命令

* `HSET key field value`：添加或者修改hash类型key的field的值
* `HGET key field`：获取一个hash类型key的field的值
* `HMSET`：批量添加多个hash类型key的field的值
* `HMGET`：批量获取多个hash类型key的field的值
* `HMGETALL`：获取一个hash类型的key中的所有field和value
* `HKEYS`：获取一个hash类型的key中的所有field
* `HVALS`：获取一个hash类型的key中的所有value
* `HINCRBY`：染一个hash类型key的字段值自增并指定步长
* `HSETNX`：添加一个hash类型的key的field值，前提是这个field不存在，否则不执行

## 五、List类型

### 特点

* Redis中的List类型与Java中的LinkedList类似，可以看成是一个**双向链表**结构，既可以支持正向检索也可以支持反向检索，支持在头部或尾部进行快速插入和删除操作。

* 特征也与LinkedList类似：

* * 有序
  * 元素可以重复
  * 插入和删除快
  * 查询速度一般

* 常用来存储一些有序数据

### 适用场景

实现简单的队列或栈，如消息队列、最新评论列表。

### List类型的常见命令

* `LPUSH key element...`：向列表左侧插入一个或多个元素
* `LPOP key`：移除并返回列表左侧第一个元素，没有则返回nil
* `RPUSH key element...`：向列表右侧插入一个或多个元素
* `RPOP key`：移除并返回列表右侧的第一个元素
* `LRANGE key star end`：返回一段角标范围内的所有元素
* `BLPOP`和`BRPOP`：与`LPOP`和`RPOP`类似，只不过在没有元素时等待指定时间，而不是直接返回nil

## 六、Set类型

### 特点

* Redis的Set结构与Java中的HashSet类似，可以看做是一个value为null的HashMap，因为也是一个hash表，因此具备与HashSet类似的特征：

* * 无序
  * 元素不可重复
  * 查找快
  * 支持交集、并集、差集等功能

### 适用场景

去重操作，如关注列表、标签系统。

### Set类型的常见命令

* `SADD key member...`：向set中添加一个或多个元素
* `SREM key member...`：移除set中的指定元素
* `SCARD key`：返回set中元素的个数
* `SISMEMBER key member`：判断衣一个元素是否存在于set中
* `SMEMBERS`：获取set中的所有元素
* `SINTER key1 key2...`：求key1与key2的交集
* `SDIFF key1 key2..`.：求key1与key2的差集
* `SUNION key1 key2...`：求key1与key2的并集

## 七、SortedSet类型

### 特点

* Redis的SortedSet是一个可排序的set集合，与Java中的TreeSet有些类似，但是底层数据结构却差别很大，SortedSet中的每个元素都带有一个score属性，可以基于score属性对元素排序，底层的实现是一个跳表（SkipList）加hash表

* SortedSet有以下特性：

* * 可排序
  * 元素不重复
  * 查询速度快

### 适用场景

因为SortedSet的可排序性，经常用来实现排行榜这样的功能。

### SortedSet类型的常见命令

* `ZADD key member`：添加一个或多个元素到sorted set ，如果已经存在则更新其score值
* `ZREM key member`：删除sorted set中的一个指定元素
* `ZSCORE key member`：获取sorted set中的指定元素的score值
* `ZRANK key member`：获取sorted set中的指定元素排名
* `ZCARD key`：获取sorted set中的元素个数
* `ZCOUNT key min max`：统计score值在给定的范围内所有元素的个数
* `ZINCRBY key increment member` ：让sorted set中的指定元素自增，步长为指定的increment值
* `ZRANGE key min max` ：按照score排序后，获取指定排名范围内的元素
* `ZRANGEBYSCORE key min max`：按照score排序后，获取指定score范围内的元素
* `ZDIFF`、`ZINTER`、`ZUNION`：求差集、交集、并集

## 参考资料

https://www.runoob.com/redis/redis-data-types.html

https://developer.aliyun.com/article/1518145

https://developer.aliyun.com/article/1505666

---

---
url: /Redis/Redis基础/5_Redis延迟队列.md
---

# Redis延迟队列

> 本文代码：[springboot\_chowder/springboot\_redisson at main · Daneliya/springboot\_chowder (github.com)](https://github.com/Daneliya/springboot_chowder/tree/main/springboot_redisson)

具体示例

[redis作延迟队列简单实现\_redisdelayqueuehandle-CSDN博客](https://blog.csdn.net/qq_43163943/article/details/120493022)

redis安装
https://blog.csdn.net/Biteht/article/details/128548659

redis延迟队列的几种方式 `KeyExpirationEventMessageListener`是Spring Data Redis中的监听器，用于监听Redis中Key的过期事件。 Redis使用一种称为“过期事件（expiry event）”的机制来处理键的过期。当一个键的过期时间到达时，Redis会生成一个事件，通知订阅了该事件的客户端 使用`ZSET`（有序集合，Sorted Set）来实现延迟任务调度（如订单超时取消）是一种有效的方法 使用 Redisson 创建延迟队列。Redisson 提供了 `RDelayedQueue` 接口和 `RQueue` 接口来实现延迟队列。

单节点及集群配置

https://www.cnblogs.com/ming-blogs/p/16268515.html

redis实现延时队列的两种方式

https://blog.csdn.net/qq\_36268452/article/details/113392170

详解！基于Redis解决业务场景中延迟队列的应用实践,你不得不服啊

https://zhuanlan.zhihu.com/p/187137027

基于Redisson实现延迟队列

https://www.jianshu.com/p/a1b3aa87f78b

分布式锁

https://www.cnblogs.com/cjsblog/p/11273205.html

https://zhuanlan.zhihu.com/p/343811173

https://www.jianshu.com/p/8853b34f7c8b

https://zhuanlan.zhihu.com/p/548827425

java.lang.AbstractMethodError: org.redisson.spring.data.connection.RedissonReactiveRedisConnection.close()V

报错原因：spring boot版本和[redission](https://so.csdn.net/so/search?q=redission\&spm=1001.2101.3001.7020)版本不兼容
报错版本：spring boot 2.0.5   <-->  redission 3.12.5

---

---
url: /Redis/Redis基础/2_RedisInsight部署使用.md
---

# RedisInsight部署使用

![image-20240108002458009](/assets/image-20240108002458009.BfHQRjcR.png)

![image-20240108002538598](/assets/image-20240108002538598.D_EoIf5-.png)

![image-20240108002559983](/assets/image-20240108002559983.BNlYwdpk.png)

![image-20240108002732304](/assets/image-20240108002732304.WfmbDIWW.png)

---

---
url: /Redis/Redis基础/RedisSearch全文搜索引擎.md
---

# RedisSearch全文搜索引擎

https://learn.lianglianglee.com/%E4%B8%93%E6%A0%8F/Redis%20%E6%A0%B8%E5%BF%83%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98/32%20%E5%AE%9E%E6%88%98%EF%BC%9ARediSearch%20%E9%AB%98%E6%80%A7%E8%83%BD%E7%9A%84%E5%85%A8%E6%96%87%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E.md

https://segmentfault.com/a/1190000044082999

https://blog.csdn.net/qq\_32447301/article/details/107576455

https://www.cnblogs.com/dyhuang/p/18191659

https://www.cnblogs.com/jingzh/p/17033403.html#13-redisearchredisjson%E5%AE%89%E8%A3%85

---

---
url: /Redis/Redis基础/RedisTemplate.md
---

# RedisTemplate

https://developer.aliyun.com/article/1548773

https://cloud.tencent.com/developer/article/1677683

---

---
url: /Java/架构设计/分布式/03.分布式消息队列/01.RocketMQ/RocketMQ-01.md
---

# 1. MQ介绍

\##1.1 为什么要用MQ

消息队列是一种“先进先出”的数据结构

![](/assets/queue1.1Ub1lY05.png)

其应用场景主要包含以下3个方面

* 应用解耦

系统的耦合性越高，容错性就越低。以电商应用为例，用户创建订单后，如果耦合调用库存系统、物流系统、支付系统，任何一个子系统出了故障或者因为升级等原因暂时不可用，都会造成下单操作异常，影响用户使用体验。

![](/assets/解耦1.Ccby4sqe.png)

使用消息队列解耦合，系统的耦合性就会提高了。比如物流系统发生故障，需要几分钟才能来修复，在这段时间内，物流系统要处理的数据被缓存到消息队列中，用户的下单操作正常完成。当物流系统回复后，补充处理存在消息队列中的订单消息即可，终端系统感知不到物流系统发生过几分钟故障。

![](/assets/解耦2.pGIvVlOc.png)

* 流量削峰

![](/assets/mq-5.BQn5IWEH.png)

应用系统如果遇到系统请求流量的瞬间猛增，有可能会将系统压垮。有了消息队列可以将大量请求缓存起来，分散到很长一段时间处理，这样可以大大提到系统的稳定性和用户体验。

![](/assets/mq-6.Ba0rTTM6.png)

一般情况，为了保证系统的稳定性，如果系统负载超过阈值，就会阻止用户请求，这会影响用户体验，而如果使用消息队列将请求缓存起来，等待系统处理完毕后通知用户下单完毕，这样总不能下单体验要好。

处于经济考量目的：

业务系统正常时段的QPS如果是1000，流量最高峰是10000，为了应对流量高峰配置高性能的服务器显然不划算，这时可以使用消息队列对峰值流量削峰

* 数据分发

![](/assets/mq-1.me0O6Ssl.png)

通过消息队列可以让数据在多个系统更加之间进行流通。数据的产生方不需要关心谁来使用数据，只需要将数据发送到消息队列，数据使用方直接在消息队列中直接获取数据即可

![](/assets/mq-2.BxDA-zWv.png)

## 1.2 MQ的优点和缺点

优点：解耦、削峰、数据分发

缺点包含以下几点：

* 系统可用性降低

  系统引入的外部依赖越多，系统稳定性越差。一旦MQ宕机，就会对业务造成影响。

  如何保证MQ的高可用？

* 系统复杂度提高

  MQ的加入大大增加了系统的复杂度，以前系统间是同步的远程调用，现在是通过MQ进行异步调用。

  如何保证消息没有被重复消费？怎么处理消息丢失情况？那么保证消息传递的顺序性？

* 一致性问题

  A系统处理完业务，通过MQ给B、C、D三个系统发消息数据，如果B系统、C系统处理成功，D系统处理失败。

  如何保证消息数据处理的一致性？

## 1.3 各种MQ产品的比较

常见的MQ产品包括Kafka、ActiveMQ、RabbitMQ、RocketMQ。

![](/assets/MQ比较.BghTv-Mb.png)

# 2. RocketMQ快速入门

RocketMQ是阿里巴巴2016年MQ中间件，使用Java语言开发，在阿里内部，RocketMQ承接了例如“双11”等高并发场景的消息流转，能够处理万亿级别的消息。

## 2.1 准备工作

### 2.1.1 下载RocketMQ

RocketMQ最新版本：4.5.1

[下载地址](https://www.apache.org/dyn/closer.cgi?path=rocketmq/4.5.1/rocketmq-all-4.5.1-bin-release.zip)

### 2.2.2 环境要求

* Linux64位系统

* JDK1.8(64位)

* 源码安装需要安装Maven 3.2.x

## 2.2 安装RocketMQ

### 2.2.1 安装步骤

本教程以二进制包方式安装

1. 解压安装包
2. 进入安装目录

### 2.2.2 目录介绍

* bin：启动脚本，包括shell脚本和CMD脚本
* conf：实例配置文件 ，包括broker配置文件、logback配置文件等
* lib：依赖jar包，包括Netty、commons-lang、FastJSON等

## 2.3 启动RocketMQ

1. 启动NameServer

```shell
# 1.启动NameServer
nohup sh bin/mqnamesrv &
# 2.查看启动日志
tail -f ~/logs/rocketmqlogs/namesrv.log
```

2. 启动Broker

```shell
# 1.启动Broker
nohup sh bin/mqbroker -n localhost:9876 &
# 2.查看启动日志
tail -f ~/logs/rocketmqlogs/broker.log 
```

* 问题描述：

  RocketMQ默认的虚拟机内存较大，启动Broker如果因为内存不足失败，需要编辑如下两个配置文件，修改JVM内存大小

```shell
# 编辑runbroker.sh和runserver.sh修改默认JVM大小
vi runbroker.sh
vi runserver.sh
```

* 参考设置：

`JAVA_OPT="${JAVA_OPT} -server -Xms256m -Xmx256m -Xmn128m -XX:MetaspaceSize=128m  -XX:MaxMetaspaceSize=320m"`

## 2.4 测试RocketMQ

### 2.4.1 发送消息

```sh
# 1.设置环境变量
export NAMESRV_ADDR=localhost:9876
# 2.使用安装包的Demo发送消息
sh tools.sh org.apache.rocketmq.example.quickstart.Producer
```

### 2.4.2 接收消息

```shell
# 1.设置环境变量
export NAMESRV_ADDR=localhost:9876
# 2.接收消息
sh tools.sh org.apache.rocketmq.example.quickstart.Consumer
```

## 2.5 关闭RocketMQ

```shell
# 1.关闭NameServer
sh bin/mqshutdown namesrv
# 2.关闭Broker
sh bin/mqshutdown broker
```

# 3. RocketMQ集群搭建

## 3.1 各角色介绍

* Producer：消息的发送者；举例：发信者
* Consumer：消息接收者；举例：收信者
* Broker：暂存和传输消息；举例：邮局
* NameServer：管理Broker；举例：各个邮局的管理机构
* Topic：区分消息的种类；一个发送者可以发送消息给一个或者多个Topic；一个消息的接收者可以订阅一个或者多个Topic消息
* Message Queue：相当于是Topic的分区；用于并行发送和接收消息

![](/assets/RocketMQ角色.C0wOeWNj.jpg)

## 3.2 集群搭建方式

### 3.2.1 集群特点

* NameServer是一个几乎无状态节点，可集群部署，节点之间无任何信息同步。

* Broker部署相对复杂，Broker分为Master与Slave，一个Master可以对应多个Slave，但是一个Slave只能对应一个Master，Master与Slave的对应关系通过指定相同的BrokerName，不同的BrokerId来定义，BrokerId为0表示Master，非0表示Slave。Master也可以部署多个。每个Broker与NameServer集群中的所有节点建立长连接，定时注册Topic信息到所有NameServer。

* Producer与NameServer集群中的其中一个节点（随机选择）建立长连接，定期从NameServer取Topic路由信息，并向提供Topic服务的Master建立长连接，且定时向Master发送心跳。Producer完全无状态，可集群部署。

* Consumer与NameServer集群中的其中一个节点（随机选择）建立长连接，定期从NameServer取Topic路由信息，并向提供Topic服务的Master、Slave建立长连接，且定时向Master、Slave发送心跳。Consumer既可以从Master订阅消息，也可以从Slave订阅消息，订阅规则由Broker配置决定。

### 3.2.3 集群模式

#### 1）单Master模式

这种方式风险较大，一旦Broker重启或者宕机时，会导致整个服务不可用。不建议线上环境使用,可以用于本地测试。

#### 2）多Master模式

一个集群无Slave，全是Master，例如2个Master或者3个Master，这种模式的优缺点如下：

* 优点：配置简单，单个Master宕机或重启维护对应用无影响，在磁盘配置为RAID10时，即使机器宕机不可恢复情况下，由于RAID10磁盘非常可靠，消息也不会丢（异步刷盘丢失少量消息，同步刷盘一条不丢），性能最高；
* 缺点：单台机器宕机期间，这台机器上未被消费的消息在机器恢复之前不可订阅，消息实时性会受到影响。

#### 3）多Master多Slave模式（异步）

每个Master配置一个Slave，有多对Master-Slave，HA采用异步复制方式，主备有短暂消息延迟（毫秒级），这种模式的优缺点如下：

* 优点：即使磁盘损坏，消息丢失的非常少，且消息实时性不会受影响，同时Master宕机后，消费者仍然可以从Slave消费，而且此过程对应用透明，不需要人工干预，性能同多Master模式几乎一样；
* 缺点：Master宕机，磁盘损坏情况下会丢失少量消息。

#### 4）多Master多Slave模式（同步）

每个Master配置一个Slave，有多对Master-Slave，HA采用同步双写方式，即只有主备都写成功，才向应用返回成功，这种模式的优缺点如下：

* 优点：数据与服务都无单点故障，Master宕机情况下，消息无延迟，服务可用性与数据可用性都非常高；
* 缺点：性能比异步复制模式略低（大约低10%左右），发送单个消息的RT会略高，且目前版本在主节点宕机后，备机不能自动切换为主机。

## 3.3 双主双从集群搭建

### 3.3.1 总体架构

消息高可用采用2m-2s（同步双写）方式

![](/assets/RocketMQ集群.CSNheTr-.png)

### 3.3.2 集群工作流程

1. 启动NameServer，NameServer起来后监听端口，等待Broker、Producer、Consumer连上来，相当于一个路由控制中心。
2. Broker启动，跟所有的NameServer保持长连接，定时发送心跳包。心跳包中包含当前Broker信息(IP+端口等)以及存储所有Topic信息。注册成功后，NameServer集群中就有Topic跟Broker的映射关系。
3. 收发消息前，先创建Topic，创建Topic时需要指定该Topic要存储在哪些Broker上，也可以在发送消息时自动创建Topic。
4. Producer发送消息，启动时先跟NameServer集群中的其中一台建立长连接，并从NameServer中获取当前发送的Topic存在哪些Broker上，轮询从队列列表中选择一个队列，然后与队列所在的Broker建立长连接从而向Broker发消息。
5. Consumer跟Producer类似，跟其中一台NameServer建立长连接，获取当前订阅Topic存在哪些Broker上，然后直接跟Broker建立连接通道，开始消费消息。

### 3.3.3 服务器环境

| **序号** | **IP**         | **角色**                 | **架构模式**    |
| -------- | -------------- | ------------------------ | --------------- |
| 1        | 192.168.25.135 | nameserver、brokerserver | Master1、Slave2 |
| 2        | 192.168.25.138 | nameserver、brokerserver | Master2、Slave1 |

### 3.3.4 Host添加信息

```bash
vim /etc/hosts
```

配置如下:

```bash
# nameserver
192.168.3.36 rocketmq-nameserver1
192.168.3.37 rocketmq-nameserver2
# broker
192.168.3.201 rocketmq-master1
192.168.3.201 rocketmq-slave2
192.168.3.37 rocketmq-master2
192.168.3.37 rocketmq-slave1
```

配置完成后, 重启网卡

```bash
systemctl restart network
```

### 3.3.5 防火墙配置

宿主机需要远程访问虚拟机的rocketmq服务和web服务，需要开放相关的端口号，简单粗暴的方式是直接关闭防火墙

```bash
# 关闭防火墙
systemctl stop firewalld.service 
# 查看防火墙的状态
firewall-cmd --state 
# 禁止firewall开机启动
systemctl disable firewalld.service
```

或者为了安全，只开放特定的端口号，RocketMQ默认使用3个端口：9876 、10911 、11011 。如果防火墙没有关闭的话，那么防火墙就必须开放这些端口：

* `nameserver` 默认使用 9876 端口
* `master` 默认使用 10911 端口
* `slave` 默认使用11011 端口

执行以下命令：

```bash
# 开放name server默认端口
firewall-cmd --remove-port=9876/tcp --permanent
# 开放master默认端口
firewall-cmd --remove-port=10911/tcp --permanent
# 开放slave默认端口 (当前集群模式可不开启)
firewall-cmd --remove-port=11011/tcp --permanent 
# 重启防火墙
firewall-cmd --reload
```

### 3.3.6 环境变量配置

```bash
vim /etc/profile
```

在profile文件的末尾加入如下命令

```bash
#set rocketmq
ROCKETMQ_HOME=/install/soft/rocketmq/rocketmq-all-4.8.0-bin-release
PATH=$PATH:$ROCKETMQ_HOME/bin
export ROCKETMQ_HOME PATH


/soft/mq/rocketmq
/soft/mq/rocketmq


ROCKETMQ_HOME=/soft/mq/rocketmq
PATH=$PATH:$ROCKETMQ_HOME/bin
export ROCKETMQ_HOME PATH
```

输入:wq! 保存并退出， 并使得配置立刻生效：

```bash
source /etc/profile


37
#set rocketmq
ROCKETMQ_HOME=/opt/rocketmq/rocketmq-4.8.0-bin
PATH=$PATH:$ROCKETMQ_HOME/bin
export ROCKETMQ_HOME PATH
```

### 3.3.7 创建消息存储路径

```bash
mkdir /usr/local/rocketmq/store
mkdir /usr/local/rocketmq/store/commitlog
mkdir /usr/local/rocketmq/store/consumequeue
mkdir /usr/local/rocketmq/store/index
```

### 3.3.8 broker配置文件

#### 1）master1

服务器：192.168.3.37

```sh
vi /opt/rocketmq/rocketmq-4.8.0-bin/conf/2m-2s-sync/broker-a.properties
```

修改配置如下：

```bash
#所属集群名字
brokerClusterName=rocketmq-cluster
#broker名字，注意此处不同的配置文件填写的不一样
brokerName=broker-a
#0 表示 Master，>0 表示 Slave
brokerId=0
#nameServer地址，分号分割
namesrvAddr=rocketmq-nameserver1:9876
#在发送消息时，自动创建服务器不存在的topic，默认创建的队列数
defaultTopicQueueNums=4
#是否允许 Broker 自动创建Topic，建议线下开启，线上关闭
autoCreateTopicEnable=true
#是否允许 Broker 自动创建订阅组，建议线下开启，线上关闭
autoCreateSubscriptionGroup=true
#Broker 对外服务的监听端口
listenPort=10911
#删除文件时间点，默认凌晨 4点
deleteWhen=04
#文件保留时间，默认 48 小时
fileReservedTime=120
#commitLog每个文件的大小默认1G
mapedFileSizeCommitLog=1073741824
#ConsumeQueue每个文件默认存30W条，根据业务情况调整
mapedFileSizeConsumeQueue=300000
#destroyMapedFileIntervalForcibly=120000
#redeleteHangedFileInterval=120000
#检测物理文件磁盘空间
diskMaxUsedSpaceRatio=88
#存储路径
storePathRootDir=/soft/mq/rocketmq/store
#commitLog 存储路径
storePathCommitLog=/soft/mq/rocketmq/store/commitlog
#消费队列存储路径存储路径
storePathConsumeQueue=/soft/mq/rocketmq/store/comsumequeue
#消息索引存储路径
storePathIndex=/soft/mq/rocketmq/store/index
#checkpoint 文件存储路径
storeCheckpoint=/soft/mq/rocketmq/store/checkpoint
#abort 文件存储路径
abortFile=/soft/mq/rocketmq/store/abort
#限制的消息大小
maxMessageSize=65536
#flushCommitLogLeastPages=4
#flushConsumeQueueLeastPages=2
#flushCommitLogThoroughInterval=10000
#flushConsumeQueueThoroughInterval=60000
#Broker 的角色
#- ASYNC_MASTER 异步复制Master
#- SYNC_MASTER 同步双写Master
#- SLAVE
brokerRole=SYNC_MASTER
#刷盘方式
#- ASYNC_FLUSH 异步刷盘
#- SYNC_FLUSH 同步刷盘
flushDiskType=SYNC_FLUSH
#checkTransactionMessageEnable=false
#发消息线程池数量
#sendMessageThreadPoolNums=128
#拉消息线程池数量
#pullMessageThreadPoolNums=128
```

#### 2）slave2

服务器：192.168.3.37

```sh
vi /opt/rocketmq/rocketmq-4.8.0-bin/conf/2m-2s-sync/broker-b-s.properties
```

修改配置如下：

```bash
#所属集群名字
brokerClusterName=rocketmq-cluster
#broker名字，注意此处不同的配置文件填写的不一样
brokerName=broker-b
#0 表示 Master，>0 表示 Slave
brokerId=1
#nameServer地址，分号分割
namesrvAddr=rocketmq-nameserver1:9876;rocketmq-nameserver2:9876
#在发送消息时，自动创建服务器不存在的topic，默认创建的队列数
defaultTopicQueueNums=4
#是否允许 Broker 自动创建Topic，建议线下开启，线上关闭
autoCreateTopicEnable=true
#是否允许 Broker 自动创建订阅组，建议线下开启，线上关闭
autoCreateSubscriptionGroup=true
#Broker 对外服务的监听端口
listenPort=11011
#删除文件时间点，默认凌晨 4点
deleteWhen=04
#文件保留时间，默认 48 小时
fileReservedTime=120
#commitLog每个文件的大小默认1G
mapedFileSizeCommitLog=1073741824
#ConsumeQueue每个文件默认存30W条，根据业务情况调整
mapedFileSizeConsumeQueue=300000
#destroyMapedFileIntervalForcibly=120000
#redeleteHangedFileInterval=120000
#检测物理文件磁盘空间
diskMaxUsedSpaceRatio=88
#存储路径
storePathRootDir=/opt/rocketmq/message2
#commitLog 存储路径
storePathCommitLog=/opt/rocketmq/message2/commitlog
#消费队列存储路径存储路径
storePathConsumeQueue=/opt/rocketmq/message2/consumequeue
#消息索引存储路径
storePathIndex=/opt/rocketmq/message2/index
#checkpoint 文件存储路径
storeCheckpoint=/opt/rocketmq/message2/checkpoint
#abort 文件存储路径
abortFile=/opt/rocketmq/message2/abort
#限制的消息大小
maxMessageSize=65536
#flushCommitLogLeastPages=4
#flushConsumeQueueLeastPages=2
#flushCommitLogThoroughInterval=10000
#flushConsumeQueueThoroughInterval=60000
#Broker 的角色
#- ASYNC_MASTER 异步复制Master
#- SYNC_MASTER 同步双写Master
#- SLAVE
brokerRole=SLAVE
#刷盘方式
#- ASYNC_FLUSH 异步刷盘
#- SYNC_FLUSH 同步刷盘
flushDiskType=ASYNC_FLUSH
#checkTransactionMessageEnable=false
#发消息线程池数量
#sendMessageThreadPoolNums=128
#拉消息线程池数量
#pullMessageThreadPoolNums=128
```

#### 3）master2

服务器：192.168.3.201

```sh
vi /usr/soft/rocketmq/conf/2m-2s-sync/broker-b.properties
```

修改配置如下：

```bash
#所属集群名字
brokerClusterName=rocketmq-cluster
#broker名字，注意此处不同的配置文件填写的不一样
brokerName=broker-b
#0 表示 Master，>0 表示 Slave
brokerId=0
#nameServer地址，分号分割
namesrvAddr=rocketmq-nameserver1:9876;rocketmq-nameserver2:9876
#在发送消息时，自动创建服务器不存在的topic，默认创建的队列数
defaultTopicQueueNums=4
#是否允许 Broker 自动创建Topic，建议线下开启，线上关闭
autoCreateTopicEnable=true
#是否允许 Broker 自动创建订阅组，建议线下开启，线上关闭
autoCreateSubscriptionGroup=true
#Broker 对外服务的监听端口
listenPort=10911
#删除文件时间点，默认凌晨 4点
deleteWhen=04
#文件保留时间，默认 48 小时
fileReservedTime=120
#commitLog每个文件的大小默认1G
mapedFileSizeCommitLog=1073741824
#ConsumeQueue每个文件默认存30W条，根据业务情况调整
mapedFileSizeConsumeQueue=300000
#destroyMapedFileIntervalForcibly=120000
#redeleteHangedFileInterval=120000
#检测物理文件磁盘空间
diskMaxUsedSpaceRatio=88
#存储路径
storePathRootDir=/install/soft/rocketmq/message
#commitLog 存储路径
storePathCommitLog=/install/soft/rocketmq/message/commitlog
#消费队列存储路径存储路径
storePathConsumeQueue=/install/soft/rocketmq/message/consumequeue
#消息索引存储路径
storePathIndex=/install/soft/rocketmq/message/index
#checkpoint 文件存储路径
storeCheckpoint=/install/soft/rocketmq/message/checkpoint
#abort 文件存储路径
abortFile=/install/soft/rocketmq/message/abort
#限制的消息大小
maxMessageSize=65536
#flushCommitLogLeastPages=4
#flushConsumeQueueLeastPages=2
#flushCommitLogThoroughInterval=10000
#flushConsumeQueueThoroughInterval=60000
#Broker 的角色
#- ASYNC_MASTER 异步复制Master
#- SYNC_MASTER 同步双写Master
#- SLAVE
brokerRole=SYNC_MASTER
#刷盘方式
#- ASYNC_FLUSH 异步刷盘
#- SYNC_FLUSH 同步刷盘
flushDiskType=SYNC_FLUSH
#checkTransactionMessageEnable=false
#发消息线程池数量
#sendMessageThreadPoolNums=128
#拉消息线程池数量
#pullMessageThreadPoolNums=128
```

#### 4）slave1

服务器：192.168.3.201

```sh
vi /usr/soft/rocketmq/conf/2m-2s-sync/broker-a-s.properties
```

修改配置如下：

```bash
#所属集群名字
brokerClusterName=rocketmq-cluster
#broker名字，注意此处不同的配置文件填写的不一样
brokerName=broker-a
#0 表示 Master，>0 表示 Slave
brokerId=1
#nameServer地址，分号分割
namesrvAddr=rocketmq-nameserver1:9876;rocketmq-nameserver2:9876
#在发送消息时，自动创建服务器不存在的topic，默认创建的队列数
defaultTopicQueueNums=4
#是否允许 Broker 自动创建Topic，建议线下开启，线上关闭
autoCreateTopicEnable=true
#是否允许 Broker 自动创建订阅组，建议线下开启，线上关闭
autoCreateSubscriptionGroup=true
#Broker 对外服务的监听端口
listenPort=11011
#删除文件时间点，默认凌晨 4点
deleteWhen=04
#文件保留时间，默认 48 小时
fileReservedTime=120
#commitLog每个文件的大小默认1G
mapedFileSizeCommitLog=1073741824
#ConsumeQueue每个文件默认存30W条，根据业务情况调整
mapedFileSizeConsumeQueue=300000
#destroyMapedFileIntervalForcibly=120000
#redeleteHangedFileInterval=120000
#检测物理文件磁盘空间
diskMaxUsedSpaceRatio=88
#存储路径
storePathRootDir=/install/soft/rocketmq/message2
#commitLog 存储路径
storePathCommitLog=/install/soft/rocketmq/message2/commitlog
#消费队列存储路径存储路径
storePathConsumeQueue=/install/soft/rocketmq/message2/consumequeue
#消息索引存储路径
storePathIndex=/install/soft/rocketmq/message2/index
#checkpoint 文件存储路径
storeCheckpoint=/install/soft/rocketmq/message2/checkpoint
#abort 文件存储路径
abortFile=/install/soft/rocketmq/message2/abort
#限制的消息大小
maxMessageSize=65536
#flushCommitLogLeastPages=4
#flushConsumeQueueLeastPages=2
#flushCommitLogThoroughInterval=10000
#flushConsumeQueueThoroughInterval=60000
#Broker 的角色
#- ASYNC_MASTER 异步复制Master
#- SYNC_MASTER 同步双写Master
#- SLAVE
brokerRole=SLAVE
#刷盘方式
#- ASYNC_FLUSH 异步刷盘
#- SYNC_FLUSH 同步刷盘
flushDiskType=ASYNC_FLUSH
#checkTransactionMessageEnable=false
#发消息线程池数量
#sendMessageThreadPoolNums=128
#拉消息线程池数量
#pullMessageThreadPoolNums=128
```

### 3.3.9 修改启动脚本文件

#### 1）runbroker.sh

```sh
vi /usr/local/rocketmq/bin/runbroker.sh
```

需要根据内存大小进行适当的对JVM参数进行调整：

```bash
#===================================================
# 开发环境配置 JVM Configuration
JAVA_OPT="${JAVA_OPT} -server -Xms256m -Xmx256m -Xmn128m"
```

\####2）runserver.sh

```sh
vim /usr/local/rocketmq/bin/runserver.sh
```

```bash
JAVA_OPT="${JAVA_OPT} -server -Xms256m -Xmx256m -Xmn128m -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=320m"
```

### 3.3.10 服务启动

#### 1）启动NameServe集群

分别在192.168.3.37和192.168.3.201启动NameServer

```bash
cd /usr/local/rocketmq/bin
nohup sh mqnamesrv &

/install/soft/rocketmq/rocketmq-all-4.8.0-bin-release/conf/2m-2s-sync
```

#### 2）启动Broker集群

* 在192.168.3.37上启动master1和slave2

master1：

```bash
cd /usr/local/rocketmq/bin
nohup sh mqbroker -c /opt/rocketmq/rocketmq-4.8.0-bin/conf/2m-2s-sync/broker-a.properties &
```

slave2：

```sh
cd /usr/local/rocketmq/bin
nohup sh mqbroker -c /opt/rocketmq/rocketmq-4.8.0-bin/conf/2m-2s-sync/broker-b-s.properties &
```

* 在192.168.3.201上启动master2和slave2

master2

```sh
cd /usr/local/rocketmq/bin
nohup sh mqbroker -c /install/soft/rocketmq/rocketmq-all-4.8.0-bin-release/conf/2m-2s-sync/broker-b.properties &
```

slave1

```sh
cd /usr/local/rocketmq/bin
nohup sh mqbroker -c /install/soft/rocketmq/rocketmq-all-4.8.0-bin-release/conf/2m-2s-sync/broker-a-s.properties &
```

### 3.3.11 查看进程状态

启动后通过JPS查看启动进程

![](/assets/jps1.uMzcqkEG.png)

### 3.3.12 查看日志

```sh
# 查看nameServer日志
tail -500f ~/logs/rocketmqlogs/namesrv.log
# 查看broker日志
tail -500f ~/logs/rocketmqlogs/broker.log
```

## 3.4 mqadmin管理工具

### 3.4.1 使用方式

进入RocketMQ安装位置，在bin目录下执行`./mqadmin {command} {args}`

\###3.4.2 命令介绍

\####1）Topic相关

\####2）集群相关

\####3）Broker相关

\####4）消息相关

#### 5）消费者、消费组相关

#### 6）连接相关

#### 7）NameServer相关

#### 8）其他

### 3.4.3 注意事项

* 几乎所有命令都需要配置-n表示NameServer地址，格式为ip:port
* 几乎所有命令都可以通过-h获取帮助
* 如果既有Broker地址（-b）配置项又有clusterName（-c）配置项，则优先以Broker地址执行命令；如果不配置Broker地址，则对集群中所有主机执行命令

## 3.5 集群监控平台搭建

### 3.5.1 概述

`RocketMQ`有一个对其扩展的开源项目[incubator-rocketmq-externals](https://github.com/apache/rocketmq-externals)，这个项目中有一个子模块叫`rocketmq-console`，这个便是管理控制台项目了，先将[incubator-rocketmq-externals](https://github.com/apache/rocketmq-externals)拉到本地，因为我们需要自己对`rocketmq-console`进行编译打包运行。

![](/assets/rocketmq-console.DSOq-V_d.png)

### 3.5.2 下载并编译打包

```sh
git clone https://github.com/apache/rocketmq-externals
cd rocketmq-console
mvn clean package -Dmaven.test.skip=true
```

注意：打包前在`rocketmq-console`中配置`namesrv`集群地址：

```sh
rocketmq.config.namesrvAddr=192.168.25.135:9876;192.168.25.138:9876
```

启动rocketmq-console：

```sh
java -jar rocketmq-console.jar
```

启动成功后，我们就可以通过浏览器访问`http://localhost:8080`进入控制台界面了，如下图：

![](/assets/rocketmq-console2.BNsM6gJ8.png)

集群状态：

![](/assets/rocketmq-console3.BNhiRcHC.png)

# 4. 消息发送样例

* 导入MQ客户端依赖

```xml
<dependency>
    <groupId>org.apache.rocketmq</groupId>
    <artifactId>rocketmq-client</artifactId>
    <version>4.4.0</version>
</dependency>
```

* 消息发送者步骤分析r

```tex
1.创建消息生产者producer，并制定生产者组名
2.指定Nameserver地址
3.启动producer
4.创建消息对象，指定主题Topic、Tag和消息体
5.发送消息
6.关闭生产者producer
```

* 消息消费者步骤分析

```tex
1.创建消费者Consumer，制定消费者组名
2.指定Nameserver地址
3.订阅主题Topic和Tag
4.设置回调函数，处理消息
5.启动消费者consumer
```

## 4.1 基本样例

### 4.1.1 消息发送

#### 1）发送同步消息

这种可靠性同步地发送方式使用的比较广泛，比如：重要的消息通知，短信通知。

```java
public class SyncProducer {
	public static void main(String[] args) throws Exception {
    	// 实例化消息生产者Producer
        DefaultMQProducer producer = new DefaultMQProducer("please_rename_unique_group_name");
    	// 设置NameServer的地址
    	producer.setNamesrvAddr("localhost:9876");
    	// 启动Producer实例
        producer.start();
    	for (int i = 0; i < 100; i++) {
    	    // 创建消息，并指定Topic，Tag和消息体
    	    Message msg = new Message("TopicTest" /* Topic */,
        	"TagA" /* Tag */,
        	("Hello RocketMQ " + i).getBytes(RemotingHelper.DEFAULT_CHARSET) /* Message body */
        	);
        	// 发送消息到一个Broker
            SendResult sendResult = producer.send(msg);
            // 通过sendResult返回消息是否成功送达
            System.out.printf("%s%n", sendResult);
    	}
    	// 如果不再发送消息，关闭Producer实例。
    	producer.shutdown();
    }
}
```

#### 2）发送异步消息

异步消息通常用在对响应时间敏感的业务场景，即发送端不能容忍长时间地等待Broker的响应。

```java
public class AsyncProducer {
	public static void main(String[] args) throws Exception {
    	// 实例化消息生产者Producer
        DefaultMQProducer producer = new DefaultMQProducer("please_rename_unique_group_name");
    	// 设置NameServer的地址
        producer.setNamesrvAddr("localhost:9876");
    	// 启动Producer实例
        producer.start();
        producer.setRetryTimesWhenSendAsyncFailed(0);
    	for (int i = 0; i < 100; i++) {
                final int index = i;
            	// 创建消息，并指定Topic，Tag和消息体
                Message msg = new Message("TopicTest",
                    "TagA",
                    "OrderID188",
                    "Hello world".getBytes(RemotingHelper.DEFAULT_CHARSET));
                // SendCallback接收异步返回结果的回调
                producer.send(msg, new SendCallback() {
                    @Override
                    public void onSuccess(SendResult sendResult) {
                        System.out.printf("%-10d OK %s %n", index,
                            sendResult.getMsgId());
                    }
                    @Override
                    public void onException(Throwable e) {
      	              System.out.printf("%-10d Exception %s %n", index, e);
      	              e.printStackTrace();
                    }
            	});
    	}
    	// 如果不再发送消息，关闭Producer实例。
    	producer.shutdown();
    }
}
```

#### 3）单向发送消息

这种方式主要用在不特别关心发送结果的场景，例如日志发送。

```java
public class OnewayProducer {
	public static void main(String[] args) throws Exception{
    	// 实例化消息生产者Producer
        DefaultMQProducer producer = new DefaultMQProducer("please_rename_unique_group_name");
    	// 设置NameServer的地址
        producer.setNamesrvAddr("localhost:9876");
    	// 启动Producer实例
        producer.start();
    	for (int i = 0; i < 100; i++) {
        	// 创建消息，并指定Topic，Tag和消息体
        	Message msg = new Message("TopicTest" /* Topic */,
                "TagA" /* Tag */,
                ("Hello RocketMQ " + i).getBytes(RemotingHelper.DEFAULT_CHARSET) /* Message body */
        	);
        	// 发送单向消息，没有任何返回结果
        	producer.sendOneway(msg);

    	}
    	// 如果不再发送消息，关闭Producer实例。
    	producer.shutdown();
    }
}
```

### 4.1.2 消费消息

#### 1）负载均衡模式

消费者采用负载均衡方式消费消息，多个消费者共同消费队列消息，每个消费者处理的消息不同

```java
public static void main(String[] args) throws Exception {
    // 实例化消息生产者,指定组名
    DefaultMQPushConsumer consumer = new DefaultMQPushConsumer("group1");
    // 指定Namesrv地址信息.
    consumer.setNamesrvAddr("localhost:9876");
    // 订阅Topic
    consumer.subscribe("Test", "*");
    //负载均衡模式消费
    consumer.setMessageModel(MessageModel.CLUSTERING);
    // 注册回调函数，处理消息
    consumer.registerMessageListener(new MessageListenerConcurrently() {
        @Override
        public ConsumeConcurrentlyStatus consumeMessage(List<MessageExt> msgs,
                                                        ConsumeConcurrentlyContext context) {
            System.out.printf("%s Receive New Messages: %s %n", 
                              Thread.currentThread().getName(), msgs);
            return ConsumeConcurrentlyStatus.CONSUME_SUCCESS;
        }
    });
    //启动消息者
    consumer.start();
    System.out.printf("Consumer Started.%n");
}
```

#### 2）广播模式

消费者采用广播的方式消费消息，每个消费者消费的消息都是相同的

```java
public static void main(String[] args) throws Exception {
    // 实例化消息生产者,指定组名
    DefaultMQPushConsumer consumer = new DefaultMQPushConsumer("group1");
    // 指定Namesrv地址信息.
    consumer.setNamesrvAddr("localhost:9876");
    // 订阅Topic
    consumer.subscribe("Test", "*");
    //广播模式消费
    consumer.setMessageModel(MessageModel.BROADCASTING);
    // 注册回调函数，处理消息
    consumer.registerMessageListener(new MessageListenerConcurrently() {
        @Override
        public ConsumeConcurrentlyStatus consumeMessage(List<MessageExt> msgs,
                                                        ConsumeConcurrentlyContext context) {
            System.out.printf("%s Receive New Messages: %s %n", 
                              Thread.currentThread().getName(), msgs);
            return ConsumeConcurrentlyStatus.CONSUME_SUCCESS;
        }
    });
    //启动消息者
    consumer.start();
    System.out.printf("Consumer Started.%n");
}
```

## 4.2 顺序消息

消息有序指的是可以按照消息的发送顺序来消费(FIFO)。RocketMQ可以严格的保证消息有序，可以分为分区有序或者全局有序。

顺序消费的原理解析，在默认的情况下消息发送会采取Round Robin轮询方式把消息发送到不同的queue(分区队列)；而消费消息的时候从多个queue上拉取消息，这种情况发送和消费是不能保证顺序。但是如果控制发送的顺序消息只依次发送到同一个queue中，消费的时候只从这个queue上依次拉取，则就保证了顺序。当发送和消费参与的queue只有一个，则是全局有序；如果多个queue参与，则为分区有序，即相对每个queue，消息都是有序的。

下面用订单进行分区有序的示例。一个订单的顺序流程是：创建、付款、推送、完成。订单号相同的消息会被先后发送到同一个队列中，消费时，同一个OrderId获取到的肯定是同一个队列。

### 4.2.1 顺序消息生产

```java
/**
* Producer，发送顺序消息
*/
public class Producer {

   public static void main(String[] args) throws Exception {
       DefaultMQProducer producer = new DefaultMQProducer("please_rename_unique_group_name");

       producer.setNamesrvAddr("127.0.0.1:9876");

       producer.start();

       String[] tags = new String[]{"TagA", "TagC", "TagD"};

       // 订单列表
       List<OrderStep> orderList = new Producer().buildOrders();

       Date date = new Date();
       SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
       String dateStr = sdf.format(date);
       for (int i = 0; i < 10; i++) {
           // 加个时间前缀
           String body = dateStr + " Hello RocketMQ " + orderList.get(i);
           Message msg = new Message("TopicTest", tags[i % tags.length], "KEY" + i, body.getBytes());

           SendResult sendResult = producer.send(msg, new MessageQueueSelector() {
               @Override
               public MessageQueue select(List<MessageQueue> mqs, Message msg, Object arg) {
                   Long id = (Long) arg;  //根据订单id选择发送queue
                   long index = id % mqs.size();
                   return mqs.get((int) index);
               }
           }, orderList.get(i).getOrderId());//订单id

           System.out.println(String.format("SendResult status:%s, queueId:%d, body:%s",
               sendResult.getSendStatus(),
               sendResult.getMessageQueue().getQueueId(),
               body));
       }

       producer.shutdown();
   }

   /**
    * 订单的步骤
    */
   private static class OrderStep {
       private long orderId;
       private String desc;

       public long getOrderId() {
           return orderId;
       }

       public void setOrderId(long orderId) {
           this.orderId = orderId;
       }

       public String getDesc() {
           return desc;
       }

       public void setDesc(String desc) {
           this.desc = desc;
       }

       @Override
       public String toString() {
           return "OrderStep{" +
               "orderId=" + orderId +
               ", desc='" + desc + '\'' +
               '}';
       }
   }

   /**
    * 生成模拟订单数据
    */
   private List<OrderStep> buildOrders() {
       List<OrderStep> orderList = new ArrayList<OrderStep>();

       OrderStep orderDemo = new OrderStep();
       orderDemo.setOrderId(15103111039L);
       orderDemo.setDesc("创建");
       orderList.add(orderDemo);

       orderDemo = new OrderStep();
       orderDemo.setOrderId(15103111065L);
       orderDemo.setDesc("创建");
       orderList.add(orderDemo);

       orderDemo = new OrderStep();
       orderDemo.setOrderId(15103111039L);
       orderDemo.setDesc("付款");
       orderList.add(orderDemo);

       orderDemo = new OrderStep();
       orderDemo.setOrderId(15103117235L);
       orderDemo.setDesc("创建");
       orderList.add(orderDemo);

       orderDemo = new OrderStep();
       orderDemo.setOrderId(15103111065L);
       orderDemo.setDesc("付款");
       orderList.add(orderDemo);

       orderDemo = new OrderStep();
       orderDemo.setOrderId(15103117235L);
       orderDemo.setDesc("付款");
       orderList.add(orderDemo);

       orderDemo = new OrderStep();
       orderDemo.setOrderId(15103111065L);
       orderDemo.setDesc("完成");
       orderList.add(orderDemo);

       orderDemo = new OrderStep();
       orderDemo.setOrderId(15103111039L);
       orderDemo.setDesc("推送");
       orderList.add(orderDemo);

       orderDemo = new OrderStep();
       orderDemo.setOrderId(15103117235L);
       orderDemo.setDesc("完成");
       orderList.add(orderDemo);

       orderDemo = new OrderStep();
       orderDemo.setOrderId(15103111039L);
       orderDemo.setDesc("完成");
       orderList.add(orderDemo);

       return orderList;
   }
}
```

### 4.2.2 顺序消费消息

```java
/**
* 顺序消息消费，带事务方式（应用可控制Offset什么时候提交）
*/
public class ConsumerInOrder {

   public static void main(String[] args) throws Exception {
       DefaultMQPushConsumer consumer = new 
           DefaultMQPushConsumer("please_rename_unique_group_name_3");
       consumer.setNamesrvAddr("127.0.0.1:9876");
       /**
        * 设置Consumer第一次启动是从队列头部开始消费还是队列尾部开始消费<br>
        * 如果非第一次启动，那么按照上次消费的位置继续消费
        */
       consumer.setConsumeFromWhere(ConsumeFromWhere.CONSUME_FROM_FIRST_OFFSET);

       consumer.subscribe("TopicTest", "TagA || TagC || TagD");

       consumer.registerMessageListener(new MessageListenerOrderly() {

           Random random = new Random();

           @Override
           public ConsumeOrderlyStatus consumeMessage(List<MessageExt> msgs, ConsumeOrderlyContext context) {
               context.setAutoCommit(true);
               for (MessageExt msg : msgs) {
                   // 可以看到每个queue有唯一的consume线程来消费, 订单对每个queue(分区)有序
                   System.out.println("consumeThread=" + Thread.currentThread().getName() + "queueId=" + msg.getQueueId() + ", content:" + new String(msg.getBody()));
               }

               try {
                   //模拟业务逻辑处理中...
                   TimeUnit.SECONDS.sleep(random.nextInt(10));
               } catch (Exception e) {
                   e.printStackTrace();
               }
               return ConsumeOrderlyStatus.SUCCESS;
           }
       });

       consumer.start();

       System.out.println("Consumer Started.");
   }
}
```

## 4.3 延时消息

比如电商里，提交了一个订单就可以发送一个延时消息，1h后去检查这个订单的状态，如果还是未付款就取消订单释放库存。

### 4.3.1 启动消息消费者

```java
public class ScheduledMessageConsumer {
   public static void main(String[] args) throws Exception {
      // 实例化消费者
      DefaultMQPushConsumer consumer = new DefaultMQPushConsumer("ExampleConsumer");
      // 订阅Topics
      consumer.subscribe("TestTopic", "*");
      // 注册消息监听者
      consumer.registerMessageListener(new MessageListenerConcurrently() {
          @Override
          public ConsumeConcurrentlyStatus consumeMessage(List<MessageExt> messages, ConsumeConcurrentlyContext context) {
              for (MessageExt message : messages) {
                  // Print approximate delay time period
                  System.out.println("Receive message[msgId=" + message.getMsgId() + "] " + (System.currentTimeMillis() - message.getStoreTimestamp()) + "ms later");
              }
              return ConsumeConcurrentlyStatus.CONSUME_SUCCESS;
          }
      });
      // 启动消费者
      consumer.start();
  }
}
```

### 4.3.2 发送延时消息

```java
public class ScheduledMessageProducer {
   public static void main(String[] args) throws Exception {
      // 实例化一个生产者来产生延时消息
      DefaultMQProducer producer = new DefaultMQProducer("ExampleProducerGroup");
      // 启动生产者
      producer.start();
      int totalMessagesToSend = 100;
      for (int i = 0; i < totalMessagesToSend; i++) {
          Message message = new Message("TestTopic", ("Hello scheduled message " + i).getBytes());
          // 设置延时等级3,这个消息将在10s之后发送(现在只支持固定的几个时间,详看delayTimeLevel)
          message.setDelayTimeLevel(3);
          // 发送消息
          producer.send(message);
      }
       // 关闭生产者
      producer.shutdown();
  }
}
```

\###4.3.3 验证

您将会看到消息的消费比存储时间晚10秒

### 4.3.4 使用限制

```java
// org/apache/rocketmq/store/config/MessageStoreConfig.java
private String messageDelayLevel = "1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h";
```

现在RocketMq并不支持任意时间的延时，需要设置几个固定的延时等级，从1s到2h分别对应着等级1到18

## 4.4 批量消息

批量发送消息能显著提高传递小消息的性能。限制是这些批量消息应该有相同的topic，相同的waitStoreMsgOK，而且不能是延时消息。此外，这一批消息的总大小不应超过4MB。

### 4.4.1 发送批量消息

如果您每次只发送不超过4MB的消息，则很容易使用批处理，样例如下：

```java
String topic = "BatchTest";
List<Message> messages = new ArrayList<>();
messages.add(new Message(topic, "TagA", "OrderID001", "Hello world 0".getBytes()));
messages.add(new Message(topic, "TagA", "OrderID002", "Hello world 1".getBytes()));
messages.add(new Message(topic, "TagA", "OrderID003", "Hello world 2".getBytes()));
try {
   producer.send(messages);
} catch (Exception e) {
   e.printStackTrace();
   //处理error
}
```

如果消息的总长度可能大于4MB时，这时候最好把消息进行分割

```java
public class ListSplitter implements Iterator<List<Message>> {
   private final int SIZE_LIMIT = 1024 * 1024 * 4;
   private final List<Message> messages;
   private int currIndex;
   public ListSplitter(List<Message> messages) {
           this.messages = messages;
   }
    @Override 
    public boolean hasNext() {
       return currIndex < messages.size();
   }
   	@Override 
    public List<Message> next() {
       int nextIndex = currIndex;
       int totalSize = 0;
       for (; nextIndex < messages.size(); nextIndex++) {
           Message message = messages.get(nextIndex);
           int tmpSize = message.getTopic().length() + message.getBody().length;
           Map<String, String> properties = message.getProperties();
           for (Map.Entry<String, String> entry : properties.entrySet()) {
               tmpSize += entry.getKey().length() + entry.getValue().length();
           }
           tmpSize = tmpSize + 20; // 增加日志的开销20字节
           if (tmpSize > SIZE_LIMIT) {
               //单个消息超过了最大的限制
               //忽略,否则会阻塞分裂的进程
               if (nextIndex - currIndex == 0) {
                  //假如下一个子列表没有元素,则添加这个子列表然后退出循环,否则只是退出循环
                  nextIndex++;
               }
               break;
           }
           if (tmpSize + totalSize > SIZE_LIMIT) {
               break;
           } else {
               totalSize += tmpSize;
           }

       }
       List<Message> subList = messages.subList(currIndex, nextIndex);
       currIndex = nextIndex;
       return subList;
   }
}
//把大的消息分裂成若干个小的消息
ListSplitter splitter = new ListSplitter(messages);
while (splitter.hasNext()) {
  try {
      List<Message>  listItem = splitter.next();
      producer.send(listItem);
  } catch (Exception e) {
      e.printStackTrace();
      //处理error
  }
}
```

## 4.5 过滤消息

在大多数情况下，TAG是一个简单而有用的设计，其可以来选择您想要的消息。例如：

```java
DefaultMQPushConsumer consumer = new DefaultMQPushConsumer("CID_EXAMPLE");
consumer.subscribe("TOPIC", "TAGA || TAGB || TAGC");
```

消费者将接收包含TAGA或TAGB或TAGC的消息。但是限制是一个消息只能有一个标签，这对于复杂的场景可能不起作用。在这种情况下，可以使用SQL表达式筛选消息。SQL特性可以通过发送消息时的属性来进行计算。在RocketMQ定义的语法下，可以实现一些简单的逻辑。下面是一个例子：

```te
------------
| message  |
|----------|  a > 5 AND b = 'abc'
| a = 10   |  --------------------> Gotten
| b = 'abc'|
| c = true |
------------
------------
| message  |
|----------|   a > 5 AND b = 'abc'
| a = 1    |  --------------------> Missed
| b = 'abc'|
| c = true |
------------
```

### 4.5.1 SQL基本语法

RocketMQ只定义了一些基本语法来支持这个特性。你也可以很容易地扩展它。

* 数值比较，比如：**>，>=，<，<=，BETWEEN，=；**
* 字符比较，比如：**=，<>，IN；**
* **IS NULL** 或者 **IS NOT NULL；**
* 逻辑符号 **AND，OR，NOT；**

常量支持类型为：

* 数值，比如：**123，3.1415；**
* 字符，比如：**'abc'，必须用单引号包裹起来；**
* **NULL**，特殊的常量
* 布尔值，**TRUE** 或 **FALSE**

只有使用push模式的消费者才能用使用SQL92标准的sql语句，接口如下：

```java
public void subscribe(finalString topic, final MessageSelector messageSelector)
```

### 4.5.2 消息生产者

发送消息时，你能通过`putUserProperty`来设置消息的属性

```java
DefaultMQProducer producer = new DefaultMQProducer("please_rename_unique_group_name");
producer.start();
Message msg = new Message("TopicTest",
   tag,
   ("Hello RocketMQ " + i).getBytes(RemotingHelper.DEFAULT_CHARSET)
);
// 设置一些属性
msg.putUserProperty("a", String.valueOf(i));
SendResult sendResult = producer.send(msg);

producer.shutdown();
```

### 4.5.3 消息消费者

用MessageSelector.bySql来使用sql筛选消息

```java
DefaultMQPushConsumer consumer = new DefaultMQPushConsumer("please_rename_unique_group_name_4");
// 只有订阅的消息有这个属性a, a >=0 and a <= 3
consumer.subscribe("TopicTest", MessageSelector.bySql("a between 0 and 3");
consumer.registerMessageListener(new MessageListenerConcurrently() {
   @Override
   public ConsumeConcurrentlyStatus consumeMessage(List<MessageExt> msgs, ConsumeConcurrentlyContext context) {
       return ConsumeConcurrentlyStatus.CONSUME_SUCCESS;
   }
});
consumer.start();
```

## 4.6 事务消息

\###4.6.1 流程分析

![](/assets/事务消息.DB3KWzi-.png)

上图说明了事务消息的大致方案，其中分为两个流程：正常事务消息的发送及提交、事务消息的补偿流程。

\####1）事务消息发送及提交

(1) 发送消息（half消息）。

(2) 服务端响应消息写入结果。

(3) 根据发送结果执行本地事务（如果写入失败，此时half消息对业务不可见，本地逻辑不执行）。

(4) 根据本地事务状态执行Commit或者Rollback（Commit操作生成消息索引，消息对消费者可见）

#### 2）事务补偿

(1) 对没有Commit/Rollback的事务消息（pending状态的消息），从服务端发起一次“回查”

(2) Producer收到回查消息，检查回查消息对应的本地事务的状态

(3) 根据本地事务状态，重新Commit或者Rollback

其中，补偿阶段用于解决消息Commit或者Rollback发生超时或者失败的情况。

#### 3）事务消息状态

事务消息共有三种状态，提交状态、回滚状态、中间状态：

* TransactionStatus.CommitTransaction: 提交事务，它允许消费者消费此消息。
* TransactionStatus.RollbackTransaction: 回滚事务，它代表该消息将被删除，不允许被消费。
* TransactionStatus.Unknown: 中间状态，它代表需要检查消息队列来确定状态。

\###4.6.1 发送事务消息

#### 1) 创建事务性生产者

使用 `TransactionMQProducer`类创建生产者，并指定唯一的 `ProducerGroup`，就可以设置自定义线程池来处理这些检查请求。执行本地事务后、需要根据执行结果对消息队列进行回复。回传的事务状态在请参考前一节。

```java
public class Producer {
    public static void main(String[] args) throws MQClientException, InterruptedException {
        //创建事务监听器
        TransactionListener transactionListener = new TransactionListenerImpl();
        //创建消息生产者
        TransactionMQProducer producer = new TransactionMQProducer("group6");
        producer.setNamesrvAddr("192.168.25.135:9876;192.168.25.138:9876");
        //生产者这是监听器
        producer.setTransactionListener(transactionListener);
        //启动消息生产者
        producer.start();
        String[] tags = new String[]{"TagA", "TagB", "TagC"};
        for (int i = 0; i < 3; i++) {
            try {
                Message msg = new Message("TransactionTopic", tags[i % tags.length], "KEY" + i,
                        ("Hello RocketMQ " + i).getBytes(RemotingHelper.DEFAULT_CHARSET));
                SendResult sendResult = producer.sendMessageInTransaction(msg, null);
                System.out.printf("%s%n", sendResult);
                TimeUnit.SECONDS.sleep(1);
            } catch (MQClientException | UnsupportedEncodingException e) {
                e.printStackTrace();
            }
        }
        //producer.shutdown();
    }
}
```

#### 2）实现事务的监听接口

当发送半消息成功时，我们使用 `executeLocalTransaction` 方法来执行本地事务。它返回前一节中提到的三个事务状态之一。`checkLocalTranscation` 方法用于检查本地事务状态，并回应消息队列的检查请求。它也是返回前一节中提到的三个事务状态之一。

```java
public class TransactionListenerImpl implements TransactionListener {

    @Override
    public LocalTransactionState executeLocalTransaction(Message msg, Object arg) {
        System.out.println("执行本地事务");
        if (StringUtils.equals("TagA", msg.getTags())) {
            return LocalTransactionState.COMMIT_MESSAGE;
        } else if (StringUtils.equals("TagB", msg.getTags())) {
            return LocalTransactionState.ROLLBACK_MESSAGE;
        } else {
            return LocalTransactionState.UNKNOW;
        }

    }

    @Override
    public LocalTransactionState checkLocalTransaction(MessageExt msg) {
        System.out.println("MQ检查消息Tag【"+msg.getTags()+"】的本地事务执行结果");
        return LocalTransactionState.COMMIT_MESSAGE;
    }
}
```

### 4.6.2 使用限制

1. 事务消息不支持延时消息和批量消息。
2. 为了避免单个消息被检查太多次而导致半队列消息累积，我们默认将单个消息的检查次数限制为 15 次，但是用户可以通过 Broker 配置文件的 `transactionCheckMax`参数来修改此限制。如果已经检查某条消息超过 N 次的话（ N = `transactionCheckMax` ） 则 Broker 将丢弃此消息，并在默认情况下同时打印错误日志。用户可以通过重写 `AbstractTransactionCheckListener` 类来修改这个行为。
3. 事务消息将在 Broker 配置文件中的参数 transactionMsgTimeout 这样的特定时间长度之后被检查。当发送事务消息时，用户还可以通过设置用户属性 CHECK\_IMMUNITY\_TIME\_IN\_SECONDS 来改变这个限制，该参数优先于 `transactionMsgTimeout` 参数。
4. 事务性消息可能不止一次被检查或消费。
5. 提交给用户的目标主题消息可能会失败，目前这依日志的记录而定。它的高可用性通过 RocketMQ 本身的高可用性机制来保证，如果希望确保事务消息不丢失、并且事务完整性得到保证，建议使用同步的双重写入机制。
6. 事务消息的生产者 ID 不能与其他类型消息的生产者 ID 共享。与其他类型的消息不同，事务消息允许反向查询、MQ服务器能通过它们的生产者 ID 查询到消费者。

---

---
url: /Java/架构设计/分布式/03.分布式消息队列/01.RocketMQ/RocketMQ-02.md
---

# 1. 案例介绍

## 1.1 业务分析

模拟电商网站购物场景中的【下单】和【支付】业务

\###1）下单

![](/assets/下单组件图.Bs70aCGL.png)

1. 用户请求订单系统下单
2. 订单系统通过RPC调用订单服务下单
3. 订单服务调用优惠券服务，扣减优惠券
4. 订单服务调用调用库存服务，校验并扣减库存
5. 订单服务调用用户服务，扣减用户余额
6. 订单服务完成确认订单

***

\###2）支付

![](/assets/支付组件图.2z6fuLHr.png)

1. 用户请求支付系统
2. 支付系统调用第三方支付平台API进行发起支付流程
3. 用户通过第三方支付平台支付成功后，第三方支付平台回调通知支付系统
4. 支付系统调用订单服务修改订单状态
5. 支付系统调用积分服务添加积分
6. 支付系统调用日志服务记录日志

## 1.2 问题分析

### 问题1

用户提交订单后，扣减库存成功、扣减优惠券成功、使用余额成功，但是在确认订单操作失败，需要对库存、库存、余额进行回退。

如何保证数据的完整性？

![](/assets/下单失败流程图.Br3USTIG.png)

使用MQ保证在下单失败后系统数据的完整性

![](/assets/下单时序图\(2\).eilRB9Nj.png)

\###问题2

用户通过第三方支付平台（支付宝、微信）支付成功后，第三方支付平台要通过回调API异步通知商家支付系统用户支付结果，支付系统根据支付结果修改订单状态、记录支付日志和给用户增加积分。

商家支付系统如何保证在收到第三方支付平台的异步通知时，如何快速给第三方支付凭条做出回应？

![](/assets/支付流程.Bu2tzjdw.png)

通过MQ进行数据分发，提高系统处理性能

![](/assets/支付成功数据分发流程图.A0XdRy6K.png)

# 2. 技术分析

## 2.1 技术选型

* SpringBoot
* Dubbo
* Zookeeper
* RocketMQ
* Mysql

![](/assets/项目结构图.DwnpAiHE.png)

## 2.2 SpringBoot整合RocketMQ

下载[rocketmq-spring](https://github.com/apache/rocketmq-spring.git)项目

将rocketmq-spring安装到本地仓库

```shell
mvn install -Dmaven.skip.test=true
```

### 2.2.1 消息生产者

#### 1）添加依赖

```xml
<parent>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-parent</artifactId>
    <version>2.0.1.RELEASE</version>
</parent>

<properties>
    <rocketmq-spring-boot-starter-version>2.0.3</rocketmq-spring-boot-starter-version>
</properties>

<dependencies>
    <dependency>
        <groupId>org.apache.rocketmq</groupId>
        <artifactId>rocketmq-spring-boot-starter</artifactId>
        <version>${rocketmq-spring-boot-starter-version}</version>
    </dependency>
    <dependency>
        <groupId>org.projectlombok</groupId>
        <artifactId>lombok</artifactId>
        <version>1.18.6</version>
    </dependency>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-test</artifactId>
        <scope>test</scope>
    </dependency>

</dependencies>
```

#### 2）配置文件

```properties
# application.properties
rocketmq.name-server=192.168.25.135:9876;192.168.25.138:9876
rocketmq.producer.group=my-group
```

#### 3）启动类

```java
@SpringBootApplication
public class MQProducerApplication {
    public static void main(String[] args) {
        SpringApplication.run(MQSpringBootApplication.class);
    }
}
```

#### 4）测试类

```java
@RunWith(SpringRunner.class)
@SpringBootTest(classes = {MQSpringBootApplication.class})
public class ProducerTest {

    @Autowired
    private RocketMQTemplate rocketMQTemplate;

    @Test
    public void test1(){
        rocketMQTemplate.convertAndSend("springboot-mq","hello springboot rocketmq");
    }
}
```

### 2.2.2 消息消费者

#### 1）添加依赖

同消息生产者

#### 2）配置文件

同消息生产者

#### 3）启动类

```java
@SpringBootApplication
public class MQConsumerApplication {
    public static void main(String[] args) {
        SpringApplication.run(MQSpringBootApplication.class);
    }
}
```

#### 4）消息监听器

```java
@Slf4j
@Component
@RocketMQMessageListener(topic = "springboot-mq",consumerGroup = "springboot-mq-consumer-1")
public class Consumer implements RocketMQListener<String> {

    @Override
    public void onMessage(String message) {
        log.info("Receive message："+message);
    }
}
```

## 2.3 SpringBoot整合Dubbo

下载[dubbo-spring-boot-starter](https://github.com/alibaba/dubbo-spring-boot-starter.git)依赖包

将`dubbo-spring-boot-starter`安装到本地仓库

```shell
mvn install -Dmaven.skip.test=true
```

![](/assets/dubbo.BI5zWRFa.png)

### 2.3.1 搭建Zookeeper集群

#### 1）准备工作

1. 安装JDK
2. 将Zookeeper上传到服务器
3. 解压Zookeeper，并创建data目录，将conf下的zoo\_sample.cfg文件改名为zoo.cfg
4. 建立`/user/local/zookeeper-cluster`,将解压后的Zookeeper复制到以下三个目录

```sh
/usr/local/zookeeper-cluster/zookeeper-1
/usr/local/zookeeper-cluster/zookeeper-2
/usr/local/zookeeper-cluster/zookeeper-3
```

5. 配置每一个 Zookeeper 的 dataDir（zoo.cfg） clientPort 分别为 2181 2182 2183

   修改`/usr/local/zookeeper-cluster/zookeeper-1/conf/zoo.cfg`

```shell
clientPort=2181
dataDir=/usr/local/zookeeper-cluster/zookeeper-1/data
```

​	修改/usr/local/zookeeper-cluster/zookeeper-2/conf/zoo.cfg

```shell
clientPort=2182
dataDir=/usr/local/zookeeper-cluster/zookeeper-2/data
```

​	修改/usr/local/zookeeper-cluster/zookeeper-3/conf/zoo.cfg

```shell
clientPort=2183
dataDir=/usr/local/zookeeper-cluster/zookeeper-3/data
```

#### 2）配置集群

1. 在每个 zookeeper 的 data 目录下创建一个 myid 文件，内容分别是 1、2、3 。这个文件就是记录每个服务器的 ID

2. 在每一个 zookeeper 的 zoo.cfg 配置客户端访问端口（clientPort）和集群服务器 IP 列表。

   集群服务器 IP 列表如下

```shell
server.1=192.168.25.140:2881:3881
server.2=192.168.25.140:2882:3882
server.3=192.168.25.140:2883:3883
```

解释：server.服务器 ID=服务器 IP 地址：服务器之间通信端口：服务器之间投票选举端口

#### 3）启动集群

启动集群就是分别启动每个实例。

![](/assets/zk.C3bDRaRm.png)

### 2.3.2 RPC服务接口

```java
public interface IUserService {
    public String sayHello(String name);
}
```

### 2.3.3 服务提供者

#### 1）添加依赖

```xml
<parent>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-parent</artifactId>
    <version>2.0.1.RELEASE</version>
</parent>

<dependencies>
    <!--dubbo-->
    <dependency>
        <groupId>com.alibaba.spring.boot</groupId>
        <artifactId>dubbo-spring-boot-starter</artifactId>
        <version>2.0.0</version>
    </dependency>
	<!--spring-boot-stater-->
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter</artifactId>
        <exclusions>
            <exclusion>
                <artifactId>log4j-to-slf4j</artifactId>
                <groupId>org.apache.logging.log4j</groupId>
            </exclusion>
        </exclusions>
    </dependency>
	<!--zookeeper-->
    <dependency>
        <groupId>org.apache.zookeeper</groupId>
        <artifactId>zookeeper</artifactId>
        <version>3.4.10</version>
        <exclusions>
            <exclusion>
                <groupId>org.slf4j</groupId>
                <artifactId>slf4j-log4j12</artifactId>
            </exclusion>
            <exclusion>
                <groupId>log4j</groupId>
                <artifactId>log4j</artifactId>
            </exclusion>
        </exclusions>
    </dependency>

    <dependency>
        <groupId>com.101tec</groupId>
        <artifactId>zkclient</artifactId>
        <version>0.9</version>
        <exclusions>
            <exclusion>
                <artifactId>slf4j-log4j12</artifactId>
                <groupId>org.slf4j</groupId>
            </exclusion>
        </exclusions>
    </dependency>
	<!--API-->
    <dependency>
        <groupId>com.itheima.demo</groupId>
        <artifactId>dubbo-api</artifactId>
        <version>1.0-SNAPSHOT</version>
    </dependency>

</dependencies>
```

#### 2）配置文件

```properties
# application.properties
spring.application.name=dubbo-demo-provider
spring.dubbo.application.id=dubbo-demo-provider
spring.dubbo.application.name=dubbo-demo-provider
spring.dubbo.registry.address=zookeeper://192.168.25.140:2181;zookeeper://192.168.25.140:2182;zookeeper://192.168.25.140:2183
spring.dubbo.server=true
spring.dubbo.protocol.name=dubbo
spring.dubbo.protocol.port=20880
```

#### 3）启动类

```java
@EnableDubboConfiguration
@SpringBootApplication
public class ProviderBootstrap {

    public static void main(String[] args) throws IOException {
        SpringApplication.run(ProviderBootstrap.class,args);
    }

}
```

#### 4）服务实现

```java
@Component
@Service(interfaceClass = IUserService.class)
public class UserServiceImpl implements IUserService{
    @Override
    public String sayHello(String name) {
        return "hello:"+name;
    }
}
```

### 2.3.4 服务消费者

#### 1）添加依赖

```xml
<parent>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-parent</artifactId>
        <version>2.0.1.RELEASE</version>
    </parent>

<dependencies>

    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-web</artifactId>
    </dependency>

    <!--dubbo-->
    <dependency>
        <groupId>com.alibaba.spring.boot</groupId>
        <artifactId>dubbo-spring-boot-starter</artifactId>
        <version>2.0.0</version>
    </dependency>

    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter</artifactId>
        <exclusions>
            <exclusion>
                <artifactId>log4j-to-slf4j</artifactId>
                <groupId>org.apache.logging.log4j</groupId>
            </exclusion>
        </exclusions>
    </dependency>

    <!--zookeeper-->
    <dependency>
        <groupId>org.apache.zookeeper</groupId>
        <artifactId>zookeeper</artifactId>
        <version>3.4.10</version>
        <exclusions>
            <exclusion>
                <groupId>org.slf4j</groupId>
                <artifactId>slf4j-log4j12</artifactId>
            </exclusion>
            <exclusion>
                <groupId>log4j</groupId>
                <artifactId>log4j</artifactId>
            </exclusion>
        </exclusions>
    </dependency>

    <dependency>
        <groupId>com.101tec</groupId>
        <artifactId>zkclient</artifactId>
        <version>0.9</version>
        <exclusions>
            <exclusion>
                <artifactId>slf4j-log4j12</artifactId>
                <groupId>org.slf4j</groupId>
            </exclusion>
        </exclusions>
    </dependency>

    <!--API-->
    <dependency>
        <groupId>com.itheima.demo</groupId>
        <artifactId>dubbo-api</artifactId>
        <version>1.0-SNAPSHOT</version>
    </dependency>

</dependencies>
```

#### 2）配置文件

```properties
# application.properties
spring.application.name=dubbo-demo-consumer
spring.dubbo.application.name=dubbo-demo-consumer
spring.dubbo.application.id=dubbo-demo-consumer
    spring.dubbo.registry.address=zookeeper://192.168.25.140:2181;zookeeper://192.168.25.140:2182;zookeeper://192.168.25.140:2183
```

#### 3）启动类

```java
@EnableDubboConfiguration
@SpringBootApplication
public class ConsumerBootstrap {
    public static void main(String[] args) {
        SpringApplication.run(ConsumerBootstrap.class);
    }
}
```

#### 4）Controller

```java
@RestController
@RequestMapping("/user")
public class UserController {

    @Reference
    private IUserService userService;

    @RequestMapping("/sayHello")
    public String sayHello(String name){
        return userService.sayHello(name);
    }

}
```

# 3. 环境搭建

## 3.1 数据库

### 1）优惠券表

| Field        | Type                | Comment                  |
| ------------ | ------------------- | ------------------------ |
| coupon\_id    | bigint(50) NOT NULL | 优惠券ID                 |
| coupon\_price | decimal(10,2) NULL  | 优惠券金额               |
| user\_id      | bigint(50) NULL     | 用户ID                   |
| order\_id     | bigint(32) NULL     | 订单ID                   |
| is\_used      | int(1) NULL         | 是否使用 0未使用 1已使用 |
| used\_time    | timestamp NULL      | 使用时间                 |

### 2）商品表

| Field        | Type                | Comment  |
| ------------ | ------------------- | -------- |
| goods\_id     | bigint(50) NOT NULL | 主键     |
| goods\_name   | varchar(255) NULL   | 商品名称 |
| goods\_number | int(11) NULL        | 商品库存 |
| goods\_price  | decimal(10,2) NULL  | 商品价格 |
| goods\_desc   | varchar(255) NULL   | 商品描述 |
| add\_time     | timestamp NULL      | 添加时间 |

### 3）订单表

| Field           | Type                | Comment                                      |
| --------------- | ------------------- | -------------------------------------------- |
| order\_id        | bigint(50) NOT NULL | 订单ID                                       |
| user\_id         | bigint(50) NULL     | 用户ID                                       |
| order\_status    | int(1) NULL         | 订单状态 0未确认 1已确认 2已取消 3无效 4退款 |
| pay\_status      | int(1) NULL         | 支付状态 0未支付 1支付中 2已支付             |
| shipping\_status | int(1) NULL         | 发货状态 0未发货 1已发货 2已退货             |
| address         | varchar(255) NULL   | 收货地址                                     |
| consignee       | varchar(255) NULL   | 收货人                                       |
| goods\_id        | bigint(50) NULL     | 商品ID                                       |
| goods\_number    | int(11) NULL        | 商品数量                                     |
| goods\_price     | decimal(10,2) NULL  | 商品价格                                     |
| goods\_amount    | decimal(10,0) NULL  | 商品总价                                     |
| shipping\_fee    | decimal(10,2) NULL  | 运费                                         |
| order\_amount    | decimal(10,2) NULL  | 订单价格                                     |
| coupon\_id       | bigint(50) NULL     | 优惠券ID                                     |
| coupon\_paid     | decimal(10,2) NULL  | 优惠券                                       |
| money\_paid      | decimal(10,2) NULL  | 已付金额                                     |
| pay\_amount      | decimal(10,2) NULL  | 支付金额                                     |
| add\_time        | timestamp NULL      | 创建时间                                     |
| confirm\_time    | timestamp NULL      | 订单确认时间                                 |
| pay\_time        | timestamp NULL      | 支付时间                                     |

### 4）订单商品日志表

| Field        | Type                 | Comment  |
| ------------ | -------------------- | -------- |
| goods\_id     | int(11) NOT NULL     | 商品ID   |
| order\_id     | varchar(32) NOT NULL | 订单ID   |
| goods\_number | int(11) NULL         | 库存数量 |
| log\_time     | datetime NULL        | 记录时间 |

### 5）用户表

| Field         | Type                | Comment  |
| ------------- | ------------------- | -------- |
| user\_id       | bigint(50) NOT NULL | 用户ID   |
| user\_name     | varchar(255) NULL   | 用户姓名 |
| user\_password | varchar(255) NULL   | 用户密码 |
| user\_mobile   | varchar(255) NULL   | 手机号   |
| user\_score    | int(11) NULL        | 积分     |
| user\_reg\_time | timestamp NULL      | 注册时间 |
| user\_money    | decimal(10,0) NULL  | 用户余额 |

### 6）用户余额日志表

| Field          | Type                | Comment                       |
| -------------- | ------------------- | ----------------------------- |
| user\_id        | bigint(50) NOT NULL | 用户ID                        |
| order\_id       | bigint(50) NOT NULL | 订单ID                        |
| money\_log\_type | int(1) NOT NULL     | 日志类型 1订单付款 2 订单退款 |
| use\_money      | decimal(10,2) NULL  | 操作金额                      |
| create\_time    | timestamp NULL      | 日志时间                      |

### 7）订单支付表

| Field      | Type                | Comment            |
| ---------- | ------------------- | ------------------ |
| pay\_id     | bigint(50) NOT NULL | 支付编号           |
| order\_id   | bigint(50) NULL     | 订单编号           |
| pay\_amount | decimal(10,2) NULL  | 支付金额           |
| is\_paid    | int(1) NULL         | 是否已支付 1否 2是 |

### 8）MQ消息生产表

| Field       | Type                  | Comment             |
| ----------- | --------------------- | ------------------- |
| id          | varchar(100) NOT NULL | 主键                |
| group\_name  | varchar(100) NULL     | 生产者组名          |
| msg\_topic   | varchar(100) NULL     | 消息主题            |
| msg\_tag     | varchar(100) NULL     | Tag                 |
| msg\_key     | varchar(100) NULL     | Key                 |
| msg\_body    | varchar(500) NULL     | 消息内容            |
| msg\_status  | int(1) NULL           | 0:未处理;1:已经处理 |
| create\_time | timestamp NOT NULL    | 记录时间            |

\###9）MQ消息消费表

| Field              | Type                  | Comment                          |
| ------------------ | --------------------- | -------------------------------- |
| msg\_id             | varchar(50) NULL      | 消息ID                           |
| group\_name         | varchar(100) NOT NULL | 消费者组名                       |
| msg\_tag            | varchar(100) NOT NULL | Tag                              |
| msg\_key            | varchar(100) NOT NULL | Key                              |
| msg\_body           | varchar(500) NULL     | 消息体                           |
| consumer\_status    | int(1) NULL           | 0:正在处理;1:处理成功;2:处理失败 |
| consumer\_times     | int(1) NULL           | 消费次数                         |
| consumer\_timestamp | timestamp NULL        | 消费时间                         |
| remark             | varchar(500) NULL     | 备注                             |

## 3.2 项目初始化

shop系统基于Maven进行项目管理

### 3.1.1 工程浏览

![](/assets/项目初始化.BEFJ31y9.png)

* 父工程：shop-parent
* 订单系统：shop-order-web
* 支付系统：shop-pay-web
* 优惠券服务：shop-coupon-service
* 订单服务：shop-order-service
* 支付服务：shop-pay-service
* 商品服务：shop-goods-service
* 用户服务：shop-user-service
* 实体类：shop-pojo
* 持久层：shop-dao
* 接口层：shop-api
* 工具工程：shop-common

共12个系统

### 3.1.2 工程关系

![](/assets/项目结构图.DwnpAiHE.png)

## 3.3 Mybatis逆向工程使用

### 1）代码生成

使用Mybatis逆向工程针对数据表生成CURD持久层代码

\###2）代码导入

* 将实体类导入到shop-pojo工程
* 在服务层工程中导入对应的Mapper类和对应配置文件

## 3.4 公共类介绍

* ID生成器

  IDWorker：Twitter雪花算法

* 异常处理类

  CustomerException：自定义异常类

  CastException：异常抛出类

* 常量类

  ShopCode：系统状态类

* 响应实体类

  Result：封装响应状态和响应信息

# 4. 下单业务

![](/assets/下单时序图\(2\).eilRB9Nj.png)

## 4.1 下单基本流程

### 1）接口定义

* IOrderService

```java
public interface IOrderService {
    /**
     * 确认订单
     * @param order
     * @return Result
     */
    Result confirmOrder(TradeOrder order);
}
```

\###2）业务类实现

```java
@Slf4j
@Component
@Service(interfaceClass = IOrderService.class)
public class OrderServiceImpl implements IOrderService {

    @Override
    public Result confirmOrder(TradeOrder order) {
        //1.校验订单
       
        //2.生成预订单
       
        try {
            //3.扣减库存
            
            //4.扣减优惠券
           
            //5.使用余额
           
            //6.确认订单
            
            //7.返回成功状态
           
        } catch (Exception e) {
            //1.确认订单失败,发送消息
            
            //2.返回失败状态
        }

    }
}
```

\###3）校验订单

![](/assets/校验订单\(2\).B8W0iocN.png)

```java
private void checkOrder(TradeOrder order) {
        //1.校验订单是否存在
        if(order==null){
            CastException.cast(ShopCode.SHOP_ORDER_INVALID);
        }
        //2.校验订单中的商品是否存在
        TradeGoods goods = goodsService.findOne(order.getGoodsId());
        if(goods==null){
            CastException.cast(ShopCode.SHOP_GOODS_NO_EXIST);
        }
        //3.校验下单用户是否存在
        TradeUser user = userService.findOne(order.getUserId());
        if(user==null){
            CastException.cast(ShopCode.SHOP_USER_NO_EXIST);
        }
        //4.校验商品单价是否合法
        if(order.getGoodsPrice().compareTo(goods.getGoodsPrice())!=0){
            CastException.cast(ShopCode.SHOP_GOODS_PRICE_INVALID);
        }
        //5.校验订单商品数量是否合法
        if(order.getGoodsNumber()>=goods.getGoodsNumber()){
            CastException.cast(ShopCode.SHOP_GOODS_NUM_NOT_ENOUGH);
        }

        log.info("校验订单通过");
}
```

\###4）生成预订单

![](/assets/生成预订单.UiMyNHzI.png)

```java
private Long savePreOrder(TradeOrder order) {
        //1.设置订单状态为不可见
        order.setOrderStatus(ShopCode.SHOP_ORDER_NO_CONFIRM.getCode());
        //2.订单ID
        order.setOrderId(idWorker.nextId());
        //核算运费是否正确
        BigDecimal shippingFee = calculateShippingFee(order.getOrderAmount());
        if (order.getShippingFee().compareTo(shippingFee) != 0) {
            CastException.cast(ShopCode.SHOP_ORDER_SHIPPINGFEE_INVALID);
        }
        //3.计算订单总价格是否正确
        BigDecimal orderAmount = order.getGoodsPrice().multiply(new BigDecimal(order.getGoodsNumber()));
        orderAmount.add(shippingFee);
        if (orderAmount.compareTo(order.getOrderAmount()) != 0) {
            CastException.cast(ShopCode.SHOP_ORDERAMOUNT_INVALID);
        }

        //4.判断优惠券信息是否合法
        Long couponId = order.getCouponId();
        if (couponId != null) {
            TradeCoupon coupon = couponService.findOne(couponId);
            //优惠券不存在
            if (coupon == null) {
                CastException.cast(ShopCode.SHOP_COUPON_NO_EXIST);
            }
            //优惠券已经使用
            if ((ShopCode.SHOP_COUPON_ISUSED.getCode().toString())
                .equals(coupon.getIsUsed().toString())) {
                CastException.cast(ShopCode.SHOP_COUPON_INVALIED);
            }
            order.setCouponPaid(coupon.getCouponPrice());
        } else {
            order.setCouponPaid(BigDecimal.ZERO);
        }

        //5.判断余额是否正确
        BigDecimal moneyPaid = order.getMoneyPaid();
        if (moneyPaid != null) {
            //比较余额是否大于0
            int r = order.getMoneyPaid().compareTo(BigDecimal.ZERO);
            //余额小于0
            if (r == -1) {
                CastException.cast(ShopCode.SHOP_MONEY_PAID_LESS_ZERO);
            }
            //余额大于0
            if (r == 1) {
                //查询用户信息
                TradeUser user = userService.findOne(order.getUserId());
                if (user == null) {
                    CastException.cast(ShopCode.SHOP_USER_NO_EXIST);
                }
            //比较余额是否大于用户账户余额
            if (user.getUserMoney().compareTo(order.getMoneyPaid().longValue()) == -1) {
                CastException.cast(ShopCode.SHOP_MONEY_PAID_INVALID);
            }
            order.setMoneyPaid(order.getMoneyPaid());
        }
    } else {
        order.setMoneyPaid(BigDecimal.ZERO);
    }
    //计算订单支付总价
    order.setPayAmount(orderAmount.subtract(order.getCouponPaid())
                       .subtract(order.getMoneyPaid()));
    //设置订单添加时间
    order.setAddTime(new Date());

    //保存预订单
    int r = orderMapper.insert(order);
    if (ShopCode.SHOP_SUCCESS.getCode() != r) {
        CastException.cast(ShopCode.SHOP_ORDER_SAVE_ERROR);
    }
    log.info("订单:["+order.getOrderId()+"]预订单生成成功");
    return order.getOrderId();
}
```

\###5）扣减库存

* 通过dubbo调用商品服务完成扣减库存

```java
private void reduceGoodsNum(TradeOrder order) {
        TradeGoodsNumberLog goodsNumberLog = new TradeGoodsNumberLog();
        goodsNumberLog.setGoodsId(order.getGoodsId());
        goodsNumberLog.setOrderId(order.getOrderId());
        goodsNumberLog.setGoodsNumber(order.getGoodsNumber());
        Result result = goodsService.reduceGoodsNum(goodsNumberLog);
        if (result.getSuccess().equals(ShopCode.SHOP_FAIL.getSuccess())) {
            CastException.cast(ShopCode.SHOP_REDUCE_GOODS_NUM_FAIL);
        }
        log.info("订单:["+order.getOrderId()+"]扣减库存["+order.getGoodsNumber()+"个]成功");
    }
```

* 商品服务GoodsService扣减库存

```java
@Override
public Result reduceGoodsNum(TradeGoodsNumberLog goodsNumberLog) {
    if (goodsNumberLog == null ||
            goodsNumberLog.getGoodsNumber() == null ||
            goodsNumberLog.getOrderId() == null ||
            goodsNumberLog.getGoodsNumber() == null ||
            goodsNumberLog.getGoodsNumber().intValue() <= 0) {
        CastException.cast(ShopCode.SHOP_REQUEST_PARAMETER_VALID);
    }
    TradeGoods goods = goodsMapper.selectByPrimaryKey(goodsNumberLog.getGoodsId());
    if(goods.getGoodsNumber()<goodsNumberLog.getGoodsNumber()){
        //库存不足
        CastException.cast(ShopCode.SHOP_GOODS_NUM_NOT_ENOUGH);
    }
    //减库存
    goods.setGoodsNumber(goods.getGoodsNumber()-goodsNumberLog.getGoodsNumber());
    goodsMapper.updateByPrimaryKey(goods);


    //记录库存操作日志
    goodsNumberLog.setGoodsNumber(-(goodsNumberLog.getGoodsNumber()));
    goodsNumberLog.setLogTime(new Date());
    goodsNumberLogMapper.insert(goodsNumberLog);

    return new Result(ShopCode.SHOP_SUCCESS.getSuccess(),ShopCode.SHOP_SUCCESS.getMessage());
}
```

\###6）扣减优惠券

* 通过dubbo完成扣减优惠券

```java
private void changeCoponStatus(TradeOrder order) {
    //判断用户是否使用优惠券
    if (!StringUtils.isEmpty(order.getCouponId())) {
        //封装优惠券对象
        TradeCoupon coupon = couponService.findOne(order.getCouponId());
        coupon.setIsUsed(ShopCode.SHOP_COUPON_ISUSED.getCode());
        coupon.setUsedTime(new Date());
        coupon.setOrderId(order.getOrderId());
        Result result = couponService.changeCouponStatus(coupon);
        //判断执行结果
        if (result.getSuccess().equals(ShopCode.SHOP_FAIL.getSuccess())) {
            //优惠券使用失败
            CastException.cast(ShopCode.SHOP_COUPON_USE_FAIL);
        }
        log.info("订单:["+order.getOrderId()+"]使用扣减优惠券["+coupon.getCouponPrice()+"元]成功");
    }

}
```

* 优惠券服务CouponService更改优惠券状态

```java
@Override
public Result changeCouponStatus(TradeCoupon coupon) {
    try {
        //判断请求参数是否合法
        if (coupon == null || StringUtils.isEmpty(coupon.getCouponId())) {
            CastException.cast(ShopCode.SHOP_REQUEST_PARAMETER_VALID);
        }
		//更新优惠券状态为已使用
        couponMapper.updateByPrimaryKey(coupon);
        return new Result(ShopCode.SHOP_SUCCESS.getSuccess(), ShopCode.SHOP_SUCCESS.getMessage());
    } catch (Exception e) {
        return new Result(ShopCode.SHOP_FAIL.getSuccess(), ShopCode.SHOP_FAIL.getMessage());
    }
}
```

\###7）扣减用户余额

* 通过用户服务完成扣减余额

```java
private void reduceMoneyPaid(TradeOrder order) {
    //判断订单中使用的余额是否合法
    if (order.getMoneyPaid() != null && order.getMoneyPaid().compareTo(BigDecimal.ZERO) == 1) {
        TradeUserMoneyLog userMoneyLog = new TradeUserMoneyLog();
        userMoneyLog.setOrderId(order.getOrderId());
        userMoneyLog.setUserId(order.getUserId());
        userMoneyLog.setUseMoney(order.getMoneyPaid());
        userMoneyLog.setMoneyLogType(ShopCode.SHOP_USER_MONEY_PAID.getCode());
        //扣减余额
        Result result = userService.changeUserMoney(userMoneyLog);
        if (result.getSuccess().equals(ShopCode.SHOP_FAIL.getSuccess())) {
            CastException.cast(ShopCode.SHOP_USER_MONEY_REDUCE_FAIL);
        }
        log.info("订单:["+order.getOrderId()+"扣减余额["+order.getMoneyPaid()+"元]成功]");
    }
}
```

* 用户服务UserService,更新余额

![](/assets/更改用户余额.LM9J0CGs.png)

```java
@Override
public Result changeUserMoney(TradeUserMoneyLog userMoneyLog) {
    //判断请求参数是否合法
    if (userMoneyLog == null
            || userMoneyLog.getUserId() == null
            || userMoneyLog.getUseMoney() == null
            || userMoneyLog.getOrderId() == null
            || userMoneyLog.getUseMoney().compareTo(BigDecimal.ZERO) <= 0) {
        CastException.cast(ShopCode.SHOP_REQUEST_PARAMETER_VALID);
    }

    //查询该订单是否存在付款记录
    TradeUserMoneyLogExample userMoneyLogExample = new TradeUserMoneyLogExample();
    userMoneyLogExample.createCriteria()
            .andUserIdEqualTo(userMoneyLog.getUserId())
            .andOrderIdEqualTo(userMoneyLog.getOrderId());
   int count = userMoneyLogMapper.countByExample(userMoneyLogExample);
   TradeUser tradeUser = new TradeUser();
   tradeUser.setUserId(userMoneyLog.getUserId());
   tradeUser.setUserMoney(userMoneyLog.getUseMoney().longValue());
   //判断余额操作行为
   //【付款操作】
   if (userMoneyLog.getMoneyLogType().equals(ShopCode.SHOP_USER_MONEY_PAID.getCode())) {
           //订单已经付款，则抛异常
           if (count > 0) {
                CastException.cast(ShopCode.SHOP_ORDER_PAY_STATUS_IS_PAY);
            }
       	   //用户账户扣减余额
           userMapper.reduceUserMoney(tradeUser);
       }
    //【退款操作】
    if (userMoneyLog.getMoneyLogType().equals(ShopCode.SHOP_USER_MONEY_REFUND.getCode())) {
         //如果订单未付款,则不能退款,抛异常
         if (count == 0) {
         CastException.cast(ShopCode.SHOP_ORDER_PAY_STATUS_NO_PAY);
     }
     //防止多次退款
     userMoneyLogExample = new TradeUserMoneyLogExample();
     userMoneyLogExample.createCriteria()
             .andUserIdEqualTo(userMoneyLog.getUserId())
                .andOrderIdEqualTo(userMoneyLog.getOrderId())
                .andMoneyLogTypeEqualTo(ShopCode.SHOP_USER_MONEY_REFUND.getCode());
     count = userMoneyLogMapper.countByExample(userMoneyLogExample);
     if (count > 0) {
         CastException.cast(ShopCode.SHOP_USER_MONEY_REFUND_ALREADY);
     }
     	//用户账户添加余额
        userMapper.addUserMoney(tradeUser);
    }


    //记录用户使用余额日志
    userMoneyLog.setCreateTime(new Date());
    userMoneyLogMapper.insert(userMoneyLog);
    return new Result(ShopCode.SHOP_SUCCESS.getSuccess(),ShopCode.SHOP_SUCCESS.getMessage());
}
```

\###8）确认订单

```java
private void updateOrderStatus(TradeOrder order) {
    order.setOrderStatus(ShopCode.SHOP_ORDER_CONFIRM.getCode());
    order.setPayStatus(ShopCode.SHOP_ORDER_PAY_STATUS_NO_PAY.getCode());
    order.setConfirmTime(new Date());
    int r = orderMapper.updateByPrimaryKey(order);
    if (r <= 0) {
        CastException.cast(ShopCode.SHOP_ORDER_CONFIRM_FAIL);
    }
    log.info("订单:["+order.getOrderId()+"]状态修改成功");
}
```

### 9）小结

```java
@Override
public Result confirmOrder(TradeOrder order) {
    //1.校验订单
    checkOrder(order);
    //2.生成预订单
    Long orderId = savePreOrder(order);
    order.setOrderId(orderId);
    try {
        //3.扣减库存
        reduceGoodsNum(order);
        //4.扣减优惠券
        changeCoponStatus(order);
        //5.使用余额
        reduceMoneyPaid(order);
        //6.确认订单
        updateOrderStatus(order);
        log.info("订单:["+orderId+"]确认成功");
        return new Result(ShopCode.SHOP_SUCCESS.getSuccess(), ShopCode.SHOP_SUCCESS.getMessage());
    } catch (Exception e) {
        //确认订单失败,发送消息
        ...
        return new Result(ShopCode.SHOP_FAIL.getSuccess(), ShopCode.SHOP_FAIL.getMessage());
    }
}
```

## 4.2 失败补偿机制

### 4.2.1 消息发送方

* 配置RocketMQ属性值

```properties
rocketmq.name-server=192.168.25.135:9876;192.168.25.138:9876
rocketmq.producer.group=orderProducerGroup

mq.order.consumer.group.name=order_orderTopic_cancel_group
mq.order.topic=orderTopic
mq.order.tag.confirm=order_confirm
mq.order.tag.cancel=order_cancel
```

* 注入模板类和属性值信息

```java
 @Autowired
 private RocketMQTemplate rocketMQTemplate;

 @Value("${mq.order.topic}")
 private String topic;

 @Value("${mq.order.tag.cancel}")
 private String cancelTag;
```

* 发送下单失败消息

```java
@Override
public Result confirmOrder(TradeOrder order) {
    //1.校验订单
    //2.生成预订
    try {
        //3.扣减库存
        //4.扣减优惠券
        //5.使用余额
        //6.确认订单
    } catch (Exception e) {
        //确认订单失败,发送消息
        CancelOrderMQ cancelOrderMQ = new CancelOrderMQ();
        cancelOrderMQ.setOrderId(order.getOrderId());
        cancelOrderMQ.setCouponId(order.getCouponId());
        cancelOrderMQ.setGoodsId(order.getGoodsId());
        cancelOrderMQ.setGoodsNumber(order.getGoodsNumber());
        cancelOrderMQ.setUserId(order.getUserId());
        cancelOrderMQ.setUserMoney(order.getMoneyPaid());
        try {
            sendMessage(topic, 
                        cancelTag, 
                        cancelOrderMQ.getOrderId().toString(), 
                    JSON.toJSONString(cancelOrderMQ));
    } catch (Exception e1) {
        e1.printStackTrace();
            CastException.cast(ShopCode.SHOP_MQ_SEND_MESSAGE_FAIL);
        }
        return new Result(ShopCode.SHOP_FAIL.getSuccess(), ShopCode.SHOP_FAIL.getMessage());
    }
}
```

```java
private void sendMessage(String topic, String tags, String keys, String body) throws Exception {
    //判断Topic是否为空
    if (StringUtils.isEmpty(topic)) {
        CastException.cast(ShopCode.SHOP_MQ_TOPIC_IS_EMPTY);
    }
    //判断消息内容是否为空
    if (StringUtils.isEmpty(body)) {
        CastException.cast(ShopCode.SHOP_MQ_MESSAGE_BODY_IS_EMPTY);
    }
    //消息体
    Message message = new Message(topic, tags, keys, body.getBytes());
    //发送消息
    rocketMQTemplate.getProducer().send(message);
}
```

### 4.2.2 消费接收方

* 配置RocketMQ属性值

```properties
rocketmq.name-server=192.168.25.135:9876;192.168.25.138:9876
mq.order.consumer.group.name=order_orderTopic_cancel_group
mq.order.topic=orderTopic
```

* 创建监听类，消费消息

```java
@Slf4j
@Component
@RocketMQMessageListener(topic = "${mq.order.topic}", 
                         consumerGroup = "${mq.order.consumer.group.name}",
                         messageModel = MessageModel.BROADCASTING)
public class CancelOrderConsumer implements RocketMQListener<MessageExt>{

    @Override
    public void onMessage(MessageExt messageExt) {
        ...
    }
}
```

#### 1）回退库存

* 流程分析

![](/assets/回退库存.DNuGkytM.png)

* 消息消费者

```java
@Slf4j
@Component
@RocketMQMessageListener(topic = "${mq.order.topic}",consumerGroup = "${mq.order.consumer.group.name}",messageModel = MessageModel.BROADCASTING )
public class CancelMQListener implements RocketMQListener<MessageExt>{


    @Value("${mq.order.consumer.group.name}")
    private String groupName;

    @Autowired
    private TradeGoodsMapper goodsMapper;

    @Autowired
    private TradeMqConsumerLogMapper mqConsumerLogMapper;

    @Autowired
    private TradeGoodsNumberLogMapper goodsNumberLogMapper;

    @Override
    public void onMessage(MessageExt messageExt) {
        String msgId=null;
        String tags=null;
        String keys=null;
        String body=null;
        try {
            //1. 解析消息内容
            msgId = messageExt.getMsgId();
            tags= messageExt.getTags();
            keys= messageExt.getKeys();
            body= new String(messageExt.getBody(),"UTF-8");

            log.info("接受消息成功");

            //2. 查询消息消费记录
            TradeMqConsumerLogKey primaryKey = new TradeMqConsumerLogKey();
            primaryKey.setMsgTag(tags);
            primaryKey.setMsgKey(keys);
            primaryKey.setGroupName(groupName);
            TradeMqConsumerLog mqConsumerLog = mqConsumerLogMapper.selectByPrimaryKey(primaryKey);

            if(mqConsumerLog!=null){
                //3. 判断如果消费过...
                //3.1 获得消息处理状态
                Integer status = mqConsumerLog.getConsumerStatus();
                //处理过...返回
                if(ShopCode.SHOP_MQ_MESSAGE_STATUS_SUCCESS.getCode().intValue()==status.intValue()){
                    log.info("消息:"+msgId+",已经处理过");
                    return;
                }

                //正在处理...返回
                if(ShopCode.SHOP_MQ_MESSAGE_STATUS_PROCESSING.getCode().intValue()==status.intValue()){
                    log.info("消息:"+msgId+",正在处理");
                    return;
                }

                //处理失败
                if(ShopCode.SHOP_MQ_MESSAGE_STATUS_FAIL.getCode().intValue()==status.intValue()){
                    //获得消息处理次数
                    Integer times = mqConsumerLog.getConsumerTimes();
                    if(times>3){
                        log.info("消息:"+msgId+",消息处理超过3次,不能再进行处理了");
                        return;
                    }
                    mqConsumerLog.setConsumerStatus(ShopCode.SHOP_MQ_MESSAGE_STATUS_PROCESSING.getCode());

                    //使用数据库乐观锁更新
                    TradeMqConsumerLogExample example = new TradeMqConsumerLogExample();
                    TradeMqConsumerLogExample.Criteria criteria = example.createCriteria();
                    criteria.andMsgTagEqualTo(mqConsumerLog.getMsgTag());
                    criteria.andMsgKeyEqualTo(mqConsumerLog.getMsgKey());
                    criteria.andGroupNameEqualTo(groupName);
                    criteria.andConsumerTimesEqualTo(mqConsumerLog.getConsumerTimes());
                    int r = mqConsumerLogMapper.updateByExampleSelective(mqConsumerLog, example);
                    if(r<=0){
                        //未修改成功,其他线程并发修改
                        log.info("并发修改,稍后处理");
                    }
                }

            }else{
                //4. 判断如果没有消费过...
                mqConsumerLog = new TradeMqConsumerLog();
                mqConsumerLog.setMsgTag(tags);
                mqConsumerLog.setMsgKey(keys);
                mqConsumerLog.setConsumerStatus(ShopCode.SHOP_MQ_MESSAGE_STATUS_PROCESSING.getCode());
                mqConsumerLog.setMsgBody(body);
                mqConsumerLog.setMsgId(msgId);
                mqConsumerLog.setConsumerTimes(0);

                //将消息处理信息添加到数据库
                mqConsumerLogMapper.insert(mqConsumerLog);
            }
            //5. 回退库存
            MQEntity mqEntity = JSON.parseObject(body, MQEntity.class);
            Long goodsId = mqEntity.getGoodsId();
            TradeGoods goods = goodsMapper.selectByPrimaryKey(goodsId);
            goods.setGoodsNumber(goods.getGoodsNumber()+mqEntity.getGoodsNum());
            goodsMapper.updateByPrimaryKey(goods);

            //记录库存操作日志
            TradeGoodsNumberLog goodsNumberLog = new TradeGoodsNumberLog();
            goodsNumberLog.setOrderId(mqEntity.getOrderId());
            goodsNumberLog.setGoodsId(goodsId);
            goodsNumberLog.setGoodsNumber(mqEntity.getGoodsNum());
            goodsNumberLog.setLogTime(new Date());
            goodsNumberLogMapper.insert(goodsNumberLog);

            //6. 将消息的处理状态改为成功
            mqConsumerLog.setConsumerStatus(ShopCode.SHOP_MQ_MESSAGE_STATUS_SUCCESS.getCode());
            mqConsumerLog.setConsumerTimestamp(new Date());
            mqConsumerLogMapper.updateByPrimaryKey(mqConsumerLog);
            log.info("回退库存成功");
        } catch (Exception e) {
            e.printStackTrace();
            TradeMqConsumerLogKey primaryKey = new TradeMqConsumerLogKey();
            primaryKey.setMsgTag(tags);
            primaryKey.setMsgKey(keys);
            primaryKey.setGroupName(groupName);
            TradeMqConsumerLog mqConsumerLog = mqConsumerLogMapper.selectByPrimaryKey(primaryKey);
            if(mqConsumerLog==null){
                //数据库未有记录
                mqConsumerLog = new TradeMqConsumerLog();
                mqConsumerLog.setMsgTag(tags);
                mqConsumerLog.setMsgKey(keys);
                mqConsumerLog.setConsumerStatus(ShopCode.SHOP_MQ_MESSAGE_STATUS_FAIL.getCode());
                mqConsumerLog.setMsgBody(body);
                mqConsumerLog.setMsgId(msgId);
                mqConsumerLog.setConsumerTimes(1);
                mqConsumerLogMapper.insert(mqConsumerLog);
            }else{
                mqConsumerLog.setConsumerTimes(mqConsumerLog.getConsumerTimes()+1);
                mqConsumerLogMapper.updateByPrimaryKeySelective(mqConsumerLog);
            }
        }

    }
}
```

#### 2）回退优惠券

```java
@Slf4j
@Component
@RocketMQMessageListener(topic = "${mq.order.topic}",consumerGroup = "${mq.order.consumer.group.name}",messageModel = MessageModel.BROADCASTING )
public class CancelMQListener implements RocketMQListener<MessageExt>{


    @Autowired
    private TradeCouponMapper couponMapper;

    @Override
    public void onMessage(MessageExt message) {

        try {
            //1. 解析消息内容
            String body = new String(message.getBody(), "UTF-8");
            MQEntity mqEntity = JSON.parseObject(body, MQEntity.class);
            log.info("接收到消息");
            //2. 查询优惠券信息
            TradeCoupon coupon = couponMapper.selectByPrimaryKey(mqEntity.getCouponId());
            //3.更改优惠券状态
            coupon.setUsedTime(null);
            coupon.setIsUsed(ShopCode.SHOP_COUPON_UNUSED.getCode());
            coupon.setOrderId(null);
            couponMapper.updateByPrimaryKey(coupon);
            log.info("回退优惠券成功");
        } catch (UnsupportedEncodingException e) {
            e.printStackTrace();
            log.error("回退优惠券失败");
        }

    }
}
```

#### 3）回退余额

```java
@Slf4j
@Component
@RocketMQMessageListener(topic = "${mq.order.topic}",consumerGroup = "${mq.order.consumer.group.name}",messageModel = MessageModel.BROADCASTING )
public class CancelMQListener implements RocketMQListener<MessageExt>{


    @Autowired
    private IUserService userService;

    @Override
    public void onMessage(MessageExt messageExt) {

        try {
            //1.解析消息
            String body = new String(messageExt.getBody(), "UTF-8");
            MQEntity mqEntity = JSON.parseObject(body, MQEntity.class);
            log.info("接收到消息");
            if(mqEntity.getUserMoney()!=null && mqEntity.getUserMoney().compareTo(BigDecimal.ZERO)>0){
                //2.调用业务层,进行余额修改
                TradeUserMoneyLog userMoneyLog = new TradeUserMoneyLog();
                userMoneyLog.setUseMoney(mqEntity.getUserMoney());
                userMoneyLog.setMoneyLogType(ShopCode.SHOP_USER_MONEY_REFUND.getCode());
                userMoneyLog.setUserId(mqEntity.getUserId());
                userMoneyLog.setOrderId(mqEntity.getOrderId());
                userService.updateMoneyPaid(userMoneyLog);
                log.info("余额回退成功");
            }
        } catch (UnsupportedEncodingException e) {
            e.printStackTrace();
            log.error("余额回退失败");
        }

    }
}
```

#### 4）取消订单

```java
@Override
    public void onMessage(MessageExt messageExt) {
        String body = new String(messageExt.getBody(), "UTF-8");
        String msgId = messageExt.getMsgId();
        String tags = messageExt.getTags();
        String keys = messageExt.getKeys();
        log.info("CancelOrderProcessor receive message:"+messageExt);
        CancelOrderMQ cancelOrderMQ = JSON.parseObject(body, CancelOrderMQ.class);
        TradeOrder order = orderService.findOne(cancelOrderMQ.getOrderId());
		order.setOrderStatus(ShopCode.SHOP_ORDER_CANCEL.getCode());
        orderService.changeOrderStatus(order);
        log.info("订单:["+order.getOrderId()+"]状态设置为取消");
        return order;
    }
```

## 4.3 测试

### 1）准备测试环境

```java
@RunWith(SpringRunner.class)
@SpringBootTest(classes = ShopOrderServiceApplication.class)
public class OrderTest {

    @Autowired
    private IOrderService orderService;
}
```

\###1）准备测试数据

* 用户数据
* 商品数据
* 优惠券数据

\###2）测试下单成功流程

```java
@Test    
public void add(){
    Long goodsId=XXXL;
    Long userId=XXXL;
    Long couponId=XXXL;

    TradeOrder order = new TradeOrder();
    order.setGoodsId(goodsId);
    order.setUserId(userId);
    order.setGoodsNumber(1);
    order.setAddress("北京");
    order.setGoodsPrice(new BigDecimal("5000"));
    order.setOrderAmount(new BigDecimal("5000"));
    order.setMoneyPaid(new BigDecimal("100"));
    order.setCouponId(couponId);
    order.setShippingFee(new BigDecimal(0));
    orderService.confirmOrder(order);
}
```

执行完毕后,查看数据库中用户的余额、优惠券数据，及订单的状态数据

\###3）测试下单失败流程

代码同上。

执行完毕后，查看用户的余额、优惠券数据是否发生更改，订单的状态是否为取消。

# 5. 支付业务

## 5.1 创建支付订单

![](/assets/创建支付订单.BHFT_5fH.png)

```java
public Result createPayment(TradePay tradePay) {
    //查询订单支付状态
    try {
        TradePayExample payExample = new TradePayExample();
        TradePayExample.Criteria criteria = payExample.createCriteria();
        criteria.andOrderIdEqualTo(tradePay.getOrderId());
        criteria.andIsPaidEqualTo(ShopCode.SHOP_ORDER_PAY_STATUS_IS_PAY.getCode());
        int count = tradePayMapper.countByExample(payExample);
        if (count > 0) {
            CastException.cast(ShopCode.SHOP_ORDER_PAY_STATUS_IS_PAY);
        }

        long payId = idWorker.nextId();
        tradePay.setPayId(payId);
        tradePay.setIsPaid(ShopCode.SHOP_ORDER_PAY_STATUS_NO_PAY.getCode());
        tradePayMapper.insert(tradePay);
        log.info("创建支付订单成功:" + payId);
    } catch (Exception e) {
        return new Result(ShopCode.SHOP_FAIL.getSuccess(), ShopCode.SHOP_FAIL.getMessage());
    }
    return new Result(ShopCode.SHOP_SUCCESS.getSuccess(), ShopCode.SHOP_SUCCESS.getMessage());
}
```

## 5.2 支付回调

### 5.2.1 流程分析

![](/assets/12.支付后回调.c86kVhcw.png)

### 5.2.2 代码实现

```java
public Result callbackPayment(TradePay tradePay) {

    if (tradePay.getIsPaid().equals(ShopCode.SHOP_ORDER_PAY_STATUS_IS_PAY.getCode())) {
        tradePay = tradePayMapper.selectByPrimaryKey(tradePay.getPayId());
        if (tradePay == null) {
            CastException.cast(ShopCode.SHOP_PAYMENT_NOT_FOUND);
        }
        tradePay.setIsPaid(ShopCode.SHOP_ORDER_PAY_STATUS_IS_PAY.getCode());
        int i = tradePayMapper.updateByPrimaryKeySelective(tradePay);
        //更新成功代表支付成功
        if (i == 1) {
            TradeMqProducerTemp mqProducerTemp = new TradeMqProducerTemp();
            mqProducerTemp.setId(String.valueOf(idWorker.nextId()));
            mqProducerTemp.setGroupName("payProducerGroup");
            mqProducerTemp.setMsgKey(String.valueOf(tradePay.getPayId()));
            mqProducerTemp.setMsgTag(topic);
            mqProducerTemp.setMsgBody(JSON.toJSONString(tradePay));
            mqProducerTemp.setCreateTime(new Date());
            mqProducerTempMapper.insert(mqProducerTemp);
            TradePay finalTradePay = tradePay;
            executorService.submit(new Runnable() {
                @Override
                public void run() {
                    try {
                        SendResult sendResult = sendMessage(topic, 
                                                            tag, 
                                                            finalTradePay.getPayId(), 
                                                            JSON.toJSONString(finalTradePay));
                        log.info(JSON.toJSONString(sendResult));
                        if (SendStatus.SEND_OK.equals(sendResult.getSendStatus())) {
                            mqProducerTempMapper.deleteByPrimaryKey(mqProducerTemp.getId());
                            System.out.println("删除消息表成功");
                        }
                    } catch (Exception e) {
                        e.printStackTrace();
                    }
                }
            });
        } else {
            CastException.cast(ShopCode.SHOP_PAYMENT_IS_PAID);
        }
    }
    return new Result(ShopCode.SHOP_SUCCESS.getSuccess(), ShopCode.SHOP_SUCCESS.getMessage());
}
```

#### 线程池优化消息发送逻辑

* 创建线程池对象

```java
@Bean
public ThreadPoolTaskExecutor getThreadPool() {

    ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();

    executor.setCorePoolSize(4);

    executor.setMaxPoolSize(8);

    executor.setQueueCapacity(100);

    executor.setKeepAliveSeconds(60);

    executor.setThreadNamePrefix("Pool-A");

    executor.setRejectedExecutionHandler(new ThreadPoolExecutor.CallerRunsPolicy());

    executor.initialize();

    return executor;

}
```

* 使用线程池

```java
@Autowired
private ThreadPoolTaskExecutor executorService;

executorService.submit(new Runnable() {
    @Override
    public void run() {
        try {
            SendResult sendResult = sendMessage(topic, tag, finalTradePay.getPayId(), JSON.toJSONString(finalTradePay));
            log.info(JSON.toJSONString(sendResult));
            if (SendStatus.SEND_OK.equals(sendResult.getSendStatus())) {
                mqProducerTempMapper.deleteByPrimaryKey(mqProducerTemp.getId());
                System.out.println("删除消息表成功");
            }
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
});
```

### 5.2.3

### 处理消息

支付成功后，支付服务payService发送MQ消息，订单服务、用户服务、日志服务需要订阅消息进行处理

1. 订单服务修改订单状态为已支付
2. 日志服务记录支付日志
3. 用户服务负责给用户增加积分

以下用订单服务为例说明消息的处理情况

#### 1）配置RocketMQ属性值

```properties
mq.pay.topic=payTopic
mq.pay.consumer.group.name=pay_payTopic_group
```

#### 2）消费消息

* 在订单服务中，配置公共的消息处理类

```java
public class BaseConsumer {

    public TradeOrder handleMessage(IOrderService 
                                    orderService, 
                                    MessageExt messageExt,Integer code) throws Exception {
        //解析消息内容
        String body = new String(messageExt.getBody(), "UTF-8");
        String msgId = messageExt.getMsgId();
        String tags = messageExt.getTags();
        String keys = messageExt.getKeys();
        OrderMQ orderMq = JSON.parseObject(body, OrderMQ.class);
        
        //查询
        TradeOrder order = orderService.findOne(orderMq.getOrderId());

        if(ShopCode.SHOP_ORDER_MESSAGE_STATUS_CANCEL.getCode().equals(code)){
            order.setOrderStatus(ShopCode.SHOP_ORDER_CANCEL.getCode());
        }

        if(ShopCode.SHOP_ORDER_MESSAGE_STATUS_ISPAID.getCode().equals(code)){
            order.setPayStatus(ShopCode.SHOP_ORDER_PAY_STATUS_IS_PAY.getCode());
        }
        orderService.changeOrderStatus(order);
        return order;
    }

}
```

* 接受订单支付成功消息

```java
@Slf4j
@Component
@RocketMQMessageListener(topic = "${mq.pay.topic}", 
                         consumerGroup = "${mq.pay.consumer.group.name}")
public class PayConsumer extends BaseConsumer implements RocketMQListener<MessageExt> {

    @Autowired
    private IOrderService orderService;

    @Override
    public void onMessage(MessageExt messageExt) {
        try {
            log.info("CancelOrderProcessor receive message:"+messageExt);
            TradeOrder order = handleMessage(orderService, 
                                             messageExt, 
                                             ShopCode.SHOP_ORDER_MESSAGE_STATUS_ISPAID.getCode());
            log.info("订单:["+order.getOrderId()+"]支付成功");
        } catch (Exception e) {
            e.printStackTrace();
            log.error("订单支付失败");
        }
    }
}
```

# 6. 整体联调

通过Rest客户端请求shop-order-web和shop-pay-web完成下单和支付操作

## 6.1 准备工作

### 1）配置RestTemplate类

```java
@Configuration
public class RestTemplateConfig {

    @Bean
    @ConditionalOnMissingBean({ RestOperations.class, RestTemplate.class })
    public RestTemplate restTemplate(ClientHttpRequestFactory factory) {

        RestTemplate restTemplate = new RestTemplate(factory);

        // 使用 utf-8 编码集的 conver 替换默认的 conver（默认的 string conver 的编码集为"ISO-8859-1"）
        List<HttpMessageConverter<?>> messageConverters = restTemplate.getMessageConverters();
        Iterator<HttpMessageConverter<?>> iterator = messageConverters.iterator();
        while (iterator.hasNext()) {
            HttpMessageConverter<?> converter = iterator.next();
            if (converter instanceof StringHttpMessageConverter) {
                iterator.remove();
            }
        }
        messageConverters.add(new StringHttpMessageConverter(Charset.forName("UTF-8")));

        return restTemplate;
    }

    @Bean
    @ConditionalOnMissingBean({ClientHttpRequestFactory.class})
    public ClientHttpRequestFactory simpleClientHttpRequestFactory() {
        SimpleClientHttpRequestFactory factory = new SimpleClientHttpRequestFactory();
        // ms
        factory.setReadTimeout(15000);
        // ms
        factory.setConnectTimeout(15000);
        return factory;
    }
}
```

### 2）配置请求地址

* 订单系统

```properties
server.host=http://localhost
server.servlet.path=/order-web
server.port=8080
shop.order.baseURI=${server.host}:${server.port}${server.servlet.path}
shop.order.confirm=/order/confirm
```

* 支付系统

```properties
server.host=http://localhost
server.servlet.path=/pay-web
server.port=9090
shop.pay.baseURI=${server.host}:${server.port}${server.servlet.path}
shop.pay.createPayment=/pay/createPayment
shop.pay.callbackPayment=/pay/callbackPayment
```

## 6.2 下单测试

```java
@RunWith(SpringRunner.class)
@ContextConfiguration(classes = ShopOrderWebApplication.class)
@TestPropertySource("classpath:application.properties")
public class OrderTest {

   @Autowired
   private RestTemplate restTemplate;

   @Value("${shop.order.baseURI}")
   private String baseURI;

   @Value("${shop.order.confirm}")
   private String confirmOrderPath;

   @Autowired
   private IDWorker idWorker;
  
  /**
    * 下单
    */
   @Test
   public void confirmOrder(){
       Long goodsId=XXXL;
       Long userId=XXXL;
       Long couponId=XXXL;

       TradeOrder order = new TradeOrder();
       order.setGoodsId(goodsId);
       order.setUserId(userId);
       order.setGoodsNumber(1);
       order.setAddress("北京");
       order.setGoodsPrice(new BigDecimal("5000"));
       order.setOrderAmount(new BigDecimal("5000"));
       order.setMoneyPaid(new BigDecimal("100"));
       order.setCouponId(couponId);
       order.setShippingFee(new BigDecimal(0));

       Result result = restTemplate.postForEntity(baseURI + confirmOrderPath, order, Result.class).getBody();
       System.out.println(result);
   }

}
```

## 6.3 支付测试

```java
@RunWith(SpringRunner.class)
@ContextConfiguration(classes = ShopPayWebApplication.class)
@TestPropertySource("classpath:application.properties")
public class PayTest {

    @Autowired
    private RestTemplate restTemplate;

    @Value("${shop.pay.baseURI}")
    private String baseURI;

    @Value("${shop.pay.createPayment}")
    private String createPaymentPath;

    @Value("${shop.pay.callbackPayment}")
    private String callbackPaymentPath;

    @Autowired
    private IDWorker idWorker;

   /**
     * 创建支付订单
     */
    @Test
    public void createPayment(){

        Long orderId = 346321587315814400L;
        TradePay pay = new TradePay();
        pay.setOrderId(orderId);
        pay.setPayAmount(new BigDecimal(4800));

        Result result = restTemplate.postForEntity(baseURI + createPaymentPath, pay, Result.class).getBody();
        System.out.println(result);
    }
   
    /**
     * 支付回调
     */
    @Test
    public void callbackPayment(){
        Long payId = 346321891507720192L;
        TradePay pay = new TradePay();
        pay.setPayId(payId);
        pay.setIsPaid(ShopCode.SHOP_ORDER_PAY_STATUS_IS_PAY.getCode());
        Result result = restTemplate.postForEntity(baseURI + callbackPaymentPath, pay, Result.class).getBody();
        System.out.println(result);

    }

}
```

---

---
url: /Java/架构设计/分布式/03.分布式消息队列/01.RocketMQ/RocketMQ-03.md
---

# 1. 高级功能

## 1.1 消息存储

分布式队列因为有高可靠性的要求，所以数据要进行持久化存储。

![](/assets/消息存储方式.7jF83-9r.png)

1. 消息生成者发送消息
2. MQ收到消息，将消息进行持久化，在存储中新增一条记录
3. 返回ACK给生产者
4. MQ push 消息给对应的消费者，然后等待消费者返回ACK
5. 如果消息消费者在指定时间内成功返回ack，那么MQ认为消息消费成功，在存储中删除消息，即执行第6步；如果MQ在指定时间内没有收到ACK，则认为消息消费失败，会尝试重新push消息,重复执行4、5、6步骤
6. MQ删除消息

### 1.1.1 存储介质

* 关系型数据库DB

Apache下开源的另外一款MQ—ActiveMQ（默认采用的KahaDB做消息存储）可选用JDBC的方式来做消息持久化，通过简单的xml配置信息即可实现JDBC消息存储。由于，普通关系型数据库（如Mysql）在单表数据量达到千万级别的情况下，其IO读写性能往往会出现瓶颈。在可靠性方面，该种方案非常依赖DB，如果一旦DB出现故障，则MQ的消息就无法落盘存储会导致线上故障

![](/assets/MySQL.CHmpZjR2.png)

* 文件系统

  目前业界较为常用的几款产品（RocketMQ/Kafka/RabbitMQ）均采用的是消息刷盘至所部署虚拟机/物理机的文件系统来做持久化（刷盘一般可以分为异步刷盘和同步刷盘两种模式）。消息刷盘为消息存储提供了一种高效率、高可靠性和高性能的数据持久化方式。除非部署MQ机器本身或是本地磁盘挂了，否则一般是不会出现无法持久化的故障问题。

  ![](/assets/磁盘.BVTUF04o.png)

\###1.1.2 性能对比

文件系统>关系型数据库DB

### 1.1.3 消息的存储和发送

#### 1）消息存储

磁盘如果使用得当，磁盘的速度完全可以匹配上网络 的数据传输速度。目前的高性能磁盘，顺序写速度可以达到600MB/s， 超过了一般网卡的传输速度。但是磁盘随机写的速度只有大概100KB/s，和顺序写的性能相差6000倍！因为有如此巨大的速度差别，好的消息队列系统会比普通的消息队列系统速度快多个数量级。RocketMQ的消息用顺序写,保证了消息存储的速度。

\####2）消息发送

Linux操作系统分为【用户态】和【内核态】，文件操作、网络操作需要涉及这两种形态的切换，免不了进行数据复制。

一台服务器 把本机磁盘文件的内容发送到客户端，一般分为两个步骤：

1）read；读取本地文件内容；

2）write；将读取的内容通过网络发送出去。

这两个看似简单的操作，实际进行了4 次数据复制，分别是：

1. 从磁盘复制数据到内核态内存；
2. 从内核态内存复 制到用户态内存；
3. 然后从用户态 内存复制到网络驱动的内核态内存；
4. 最后是从网络驱动的内核态内存复 制到网卡中进行传输。

![](/assets/文件操作和网络操作.Ce-fFOsF.png)通过使用mmap的方式，可以省去向用户态的内存复制，提高速度。这种机制在Java中是通过MappedByteBuffer实现的

RocketMQ充分利用了上述特性，也就是所谓的“零拷贝”技术，提高消息存盘和网络发送的速度。

> 这里需要注意的是，采用MappedByteBuffer这种内存映射的方式有几个限制，其中之一是一次只能映射1.5~2G 的文件至用户态的虚拟内存，这也是为何RocketMQ默认设置单个CommitLog日志数据文件为1G的原因了

### 1.1.4 消息存储结构

RocketMQ消息的存储是由ConsumeQueue和CommitLog配合完成 的，消息真正的物理存储文件是CommitLog，ConsumeQueue是消息的逻辑队列，类似数据库的索引文件，存储的是指向物理存储的地址。每 个Topic下的每个Message Queue都有一个对应的ConsumeQueue文件。

![](/assets/消息存储结构.CcBaUnuh.png)

* CommitLog：存储消息的元数据
* ConsumerQueue：存储消息在CommitLog的索引
* IndexFile：为了消息查询提供了一种通过key或时间区间来查询消息的方法，这种通过IndexFile来查找消息的方法不影响发送与消费消息的主流程

### 1.1.5 刷盘机制

RocketMQ的消息是存储到磁盘上的，这样既能保证断电后恢复， 又可以让存储的消息量超出内存的限制。RocketMQ为了提高性能，会尽可能地保证磁盘的顺序写。消息在通过Producer写入RocketMQ的时 候，有两种写磁盘方式，分布式同步刷盘和异步刷盘。

![](/assets/同步刷盘和异步刷盘.1mIk95Yu.png)

#### 1）同步刷盘

在返回写成功状态时，消息已经被写入磁盘。具体流程是，消息写入内存的PAGECACHE后，立刻通知刷盘线程刷盘， 然后等待刷盘完成，刷盘线程执行完成后唤醒等待的线程，返回消息写 成功的状态。

#### 2）异步刷盘

在返回写成功状态时，消息可能只是被写入了内存的PAGECACHE，写操作的返回快，吞吐量大；当内存里的消息量积累到一定程度时，统一触发写磁盘动作，快速写入。

\####3）配置

**同步刷盘还是异步刷盘，都是通过Broker配置文件里的flushDiskType 参数设置的，这个参数被配置成SYNC\_FLUSH、ASYNC\_FLUSH中的 一个。**

## 1.2 高可用性机制

![](/assets/RocketMQ角色.C0wOeWNj.jpg)

RocketMQ分布式集群是通过Master和Slave的配合达到高可用性的。

Master和Slave的区别：在Broker的配置文件中，参数 brokerId的值为0表明这个Broker是Master，大于0表明这个Broker是 Slave，同时brokerRole参数也会说明这个Broker是Master还是Slave。

Master角色的Broker支持读和写，Slave角色的Broker仅支持读，也就是 Producer只能和Master角色的Broker连接写入消息；Consumer可以连接 Master角色的Broker，也可以连接Slave角色的Broker来读取消息。

### 1.2.1 消息消费高可用

在Consumer的配置文件中，并不需要设置是从Master读还是从Slave 读，当Master不可用或者繁忙的时候，Consumer会被自动切换到从Slave 读。有了自动切换Consumer这种机制，当一个Master角色的机器出现故障后，Consumer仍然可以从Slave读取消息，不影响Consumer程序。这就达到了消费端的高可用性。

### 1.2.2 消息发送高可用

在创建Topic的时候，把Topic的多个Message Queue创建在多个Broker组上（相同Broker名称，不同 brokerId的机器组成一个Broker组），这样当一个Broker组的Master不可 用后，其他组的Master仍然可用，Producer仍然可以发送消息。 RocketMQ目前还不支持把Slave自动转成Master，如果机器资源不足， 需要把Slave转成Master，则要手动停止Slave角色的Broker，更改配置文 件，用新的配置文件启动Broker。

![](/assets/消息发送高可用设计.BdooEewY.jpg)

### 1.2.3 消息主从复制

如果一个Broker组有Master和Slave，消息需要从Master复制到Slave 上，有同步和异步两种复制方式。

\####1）同步复制

同步复制方式是等Master和Slave均写 成功后才反馈给客户端写成功状态；

在同步复制方式下，如果Master出故障， Slave上有全部的备份数据，容易恢复，但是同步复制会增大数据写入 延迟，降低系统吞吐量。

\####2）异步复制

异步复制方式是只要Master写成功 即可反馈给客户端写成功状态。

在异步复制方式下，系统拥有较低的延迟和较高的吞吐量，但是如果Master出了故障，有些数据因为没有被写 入Slave，有可能会丢失；

\####3）配置

同步复制和异步复制是通过Broker配置文件里的brokerRole参数进行设置的，这个参数可以被设置成ASYNC\_MASTER、 SYNC\_MASTER、SLAVE三个值中的一个。

\####4）总结

![](/assets/复制刷盘.CBID-BG9.png)

实际应用中要结合业务场景，合理设置刷盘方式和主从复制方式， 尤其是SYNC\_FLUSH方式，由于频繁地触发磁盘写动作，会明显降低 性能。通常情况下，应该把Master和Save配置成ASYNC\_FLUSH的刷盘 方式，主从之间配置成SYNC\_MASTER的复制方式，这样即使有一台 机器出故障，仍然能保证数据不丢，是个不错的选择。

## 1.3 负载均衡

### 1.3.1 Producer负载均衡

Producer端，每个实例在发消息的时候，默认会轮询所有的message queue发送，以达到让消息平均落在不同的queue上。而由于queue可以散落在不同的broker，所以消息就发送到不同的broker下，如下图：

![](/assets/producer负载均衡.BQfyoRzy.png)

图中箭头线条上的标号代表顺序，发布方会把第一条消息发送至 Queue 0，然后第二条消息发送至 Queue 1，以此类推。

### 1.3.2 Consumer负载均衡

#### 1）集群模式

在集群消费模式下，每条消息只需要投递到订阅这个topic的Consumer Group下的一个实例即可。RocketMQ采用主动拉取的方式拉取并消费消息，在拉取的时候需要明确指定拉取哪一条message queue。

而每当实例的数量有变更，都会触发一次所有实例的负载均衡，这时候会按照queue的数量和实例的数量平均分配queue给每个实例。

默认的分配算法是AllocateMessageQueueAveragely，如下图：

![](/assets/consumer负载均衡.ppynQuJv.png)

还有另外一种平均的算法是AllocateMessageQueueAveragelyByCircle，也是平均分摊每一条queue，只是以环状轮流分queue的形式，如下图：

![](/assets/consumer负载均衡2.BmRyGgw5.png)

需要注意的是，集群模式下，queue都是只允许分配只一个实例，这是由于如果多个实例同时消费一个queue的消息，由于拉取哪些消息是consumer主动控制的，那样会导致同一个消息在不同的实例下被消费多次，所以算法上都是一个queue只分给一个consumer实例，一个consumer实例可以允许同时分到不同的queue。

通过增加consumer实例去分摊queue的消费，可以起到水平扩展的消费能力的作用。而有实例下线的时候，会重新触发负载均衡，这时候原来分配到的queue将分配到其他实例上继续消费。

但是如果consumer实例的数量比message queue的总数量还多的话，多出来的consumer实例将无法分到queue，也就无法消费到消息，也就无法起到分摊负载的作用了。所以需要控制让queue的总数量大于等于consumer的数量。

\####2）广播模式

由于广播模式下要求一条消息需要投递到一个消费组下面所有的消费者实例，所以也就没有消息被分摊消费的说法。

在实现上，其中一个不同就是在consumer分配queue的时候，所有consumer都分到所有的queue。

![](/assets/consumer负载均衡3.Dpg42aP2.png)

## 1.4 消息重试

### 1.4.1 顺序消息的重试

对于顺序消息，当消费者消费消息失败后，消息队列 RocketMQ 会自动不断进行消息重试（每次间隔时间为 1 秒），这时，应用会出现消息消费被阻塞的情况。因此，在使用顺序消息时，务必保证应用能够及时监控并处理消费失败的情况，避免阻塞现象的发生。

### 1.4.2 无序消息的重试

对于无序消息（普通、定时、延时、事务消息），当消费者消费消息失败时，您可以通过设置返回状态达到消息重试的结果。

无序消息的重试只针对集群消费方式生效；广播方式不提供失败重试特性，即消费失败后，失败消息不再重试，继续消费新的消息。

#### 1）重试次数

消息队列 RocketMQ 默认允许每条消息最多重试 16 次，每次重试的间隔时间如下：

| 第几次重试 | 与上次重试的间隔时间 | 第几次重试 | 与上次重试的间隔时间 |
| :--------: | :------------------: | :--------: | :------------------: |
|     1      |        10 秒         |     9      |        7 分钟        |
|     2      |        30 秒         |     10     |        8 分钟        |
|     3      |        1 分钟        |     11     |        9 分钟        |
|     4      |        2 分钟        |     12     |       10 分钟        |
|     5      |        3 分钟        |     13     |       20 分钟        |
|     6      |        4 分钟        |     14     |       30 分钟        |
|     7      |        5 分钟        |     15     |        1 小时        |
|     8      |        6 分钟        |     16     |        2 小时        |

如果消息重试 16 次后仍然失败，消息将不再投递。如果严格按照上述重试时间间隔计算，某条消息在一直消费失败的前提下，将会在接下来的 4 小时 46 分钟之内进行 16 次重试，超过这个时间范围消息将不再重试投递。

**注意：** 一条消息无论重试多少次，这些重试消息的 Message ID 不会改变。

#### 2）配置方式

**消费失败后，重试配置方式**

集群消费方式下，消息消费失败后期望消息重试，需要在消息监听器接口的实现中明确进行配置（三种方式任选一种）：

* 返回 Action.ReconsumeLater （推荐）
* 返回 Null
* 抛出异常

```java
public class MessageListenerImpl implements MessageListener {
    @Override
    public Action consume(Message message, ConsumeContext context) {
        //处理消息
        doConsumeMessage(message);
        //方式1：返回 Action.ReconsumeLater，消息将重试
        return Action.ReconsumeLater;
        //方式2：返回 null，消息将重试
        return null;
        //方式3：直接抛出异常， 消息将重试
        throw new RuntimeException("Consumer Message exceotion");
    }
}
```

**消费失败后，不重试配置方式**

集群消费方式下，消息失败后期望消息不重试，需要捕获消费逻辑中可能抛出的异常，最终返回 Action.CommitMessage，此后这条消息将不会再重试。

```java
public class MessageListenerImpl implements MessageListener {
    @Override
    public Action consume(Message message, ConsumeContext context) {
        try {
            doConsumeMessage(message);
        } catch (Throwable e) {
            //捕获消费逻辑中的所有异常，并返回 Action.CommitMessage;
            return Action.CommitMessage;
        }
        //消息处理正常，直接返回 Action.CommitMessage;
        return Action.CommitMessage;
    }
}
```

**自定义消息最大重试次数**

消息队列 RocketMQ 允许 Consumer 启动的时候设置最大重试次数，重试时间间隔将按照如下策略：

* 最大重试次数小于等于 16 次，则重试时间间隔同上表描述。
* 最大重试次数大于 16 次，超过 16 次的重试时间间隔均为每次 2 小时。

```java
Properties properties = new Properties();
//配置对应 Group ID 的最大消息重试次数为 20 次
properties.put(PropertyKeyConst.MaxReconsumeTimes,"20");
Consumer consumer =ONSFactory.createConsumer(properties);
```

> 注意：

* 消息最大重试次数的设置对相同 Group ID 下的所有 Consumer 实例有效。
* 如果只对相同 Group ID 下两个 Consumer 实例中的其中一个设置了 MaxReconsumeTimes，那么该配置对两个 Consumer 实例均生效。
* 配置采用覆盖的方式生效，即最后启动的 Consumer 实例会覆盖之前的启动实例的配置

**获取消息重试次数**

消费者收到消息后，可按照如下方式获取消息的重试次数：

```java
public class MessageListenerImpl implements MessageListener {
    @Override
    public Action consume(Message message, ConsumeContext context) {
        //获取消息的重试次数
        System.out.println(message.getReconsumeTimes());
        return Action.CommitMessage;
    }
}
```

## 1.5 死信队列

当一条消息初次消费失败，消息队列 RocketMQ 会自动进行消息重试；达到最大重试次数后，若消费依然失败，则表明消费者在正常情况下无法正确地消费该消息，此时，消息队列 RocketMQ 不会立刻将消息丢弃，而是将其发送到该消费者对应的特殊队列中。

在消息队列 RocketMQ 中，这种正常情况下无法被消费的消息称为死信消息（Dead-Letter Message），存储死信消息的特殊队列称为死信队列（Dead-Letter Queue）。

### 1.5.1 死信特性

死信消息具有以下特性

* 不会再被消费者正常消费。
* 有效期与正常消息相同，均为 3 天，3 天后会被自动删除。因此，请在死信消息产生后的 3 天内及时处理。

死信队列具有以下特性：

* 一个死信队列对应一个 Group ID， 而不是对应单个消费者实例。
* 如果一个 Group ID 未产生死信消息，消息队列 RocketMQ 不会为其创建相应的死信队列。
* 一个死信队列包含了对应 Group ID 产生的所有死信消息，不论该消息属于哪个 Topic。

### 1.5.2 查看死信信息

1. 在控制台查询出现死信队列的主题信息

![](/assets/死信队列主题.BbncNCG_.png)

2. 在消息界面根据主题查询死信消息

![](/assets/死信队列主题2.bpuTHNOJ.png)

3. 选择重新发送消息

一条消息进入死信队列，意味着某些因素导致消费者无法正常消费该消息，因此，通常需要您对其进行特殊处理。排查可疑因素并解决问题后，可以在消息队列 RocketMQ 控制台重新发送该消息，让消费者重新消费一次。

## 1.6 消费幂等

消息队列 RocketMQ 消费者在接收到消息以后，有必要根据业务上的唯一 Key 对消息做幂等处理的必要性。

### 1.6.1 消费幂等的必要性

在互联网应用中，尤其在网络不稳定的情况下，消息队列 RocketMQ 的消息有可能会出现重复，这个重复简单可以概括为以下情况：

* 发送时消息重复

  当一条消息已被成功发送到服务端并完成持久化，此时出现了网络闪断或者客户端宕机，导致服务端对客户端应答失败。 如果此时生产者意识到消息发送失败并尝试再次发送消息，消费者后续会收到两条内容相同并且 Message ID 也相同的消息。

* 投递时消息重复

  消息消费的场景下，消息已投递到消费者并完成业务处理，当客户端给服务端反馈应答的时候网络闪断。 为了保证消息至少被消费一次，消息队列 RocketMQ 的服务端将在网络恢复后再次尝试投递之前已被处理过的消息，消费者后续会收到两条内容相同并且 Message ID 也相同的消息。

* 负载均衡时消息重复（包括但不限于网络抖动、Broker 重启以及订阅方应用重启）

  当消息队列 RocketMQ 的 Broker 或客户端重启、扩容或缩容时，会触发 Rebalance，此时消费者可能会收到重复消息。

### 1.6.2 处理方式

因为 Message ID 有可能出现冲突（重复）的情况，所以真正安全的幂等处理，不建议以 Message ID 作为处理依据。 最好的方式是以业务唯一标识作为幂等处理的关键依据，而业务的唯一标识可以通过消息 Key 进行设置：

```java
Message message = new Message();
message.setKey("ORDERID_100");
SendResult sendResult = producer.send(message);
```

订阅方收到消息时可以根据消息的 Key 进行幂等处理：

```java
consumer.subscribe("ons_test", "*", new MessageListener() {
    public Action consume(Message message, ConsumeContext context) {
        String key = message.getKey()
        // 根据业务唯一标识的 key 做幂等处理
    }
});
```

# 2. 源码分析

## 2.1 环境搭建

依赖工具

* JDK ：1.8+
* Maven
* IntelliJ IDEA

### 2.1.1 源码拉取

从官方仓库 <https://github.com/apache/rocketmq> `clone`或者`download`源码。

![](/assets/源码1.DUsIacoC.png)

**源码目录结构：**

* broker: broker 模块（broke 启动进程）

* client ：消息客户端，包含消息生产者、消息消费者相关类

* common ：公共包

* dev ：开发者信息（非源代码）

* distribution ：部署实例文件夹（非源代码）

* example: RocketMQ 例代码

* filter ：消息过滤相关基础类

* filtersrv：消息过滤服务器实现相关类（Filter启动进程）

* logappender：日志实现相关类

* namesrv：NameServer实现相关类（NameServer启动进程）

* openmessageing：消息开放标准

* remoting：远程通信模块，给予Netty

* srcutil：服务工具类

* store：消息存储实现相关类

* style：checkstyle相关实现

* test：测试相关类

* tools：工具类，监控命令相关实现类

\###2.1.2 导入IDEA

![](/assets/源码2.BXdLEzgO.png)

**执行安装**

```sh
clean install -Dmaven.test.skip=true
```

### 2.1.3 调试

创建`conf`配置文件夹,从`distribution`拷贝`broker.conf`和`logback_broker.xml`和`logback_namesrv.xml`

![](/assets/源码6.V1AgOx_l.png)

#### 1）启动NameServer

* 展开namesrv模块，右键NamesrvStartup.java

![](/assets/源码3.BeK5MVN5.png)

* 配置**ROCKETMQ\_HOME**

![](/assets/源码4.CPNidW1D.png)

![](/assets/源码5.C0wu25pU.png)

* 重新启动

  控制台打印结果

```sh
The Name Server boot success. serializeType=JSON
```

#### 2）启动Broker

* `broker.conf`配置文件内容

```properties
brokerClusterName = DefaultCluster
brokerName = broker-a
brokerId = 0
# namesrvAddr地址
namesrvAddr=127.0.0.1:9876
deleteWhen = 04
fileReservedTime = 48
brokerRole = ASYNC_MASTER
flushDiskType = ASYNC_FLUSH
autoCreateTopicEnable=true

# 存储路径
storePathRootDir=E:\\RocketMQ\\data\\rocketmq\\dataDir
# commitLog路径
storePathCommitLog=E:\\RocketMQ\\data\\rocketmq\\dataDir\\commitlog
# 消息队列存储路径
storePathConsumeQueue=E:\\RocketMQ\\data\\rocketmq\\dataDir\\consumequeue
# 消息索引存储路径
storePathIndex=E:\\RocketMQ\\data\\rocketmq\\dataDir\\index
# checkpoint文件路径
storeCheckpoint=E:\\RocketMQ\\data\\rocketmq\\dataDir\\checkpoint
# abort文件存储路径
abortFile=E:\\RocketMQ\\data\\rocketmq\\dataDir\\abort
```

* 创建数据文件夹`dataDir`
* 启动`BrokerStartup`,配置`broker.conf`和`ROCKETMQ_HOME`

![](/assets/源码7.Ck1GRMyP.png)

![](/assets/源码8.Dc2HIZPz.png)

\####3）发送消息

* 进入example模块的`org.apache.rocketmq.example.quickstart`
* 指定Namesrv地址

```java
DefaultMQProducer producer = new DefaultMQProducer("please_rename_unique_group_name");
producer.setNamesrvAddr("127.0.0.1:9876");
```

* 运行`main`方法，发送消息

#### 4）消费消息

* 进入example模块的`org.apache.rocketmq.example.quickstart`
* 指定Namesrv地址

```java
DefaultMQPushConsumer consumer = new DefaultMQPushConsumer("please_rename_unique_group_name_4");
consumer.setNamesrvAddr("127.0.0.1:9876");
```

* 运行`main`方法，消费消息

## 2.2 NameServer

### 2.2.1 架构设计

消息中间件的设计思路一般是基于主题订阅发布的机制，消息生产者（Producer）发送某一个主题到消息服务器，消息服务器负责将消息持久化存储，消息消费者（Consumer）订阅该兴趣的主题，消息服务器根据订阅信息（路由信息）将消息推送到消费者（Push模式）或者消费者主动向消息服务器拉去（Pull模式），从而实现消息生产者与消息消费者解耦。为了避免消息服务器的单点故障导致的整个系统瘫痪，通常会部署多台消息服务器共同承担消息的存储。那消息生产者如何知道消息要发送到哪台消息服务器呢？如果某一台消息服务器宕机了，那么消息生产者如何在不重启服务情况下感知呢？

NameServer就是为了解决以上问题设计的。

![](/assets/RocketMQ角色.C0wOeWNj.jpg)

Broker消息服务器在启动的时向所有NameServer注册，消息生产者（Producer）在发送消息时之前先从NameServer获取Broker服务器地址列表，然后根据负载均衡算法从列表中选择一台服务器进行发送。NameServer与每台Broker保持长连接，并间隔30S检测Broker是否存活，如果检测到Broker宕机，则从路由注册表中删除。但是路由变化不会马上通知消息生产者。这样设计的目的是为了降低NameServer实现的复杂度，在消息发送端提供容错机制保证消息发送的可用性。

NameServer本身的高可用是通过部署多台NameServer来实现，但彼此之间不通讯，也就是NameServer服务器之间在某一个时刻的数据并不完全相同，但这对消息发送并不会造成任何影响，这也是NameServer设计的一个亮点，总之，RocketMQ设计追求简单高效。

### 2.2.2 启动流程

![](/assets/NameServer启动流程.Btr8Ju4B.png)

启动类：`org.apache.rocketmq.namesrv.NamesrvStartup`

\####步骤一

解析配置文件，填充NameServerConfig、NettyServerConfig属性值，并创建NamesrvController

***代码：NamesrvController#createNamesrvController***

```java
//创建NamesrvConfig
final NamesrvConfig namesrvConfig = new NamesrvConfig();
//创建NettyServerConfig
final NettyServerConfig nettyServerConfig = new NettyServerConfig();
//设置启动端口号
nettyServerConfig.setListenPort(9876);
//解析启动-c参数
if (commandLine.hasOption('c')) {
    String file = commandLine.getOptionValue('c');
    if (file != null) {
        InputStream in = new BufferedInputStream(new FileInputStream(file));
        properties = new Properties();
        properties.load(in);
        MixAll.properties2Object(properties, namesrvConfig);
        MixAll.properties2Object(properties, nettyServerConfig);

        namesrvConfig.setConfigStorePath(file);

        System.out.printf("load config properties file OK, %s%n", file);
        in.close();
    }
}
//解析启动-p参数
if (commandLine.hasOption('p')) {
    InternalLogger console = InternalLoggerFactory.getLogger(LoggerName.NAMESRV_CONSOLE_NAME);
    MixAll.printObjectProperties(console, namesrvConfig);
    MixAll.printObjectProperties(console, nettyServerConfig);
    System.exit(0);
}
//将启动参数填充到namesrvConfig,nettyServerConfig
MixAll.properties2Object(ServerUtil.commandLine2Properties(commandLine), namesrvConfig);

//创建NameServerController
final NamesrvController controller = new NamesrvController(namesrvConfig, nettyServerConfig);
```

**NamesrvConfig属性**

```java
private String rocketmqHome = System.getProperty(MixAll.ROCKETMQ_HOME_PROPERTY, System.getenv(MixAll.ROCKETMQ_HOME_ENV));
private String kvConfigPath = System.getProperty("user.home") + File.separator + "namesrv" + File.separator + "kvConfig.json";
private String configStorePath = System.getProperty("user.home") + File.separator + "namesrv" + File.separator + "namesrv.properties";
private String productEnvName = "center";
private boolean clusterTest = false;
private boolean orderMessageEnable = false;
```

\*\*rocketmqHome：\*\*rocketmq主目录

\*\*kvConfig：\*\*NameServer存储KV配置属性的持久化路径

\*\*configStorePath：\*\*nameServer默认配置文件路径

\*\*orderMessageEnable：\*\*是否支持顺序消息

**NettyServerConfig属性**

```java
private int listenPort = 8888;
private int serverWorkerThreads = 8;
private int serverCallbackExecutorThreads = 0;
private int serverSelectorThreads = 3;
private int serverOnewaySemaphoreValue = 256;
private int serverAsyncSemaphoreValue = 64;
private int serverChannelMaxIdleTimeSeconds = 120;
private int serverSocketSndBufSize = NettySystemConfig.socketSndbufSize;
private int serverSocketRcvBufSize = NettySystemConfig.socketRcvbufSize;
private boolean serverPooledByteBufAllocatorEnable = true;
private boolean useEpollNativeSelector = false;
```

\*\*listenPort：\*\*NameServer监听端口，该值默认会被初始化为9876
\*\*serverWorkerThreads：\*\*Netty业务线程池线程个数
\*\*serverCallbackExecutorThreads：\*\*Netty public任务线程池线程个数，Netty网络设计，根据业务类型会创建不同的线程池，比如处理消息发送、消息消费、心跳检测等。如果该业务类型未注册线程池，则由public线程池执行。
\*\*serverSelectorThreads：\*\*IO线程池个数，主要是NameServer、Broker端解析请求、返回相应的线程个数，这类线程主要是处理网路请求的，解析请求包，然后转发到各个业务线程池完成具体的操作，然后将结果返回给调用方;
\*\*serverOnewaySemaphoreValue：\*\*send oneway消息请求并发读（Broker端参数）;
\*\*serverAsyncSemaphoreValue：\*\*异步消息发送最大并发度;
\*\*serverChannelMaxIdleTimeSeconds ：\*\*网络连接最大的空闲时间，默认120s。
\*\*serverSocketSndBufSize：\*\*网络socket发送缓冲区大小。
**serverSocketRcvBufSize：** 网络接收端缓存区大小。
\*\*serverPooledByteBufAllocatorEnable：\*\*ByteBuffer是否开启缓存;
\*\*useEpollNativeSelector：\*\*是否启用Epoll IO模型。

#### 步骤二

根据启动属性创建NamesrvController实例，并初始化该实例。NameServerController实例为NameServer核心控制器

***代码：NamesrvController#initialize***

```java
public boolean initialize() {
	//加载KV配置
    this.kvConfigManager.load();
	//创建NettyServer网络处理对象
    this.remotingServer = new NettyRemotingServer(this.nettyServerConfig, this.brokerHousekeepingService);
	//开启定时任务:每隔10s扫描一次Broker,移除不活跃的Broker
    this.remotingExecutor =
        Executors.newFixedThreadPool(nettyServerConfig.getServerWorkerThreads(), new ThreadFactoryImpl("RemotingExecutorThread_"));
    this.registerProcessor();
    this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() {
        @Override
        public void run() {
            NamesrvController.this.routeInfoManager.scanNotActiveBroker();
        }
    }, 5, 10, TimeUnit.SECONDS);
	//开启定时任务:每隔10min打印一次KV配置
	this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() {

        @Override
        public void run() {
            NamesrvController.this.kvConfigManager.printAllPeriodically();
        }
    }, 1, 10, TimeUnit.MINUTES);
    return true;
}
```

#### 步骤三

在JVM进程关闭之前，先将线程池关闭，及时释放资源

***代码：NamesrvStartup#start***

```java
//注册JVM钩子函数代码
Runtime.getRuntime().addShutdownHook(new ShutdownHookThread(log, new Callable<Void>() {
    @Override
    public Void call() throws Exception {
        //释放资源
        controller.shutdown();
        return null;
    }
}));
```

### 2.2.3 路由管理

NameServer的主要作用是为消息的生产者和消息消费者提供关于主题Topic的路由信息，那么NameServer需要存储路由的基础信息，还要管理Broker节点，包括路由注册、路由删除等。

#### 2.2.3.1 路由元信息

***代码：RouteInfoManager***

```java
private final HashMap<String/* topic */, List<QueueData>> topicQueueTable;
private final HashMap<String/* brokerName */, BrokerData> brokerAddrTable;
private final HashMap<String/* clusterName */, Set<String/* brokerName */>> clusterAddrTable;
private final HashMap<String/* brokerAddr */, BrokerLiveInfo> brokerLiveTable;
private final HashMap<String/* brokerAddr */, List<String>/* Filter Server */> filterServerTable;
```

![](/assets/路由实体图.C0P2NSVn.png)

\*\*topicQueueTable：\*\*Topic消息队列路由信息，消息发送时根据路由表进行负载均衡

\*\*brokerAddrTable：\*\*Broker基础信息，包括brokerName、所属集群名称、主备Broker地址

\*\*clusterAddrTable：\*\*Broker集群信息，存储集群中所有Broker名称

\*\*brokerLiveTable：\*\*Broker状态信息，NameServer每次收到心跳包是会替换该信息

\*\*filterServerTable：\*\*Broker上的FilterServer列表，用于类模式消息过滤。

> RocketMQ基于定于发布机制，一个Topic拥有多个消息队列，一个Broker为每一个主题创建4个读队列和4个写队列。多个Broker组成一个集群，集群由相同的多台Broker组成Master-Slave架构，brokerId为0代表Master，大于0为Slave。BrokerLiveInfo中的lastUpdateTimestamp存储上次收到Broker心跳包的时间。

![](/assets/实体数据实例.BfHDlo1J.png)

![](/assets/实体数据实例2.CBNpsplc.png)

#### 2.2.3.2 路由注册

\#####1）发送心跳包

![](/assets/路由注册.DsniDjg7.png)

RocketMQ路由注册是通过Broker与NameServer的心跳功能实现的。Broker启动时向集群中所有的NameServer发送心跳信息，每隔30s向集群中所有NameServer发送心跳包，NameServer收到心跳包时会更新brokerLiveTable缓存中BrokerLiveInfo的lastUpdataTimeStamp信息，然后NameServer每隔10s扫描brokerLiveTable，如果连续120S没有收到心跳包，NameServer将移除Broker的路由信息同时关闭Socket连接。

***代码：BrokerController#start***

```java
//注册Broker信息
this.registerBrokerAll(true, false, true);
//每隔30s上报Broker信息到NameServer
this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() {

    @Override
    public void run() {
        try {
            BrokerController.this.registerBrokerAll(true, false, brokerConfig.isForceRegister());
        } catch (Throwable e) {
            log.error("registerBrokerAll Exception", e);
        }
    }
}, 1000 * 10, Math.max(10000, Math.min(brokerConfig.getRegisterNameServerPeriod(), 60000)), 
                                                  TimeUnit.MILLISECONDS);

```

***代码：BrokerOuterAPI#registerBrokerAll***

```java
//获得nameServer地址信息
List<String> nameServerAddressList = this.remotingClient.getNameServerAddressList();
//遍历所有nameserver列表
if (nameServerAddressList != null && nameServerAddressList.size() > 0) {

    //封装请求头
    final RegisterBrokerRequestHeader requestHeader = new RegisterBrokerRequestHeader();
    requestHeader.setBrokerAddr(brokerAddr);
    requestHeader.setBrokerId(brokerId);
    requestHeader.setBrokerName(brokerName);
    requestHeader.setClusterName(clusterName);
    requestHeader.setHaServerAddr(haServerAddr);
    requestHeader.setCompressed(compressed);
	//封装请求体
    RegisterBrokerBody requestBody = new RegisterBrokerBody();
    requestBody.setTopicConfigSerializeWrapper(topicConfigWrapper);
    requestBody.setFilterServerList(filterServerList);
    final byte[] body = requestBody.encode(compressed);
    final int bodyCrc32 = UtilAll.crc32(body);
    requestHeader.setBodyCrc32(bodyCrc32);
    final CountDownLatch countDownLatch = new CountDownLatch(nameServerAddressList.size());
    for (final String namesrvAddr : nameServerAddressList) {
        brokerOuterExecutor.execute(new Runnable() {
            @Override
            public void run() {
                try {
                    //分别向NameServer注册
                    RegisterBrokerResult result = registerBroker(namesrvAddr,oneway, timeoutMills,requestHeader,body);
                    if (result != null) {
                        registerBrokerResultList.add(result);
                    }

                    log.info("register broker[{}]to name server {} OK", brokerId, namesrvAddr);
                } catch (Exception e) {
                    log.warn("registerBroker Exception, {}", namesrvAddr, e);
                } finally {
                    countDownLatch.countDown();
                }
            }
        });
    }

    try {
        countDownLatch.await(timeoutMills, TimeUnit.MILLISECONDS);
    } catch (InterruptedException e) {
    }
}
```

***代码：BrokerOutAPI#registerBroker***

```java
if (oneway) {
    try {
        this.remotingClient.invokeOneway(namesrvAddr, request, timeoutMills);
    } catch (RemotingTooMuchRequestException e) {
        // Ignore
    }
    return null;
}
RemotingCommand response = this.remotingClient.invokeSync(namesrvAddr, request, timeoutMills);
```

##### 2）处理心跳包

![](/assets/NameServer处理路由注册.1MYDtZDV.png)

`org.apache.rocketmq.namesrv.processor.DefaultRequestProcessor`网路处理类解析请求类型，如果请求类型是&#x4E3A;***REGISTER\_BROKER***，则将请求转发到`RouteInfoManager#regiesterBroker`

***代码：DefaultRequestProcessor#processRequest***

```java
//判断是注册Broker信息
case RequestCode.REGISTER_BROKER:
	Version brokerVersion = MQVersion.value2Version(request.getVersion());
	if (brokerVersion.ordinal() >= MQVersion.Version.V3_0_11.ordinal()) {
	    return this.registerBrokerWithFilterServer(ctx, request);
	} else {
        //注册Broker信息
	    return this.registerBroker(ctx, request);
	}
```

***代码：DefaultRequestProcessor#registerBroker***

```java
RegisterBrokerResult result = this.namesrvController.getRouteInfoManager().registerBroker(
    requestHeader.getClusterName(),
    requestHeader.getBrokerAddr(),
    requestHeader.getBrokerName(),
    requestHeader.getBrokerId(),
    requestHeader.getHaServerAddr(),
    topicConfigWrapper,
    null,
    ctx.channel()
);
```

***代码：RouteInfoManager#registerBroker***

维护路由信息

```java
//加锁
this.lock.writeLock().lockInterruptibly();
//维护clusterAddrTable
Set<String> brokerNames = this.clusterAddrTable.get(clusterName);
if (null == brokerNames) {
    brokerNames = new HashSet<String>();
    this.clusterAddrTable.put(clusterName, brokerNames);
}
brokerNames.add(brokerName);
```

```java
//维护brokerAddrTable
BrokerData brokerData = this.brokerAddrTable.get(brokerName);
//第一次注册,则创建brokerData
if (null == brokerData) {
    registerFirst = true;
    brokerData = new BrokerData(clusterName, brokerName, new HashMap<Long, String>());
    this.brokerAddrTable.put(brokerName, brokerData);
}
//非第一次注册,更新Broker
Map<Long, String> brokerAddrsMap = brokerData.getBrokerAddrs();
Iterator<Entry<Long, String>> it = brokerAddrsMap.entrySet().iterator();
while (it.hasNext()) {
    Entry<Long, String> item = it.next();
    if (null != brokerAddr && brokerAddr.equals(item.getValue()) && brokerId != item.getKey()) {
        it.remove();
    }
}
String oldAddr = brokerData.getBrokerAddrs().put(brokerId, brokerAddr);
registerFirst = registerFirst || (null == oldAddr);
```

```java
//维护topicQueueTable
if (null != topicConfigWrapper && MixAll.MASTER_ID == brokerId) {
    if (this.isBrokerTopicConfigChanged(brokerAddr, topicConfigWrapper.getDataVersion()) || 
        registerFirst) {
        ConcurrentMap<String, TopicConfig> tcTable = topicConfigWrapper.getTopicConfigTable();
        if (tcTable != null) {
            for (Map.Entry<String, TopicConfig> entry : tcTable.entrySet()) {
                this.createAndUpdateQueueData(brokerName, entry.getValue());
            }
        }
    }
}
```

***代码：RouteInfoManager#createAndUpdateQueueData***

```java
private void createAndUpdateQueueData(final String brokerName, final TopicConfig topicConfig) {
    //创建QueueData
	QueueData queueData = new QueueData();
	queueData.setBrokerName(brokerName);
	queueData.setWriteQueueNums(topicConfig.getWriteQueueNums());
	queueData.setReadQueueNums(topicConfig.getReadQueueNums());
	queueData.setPerm(topicConfig.getPerm());
	queueData.setTopicSynFlag(topicConfig.getTopicSysFlag());
	//获得topicQueueTable中队列集合
	List<QueueData> queueDataList = this.topicQueueTable.get(topicConfig.getTopicName());
    //topicQueueTable为空,则直接添加queueData到队列集合
	if (null == queueDataList) {
	    queueDataList = new LinkedList<QueueData>();
	    queueDataList.add(queueData);
	    this.topicQueueTable.put(topicConfig.getTopicName(), queueDataList);
	    log.info("new topic registered, {} {}", topicConfig.getTopicName(), queueData);
	} else {
        //判断是否是新的队列
	    boolean addNewOne = true;
	    Iterator<QueueData> it = queueDataList.iterator();
	    while (it.hasNext()) {
	        QueueData qd = it.next();
            //如果brokerName相同,代表不是新的队列
	        if (qd.getBrokerName().equals(brokerName)) {
	            if (qd.equals(queueData)) {
	                addNewOne = false;
	        } else {
	                    log.info("topic changed, {} OLD: {} NEW: {}", topicConfig.getTopicName(), qd,
	                        queueData);
	                    it.remove();
	                }
	            }
	        }
		//如果是新的队列,则添加队列到queueDataList
        if (addNewOne) {
            queueDataList.add(queueData);
        }
    }
}
```

```java
//维护brokerLiveTable
BrokerLiveInfo prevBrokerLiveInfo = this.brokerLiveTable.put(brokerAddr,new BrokerLiveInfo(
    System.currentTimeMillis(),
    topicConfigWrapper.getDataVersion(),
    channel,
    haServerAddr));
```

```java
//维护filterServerList
if (filterServerList != null) {
    if (filterServerList.isEmpty()) {
        this.filterServerTable.remove(brokerAddr);
    } else {
        this.filterServerTable.put(brokerAddr, filterServerList);
    }
}

if (MixAll.MASTER_ID != brokerId) {
    String masterAddr = brokerData.getBrokerAddrs().get(MixAll.MASTER_ID);
    if (masterAddr != null) {
        BrokerLiveInfo brokerLiveInfo = this.brokerLiveTable.get(masterAddr);
        if (brokerLiveInfo != null) {
            result.setHaServerAddr(brokerLiveInfo.getHaServerAddr());
            result.setMasterAddr(masterAddr);
        }
    }
}
```

#### 2.2.3.3 路由删除

`Broker`每隔30s向`NameServer`发送一个心跳包，心跳包包含`BrokerId`，`Broker`地址，`Broker`名称，`Broker`所属集群名称、`Broker`关联的`FilterServer`列表。但是如果`Broker`宕机，`NameServer`无法收到心跳包，此时`NameServer`如何来剔除这些失效的`Broker`呢？`NameServer`会每隔10s扫描`brokerLiveTable`状态表，如果`BrokerLive`的**lastUpdateTimestamp**的时间戳距当前时间超过120s，则认为`Broker`失效，移除该`Broker`，关闭与`Broker`连接，同时更新`topicQueueTable`、`brokerAddrTable`、`brokerLiveTable`、`filterServerTable`。

**RocketMQ有两个触发点来删除路由信息**：

* NameServer定期扫描brokerLiveTable检测上次心跳包与当前系统的时间差，如果时间超过120s，则需要移除broker。
* Broker在正常关闭的情况下，会执行unregisterBroker指令

这两种方式路由删除的方法都是一样的，就是从相关路由表中删除与该broker相关的信息。

![](/assets/路由删除.DrMGrMuC.png)

***代码：NamesrvController#initialize***

```java
//每隔10s扫描一次为活跃Broker
this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() {

    @Override
    public void run() {
        NamesrvController.this.routeInfoManager.scanNotActiveBroker();
    }
}, 5, 10, TimeUnit.SECONDS);
```

***代码：RouteInfoManager#scanNotActiveBroker***

```java
public void scanNotActiveBroker() {
    //获得brokerLiveTable
    Iterator<Entry<String, BrokerLiveInfo>> it = this.brokerLiveTable.entrySet().iterator();
    //遍历brokerLiveTable
    while (it.hasNext()) {
        Entry<String, BrokerLiveInfo> next = it.next();
        long last = next.getValue().getLastUpdateTimestamp();
        //如果收到心跳包的时间距当时时间是否超过120s
        if ((last + BROKER_CHANNEL_EXPIRED_TIME) < System.currentTimeMillis()) {
            //关闭连接
            RemotingUtil.closeChannel(next.getValue().getChannel());
            //移除broker
            it.remove();
            //维护路由表
            this.onChannelDestroy(next.getKey(), next.getValue().getChannel());
        }
    }
}
```

***代码：RouteInfoManager#onChannelDestroy***

```java
//申请写锁,根据brokerAddress从brokerLiveTable和filterServerTable移除
this.lock.writeLock().lockInterruptibly();
this.brokerLiveTable.remove(brokerAddrFound);
this.filterServerTable.remove(brokerAddrFound);
```

```java
//维护brokerAddrTable
String brokerNameFound = null;
boolean removeBrokerName = false;
Iterator<Entry<String, BrokerData>> itBrokerAddrTable =this.brokerAddrTable.entrySet().iterator();
//遍历brokerAddrTable
while (itBrokerAddrTable.hasNext() && (null == brokerNameFound)) {
    BrokerData brokerData = itBrokerAddrTable.next().getValue();
    //遍历broker地址
    Iterator<Entry<Long, String>> it = brokerData.getBrokerAddrs().entrySet().iterator();
    while (it.hasNext()) {
        Entry<Long, String> entry = it.next();
        Long brokerId = entry.getKey();
        String brokerAddr = entry.getValue();
        //根据broker地址移除brokerAddr
        if (brokerAddr.equals(brokerAddrFound)) {
            brokerNameFound = brokerData.getBrokerName();
            it.remove();
            log.info("remove brokerAddr[{}, {}] from brokerAddrTable, because channel destroyed",
                brokerId, brokerAddr);
            break;
        }
    }
	//如果当前主题只包含待移除的broker,则移除该topic
    if (brokerData.getBrokerAddrs().isEmpty()) {
        removeBrokerName = true;
        itBrokerAddrTable.remove();
        log.info("remove brokerName[{}] from brokerAddrTable, because channel destroyed",
            brokerData.getBrokerName());
    }
}
```

```java
//维护clusterAddrTable
if (brokerNameFound != null && removeBrokerName) {
    Iterator<Entry<String, Set<String>>> it = this.clusterAddrTable.entrySet().iterator();
    //遍历clusterAddrTable
    while (it.hasNext()) {
        Entry<String, Set<String>> entry = it.next();
        //获得集群名称
        String clusterName = entry.getKey();
        //获得集群中brokerName集合
        Set<String> brokerNames = entry.getValue();
        //从brokerNames中移除brokerNameFound
        boolean removed = brokerNames.remove(brokerNameFound);
        if (removed) {
            log.info("remove brokerName[{}], clusterName[{}] from clusterAddrTable, because channel destroyed",
                brokerNameFound, clusterName);

            if (brokerNames.isEmpty()) {
                log.info("remove the clusterName[{}] from clusterAddrTable, because channel destroyed and no broker in this cluster",
                    clusterName);
                //如果集群中不包含任何broker,则移除该集群
                it.remove();
            }

            break;
        }
    }
}
```

```java
//维护topicQueueTable队列
if (removeBrokerName) {
    //遍历topicQueueTable
    Iterator<Entry<String, List<QueueData>>> itTopicQueueTable =
        this.topicQueueTable.entrySet().iterator();
    while (itTopicQueueTable.hasNext()) {
        Entry<String, List<QueueData>> entry = itTopicQueueTable.next();
        //主题名称
        String topic = entry.getKey();
        //队列集合
        List<QueueData> queueDataList = entry.getValue();
		//遍历该主题队列
        Iterator<QueueData> itQueueData = queueDataList.iterator();
        while (itQueueData.hasNext()) {
            //从队列中移除为活跃broker信息
            QueueData queueData = itQueueData.next();
            if (queueData.getBrokerName().equals(brokerNameFound)) {
                itQueueData.remove();
                log.info("remove topic[{} {}], from topicQueueTable, because channel destroyed",
                    topic, queueData);
            }
        }
		//如果该topic的队列为空,则移除该topic
        if (queueDataList.isEmpty()) {
            itTopicQueueTable.remove();
            log.info("remove topic[{}] all queue, from topicQueueTable, because channel destroyed",
                topic);
        }
    }
}
```

```java
//释放写锁
finally {
    this.lock.writeLock().unlock();
}
```

#### 2.2.3.4 路由发现

RocketMQ路由发现是非实时的，当Topic路由出现变化后，NameServer不会主动推送给客户端，而是由客户端定时拉取主题最新的路由。

***代码：DefaultRequestProcessor#getRouteInfoByTopic***

```java
public RemotingCommand getRouteInfoByTopic(ChannelHandlerContext ctx,
    RemotingCommand request) throws RemotingCommandException {
    final RemotingCommand response = RemotingCommand.createResponseCommand(null);
    final GetRouteInfoRequestHeader requestHeader =
        (GetRouteInfoRequestHeader) request.decodeCommandCustomHeader(GetRouteInfoRequestHeader.class);
	//调用RouteInfoManager的方法,从路由表topicQueueTable、brokerAddrTable、filterServerTable中分别填充TopicRouteData的List<QueueData>、List<BrokerData>、filterServer
    TopicRouteData topicRouteData = this.namesrvController.getRouteInfoManager().pickupTopicRouteData(requestHeader.getTopic());
	//如果找到主题对应你的路由信息并且该主题为顺序消息，则从NameServer KVConfig中获取关于顺序消息相关的配置填充路由信息
    if (topicRouteData != null) {
        if (this.namesrvController.getNamesrvConfig().isOrderMessageEnable()) {
            String orderTopicConf =
                this.namesrvController.getKvConfigManager().getKVConfig(NamesrvUtil.NAMESPACE_ORDER_TOPIC_CONFIG,
                    requestHeader.getTopic());
            topicRouteData.setOrderTopicConf(orderTopicConf);
        }

        byte[] content = topicRouteData.encode();
        response.setBody(content);
        response.setCode(ResponseCode.SUCCESS);
        response.setRemark(null);
        return response;
    }

    response.setCode(ResponseCode.TOPIC_NOT_EXIST);
    response.setRemark("No topic route info in name server for the topic: " + requestHeader.getTopic()
        + FAQUrl.suggestTodo(FAQUrl.APPLY_TOPIC_URL));
    return response;
}
```

### 2.2.4 小结

![](/assets/NameServer小结.CvQwN8RQ.png)

## 2.3 Producer

消息生产者的代码都在client模块中，相对于RocketMQ来讲，消息生产者就是客户端，也是消息的提供者。

![](/assets/DefaultMQProducer类图.BQC9G-JD.png)

\###2.3.1 方法和属性

\####1）主要方法介绍

![](/assets/MQAdmin.D1G4av3E.png)

* ```java
  //创建主题
  void createTopic(final String key, final String newTopic, final int queueNum) throws MQClientException;
  ```

* ```java
  //根据时间戳从队列中查找消息偏移量
  long searchOffset(final MessageQueue mq, final long timestamp)
  ```

* ```java
  //查找消息队列中最大的偏移量
  long maxOffset(final MessageQueue mq) throws MQClientException;
  ```

* ```java
  //查找消息队列中最小的偏移量
  long minOffset(final MessageQueue mq) 
  ```

* ```java
  //根据偏移量查找消息
  MessageExt viewMessage(final String offsetMsgId) throws RemotingException, MQBrokerException,
          InterruptedException, MQClientException;
  ```

* ```java
  //根据条件查找消息
  QueryResult queryMessage(final String topic, final String key, final int maxNum, final long begin,
          final long end) throws MQClientException, InterruptedException;
  ```

* ```java
  //根据消息ID和主题查找消息
  MessageExt viewMessage(String topic,String msgId) throws RemotingException, MQBrokerException, InterruptedException, MQClientException;
  ```

![](/assets/MQProducer.CEe92xUC.png)

* ```java
  //启动
  void start() throws MQClientException;
  ```

* ```java
  //关闭
  void shutdown();
  ```

* ```java
  //查找该主题下所有消息
  List<MessageQueue> fetchPublishMessageQueues(final String topic) throws MQClientException;
  ```

* ```java
  //同步发送消息
  SendResult send(final Message msg) throws MQClientException, RemotingException, MQBrokerException,
          InterruptedException;
  ```

* ```java
  //同步超时发送消息
  SendResult send(final Message msg, final long timeout) throws MQClientException,
          RemotingException, MQBrokerException, InterruptedException;
  ```

* ```java
  //异步发送消息
  void send(final Message msg, final SendCallback sendCallback) throws MQClientException,
          RemotingException, InterruptedException;
  ```

* ```java
  //异步超时发送消息
  void send(final Message msg, final SendCallback sendCallback, final long timeout)
      throws MQClientException, RemotingException, InterruptedException;
  ```

* ```java
  //发送单向消息
  void sendOneway(final Message msg) throws MQClientException, RemotingException,
      InterruptedException;
  ```

* ```java
  //选择指定队列同步发送消息
  SendResult send(final Message msg, final MessageQueue mq) throws MQClientException,
      RemotingException, MQBrokerException, InterruptedException;
  ```

* ```java
  //选择指定队列异步发送消息
  void send(final Message msg, final MessageQueue mq, final SendCallback sendCallback)
      throws MQClientException, RemotingException, InterruptedException;
  ```

* ```java
  //选择指定队列单项发送消息
  void sendOneway(final Message msg, final MessageQueue mq) throws MQClientException,
      RemotingException, InterruptedException;
  ```

* ```java
  //批量发送消息
  SendResult send(final Collection<Message> msgs) throws MQClientException, RemotingException, MQBrokerException,InterruptedException;
  ```

\####2）属性介绍

![](/assets/DefaultMQProducer属性.xaDPM2VH.png)

```java
producerGroup：生产者所属组
createTopicKey：默认Topic
defaultTopicQueueNums：默认主题在每一个Broker队列数量
sendMsgTimeout：发送消息默认超时时间，默认3s
compressMsgBodyOverHowmuch：消息体超过该值则启用压缩，默认4k
retryTimesWhenSendFailed：同步方式发送消息重试次数，默认为2，总共执行3次
retryTimesWhenSendAsyncFailed：异步方法发送消息重试次数，默认为2
retryAnotherBrokerWhenNotStoreOK：消息重试时选择另外一个Broker时，是否不等待存储结果就返回，默认为false
maxMessageSize：允许发送的最大消息长度，默认为4M
```

### 2.3.2 启动流程

![](/assets/生产者启动流程.C8hTuk2D.png)

***代码：DefaultMQProducerImpl#start***

```java
//检查生产者组是否满足要求
this.checkConfig();
//更改当前instanceName为进程ID
if (!this.defaultMQProducer.getProducerGroup().equals(MixAll.CLIENT_INNER_PRODUCER_GROUP)) {
    this.defaultMQProducer.changeInstanceNameToPID();
}
//获得MQ客户端实例
this.mQClientFactory = MQClientManager.getInstance().getAndCreateMQClientInstance(this.defaultMQProducer, rpcHook);
```

> 整个JVM中只存在一个MQClientManager实例，维护一个MQClientInstance缓存表
>
> ConcurrentMap\<String/\* clientId \*/, MQClientInstance> factoryTable = new ConcurrentHashMap\<String,MQClientInstance>();
>
> 同一个clientId只会创建一个MQClientInstance。
>
> MQClientInstance封装了RocketMQ网络处理API，是消息生产者和消息消费者与NameServer、Broker打交道的网络通道

***代码：MQClientManager#getAndCreateMQClientInstance***

```java
public MQClientInstance getAndCreateMQClientInstance(final ClientConfig clientConfig, 
                                                     RPCHook rpcHook) {
    //构建客户端ID
    String clientId = clientConfig.buildMQClientId();
    //根据客户端ID或者客户端实例
    MQClientInstance instance = this.factoryTable.get(clientId);
    //实例如果为空就创建新的实例,并添加到实例表中
    if (null == instance) {
        instance =
            new MQClientInstance(clientConfig.cloneClientConfig(),
                this.factoryIndexGenerator.getAndIncrement(), clientId, rpcHook);
        MQClientInstance prev = this.factoryTable.putIfAbsent(clientId, instance);
        if (prev != null) {
            instance = prev;
            log.warn("Returned Previous MQClientInstance for clientId:[{}]", clientId);
        } else {
            log.info("Created new MQClientInstance for clientId:[{}]", clientId);
        }
    }

    return instance;
}
```

***代码：DefaultMQProducerImpl#start***

```java
//注册当前生产者到到MQClientInstance管理中,方便后续调用网路请求
boolean registerOK = mQClientFactory.registerProducer(this.defaultMQProducer.getProducerGroup(), this);
if (!registerOK) {
    this.serviceState = ServiceState.CREATE_JUST;
    throw new MQClientException("The producer group[" + this.defaultMQProducer.getProducerGroup()
        + "] has been created before, specify another name please." + FAQUrl.suggestTodo(FAQUrl.GROUP_NAME_DUPLICATE_URL),
        null);
}
//启动生产者
if (startFactory) {
    mQClientFactory.start();
}
```

### 2.3.3 消息发送

![](/assets/消息发送.FuaEwfiF.png)

***代码：DefaultMQProducerImpl#send(Message msg)***

```java
//发送消息
public SendResult send(Message msg) {
    return send(msg, this.defaultMQProducer.getSendMsgTimeout());
}
```

***代码：DefaultMQProducerImpl#send(Message msg,long timeout)***

```java
//发送消息,默认超时时间为3s
public SendResult send(Message msg,long timeout){
    return this.sendDefaultImpl(msg, CommunicationMode.SYNC, null, timeout);
}
```

***代码：DefaultMQProducerImpl#sendDefaultImpl***

```java
//校验消息
Validators.checkMessage(msg, this.defaultMQProducer);
```

\####1）验证消息

***代码：Validators#checkMessage***

```java
public static void checkMessage(Message msg, DefaultMQProducer defaultMQProducer)
    throws MQClientException {
    //判断是否为空
    if (null == msg) {
        throw new MQClientException(ResponseCode.MESSAGE_ILLEGAL, "the message is null");
    }
    // 校验主题
    Validators.checkTopic(msg.getTopic());
		
    // 校验消息体
    if (null == msg.getBody()) {
        throw new MQClientException(ResponseCode.MESSAGE_ILLEGAL, "the message body is null");
    }

    if (0 == msg.getBody().length) {
        throw new MQClientException(ResponseCode.MESSAGE_ILLEGAL, "the message body length is zero");
    }

    if (msg.getBody().length > defaultMQProducer.getMaxMessageSize()) {
        throw new MQClientException(ResponseCode.MESSAGE_ILLEGAL,
            "the message body size over max value, MAX: " + defaultMQProducer.getMaxMessageSize());
    }
}
```

\####2）查找路由

***代码：DefaultMQProducerImpl#tryToFindTopicPublishInfo***

```java
private TopicPublishInfo tryToFindTopicPublishInfo(final String topic) {
    //从缓存中获得主题的路由信息
    TopicPublishInfo topicPublishInfo = this.topicPublishInfoTable.get(topic);
    //路由信息为空,则从NameServer获取路由
    if (null == topicPublishInfo || !topicPublishInfo.ok()) {
        this.topicPublishInfoTable.putIfAbsent(topic, new TopicPublishInfo());
        this.mQClientFactory.updateTopicRouteInfoFromNameServer(topic);
        topicPublishInfo = this.topicPublishInfoTable.get(topic);
    }

    if (topicPublishInfo.isHaveTopicRouterInfo() || topicPublishInfo.ok()) {
        return topicPublishInfo;
    } else {
        //如果未找到当前主题的路由信息,则用默认主题继续查找
        this.mQClientFactory.updateTopicRouteInfoFromNameServer(topic, true, this.defaultMQProducer);
        topicPublishInfo = this.topicPublishInfoTable.get(topic);
        return topicPublishInfo;
    }
}
```

![](/assets/Topic路由信息.Btv3-9VA.png)

***代码：TopicPublishInfo***

```java
public class TopicPublishInfo {
    private boolean orderTopic = false;	//是否是顺序消息
    private boolean haveTopicRouterInfo = false; 
    private List<MessageQueue> messageQueueList = new ArrayList<MessageQueue>();	//该主题消息队列
    private volatile ThreadLocalIndex sendWhichQueue = new ThreadLocalIndex();//每选择一次消息队列,该值+1
    private TopicRouteData topicRouteData;//关联Topic路由元信息
}
```

***代码：MQClientInstance#updateTopicRouteInfoFromNameServer***

```java
TopicRouteData topicRouteData;
//使用默认主题从NameServer获取路由信息
if (isDefault && defaultMQProducer != null) {
    topicRouteData = this.mQClientAPIImpl.getDefaultTopicRouteInfoFromNameServer(defaultMQProducer.getCreateTopicKey(),
        1000 * 3);
    if (topicRouteData != null) {
        for (QueueData data : topicRouteData.getQueueDatas()) {
            int queueNums = Math.min(defaultMQProducer.getDefaultTopicQueueNums(), data.getReadQueueNums());
            data.setReadQueueNums(queueNums);
            data.setWriteQueueNums(queueNums);
        }
    }
} else {
    //使用指定主题从NameServer获取路由信息
    topicRouteData = this.mQClientAPIImpl.getTopicRouteInfoFromNameServer(topic, 1000 * 3);
}
```

***代码：MQClientInstance#updateTopicRouteInfoFromNameServer***

```java
//判断路由是否需要更改
TopicRouteData old = this.topicRouteTable.get(topic);
boolean changed = topicRouteDataIsChange(old, topicRouteData);
if (!changed) {
    changed = this.isNeedUpdateTopicRouteInfo(topic);
} else {
    log.info("the topic[{}] route info changed, old[{}] ,new[{}]", topic, old, topicRouteData);
}
```

***代码：MQClientInstance#updateTopicRouteInfoFromNameServer***

```java
if (changed) {
    //将topicRouteData转换为发布队列
    TopicPublishInfo publishInfo = topicRouteData2TopicPublishInfo(topic, topicRouteData);
    publishInfo.setHaveTopicRouterInfo(true);
    //遍历生产
    Iterator<Entry<String, MQProducerInner>> it = this.producerTable.entrySet().iterator();
    while (it.hasNext()) {
        Entry<String, MQProducerInner> entry = it.next();
        MQProducerInner impl = entry.getValue();
        if (impl != null) {
            //生产者不为空时,更新publishInfo信息
            impl.updateTopicPublishInfo(topic, publishInfo);
        }
    }
}
```

***代码：MQClientInstance#topicRouteData2TopicPublishInfo***

```java
public static TopicPublishInfo topicRouteData2TopicPublishInfo(final String topic, final TopicRouteData route) {
    	//创建TopicPublishInfo对象
        TopicPublishInfo info = new TopicPublishInfo();
    	//关联topicRoute
        info.setTopicRouteData(route);
    	//顺序消息,更新TopicPublishInfo
        if (route.getOrderTopicConf() != null && route.getOrderTopicConf().length() > 0) {
            String[] brokers = route.getOrderTopicConf().split(";");
            for (String broker : brokers) {
                String[] item = broker.split(":");
                int nums = Integer.parseInt(item[1]);
                for (int i = 0; i < nums; i++) {
                    MessageQueue mq = new MessageQueue(topic, item[0], i);
                    info.getMessageQueueList().add(mq);
                }
            }

            info.setOrderTopic(true);
        } else {
            //非顺序消息更新TopicPublishInfo
            List<QueueData> qds = route.getQueueDatas();
            Collections.sort(qds);
            //遍历topic队列信息
            for (QueueData qd : qds) {
                //是否是写队列
                if (PermName.isWriteable(qd.getPerm())) {
                    BrokerData brokerData = null;
                    //遍历写队列Broker
                    for (BrokerData bd : route.getBrokerDatas()) {
                        //根据名称获得读队列对应的Broker
                        if (bd.getBrokerName().equals(qd.getBrokerName())) {
                        brokerData = bd;
                        break;
                    }
                }

                if (null == brokerData) {
                    continue;
                }

                if (!brokerData.getBrokerAddrs().containsKey(MixAll.MASTER_ID)) {
                    continue;
                }
				//封装TopicPublishInfo写队列
                for (int i = 0; i < qd.getWriteQueueNums(); i++) {
                    MessageQueue mq = new MessageQueue(topic, qd.getBrokerName(), i);
                    info.getMessageQueueList().add(mq);
                }
            }
        }

        info.setOrderTopic(false);
    }
	//返回TopicPublishInfo对象
    return info;
}
```

#### 3）选择队列

* 默认不启用Broker故障延迟机制

***代码：TopicPublishInfo#selectOneMessageQueue(lastBrokerName)***

```java
public MessageQueue selectOneMessageQueue(final String lastBrokerName) {
    //第一次选择队列
    if (lastBrokerName == null) {
        return selectOneMessageQueue();
    } else {
        //sendWhichQueue
        int index = this.sendWhichQueue.getAndIncrement();
        //遍历消息队列集合
        for (int i = 0; i < this.messageQueueList.size(); i++) {
            //sendWhichQueue自增后取模
            int pos = Math.abs(index++) % this.messageQueueList.size();
            if (pos < 0)
                pos = 0;
            //规避上次Broker队列
            MessageQueue mq = this.messageQueueList.get(pos);
            if (!mq.getBrokerName().equals(lastBrokerName)) {
                return mq;
            }
        }
        //如果以上情况都不满足,返回sendWhichQueue取模后的队列
        return selectOneMessageQueue();
    }
}
```

***代码：TopicPublishInfo#selectOneMessageQueue()***

```java
//第一次选择队列
public MessageQueue selectOneMessageQueue() {
    //sendWhichQueue自增
    int index = this.sendWhichQueue.getAndIncrement();
    //对队列大小取模
    int pos = Math.abs(index) % this.messageQueueList.size();
    if (pos < 0)
        pos = 0;
    //返回对应的队列
    return this.messageQueueList.get(pos);
}
```

* 启用Broker故障延迟机制

```java
public MessageQueue selectOneMessageQueue(final TopicPublishInfo tpInfo, final String lastBrokerName) {
    //Broker故障延迟机制
    if (this.sendLatencyFaultEnable) {
        try {
            //对sendWhichQueue自增
            int index = tpInfo.getSendWhichQueue().getAndIncrement();
            //对消息队列轮询获取一个队列
            for (int i = 0; i < tpInfo.getMessageQueueList().size(); i++) {
                int pos = Math.abs(index++) % tpInfo.getMessageQueueList().size();
                if (pos < 0)
                    pos = 0;
                MessageQueue mq = tpInfo.getMessageQueueList().get(pos);
                //验证该队列是否可用
                if (latencyFaultTolerance.isAvailable(mq.getBrokerName())) {
                    //可用
                    if (null == lastBrokerName || mq.getBrokerName().equals(lastBrokerName))
                        return mq;
                }
            }
			//从规避的Broker中选择一个可用的Broker
            final String notBestBroker = latencyFaultTolerance.pickOneAtLeast();
            //获得Broker的写队列集合
            int writeQueueNums = tpInfo.getQueueIdByBroker(notBestBroker);
            if (writeQueueNums > 0) {
                //获得一个队列,指定broker和队列ID并返回
                final MessageQueue mq = tpInfo.selectOneMessageQueue();
                if (notBestBroker != null) {
                    mq.setBrokerName(notBestBroker);
                    mq.setQueueId(tpInfo.getSendWhichQueue().getAndIncrement() % writeQueueNums);
                }
                return mq;
            } else {
                latencyFaultTolerance.remove(notBestBroker);
            }
        } catch (Exception e) {
            log.error("Error occurred when selecting message queue", e);
        }

        return tpInfo.selectOneMessageQueue();
    }

    return tpInfo.selectOneMessageQueue(lastBrokerName);
}
```

![](/assets/Broker故障延迟机制核心类.x3BvriwO.png)

* 延迟机制接口规范

```java
public interface LatencyFaultTolerance<T> {
    //更新失败条目
    void updateFaultItem(final T name, final long currentLatency, final long notAvailableDuration);
	//判断Broker是否可用
    boolean isAvailable(final T name);
	//移除Fault条目
    void remove(final T name);
	//尝试从规避的Broker中选择一个可用的Broker
    T pickOneAtLeast();
}
```

* FaultItem：失败条目

```java
class FaultItem implements Comparable<FaultItem> {
    //条目唯一键,这里为brokerName
    private final String name;
    //本次消息发送延迟
    private volatile long currentLatency;
    //故障规避开始时间
    private volatile long startTimestamp;
}
```

* 消息失败策略

```java
public class MQFaultStrategy {
   //根据currentLatency本地消息发送延迟,从latencyMax尾部向前找到第一个比currentLatency小的索引,如果没有找到,返回0
	private long[] latencyMax = {50L, 100L, 550L, 1000L, 2000L, 3000L, 15000L};
    //根据这个索引从notAvailableDuration取出对应的时间,在该时长内,Broker设置为不可用
	private long[] notAvailableDuration = {0L, 0L, 30000L, 60000L, 120000L, 180000L, 600000L};
}
```

***原理分析***

***代码：DefaultMQProducerImpl#sendDefaultImpl***

```java
sendResult = this.sendKernelImpl(msg, 
                                 mq, 
                                 communicationMode, 
                                 sendCallback, 
                                 topicPublishInfo, 
                                 timeout - costTime);
endTimestamp = System.currentTimeMillis();
this.updateFaultItem(mq.getBrokerName(), endTimestamp - beginTimestampPrev, false);
```

如果上述发送过程出现异常，则调用`DefaultMQProducerImpl#updateFaultItem`

```java
public void updateFaultItem(final String brokerName, final long currentLatency, boolean isolation) {
    //参数一：broker名称
    //参数二:本次消息发送延迟时间
    //参数三:是否隔离
    this.mqFaultStrategy.updateFaultItem(brokerName, currentLatency, isolation);
}
```

***代码：MQFaultStrategy#updateFaultItem***

```java
public void updateFaultItem(final String brokerName, final long currentLatency, boolean isolation) {
    if (this.sendLatencyFaultEnable) {
        //计算broker规避的时长
        long duration = computeNotAvailableDuration(isolation ? 30000 : currentLatency);
        //更新该FaultItem规避时长
        this.latencyFaultTolerance.updateFaultItem(brokerName, currentLatency, duration);
    }
}
```

***代码：MQFaultStrategy#computeNotAvailableDuration***

```java
private long computeNotAvailableDuration(final long currentLatency) {
    //遍历latencyMax
    for (int i = latencyMax.length - 1; i >= 0; i--) {
        //找到第一个比currentLatency的latencyMax值
        if (currentLatency >= latencyMax[i])
            return this.notAvailableDuration[i];
    }
    //没有找到则返回0
    return 0;
}
```

***代码：LatencyFaultToleranceImpl#updateFaultItem***

```java
public void updateFaultItem(final String name, final long currentLatency, final long notAvailableDuration) {
    //获得原FaultItem
    FaultItem old = this.faultItemTable.get(name);
    //为空新建faultItem对象,设置规避时长和开始时间
    if (null == old) {
        final FaultItem faultItem = new FaultItem(name);
        faultItem.setCurrentLatency(currentLatency);
        faultItem.setStartTimestamp(System.currentTimeMillis() + notAvailableDuration);

        old = this.faultItemTable.putIfAbsent(name, faultItem);
        if (old != null) {
            old.setCurrentLatency(currentLatency);
            old.setStartTimestamp(System.currentTimeMillis() + notAvailableDuration);
        }
    } else {
        //更新规避时长和开始时间
        old.setCurrentLatency(currentLatency);
        old.setStartTimestamp(System.currentTimeMillis() + notAvailableDuration);
    }
}
```

\####4）发送消息

消息发送API核心入&#x53E3;***DefaultMQProducerImpl#sendKernelImpl***

```java
private SendResult sendKernelImpl(
    final Message msg,	//待发送消息
    final MessageQueue mq,	//消息发送队列
    final CommunicationMode communicationMode,		//消息发送内模式
    final SendCallback sendCallback,	pp	//异步消息回调函数
    final TopicPublishInfo topicPublishInfo,	//主题路由信息
    final long timeout	//超时时间
    )
```

***代码：DefaultMQProducerImpl#sendKernelImpl***

```java
//获得broker网络地址信息
String brokerAddr = this.mQClientFactory.findBrokerAddressInPublish(mq.getBrokerName());
if (null == brokerAddr) {
    //没有找到从NameServer更新broker网络地址信息
    tryToFindTopicPublishInfo(mq.getTopic());
    brokerAddr = this.mQClientFactory.findBrokerAddressInPublish(mq.getBrokerName());
}
```

```java
//为消息分类唯一ID
if (!(msg instanceof MessageBatch)) {
    MessageClientIDSetter.setUniqID(msg);
}

boolean topicWithNamespace = false;
if (null != this.mQClientFactory.getClientConfig().getNamespace()) {
    msg.setInstanceId(this.mQClientFactory.getClientConfig().getNamespace());
    topicWithNamespace = true;
}
//消息大小超过4K,启用消息压缩
int sysFlag = 0;
boolean msgBodyCompressed = false;
if (this.tryToCompressMessage(msg)) {
    sysFlag |= MessageSysFlag.COMPRESSED_FLAG;
    msgBodyCompressed = true;
}
//如果是事务消息,设置消息标记MessageSysFlag.TRANSACTION_PREPARED_TYPE
final String tranMsg = msg.getProperty(MessageConst.PROPERTY_TRANSACTION_PREPARED);
if (tranMsg != null && Boolean.parseBoolean(tranMsg)) {
    sysFlag |= MessageSysFlag.TRANSACTION_PREPARED_TYPE;
}
```

```java
//如果注册了消息发送钩子函数,在执行消息发送前的增强逻辑
if (this.hasSendMessageHook()) {
    context = new SendMessageContext();
    context.setProducer(this);
    context.setProducerGroup(this.defaultMQProducer.getProducerGroup());
    context.setCommunicationMode(communicationMode);
    context.setBornHost(this.defaultMQProducer.getClientIP());
    context.setBrokerAddr(brokerAddr);
    context.setMessage(msg);
    context.setMq(mq);
    context.setNamespace(this.defaultMQProducer.getNamespace());
    String isTrans = msg.getProperty(MessageConst.PROPERTY_TRANSACTION_PREPARED);
    if (isTrans != null && isTrans.equals("true")) {
        context.setMsgType(MessageType.Trans_Msg_Half);
    }

    if (msg.getProperty("__STARTDELIVERTIME") != null || msg.getProperty(MessageConst.PROPERTY_DELAY_TIME_LEVEL) != null) {
        context.setMsgType(MessageType.Delay_Msg);
    }
    this.executeSendMessageHookBefore(context);
}
```

***代码：SendMessageHook***

```java
public interface SendMessageHook {
    String hookName();

    void sendMessageBefore(final SendMessageContext context);

    void sendMessageAfter(final SendMessageContext context);
}
```

***代码：DefaultMQProducerImpl#sendKernelImpl***

```java
//构建消息发送请求包
SendMessageRequestHeader requestHeader = new SendMessageRequestHeader();
//生产者组
requestHeader.setProducerGroup(this.defaultMQProducer.getProducerGroup());
//主题
requestHeader.setTopic(msg.getTopic());
//默认创建主题Key
requestHeader.setDefaultTopic(this.defaultMQProducer.getCreateTopicKey());
//该主题在单个Broker默认队列树
requestHeader.setDefaultTopicQueueNums(this.defaultMQProducer.getDefaultTopicQueueNums());
//队列ID
requestHeader.setQueueId(mq.getQueueId());
//消息系统标记
requestHeader.setSysFlag(sysFlag);
//消息发送时间
requestHeader.setBornTimestamp(System.currentTimeMillis());
//消息标记
requestHeader.setFlag(msg.getFlag());
//消息扩展信息
requestHeader.setProperties(MessageDecoder.messageProperties2String(msg.getProperties()));
//消息重试次数
requestHeader.setReconsumeTimes(0);
requestHeader.setUnitMode(this.isUnitMode());
//是否是批量消息等
requestHeader.setBatch(msg instanceof MessageBatch);
if (requestHeader.getTopic().startsWith(MixAll.RETRY_GROUP_TOPIC_PREFIX)) {
    String reconsumeTimes = MessageAccessor.getReconsumeTime(msg);
    if (reconsumeTimes != null) {
        requestHeader.setReconsumeTimes(Integer.valueOf(reconsumeTimes));
        MessageAccessor.clearProperty(msg, MessageConst.PROPERTY_RECONSUME_TIME);
    }

    String maxReconsumeTimes = MessageAccessor.getMaxReconsumeTimes(msg);
    if (maxReconsumeTimes != null) {
        requestHeader.setMaxReconsumeTimes(Integer.valueOf(maxReconsumeTimes));
        MessageAccessor.clearProperty(msg, MessageConst.PROPERTY_MAX_RECONSUME_TIMES);
    }
}
```

```java
case ASYNC:		//异步发送
    Message tmpMessage = msg;
    boolean messageCloned = false;
    if (msgBodyCompressed) {
        //If msg body was compressed, msgbody should be reset using prevBody.
        //Clone new message using commpressed message body and recover origin massage.
        //Fix bug:https://github.com/apache/rocketmq-externals/issues/66
        tmpMessage = MessageAccessor.cloneMessage(msg);
        messageCloned = true;
        msg.setBody(prevBody);
    }

    if (topicWithNamespace) {
        if (!messageCloned) {
            tmpMessage = MessageAccessor.cloneMessage(msg);
            messageCloned = true;
        }
        msg.setTopic(NamespaceUtil.withoutNamespace(msg.getTopic(), 
                                                    this.defaultMQProducer.getNamespace()));
    }

		long costTimeAsync = System.currentTimeMillis() - beginStartTime;
		if (timeout < costTimeAsync) {
		    throw new RemotingTooMuchRequestException("sendKernelImpl call timeout");
		}
		sendResult = this.mQClientFactory.getMQClientAPIImpl().sendMessage(
        			brokerAddr,
        			mq.getBrokerName(),
        			tmpMessage,
        			requestHeader,
        			timeout - costTimeAsync,
        			communicationMode,
        			sendCallback,
        			topicPublishInfo,
        			this.mQClientFactory,
        			this.defaultMQProducer.getRetryTimesWhenSendAsyncFailed(),
        			context,
        			this);
    	break;
case ONEWAY:
case SYNC:		//同步发送
    long costTimeSync = System.currentTimeMillis() - beginStartTime;
        if (timeout < costTimeSync) {
            throw new RemotingTooMuchRequestException("sendKernelImpl call timeout");
        }
        sendResult = this.mQClientFactory.getMQClientAPIImpl().sendMessage(
            brokerAddr,
            mq.getBrokerName(),
            msg,
            requestHeader,
            timeout - costTimeSync,
            communicationMode,
            context,
            this);
        break;
    default:
        assert false;
        break;
}
```

```java
//如果注册了钩子函数,则发送完毕后执行钩子函数
if (this.hasSendMessageHook()) {
    context.setSendResult(sendResult);
    this.executeSendMessageHookAfter(context);
}
```

### 2.3.4 批量消息发送

![](/assets/发送批量消息.BxBJj_3n.png)

批量消息发送是将同一个主题的多条消息一起打包发送到消息服务端，减少网络调用次数，提高网络传输效率。当然，并不是在同一批次中发送的消息数量越多越好，其判断依据是单条消息的长度，如果单条消息内容比较长，则打包多条消息发送会影响其他线程发送消息的响应时间，并且单批次消息总长度不能超过DefaultMQProducer#maxMessageSize。

批量消息发送要解决的问题是如何将这些消息编码以便服务端能够正确解码出每条消息的消息内容。

***代码：DefaultMQProducer#send***

```java
public SendResult send(Collection<Message> msgs) 
    throws MQClientException, RemotingException, MQBrokerException, InterruptedException {
    //压缩消息集合成一条消息,然后发送出去
    return this.defaultMQProducerImpl.send(batch(msgs));
}
```

***代码：DefaultMQProducer#batch***

```java
private MessageBatch batch(Collection<Message> msgs) throws MQClientException {
    MessageBatch msgBatch;
    try {
        //将集合消息封装到MessageBatch
        msgBatch = MessageBatch.generateFromList(msgs);
        //遍历消息集合,检查消息合法性,设置消息ID,设置Topic
        for (Message message : msgBatch) {
            Validators.checkMessage(message, this);
            MessageClientIDSetter.setUniqID(message);
            message.setTopic(withNamespace(message.getTopic()));
        }
        //压缩消息,设置消息body
        msgBatch.setBody(msgBatch.encode());
    } catch (Exception e) {
        throw new MQClientException("Failed to initiate the MessageBatch", e);
    }
    //设置msgBatch的topic
    msgBatch.setTopic(withNamespace(msgBatch.getTopic()));
    return msgBatch;
}
```

## 2.4 消息存储

\###2.4.1 消息存储核心类

![](/assets/DefaultMessageStore.Bumm_kgJ.png)

```java
private final MessageStoreConfig messageStoreConfig;	//消息配置属性
private final CommitLog commitLog;		//CommitLog文件存储的实现类
private final ConcurrentMap<String/* topic */, ConcurrentMap<Integer/* queueId */, ConsumeQueue>> consumeQueueTable;	//消息队列存储缓存表,按照消息主题分组
private final FlushConsumeQueueService flushConsumeQueueService;	//消息队列文件刷盘线程
private final CleanCommitLogService cleanCommitLogService;	//清除CommitLog文件服务
private final CleanConsumeQueueService cleanConsumeQueueService;	//清除ConsumerQueue队列文件服务
private final IndexService indexService;	//索引实现类
private final AllocateMappedFileService allocateMappedFileService;	//MappedFile分配服务
private final ReputMessageService reputMessageService;//CommitLog消息分发,根据CommitLog文件构建ConsumerQueue、IndexFile文件
private final HAService haService;	//存储HA机制
private final ScheduleMessageService scheduleMessageService;	//消息服务调度线程
private final StoreStatsService storeStatsService;	//消息存储服务
private final TransientStorePool transientStorePool;	//消息堆外内存缓存
private final BrokerStatsManager brokerStatsManager;	//Broker状态管理器
private final MessageArrivingListener messageArrivingListener;	//消息拉取长轮询模式消息达到监听器
private final BrokerConfig brokerConfig;	//Broker配置类
private StoreCheckpoint storeCheckpoint;	//文件刷盘监测点
private final LinkedList<CommitLogDispatcher> dispatcherList;	//CommitLog文件转发请求
```

### 2.4.2 消息存储流程

![](/assets/消息存储流程.BUx6tpXC.png)

***消息存储入口：DefaultMessageStore#putMessage***

```java
//判断Broker角色如果是从节点,则无需写入
if (BrokerRole.SLAVE == this.messageStoreConfig.getBrokerRole()) {
        long value = this.printTimes.getAndIncrement();
        if ((value % 50000) == 0) {
            log.warn("message store is slave mode, so putMessage is forbidden ");
        }

    return new PutMessageResult(PutMessageStatus.SERVICE_NOT_AVAILABLE, null);
}

//判断当前写入状态如果是正在写入,则不能继续
if (!this.runningFlags.isWriteable()) {
        long value = this.printTimes.getAndIncrement();
    	return new PutMessageResult(PutMessageStatus.SERVICE_NOT_AVAILABLE, null);
} else {
    this.printTimes.set(0);
}
//判断消息主题长度是否超过最大限制
if (msg.getTopic().length() > Byte.MAX_VALUE) {
    log.warn("putMessage message topic length too long " + msg.getTopic().length());
    return new PutMessageResult(PutMessageStatus.MESSAGE_ILLEGAL, null);
}
//判断消息属性长度是否超过限制
if (msg.getPropertiesString() != null && msg.getPropertiesString().length() > Short.MAX_VALUE) {
    log.warn("putMessage message properties length too long " + msg.getPropertiesString().length());
    return new PutMessageResult(PutMessageStatus.PROPERTIES_SIZE_EXCEEDED, null);
}
//判断系统PageCache缓存去是否占用
if (this.isOSPageCacheBusy()) {
    return new PutMessageResult(PutMessageStatus.OS_PAGECACHE_BUSY, null);
}

//将消息写入CommitLog文件
PutMessageResult result = this.commitLog.putMessage(msg);
```

***代码：CommitLog#putMessage***

```java
//记录消息存储时间
msg.setStoreTimestamp(beginLockTimestamp);

//判断如果mappedFile如果为空或者已满,创建新的mappedFile文件
if (null == mappedFile || mappedFile.isFull()) {
    mappedFile = this.mappedFileQueue.getLastMappedFile(0); 
}
//如果创建失败,直接返回
if (null == mappedFile) {
    log.error("create mapped file1 error, topic: " + msg.getTopic() + " clientAddr: " + msg.getBornHostString());
    beginTimeInLock = 0;
    return new PutMessageResult(PutMessageStatus.CREATE_MAPEDFILE_FAILED, null);
}

//写入消息到mappedFile中
result = mappedFile.appendMessage(msg, this.appendMessageCallback);
```

***代码：MappedFile#appendMessagesInner***

```java
//获得文件的写入指针
int currentPos = this.wrotePosition.get();

//如果指针大于文件大小则直接返回
if (currentPos < this.fileSize) {
    //通过writeBuffer.slice()创建一个与MappedFile共享的内存区,并设置position为当前指针
    ByteBuffer byteBuffer = writeBuffer != null ? writeBuffer.slice() : this.mappedByteBuffer.slice();
    byteBuffer.position(currentPos);
    AppendMessageResult result = null;
    if (messageExt instanceof MessageExtBrokerInner) {
       	//通过回调方法写入
        result = cb.doAppend(this.getFileFromOffset(), byteBuffer, this.fileSize - currentPos, (MessageExtBrokerInner) messageExt);
    } else if (messageExt instanceof MessageExtBatch) {
        result = cb.doAppend(this.getFileFromOffset(), byteBuffer, this.fileSize - currentPos, (MessageExtBatch) messageExt);
    } else {
        return new AppendMessageResult(AppendMessageStatus.UNKNOWN_ERROR);
    }
    this.wrotePosition.addAndGet(result.getWroteBytes());
    this.storeTimestamp = result.getStoreTimestamp();
    return result;
}
```

***代码：CommitLog#doAppend***

```java
//文件写入位置
long wroteOffset = fileFromOffset + byteBuffer.position();
//设置消息ID
this.resetByteBuffer(hostHolder, 8);
String msgId = MessageDecoder.createMessageId(this.msgIdMemory, msgInner.getStoreHostBytes(hostHolder), wroteOffset);

//获得该消息在消息队列中的偏移量
keyBuilder.setLength(0);
keyBuilder.append(msgInner.getTopic());
keyBuilder.append('-');
keyBuilder.append(msgInner.getQueueId());
String key = keyBuilder.toString();
Long queueOffset = CommitLog.this.topicQueueTable.get(key);
if (null == queueOffset) {
    queueOffset = 0L;
    CommitLog.this.topicQueueTable.put(key, queueOffset);
}

//获得消息属性长度
final byte[] propertiesData =msgInner.getPropertiesString() == null ? null : msgInner.getPropertiesString().getBytes(MessageDecoder.CHARSET_UTF8);

final int propertiesLength = propertiesData == null ? 0 : propertiesData.length;

if (propertiesLength > Short.MAX_VALUE) {
    log.warn("putMessage message properties length too long. length={}", propertiesData.length);
    return new AppendMessageResult(AppendMessageStatus.PROPERTIES_SIZE_EXCEEDED);
}

//获得消息主题大小
final byte[] topicData = msgInner.getTopic().getBytes(MessageDecoder.CHARSET_UTF8);
final int topicLength = topicData.length;

//获得消息体大小
final int bodyLength = msgInner.getBody() == null ? 0 : msgInner.getBody().length;
//计算消息总长度
final int msgLen = calMsgLength(bodyLength, topicLength, propertiesLength);
```

***代码：CommitLog#calMsgLength***

```java
protected static int calMsgLength(int bodyLength, int topicLength, int propertiesLength) {
    final int msgLen = 4 //TOTALSIZE
        + 4 //MAGICCODE  
        + 4 //BODYCRC
        + 4 //QUEUEID
        + 4 //FLAG
        + 8 //QUEUEOFFSET
        + 8 //PHYSICALOFFSET
        + 4 //SYSFLAG
        + 8 //BORNTIMESTAMP
        + 8 //BORNHOST
        + 8 //STORETIMESTAMP
        + 8 //STOREHOSTADDRESS
        + 4 //RECONSUMETIMES
        + 8 //Prepared Transaction Offset
        + 4 + (bodyLength > 0 ? bodyLength : 0) //BODY
        + 1 + topicLength //TOPIC
        + 2 + (propertiesLength > 0 ? propertiesLength : 0) //propertiesLength
        + 0;
    return msgLen;
}
```

***代码：CommitLog#doAppend***

```java
//消息长度不能超过4M
if (msgLen > this.maxMessageSize) {
    CommitLog.log.warn("message size exceeded, msg total size: " + msgLen + ", msg body size: " + bodyLength
        + ", maxMessageSize: " + this.maxMessageSize);
    return new AppendMessageResult(AppendMessageStatus.MESSAGE_SIZE_EXCEEDED);
}

//消息是如果没有足够的存储空间则新创建CommitLog文件
if ((msgLen + END_FILE_MIN_BLANK_LENGTH) > maxBlank) {
    this.resetByteBuffer(this.msgStoreItemMemory, maxBlank);
    // 1 TOTALSIZE
    this.msgStoreItemMemory.putInt(maxBlank);
    // 2 MAGICCODE
    this.msgStoreItemMemory.putInt(CommitLog.BLANK_MAGIC_CODE);
    // 3 The remaining space may be any value
    // Here the length of the specially set maxBlank
    final long beginTimeMills = CommitLog.this.defaultMessageStore.now();
    byteBuffer.put(this.msgStoreItemMemory.array(), 0, maxBlank);
    return new AppendMessageResult(AppendMessageStatus.END_OF_FILE, wroteOffset, maxBlank, msgId, msgInner.getStoreTimestamp(),
        queueOffset, CommitLog.this.defaultMessageStore.now() - beginTimeMills);
}

//将消息存储到ByteBuffer中,返回AppendMessageResult
final long beginTimeMills = CommitLog.this.defaultMessageStore.now();
// Write messages to the queue buffer
byteBuffer.put(this.msgStoreItemMemory.array(), 0, msgLen);
AppendMessageResult result = new AppendMessageResult(AppendMessageStatus.PUT_OK, wroteOffset, 
                                                     msgLen, msgId,msgInner.getStoreTimestamp(), 
                                                     queueOffset, 
                                                     CommitLog.this.defaultMessageStore.now() 
                                                     -beginTimeMills);
switch (tranType) {
    case MessageSysFlag.TRANSACTION_PREPARED_TYPE:
    case MessageSysFlag.TRANSACTION_ROLLBACK_TYPE:
        break;
    case MessageSysFlag.TRANSACTION_NOT_TYPE:
    case MessageSysFlag.TRANSACTION_COMMIT_TYPE:
        //更新消息队列偏移量
        CommitLog.this.topicQueueTable.put(key, ++queueOffset);
        break;
    default:
        break;
}
```

***代码：CommitLog#putMessage***

```java
//释放锁
putMessageLock.unlock();
//刷盘
handleDiskFlush(result, putMessageResult, msg);
//执行HA主从同步
handleHA(result, putMessageResult, msg);
```

### 2.4.3 存储文件

![](./img/%E5%AD%98%E5%82%A8%E6%96%87%E4%BB%B6.png)

* commitLog：消息存储目录
* config：运行期间一些配置信息
* consumerqueue：消息消费队列存储目录
* index：消息索引文件存储目录
* abort：如果存在改文件寿命Broker非正常关闭
* checkpoint：文件检查点，存储CommitLog文件最后一次刷盘时间戳、consumerquueue最后一次刷盘时间，index索引文件最后一次刷盘时间戳。

### 2.4.4 存储文件内存映射

RocketMQ通过使用内存映射文件提高IO访问性能，无论是CommitLog、ConsumerQueue还是IndexFile，单个文件都被设计为固定长度，如果一个文件写满以后再创建一个新文件，文件名就为该文件第一条消息对应的全局物理偏移量。

\####1）MappedFileQueue

![](/assets/MappedFileQueue.B6hcqfiy.png)

```java
String storePath;	//存储目录
int mappedFileSize;	// 单个文件大小
CopyOnWriteArrayList<MappedFile> mappedFiles;	//MappedFile文件集合
AllocateMappedFileService allocateMappedFileService;	//创建MapFile服务类
long flushedWhere = 0;		//当前刷盘指针
long committedWhere = 0;	//当前数据提交指针,内存中ByteBuffer当前的写指针,该值大于等于flushWhere
```

* 根据存储时间查询MappedFile

```java
public MappedFile getMappedFileByTime(final long timestamp) {
    Object[] mfs = this.copyMappedFiles(0);
	
    if (null == mfs)
        return null;
	//遍历MappedFile文件数组
    for (int i = 0; i < mfs.length; i++) {
        MappedFile mappedFile = (MappedFile) mfs[i];
        //MappedFile文件的最后修改时间大于指定时间戳则返回该文件
        if (mappedFile.getLastModifiedTimestamp() >= timestamp) {
            return mappedFile;
        }
    }

    return (MappedFile) mfs[mfs.length - 1];
}
```

* 根据消息偏移量offset查找MappedFile

```java
public MappedFile findMappedFileByOffset(final long offset, final boolean returnFirstOnNotFound) {
    try {
        //获得第一个MappedFile文件
        MappedFile firstMappedFile = this.getFirstMappedFile();
        //获得最后一个MappedFile文件
        MappedFile lastMappedFile = this.getLastMappedFile();
        //第一个文件和最后一个文件均不为空,则进行处理
        if (firstMappedFile != null && lastMappedFile != null) {
            if (offset < firstMappedFile.getFileFromOffset() || 
                offset >= lastMappedFile.getFileFromOffset() + this.mappedFileSize) {
            } else {
                //获得文件索引
                int index = (int) ((offset / this.mappedFileSize) 
                                   - (firstMappedFile.getFileFromOffset() / this.mappedFileSize));
                MappedFile targetFile = null;
                try {
                    //根据索引返回目标文件
                    targetFile = this.mappedFiles.get(index);
                } catch (Exception ignored) {
                }

                if (targetFile != null && offset >= targetFile.getFileFromOffset()
                    && offset < targetFile.getFileFromOffset() + this.mappedFileSize) {
                    return targetFile;
                }

                for (MappedFile tmpMappedFile : this.mappedFiles) {
                    if (offset >= tmpMappedFile.getFileFromOffset()
                        && offset < tmpMappedFile.getFileFromOffset() + this.mappedFileSize) {
                        return tmpMappedFile;
                    }
                }
            }

            if (returnFirstOnNotFound) {
                return firstMappedFile;
            }
        }
    } catch (Exception e) {
        log.error("findMappedFileByOffset Exception", e);
    }

    return null;
}
```

* 获取存储文件最小偏移量

```java
public long getMinOffset() {

    if (!this.mappedFiles.isEmpty()) {
        try {
            return this.mappedFiles.get(0).getFileFromOffset();
        } catch (IndexOutOfBoundsException e) {
            //continue;
        } catch (Exception e) {
            log.error("getMinOffset has exception.", e);
        }
    }
    return -1;
}
```

* 获取存储文件最大偏移量

```java
public long getMaxOffset() {
    MappedFile mappedFile = getLastMappedFile();
    if (mappedFile != null) {
        return mappedFile.getFileFromOffset() + mappedFile.getReadPosition();
    }
    return 0;
}
```

* 返回存储文件当前写指针

```java
public long getMaxWrotePosition() {
    MappedFile mappedFile = getLastMappedFile();
    if (mappedFile != null) {
        return mappedFile.getFileFromOffset() + mappedFile.getWrotePosition();
    }
    return 0;
}
```

\####2）MappedFile

![](/assets/MappedFile.928841cd.png)

```java
int OS_PAGE_SIZE = 1024 * 4;		//操作系统每页大小,默认4K
AtomicLong TOTAL_MAPPED_VIRTUAL_MEMORY = new AtomicLong(0);	//当前JVM实例中MappedFile虚拟内存
AtomicInteger TOTAL_MAPPED_FILES = new AtomicInteger(0);	//当前JVM实例中MappedFile对象个数
AtomicInteger wrotePosition = new AtomicInteger(0);	//当前文件的写指针
AtomicInteger committedPosition = new AtomicInteger(0);	//当前文件的提交指针
AtomicInteger flushedPosition = new AtomicInteger(0);	//刷写到磁盘指针
int fileSize;	//文件大小
FileChannel fileChannel;	//文件通道	
ByteBuffer writeBuffer = null;	//堆外内存ByteBuffer
TransientStorePool transientStorePool = null;	//堆外内存池
String fileName;	//文件名称
long fileFromOffset;	//该文件的处理偏移量
File file;	//物理文件
MappedByteBuffer mappedByteBuffer;	//物理文件对应的内存映射Buffer
volatile long storeTimestamp = 0;	//文件最后一次内容写入时间
boolean firstCreateInQueue = false;	//是否是MappedFileQueue队列中第一个文件
```

***MappedFile初始化***

* 未开启`transientStorePoolEnable`。`transientStorePoolEnable=true`为`true`表示数据先存储到堆外内存，然后通过`Commit`线程将数据提交到内存映射Buffer中，再通过`Flush`线程将内存映射`Buffer`中数据持久化磁盘。

```java
private void init(final String fileName, final int fileSize) throws IOException {
    this.fileName = fileName;
    this.fileSize = fileSize;
    this.file = new File(fileName);
    this.fileFromOffset = Long.parseLong(this.file.getName());
    boolean ok = false;
	
    ensureDirOK(this.file.getParent());

    try {
        this.fileChannel = new RandomAccessFile(this.file, "rw").getChannel();
        this.mappedByteBuffer = this.fileChannel.map(MapMode.READ_WRITE, 0, fileSize);
        TOTAL_MAPPED_VIRTUAL_MEMORY.addAndGet(fileSize);
        TOTAL_MAPPED_FILES.incrementAndGet();
        ok = true;
    } catch (FileNotFoundException e) {
        log.error("create file channel " + this.fileName + " Failed. ", e);
        throw e;
    } catch (IOException e) {
        log.error("map file " + this.fileName + " Failed. ", e);
        throw e;
    } finally {
        if (!ok && this.fileChannel != null) {
            this.fileChannel.close();
        }
    }
}
```

开启`transientStorePoolEnable`

```java
public void init(final String fileName, final int fileSize,
    final TransientStorePool transientStorePool) throws IOException {
    init(fileName, fileSize);
    this.writeBuffer = transientStorePool.borrowBuffer();	//初始化writeBuffer
    this.transientStorePool = transientStorePool;
}
```

***MappedFile提交***

提交数据到FileChannel，commitLeastPages为本次提交最小的页数，如果待提交数据不满commitLeastPages，则不执行本次提交操作。如果writeBuffer如果为空，直接返回writePosition指针，无需执行commit操作，表名commit操作主体是writeBuffer。

```java
public int commit(final int commitLeastPages) {
    if (writeBuffer == null) {
        //no need to commit data to file channel, so just regard wrotePosition as committedPosition.
        return this.wrotePosition.get();
    }
    //判断是否满足提交条件
    if (this.isAbleToCommit(commitLeastPages)) {
        if (this.hold()) {
            commit0(commitLeastPages);
            this.release();
        } else {
            log.warn("in commit, hold failed, commit offset = " + this.committedPosition.get());
        }
    }

    // 所有数据提交后,清空缓冲区
    if (writeBuffer != null && this.transientStorePool != null && this.fileSize == this.committedPosition.get()) {
        this.transientStorePool.returnBuffer(writeBuffer);
        this.writeBuffer = null;
    }

    return this.committedPosition.get();
}
```

***MappedFile#isAbleToCommit***

判断是否执行commit操作，如果文件已满返回true；如果commitLeastpages大于0，则比较writePosition与上一次提交的指针commitPosition的差值，除以OS\_PAGE\_SIZE得到当前脏页的数量，如果大于commitLeastPages则返回true，如果commitLeastpages小于0表示只要存在脏页就提交。

```java
protected boolean isAbleToCommit(final int commitLeastPages) {
    //已经刷盘指针
    int flush = this.committedPosition.get();
    //文件写指针
    int write = this.wrotePosition.get();
	//写满刷盘
    if (this.isFull()) {
        return true;
    }

    if (commitLeastPages > 0) {
        //文件内容达到commitLeastPages页数,则刷盘
        return ((write / OS_PAGE_SIZE) - (flush / OS_PAGE_SIZE)) >= commitLeastPages;
    }

    return write > flush;
}
```

***MappedFile#commit0***

具体提交的实现，首先创建WriteBuffer区共享缓存区，然后将新创建的position回退到上一次提交的位置（commitPosition），设置limit为wrotePosition（当前最大有效数据指针），然后把commitPosition到wrotePosition的数据写入到FileChannel中，然后更新committedPosition指针为wrotePosition。commit的作用就是将MappedFile的writeBuffer中数据提交到文件通道FileChannel中。

```java
protected void commit0(final int commitLeastPages) {
    //写指针
    int writePos = this.wrotePosition.get();
    //上次提交指针
    int lastCommittedPosition = this.committedPosition.get();

    if (writePos - this.committedPosition.get() > 0) {
        try {
            //复制共享内存区域
            ByteBuffer byteBuffer = writeBuffer.slice();
            //设置提交位置是上次提交位置
            byteBuffer.position(lastCommittedPosition);
            //最大提交数量
            byteBuffer.limit(writePos);
            //设置fileChannel位置为上次提交位置
            this.fileChannel.position(lastCommittedPosition);
            //将lastCommittedPosition到writePos的数据复制到FileChannel中
            this.fileChannel.write(byteBuffer);
            //重置提交位置
            this.committedPosition.set(writePos);
        } catch (Throwable e) {
            log.error("Error occurred when commit data to FileChannel.", e);
        }
    }
}
```

***MappedFile#flush***

刷写磁盘，直接调用MappedByteBuffer或fileChannel的force方法将内存中的数据持久化到磁盘，那么flushedPosition应该等于MappedByteBuffer中的写指针；如果writeBuffer不为空，则flushPosition应该等于上一次的commit指针；因为上一次提交的数据就是进入到MappedByteBuffer中的数据；如果writeBuffer为空，数据时直接进入到MappedByteBuffer，wrotePosition代表的是MappedByteBuffer中的指针，故设置flushPosition为wrotePosition。

![](/assets/flush.C2MqItpC.jpg)

```java
public int flush(final int flushLeastPages) {
    //数据达到刷盘条件
    if (this.isAbleToFlush(flushLeastPages)) {
        //加锁，同步刷盘
        if (this.hold()) {
            //获得读指针
            int value = getReadPosition();
            try {
                //数据从writeBuffer提交数据到fileChannel再刷新到磁盘
                if (writeBuffer != null || this.fileChannel.position() != 0) {
                    this.fileChannel.force(false);
                } else {
                    //从mmap刷新数据到磁盘
                    this.mappedByteBuffer.force();
                }
            } catch (Throwable e) {
                log.error("Error occurred when force data to disk.", e);
            }
			//更新刷盘位置
            this.flushedPosition.set(value);
            this.release();
        } else {
            log.warn("in flush, hold failed, flush offset = " + this.flushedPosition.get());
            this.flushedPosition.set(getReadPosition());
        }
    }
    return this.getFlushedPosition();
}
```

***MappedFile#getReadPosition***

获取当前文件最大可读指针。如果writeBuffer为空，则直接返回当前的写指针；如果writeBuffer不为空，则返回上一次提交的指针。在MappedFile设置中,只有提交了的数据（写入到MappedByteBuffer或FileChannel中的数据）才是安全的数据

```java
public int getReadPosition() {
    //如果writeBuffer为空,刷盘的位置就是应该等于上次commit的位置,如果为空则为mmap的写指针
    return this.writeBuffer == null ? this.wrotePosition.get() : this.committedPosition.get();
}
```

***MappedFile#selectMappedBuffer***

查找pos到当前最大可读之间的数据，由于在整个写入期间都未曾改MappedByteBuffer的指针，如果mappedByteBuffer.slice()方法返回的共享缓存区空间为整个MappedFile，然后通过设置ByteBuffer的position为待查找的值，读取字节长度当前可读最大长度，最终返回的ByteBuffer的limit为size。整个共享缓存区的容量为（MappedFile#fileSize-pos）。故在操作SelectMappedBufferResult不能对包含在里面的ByteBuffer调用filp方法。

```java
public SelectMappedBufferResult selectMappedBuffer(int pos) {
    //获得最大可读指针
    int readPosition = getReadPosition();
    //pos小于最大可读指针,并且大于0
    if (pos < readPosition && pos >= 0) {
        if (this.hold()) {
            //复制mappedByteBuffer读共享区
            ByteBuffer byteBuffer = this.mappedByteBuffer.slice();
            //设置读指针位置
            byteBuffer.position(pos);
            //获得可读范围
            int size = readPosition - pos;
            //设置最大刻度范围
            ByteBuffer byteBufferNew = byteBuffer.slice();
            byteBufferNew.limit(size);
            return new SelectMappedBufferResult(this.fileFromOffset + pos, byteBufferNew, size, this);
        }
    }

    return null;
}
```

***MappedFile#shutdown***

MappedFile文件销毁的实现方法为public boolean destory(long intervalForcibly)，intervalForcibly表示拒绝被销毁的最大存活时间。

```java
public void shutdown(final long intervalForcibly) {
    if (this.available) {
        //关闭MapedFile
        this.available = false;
        //设置当前关闭时间戳
        this.firstShutdownTimestamp = System.currentTimeMillis();
        //释放资源
        this.release();
    } else if (this.getRefCount() > 0) {
        if ((System.currentTimeMillis() - this.firstShutdownTimestamp) >= intervalForcibly) {
            this.refCount.set(-1000 - this.getRefCount());
            this.release();
        }
    }
}
```

#### 3）TransientStorePool

短暂的存储池。RocketMQ单独创建一个MappedByteBuffer内存缓存池，用来临时存储数据，数据先写入该内存映射中，然后由commit线程定时将数据从该内存复制到与目标物理文件对应的内存映射中。RocketMQ引入该机制主要的原因是提供一种内存锁定，将当前堆外内存一直锁定在内存中，避免被进程将内存交换到磁盘。

![](/assets/TransientStorePool.BLOOGRQD.png)

```java
private final int poolSize;		//availableBuffers个数
private final int fileSize;		//每隔ByteBuffer大小
private final Deque<ByteBuffer> availableBuffers;	//ByteBuffer容器。双端队列
```

***初始化***

```java
public void init() {
    //创建poolSize个堆外内存
    for (int i = 0; i < poolSize; i++) {
        ByteBuffer byteBuffer = ByteBuffer.allocateDirect(fileSize);
        final long address = ((DirectBuffer) byteBuffer).address();
        Pointer pointer = new Pointer(address);
        //使用com.sun.jna.Library类库将该批内存锁定,避免被置换到交换区,提高存储性能
        LibC.INSTANCE.mlock(pointer, new NativeLong(fileSize));

        availableBuffers.offer(byteBuffer);
    }
}
```

### 2.4.5 实时更新消息消费队列与索引文件

消息消费队文件、消息属性索引文件都是基于CommitLog文件构建的，当消息生产者提交的消息存储在CommitLog文件中，ConsumerQueue、IndexFile需要及时更新，否则消息无法及时被消费，根据消息属性查找消息也会出现较大延迟。RocketMQ通过开启一个线程ReputMessageService来准实时转发CommitLog文件更新事件，相应的任务处理器根据转发的消息及时更新ConsumerQueue、IndexFile文件。

![](/assets/消息存储结构.CcBaUnuh.png)

![](/assets/构建消息消费队列和索引文件.aZquF3xA.png)

***代码：DefaultMessageStore：start***

```java
//设置CommitLog内存中最大偏移量
this.reputMessageService.setReputFromOffset(maxPhysicalPosInLogicQueue);
//启动
this.reputMessageService.start();
```

***代码：DefaultMessageStore：run***

```java
public void run() {
    DefaultMessageStore.log.info(this.getServiceName() + " service started");
	//每隔1毫秒就继续尝试推送消息到消息消费队列和索引文件
    while (!this.isStopped()) {
        try {
            Thread.sleep(1);
            this.doReput();
        } catch (Exception e) {
            DefaultMessageStore.log.warn(this.getServiceName() + " service has exception. ", e);
        }
    }

    DefaultMessageStore.log.info(this.getServiceName() + " service end");
}
```

***代码：DefaultMessageStore：deReput***

```java
//从result中循环遍历消息,一次读一条,创建DispatherRequest对象。
for (int readSize = 0; readSize < result.getSize() && doNext; ) {
	DispatchRequest dispatchRequest =                               DefaultMessageStore.this.commitLog.checkMessageAndReturnSize(result.getByteBuffer(), false, false);
	int size = dispatchRequest.getBufferSize() == -1 ? dispatchRequest.getMsgSize() : dispatchRequest.getBufferSize();

	if (dispatchRequest.isSuccess()) {
	    if (size > 0) {
	        DefaultMessageStore.this.doDispatch(dispatchRequest);
	    }
    }
}
```

***DispatchRequest***

![](/assets/DispatchRequest.mdB_2y5N.png)

```java
String topic; //消息主题名称
int queueId;  //消息队列ID
long commitLogOffset;	//消息物理偏移量
int msgSize;	//消息长度
long tagsCode;	//消息过滤tag hashCode
long storeTimestamp;	//消息存储时间戳
long consumeQueueOffset;	//消息队列偏移量
String keys;	//消息索引key
boolean success;	//是否成功解析到完整的消息
String uniqKey;	//消息唯一键
int sysFlag;	//消息系统标记
long preparedTransactionOffset;	//消息预处理事务偏移量
Map<String, String> propertiesMap;	//消息属性
byte[] bitMap;	//位图
```

#### 1）转发到ConsumerQueue

![](/assets/消息分发到消息消费队列.DjXRZbyk.png)

```java
class CommitLogDispatcherBuildConsumeQueue implements CommitLogDispatcher {
    @Override
    public void dispatch(DispatchRequest request) {
        final int tranType = MessageSysFlag.getTransactionValue(request.getSysFlag());
        switch (tranType) {
            case MessageSysFlag.TRANSACTION_NOT_TYPE:
            case MessageSysFlag.TRANSACTION_COMMIT_TYPE:
                //消息分发
                DefaultMessageStore.this.putMessagePositionInfo(request);
                break;
            case MessageSysFlag.TRANSACTION_PREPARED_TYPE:
            case MessageSysFlag.TRANSACTION_ROLLBACK_TYPE:
                break;
        }
    }
}
```

***代码：DefaultMessageStore#putMessagePositionInfo***

```java
public void putMessagePositionInfo(DispatchRequest dispatchRequest) {
    //获得消费队列
    ConsumeQueue cq = this.findConsumeQueue(dispatchRequest.getTopic(), dispatchRequest.getQueueId());
    //消费队列分发消息
    cq.putMessagePositionInfoWrapper(dispatchRequest);
}
```

***代码：DefaultMessageStore#putMessagePositionInfo***

```java
//依次将消息偏移量、消息长度、tag写入到ByteBuffer中
this.byteBufferIndex.flip();
this.byteBufferIndex.limit(CQ_STORE_UNIT_SIZE);
this.byteBufferIndex.putLong(offset);
this.byteBufferIndex.putInt(size);
this.byteBufferIndex.putLong(tagsCode);
//获得内存映射文件
MappedFile mappedFile = this.mappedFileQueue.getLastMappedFile(expectLogicOffset);
if (mappedFile != null) {
    //将消息追加到内存映射文件,异步输盘
    return mappedFile.appendMessage(this.byteBufferIndex.array());
}
```

#### 2）转发到Index

![](/assets/消息分发到索引文件.BXPMJdc2.png)

```java
class CommitLogDispatcherBuildIndex implements CommitLogDispatcher {

    @Override
    public void dispatch(DispatchRequest request) {
        if (DefaultMessageStore.this.messageStoreConfig.isMessageIndexEnable()) {
            DefaultMessageStore.this.indexService.buildIndex(request);
        }
    }
}
```

***代码：DefaultMessageStore#buildIndex***

```java
public void buildIndex(DispatchRequest req) {
    //获得索引文件
    IndexFile indexFile = retryGetAndCreateIndexFile();
    if (indexFile != null) {
        //获得文件最大物理偏移量
        long endPhyOffset = indexFile.getEndPhyOffset();
        DispatchRequest msg = req;
        String topic = msg.getTopic();
        String keys = msg.getKeys();
        //如果该消息的物理偏移量小于索引文件中的最大物理偏移量,则说明是重复数据,忽略本次索引构建
        if (msg.getCommitLogOffset() < endPhyOffset) {
            return;
        }

        final int tranType = MessageSysFlag.getTransactionValue(msg.getSysFlag());
        switch (tranType) {
            case MessageSysFlag.TRANSACTION_NOT_TYPE:
            case MessageSysFlag.TRANSACTION_PREPARED_TYPE:
            case MessageSysFlag.TRANSACTION_COMMIT_TYPE:
                break;
            case MessageSysFlag.TRANSACTION_ROLLBACK_TYPE:
                return;
        }
		
        //如果消息ID不为空,则添加到Hash索引中
        if (req.getUniqKey() != null) {
            indexFile = putKey(indexFile, msg, buildKey(topic, req.getUniqKey()));
            if (indexFile == null) {
                return;
            }
        }
		//构建索引key,RocketMQ支持为同一个消息建立多个索引,多个索引键空格隔开.
        if (keys != null && keys.length() > 0) {
            String[] keyset = keys.split(MessageConst.KEY_SEPARATOR);
            for (int i = 0; i < keyset.length; i++) {
                String key = keyset[i];
                if (key.length() > 0) {
                    indexFile = putKey(indexFile, msg, buildKey(topic, key));
                    if (indexFile == null) {

                        return;
                    }
                }
            }
        }
    } else {
        log.error("build index error, stop building index");
    }
}
```

### 2.4.6 消息队列和索引文件恢复

由于RocketMQ存储首先将消息全量存储在CommitLog文件中，然后异步生成转发任务更新ConsumerQueue和Index文件。如果消息成功存储到CommitLog文件中，转发任务未成功执行，此时消息服务器Broker由于某个愿意宕机，导致CommitLog、ConsumerQueue、IndexFile文件数据不一致。如果不加以人工修复的话，会有一部分消息即便在CommitLog中文件中存在，但由于没有转发到ConsumerQueue，这部分消息将永远复发被消费者消费。

![](/assets/文件恢复总体流程.CDbW5qG_.png)

\####1）存储文件加载

***代码：DefaultMessageStore#load***

判断上一次是否异常退出。实现机制是Broker在启动时创建abort文件，在退出时通过JVM钩子函数删除abort文件。如果下次启动时存在abort文件。说明Broker时异常退出的，CommitLog与ConsumerQueue数据有可能不一致，需要进行修复。

```java
//判断临时文件是否存在
boolean lastExitOK = !this.isTempFileExist();
//根据临时文件判断当前Broker是否异常退出
private boolean isTempFileExist() {
    String fileName = StorePathConfigHelper
        .getAbortFile(this.messageStoreConfig.getStorePathRootDir());
    File file = new File(fileName);
    return file.exists();
}
```

***代码：DefaultMessageStore#load***

```java
//加载延时队列
if (null != scheduleMessageService) {
    result = result && this.scheduleMessageService.load();
}

// 加载CommitLog文件
result = result && this.commitLog.load();

// 加载消费队列文件
result = result && this.loadConsumeQueue();

if (result) {
	//加载存储监测点,监测点主要记录CommitLog文件、ConsumerQueue文件、Index索引文件的刷盘点
    this.storeCheckpoint =new StoreCheckpoint(StorePathConfigHelper.getStoreCheckpoint(this.messageStoreConfig.getStorePathRootDir()));
	//加载index文件
    this.indexService.load(lastExitOK);
	//根据Broker是否异常退出,执行不同的恢复策略
    this.recover(lastExitOK);
}
```

***代码：MappedFileQueue#load***

加载CommitLog到映射文件

```java
//指向CommitLog文件目录
File dir = new File(this.storePath);
//获得文件数组
File[] files = dir.listFiles();
if (files != null) {
    // 文件排序
    Arrays.sort(files);
    //遍历文件
    for (File file : files) {
		//如果文件大小和配置文件不一致,退出
        if (file.length() != this.mappedFileSize) {
            
            return false;
        }

        try {
            //创建映射文件
            MappedFile mappedFile = new MappedFile(file.getPath(), mappedFileSize);
            mappedFile.setWrotePosition(this.mappedFileSize);
            mappedFile.setFlushedPosition(this.mappedFileSize);
            mappedFile.setCommittedPosition(this.mappedFileSize);
            //将映射文件添加到队列
            this.mappedFiles.add(mappedFile);
            log.info("load " + file.getPath() + " OK");
        } catch (IOException e) {
            log.error("load file " + file + " error", e);
            return false;
        }
    }
}

return true;
```

***代码：DefaultMessageStore#loadConsumeQueue***

加载消息消费队列

```java
//执行消费队列目录
File dirLogic = new File(StorePathConfigHelper.getStorePathConsumeQueue(this.messageStoreConfig.getStorePathRootDir()));
//遍历消费队列目录
File[] fileTopicList = dirLogic.listFiles();
if (fileTopicList != null) {

    for (File fileTopic : fileTopicList) {
        //获得子目录名称,即topic名称
        String topic = fileTopic.getName();
		//遍历子目录下的消费队列文件
        File[] fileQueueIdList = fileTopic.listFiles();
        if (fileQueueIdList != null) {
            //遍历文件
            for (File fileQueueId : fileQueueIdList) {
                //文件名称即队列ID
                int queueId;
                try {
                    queueId = Integer.parseInt(fileQueueId.getName());
                } catch (NumberFormatException e) {
                    continue;
                }
                //创建消费队列并加载到内存
                ConsumeQueue logic = new ConsumeQueue(
                    topic,
                    queueId,
                    StorePathConfigHelper.getStorePathConsumeQueue(this.messageStoreConfig.getStorePathRootDir()),
            this.getMessageStoreConfig().getMapedFileSizeConsumeQueue(),
                    this);
                this.putConsumeQueue(topic, queueId, logic);
                if (!logic.load()) {
                    return false;
                }
            }
        }
    }
}

log.info("load logics queue all over, OK");

return true;
```

***代码：IndexService#load***

加载索引文件

```java
public boolean load(final boolean lastExitOK) {
    //索引文件目录
    File dir = new File(this.storePath);
    //遍历索引文件
    File[] files = dir.listFiles();
    if (files != null) {
        //文件排序
        Arrays.sort(files);
        //遍历文件
        for (File file : files) {
            try {
                //加载索引文件
                IndexFile f = new IndexFile(file.getPath(), this.hashSlotNum, this.indexNum, 0, 0);
                f.load();

                if (!lastExitOK) {
                    //索引文件上次的刷盘时间小于该索引文件的消息时间戳,该文件将立即删除
                    if (f.getEndTimestamp() > this.defaultMessageStore.getStoreCheckpoint()
                        .getIndexMsgTimestamp()) {
                        f.destroy(0);
                        continue;
                    }
                }
				//将索引文件添加到队列
                log.info("load index file OK, " + f.getFileName());
                this.indexFileList.add(f);
            } catch (IOException e) {
                log.error("load file {} error", file, e);
                return false;
            } catch (NumberFormatException e) {
                log.error("load file {} error", file, e);
            }
        }
    }

    return true;
}
```

***代码：DefaultMessageStore#recover***

文件恢复，根据Broker是否正常退出执行不同的恢复策略

```java
private void recover(final boolean lastExitOK) {
    //获得最大的物理便宜消费队列
    long maxPhyOffsetOfConsumeQueue = this.recoverConsumeQueue();

    if (lastExitOK) {
        //正常恢复
        this.commitLog.recoverNormally(maxPhyOffsetOfConsumeQueue);
    } else {
        //异常恢复
        this.commitLog.recoverAbnormally(maxPhyOffsetOfConsumeQueue);
    }
	//在CommitLog中保存每个消息消费队列当前的存储逻辑偏移量
    this.recoverTopicQueueTable();
}
```

***代码：DefaultMessageStore#recoverTopicQueueTable***

恢复ConsumerQueue后，将在CommitLog实例中保存每隔消息队列当前的存储逻辑偏移量，这也是消息中不仅存储主题、消息队列ID、还存储了消息队列的关键所在。

```java
public void recoverTopicQueueTable() {
    HashMap<String/* topic-queueid */, Long/* offset */> table = new HashMap<String, Long>(1024);
    //CommitLog最小偏移量
    long minPhyOffset = this.commitLog.getMinOffset();
    //遍历消费队列,将消费队列保存在CommitLog中
    for (ConcurrentMap<Integer, ConsumeQueue> maps : this.consumeQueueTable.values()) {
        for (ConsumeQueue logic : maps.values()) {
            String key = logic.getTopic() + "-" + logic.getQueueId();
            table.put(key, logic.getMaxOffsetInQueue());
            logic.correctMinOffset(minPhyOffset);
        }
    }
    this.commitLog.setTopicQueueTable(table);
}
```

\####2）正常恢复

***代码：CommitLog#recoverNormally***

```java
public void recoverNormally(long maxPhyOffsetOfConsumeQueue) {
	
    final List<MappedFile> mappedFiles = this.mappedFileQueue.getMappedFiles();
    if (!mappedFiles.isEmpty()) {
         //Broker正常停止再重启时,从倒数第三个开始恢复,如果不足3个文件,则从第一个文件开始恢复。
        int index = mappedFiles.size() - 3;
        if (index < 0)
            index = 0;
        MappedFile mappedFile = mappedFiles.get(index);
        ByteBuffer byteBuffer = mappedFile.sliceByteBuffer();
        long processOffset = mappedFile.getFileFromOffset();
        //代表当前已校验通过的offset
        long mappedFileOffset = 0;
        while (true) {
            //查找消息
            DispatchRequest dispatchRequest = this.checkMessageAndReturnSize(byteBuffer, checkCRCOnRecover);
            //消息长度
            int size = dispatchRequest.getMsgSize();
           	//查找结果为true,并且消息长度大于0,表示消息正确.mappedFileOffset向前移动本消息长度
            if (dispatchRequest.isSuccess() && size > 0) {
                mappedFileOffset += size;
            }
			//如果查找结果为true且消息长度等于0,表示已到该文件末尾,如果还有下一个文件,则重置processOffset和MappedFileOffset重复查找下一个文件,否则跳出循环。
            else if (dispatchRequest.isSuccess() && size == 0) {
              index++;
              if (index >= mappedFiles.size()) {
                  // Current branch can not happen
                  break;
              } else {
                  //取出每个文件
                  mappedFile = mappedFiles.get(index);
                  byteBuffer = mappedFile.sliceByteBuffer();
                  processOffset = mappedFile.getFileFromOffset();
                  mappedFileOffset = 0;
                  
          		}
            }
            // 查找结果为false，表明该文件未填满所有消息，跳出循环，结束循环
            else if (!dispatchRequest.isSuccess()) {
                log.info("recover physics file end, " + mappedFile.getFileName());
                break;
            }
        }
		//更新MappedFileQueue的flushedWhere和committedWhere指针
        processOffset += mappedFileOffset;
        this.mappedFileQueue.setFlushedWhere(processOffset);
        this.mappedFileQueue.setCommittedWhere(processOffset);
        //删除offset之后的所有文件
        this.mappedFileQueue.truncateDirtyFiles(processOffset);

        
        if (maxPhyOffsetOfConsumeQueue >= processOffset) {
            this.defaultMessageStore.truncateDirtyLogicFiles(processOffset);
        }
    } else {
        this.mappedFileQueue.setFlushedWhere(0);
        this.mappedFileQueue.setCommittedWhere(0);
        this.defaultMessageStore.destroyLogics();
    }
}
```

***代码：MappedFileQueue#truncateDirtyFiles***

```java
public void truncateDirtyFiles(long offset) {
    List<MappedFile> willRemoveFiles = new ArrayList<MappedFile>();
	//遍历目录下文件
    for (MappedFile file : this.mappedFiles) {
        //文件尾部的偏移量
        long fileTailOffset = file.getFileFromOffset() + this.mappedFileSize;
        //文件尾部的偏移量大于offset
        if (fileTailOffset > offset) {
            //offset大于文件的起始偏移量
            if (offset >= file.getFileFromOffset()) {
                //更新wrotePosition、committedPosition、flushedPosistion
                file.setWrotePosition((int) (offset % this.mappedFileSize));
                file.setCommittedPosition((int) (offset % this.mappedFileSize));
                file.setFlushedPosition((int) (offset % this.mappedFileSize));
            } else {
                //offset小于文件的起始偏移量,说明该文件是有效文件后面创建的,释放mappedFile占用内存,删除文件
                file.destroy(1000);
                willRemoveFiles.add(file);
            }
        }
    }

    this.deleteExpiredFile(willRemoveFiles);
}
```

\####3）异常恢复

Broker异常停止文件恢复的实现为CommitLog#recoverAbnormally。异常文件恢复步骤与正常停止文件恢复流程基本相同，其主要差别有两个。首先，正常停止默认从倒数第三个文件开始进行恢复，而异常停止则需要从最后一个文件往前走，找到第一个消息存储正常的文件。其次，如果CommitLog目录没有消息文件，如果消息消费队列目录下存在文件，则需要销毁。

***代码：CommitLog#recoverAbnormally***

```java
if (!mappedFiles.isEmpty()) {
    // Looking beginning to recover from which file
    int index = mappedFiles.size() - 1;
    MappedFile mappedFile = null;
    for (; index >= 0; index--) {
        mappedFile = mappedFiles.get(index);
        //判断消息文件是否是一个正确的文件
        if (this.isMappedFileMatchedRecover(mappedFile)) {
            log.info("recover from this mapped file " + mappedFile.getFileName());
            break;
        }
    }
	//根据索引取出mappedFile文件
    if (index < 0) {
        index = 0;
        mappedFile = mappedFiles.get(index);
    }
    //...验证消息的合法性,并将消息转发到消息消费队列和索引文件
       
}else{
    //未找到mappedFile,重置flushWhere、committedWhere都为0，销毁消息队列文件
    this.mappedFileQueue.setFlushedWhere(0);
    this.mappedFileQueue.setCommittedWhere(0);
    this.defaultMessageStore.destroyLogics();
}
```

### 2.4.7 刷盘机制

RocketMQ的存储是基于JDK NIO的内存映射机制（MappedByteBuffer）的，消息存储首先将消息追加到内存，再根据配置的刷盘策略在不同时间进行刷写磁盘。

#### 同步刷盘

消息追加到内存后，立即将数据刷写到磁盘文件

![](/assets/同步刷盘流程.C7nsVrer.png)

***代码：CommitLog#handleDiskFlush***

```java
//刷盘服务
final GroupCommitService service = (GroupCommitService) this.flushCommitLogService;
if (messageExt.isWaitStoreMsgOK()) {
    //封装刷盘请求
    GroupCommitRequest request = new GroupCommitRequest(result.getWroteOffset() + result.getWroteBytes());
    //提交刷盘请求
    service.putRequest(request);
    //线程阻塞5秒，等待刷盘结束
    boolean flushOK = request.waitForFlush(this.defaultMessageStore.getMessageStoreConfig().getSyncFlushTimeout());
    if (!flushOK) {
        putMessageResult.setPutMessageStatus(PutMessageStatus.FLUSH_DISK_TIMEOUT);
    }
```

***GroupCommitRequest***

![](/assets/GroupCommitRequest.DYOjXhjc.png)

```java
long nextOffset;	//刷盘点偏移量
CountDownLatch countDownLatch = new CountDownLatch(1);	//倒计树锁存器
volatile boolean flushOK = false;	//刷盘结果;默认为false
```

***代码：GroupCommitService#run***

```java
public void run() {
    CommitLog.log.info(this.getServiceName() + " service started");

    while (!this.isStopped()) {
        try {
            //线程等待10ms
            this.waitForRunning(10);
            //执行提交
            this.doCommit();
        } catch (Exception e) {
            CommitLog.log.warn(this.getServiceName() + " service has exception. ", e);
        }
    }
	...
}
```

***代码：GroupCommitService#doCommit***

```java
private void doCommit() {
    //加锁
    synchronized (this.requestsRead) {
        if (!this.requestsRead.isEmpty()) {
            //遍历requestsRead
            for (GroupCommitRequest req : this.requestsRead) {
                // There may be a message in the next file, so a maximum of
                // two times the flush
                boolean flushOK = false;
                for (int i = 0; i < 2 && !flushOK; i++) {
                    flushOK = CommitLog.this.mappedFileQueue.getFlushedWhere() >= req.getNextOffset();
					//刷盘
                    if (!flushOK) {
                        CommitLog.this.mappedFileQueue.flush(0);
                    }
                }
				//唤醒发送消息客户端
                req.wakeupCustomer(flushOK);
            }
			
            //更新刷盘监测点
            long storeTimestamp = CommitLog.this.mappedFileQueue.getStoreTimestamp();
            if (storeTimestamp > 0) {               CommitLog.this.defaultMessageStore.getStoreCheckpoint().setPhysicMsgTimestamp(storeTimestamp);
            }
			
            this.requestsRead.clear();
        } else {
            // Because of individual messages is set to not sync flush, it
            // will come to this process
            CommitLog.this.mappedFileQueue.flush(0);
        }
    }
}
```

#### 异步刷盘

在消息追加到内存后，立即返回给消息发送端。如果开启transientStorePoolEnable，RocketMQ会单独申请一个与目标物理文件（commitLog）同样大小的堆外内存，该堆外内存将使用内存锁定，确保不会被置换到虚拟内存中去，消息首先追加到堆外内存，然后提交到物理文件的内存映射中，然后刷写到磁盘。如果未开启transientStorePoolEnable，消息直接追加到物理文件直接映射文件中，然后刷写到磁盘中。

![](/assets/异步刷盘流程.BF-Uj8ik.png)

开启transientStorePoolEnable后异步刷盘步骤:

1. 将消息直接追加到ByteBuffer（堆外内存）
2. CommitRealTimeService线程每隔200ms将ByteBuffer新追加内容提交到MappedByteBuffer中
3. MappedByteBuffer在内存中追加提交的内容，wrotePosition指针向后移动
4. commit操作成功返回，将committedPosition位置恢复
5. FlushRealTimeService线程默认每500ms将MappedByteBuffer中新追加的内存刷写到磁盘

***代码：CommitLog$CommitRealTimeService#run***

提交线程工作机制

```java
//间隔时间,默认200ms
int interval = CommitLog.this.defaultMessageStore.getMessageStoreConfig().getCommitIntervalCommitLog();

//一次提交的至少页数
int commitDataLeastPages = CommitLog.this.defaultMessageStore.getMessageStoreConfig().getCommitCommitLogLeastPages();

//两次真实提交的最大间隔,默认200ms
int commitDataThoroughInterval =
CommitLog.this.defaultMessageStore.getMessageStoreConfig().getCommitCommitLogThoroughInterval();

//上次提交间隔超过commitDataThoroughInterval,则忽略提交commitDataThoroughInterval参数,直接提交
long begin = System.currentTimeMillis();
if (begin >= (this.lastCommitTimestamp + commitDataThoroughInterval)) {
    this.lastCommitTimestamp = begin;
    commitDataLeastPages = 0;
}

//执行提交操作,将待提交数据提交到物理文件的内存映射区
boolean result = CommitLog.this.mappedFileQueue.commit(commitDataLeastPages);
long end = System.currentTimeMillis();
if (!result) {
    this.lastCommitTimestamp = end; // result = false means some data committed.
    //now wake up flush thread.
    //唤醒刷盘线程
    flushCommitLogService.wakeup();
}

if (end - begin > 500) {
    log.info("Commit data to file costs {} ms", end - begin);
}
this.waitForRunning(interval);
```

***代码：CommitLog$FlushRealTimeService#run***

刷盘线程工作机制

```java
//表示await方法等待,默认false
boolean flushCommitLogTimed = CommitLog.this.defaultMessageStore.getMessageStoreConfig().isFlushCommitLogTimed();
//线程执行时间间隔
int interval = CommitLog.this.defaultMessageStore.getMessageStoreConfig().getFlushIntervalCommitLog();
//一次刷写任务至少包含页数
int flushPhysicQueueLeastPages = CommitLog.this.defaultMessageStore.getMessageStoreConfig().getFlushCommitLogLeastPages();
//两次真实刷写任务最大间隔
int flushPhysicQueueThoroughInterval =
CommitLog.this.defaultMessageStore.getMessageStoreConfig().getFlushCommitLogThoroughInterval();
...
//距离上次提交间隔超过flushPhysicQueueThoroughInterval,则本次刷盘任务将忽略flushPhysicQueueLeastPages,直接提交
long currentTimeMillis = System.currentTimeMillis();
if (currentTimeMillis >= (this.lastFlushTimestamp + flushPhysicQueueThoroughInterval)) {
    this.lastFlushTimestamp = currentTimeMillis;
    flushPhysicQueueLeastPages = 0;
    printFlushProgress = (printTimes++ % 10) == 0;
}
...
//执行一次刷盘前,先等待指定时间间隔
if (flushCommitLogTimed) {
    Thread.sleep(interval);
} else {
    this.waitForRunning(interval);
}
...
long begin = System.currentTimeMillis();
//刷写磁盘
CommitLog.this.mappedFileQueue.flush(flushPhysicQueueLeastPages);
long storeTimestamp = CommitLog.this.mappedFileQueue.getStoreTimestamp();
if (storeTimestamp > 0) {
//更新存储监测点文件的时间戳
CommitLog.this.defaultMessageStore.getStoreCheckpoint().setPhysicMsgTimestamp(storeTimestamp);

```

### 2.4.8 过期文件删除机制

由于RocketMQ操作CommitLog、ConsumerQueue文件是基于内存映射机制并在启动的时候回加载CommitLog、ConsumerQueue目录下的所有文件，为了避免内存与磁盘的浪费，不可能将消息永久存储在消息服务器上，所以要引入一种机制来删除已过期的文件。RocketMQ顺序写CommitLog、ConsumerQueue文件，所有写操作全部落在最后一个CommitLog或者ConsumerQueue文件上，之前的文件在下一个文件创建后将不会再被更新。RocketMQ清除过期文件的方法时：如果当前文件在在一定时间间隔内没有再次被消费，则认为是过期文件，可以被删除，RocketMQ不会关注这个文件上的消息是否全部被消费。默认每个文件的过期时间为72小时，通过在Broker配置文件中设置fileReservedTime来改变过期时间，单位为小时。

***代码：DefaultMessageStore#addScheduleTask***

```java
private void addScheduleTask() {
	//每隔10s调度一次清除文件
    this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() {
        @Override
        public void run() {
            DefaultMessageStore.this.cleanFilesPeriodically();
        }
    }, 1000 * 60, this.messageStoreConfig.getCleanResourceInterval(), TimeUnit.MILLISECONDS);
	...
}
```

***代码：DefaultMessageStore#cleanFilesPeriodically***

```java
private void cleanFilesPeriodically() {
    //清除存储文件
    this.cleanCommitLogService.run();
    //清除消息消费队列文件
    this.cleanConsumeQueueService.run();
}
```

***代码：DefaultMessageStore#deleteExpiredFiles***

```java
private void deleteExpiredFiles() {
    //删除的数量
    int deleteCount = 0;
    //文件保留的时间
    long fileReservedTime = DefaultMessageStore.this.getMessageStoreConfig().getFileReservedTime();
    //删除物理文件的间隔
    int deletePhysicFilesInterval = DefaultMessageStore.this.getMessageStoreConfig().getDeleteCommitLogFilesInterval();
    //线程被占用,第一次拒绝删除后能保留的最大时间,超过该时间,文件将被强制删除
    int destroyMapedFileIntervalForcibly = DefaultMessageStore.this.getMessageStoreConfig().getDestroyMapedFileIntervalForcibly();

boolean timeup = this.isTimeToDelete();
boolean spacefull = this.isSpaceToDelete();
boolean manualDelete = this.manualDeleteFileSeveralTimes > 0;
if (timeup || spacefull || manualDelete) {
	...执行删除逻辑
}else{
    ...无作为
}
```

删除文件操作的条件

1. 指定删除文件的时间点，RocketMQ通过deleteWhen设置一天的固定时间执行一次删除过期文件操作，默认4点
2. 磁盘空间如果不充足，删除过期文件
3. 预留，手工触发。

***代码：CleanCommitLogService#isSpaceToDelete***

当磁盘空间不足时执行删除过期文件

```java
private boolean isSpaceToDelete() {
    //磁盘分区的最大使用量
    double ratio = DefaultMessageStore.this.getMessageStoreConfig().getDiskMaxUsedSpaceRatio() / 100.0;
	//是否需要立即执行删除过期文件操作
    cleanImmediately = false;

    {
        String storePathPhysic = DefaultMessageStore.this.getMessageStoreConfig().getStorePathCommitLog();
        //当前CommitLog目录所在的磁盘分区的磁盘使用率
        double physicRatio = UtilAll.getDiskPartitionSpaceUsedPercent(storePathPhysic);
        //diskSpaceWarningLevelRatio:磁盘使用率警告阈值,默认0.90
        if (physicRatio > diskSpaceWarningLevelRatio) {
            boolean diskok = DefaultMessageStore.this.runningFlags.getAndMakeDiskFull();
            if (diskok) {
                DefaultMessageStore.log.error("physic disk maybe full soon " + physicRatio + ", so mark disk full");
            }
			//diskSpaceCleanForciblyRatio:强制清除阈值,默认0.85
            cleanImmediately = true;
        } else if (physicRatio > diskSpaceCleanForciblyRatio) {
            cleanImmediately = true;
        } else {
            boolean diskok = DefaultMessageStore.this.runningFlags.getAndMakeDiskOK();
            if (!diskok) {
            DefaultMessageStore.log.info("physic disk space OK " + physicRatio + ", so mark disk ok");
        }
    }

    if (physicRatio < 0 || physicRatio > ratio) {
        DefaultMessageStore.log.info("physic disk maybe full soon, so reclaim space, " + physicRatio);
        return true;
    }
}
```

***代码：MappedFileQueue#deleteExpiredFileByTime***

执行文件销毁和删除

```java
for (int i = 0; i < mfsLength; i++) {
    //遍历每隔文件
    MappedFile mappedFile = (MappedFile) mfs[i];
    //计算文件存活时间
    long liveMaxTimestamp = mappedFile.getLastModifiedTimestamp() + expiredTime;
    //如果超过72小时,执行文件删除
    if (System.currentTimeMillis() >= liveMaxTimestamp || cleanImmediately) {
        if (mappedFile.destroy(intervalForcibly)) {
            files.add(mappedFile);
            deleteCount++;

            if (files.size() >= DELETE_FILES_BATCH_MAX) {
                break;
            }

            if (deleteFilesInterval > 0 && (i + 1) < mfsLength) {
                try {
                    Thread.sleep(deleteFilesInterval);
                } catch (InterruptedException e) {
                }
            }
        } else {
            break;
        }
    } else {
        //avoid deleting files in the middle
        break;
    }
}
```

### 2.4.9 小结

RocketMQ的存储文件包括消息文件（Commitlog）、消息消费队列文件（ConsumerQueue）、Hash索引文件（IndexFile）、监测点文件（checkPoint）、abort（关闭异常文件）。单个消息存储文件、消息消费队列文件、Hash索引文件长度固定以便使用内存映射机制进行文件的读写操作。RocketMQ组织文件以文件的起始偏移量来命令文件，这样根据偏移量能快速定位到真实的物理文件。RocketMQ基于内存映射文件机制提供了同步刷盘和异步刷盘两种机制，异步刷盘是指在消息存储时先追加到内存映射文件，然后启动专门的刷盘线程定时将内存中的文件数据刷写到磁盘。

CommitLog，消息存储文件，RocketMQ为了保证消息发送的高吞吐量，采用单一文件存储所有主题消息，保证消息存储是完全的顺序写，但这样给文件读取带来了不便，为此RocketMQ为了方便消息消费构建了消息消费队列文件，基于主题与队列进行组织，同时RocketMQ为消息实现了Hash索引，可以为消息设置索引键，根据所以能够快速从CommitLog文件中检索消息。

当消息达到CommitLog后，会通过ReputMessageService线程接近实时地将消息转发给消息消费队列文件与索引文件。为了安全起见，RocketMQ引入abort文件，记录Broker的停机是否是正常关闭还是异常关闭，在重启Broker时为了保证CommitLog文件，消息消费队列文件与Hash索引文件的正确性，分别采用不同策略来恢复文件。

RocketMQ不会永久存储消息文件、消息消费队列文件，而是启动文件过期机制并在磁盘空间不足或者默认凌晨4点删除过期文件，文件保存72小时并且在删除文件时并不会判断该消息文件上的消息是否被消费。

## 2.5 Consumer

### 2.5.1 消息消费概述

消息消费以组的模式开展，一个消费组内可以包含多个消费者，每一个消费者组可订阅多个主题，消费组之间有ff式和广播模式两种消费模式。集群模式，主题下的同一条消息只允许被其中一个消费者消费。广播模式，主题下的同一条消息，将被集群内的所有消费者消费一次。消息服务器与消费者之间的消息传递也有两种模式：推模式、拉模式。所谓的拉模式，是消费端主动拉起拉消息请求，而推模式是消息达到消息服务器后，推送给消息消费者。RocketMQ消息推模式的实现基于拉模式，在拉模式上包装一层，一个拉取任务完成后开始下一个拉取任务。

集群模式下，多个消费者如何对消息队列进行负载呢？消息队列负载机制遵循一个通用思想：一个消息队列同一个时间只允许被一个消费者消费，一个消费者可以消费多个消息队列。

RocketMQ支持局部顺序消息消费，也就是保证同一个消息队列上的消息顺序消费。不支持消息全局顺序消费，如果要实现某一个主题的全局顺序消费，可以将该主题的队列数设置为1，牺牲高可用性。

\###2.5.2 消息消费初探

**消息推送模式**

![](/assets/消息推送.Bl4fmYm1.png)

**消息消费重要方法**

```java
void sendMessageBack(final MessageExt msg, final int delayLevel, final String brokerName)：发送消息确认
Set<MessageQueue> fetchSubscribeMessageQueues(final String topic) :获取消费者对主题分配了那些消息队列
void registerMessageListener(final MessageListenerConcurrently messageListener)：注册并发事件监听器
void registerMessageListener(final MessageListenerOrderly messageListener)：注册顺序消息事件监听器
void subscribe(final String topic, final String subExpression)：基于主题订阅消息，消息过滤使用表达式
void subscribe(final String topic, final String fullClassName,final String filterClassSource)：基于主题订阅消息，消息过滤使用类模式
void subscribe(final String topic, final MessageSelector selector) ：订阅消息，并指定队列选择器
void unsubscribe(final String topic)：取消消息订阅
```

**DefaultMQPushConsumer**

![](/assets/DefaultMQPushConsumer.DKJW_stD.png)

```java
//消费者组
private String consumerGroup;	
//消息消费模式
private MessageModel messageModel = MessageModel.CLUSTERING;	
//指定消费开始偏移量（最大偏移量、最小偏移量、启动时间戳）开始消费
private ConsumeFromWhere consumeFromWhere = ConsumeFromWhere.CONSUME_FROM_LAST_OFFSET;
//集群模式下的消息队列负载策略
private AllocateMessageQueueStrategy allocateMessageQueueStrategy;
//订阅信息
private Map<String /* topic */, String /* sub expression */> subscription = new HashMap<String, String>();
//消息业务监听器
private MessageListener messageListener;
//消息消费进度存储器
private OffsetStore offsetStore;
//消费者最小线程数量
private int consumeThreadMin = 20;
//消费者最大线程数量
private int consumeThreadMax = 20;
//并发消息消费时处理队列最大跨度
private int consumeConcurrentlyMaxSpan = 2000;
//每1000次流控后打印流控日志
private int pullThresholdForQueue = 1000;
//推模式下任务间隔时间
private long pullInterval = 0;
//推模式下任务拉取的条数,默认32条
private int pullBatchSize = 32;
//每次传入MessageListener#consumerMessage中消息的数量
private int consumeMessageBatchMaxSize = 1;
//是否每次拉取消息都订阅消息
private boolean postSubscriptionWhenPull = false;
//消息重试次数,-1代表16次
private int maxReconsumeTimes = -1;
//消息消费超时时间
private long consumeTimeout = 15;
```

### 2.5.3 消费者启动流程

![](/assets/消息消费启动流程.DY8yZiuM.png)

***代码：DefaultMQPushConsumerImpl#start***

```java
public synchronized void start() throws MQClientException {
    switch (this.serviceState) {
        case CREATE_JUST:
            
                this.defaultMQPushConsumer.getMessageModel(), this.defaultMQPushConsumer.isUnitMode());
            this.serviceState = ServiceState.START_FAILED;
			//检查消息者是否合法
            this.checkConfig();
			//构建主题订阅信息
            this.copySubscription();
			//设置消费者客户端实例名称为进程ID
            if (this.defaultMQPushConsumer.getMessageModel() == MessageModel.CLUSTERING) {
                this.defaultMQPushConsumer.changeInstanceNameToPID();
            }
			//创建MQClient实例
            this.mQClientFactory = MQClientManager.getInstance().getAndCreateMQClientInstance(this.defaultMQPushConsumer, this.rpcHook);
			//构建rebalanceImpl
            this.rebalanceImpl.setConsumerGroup(this.defaultMQPushConsumer.getConsumerGroup());
            this.rebalanceImpl.setMessageModel(this.defaultMQPushConsumer.getMessageModel());
            this.rebalanceImpl.setAllocateMessageQueueStrategy(this.defaultMQPushConsumer.getAllocateMessageQueueStrategy());
            this.rebalanceImpl.setmQClientFactory(this.mQClientFactor
            this.pullAPIWrapper = new PullAPIWrapper(
                mQClientFactory,
                this.defaultMQPushConsumer.getConsumerGroup(), isUnitMode());
            this.pullAPIWrapper.registerFilterMessageHook(filterMessageHookLis
            if (this.defaultMQPushConsumer.getOffsetStore() != null) {
                this.offsetStore = this.defaultMQPushConsumer.getOffsetStore();
            } else {
           		switch (this.defaultMQPushConsumer.getMessageModel()) {
               
           	    case BROADCASTING:	 //消息消费广播模式,将消费进度保存在本地
           	        this.offsetStore = new LocalFileOffsetStore(this.mQClientFactory, this.defaultMQPushConsumer.getConsumerGroup());
           	            break;
           	        case CLUSTERING:	//消息消费集群模式,将消费进度保存在远端Broker
           	            this.offsetStore = new RemoteBrokerOffsetStore(this.mQClientFactory, this.defaultMQPushConsumer.getConsumerGroup());
           	            break;
           	        default:
           	            break;
           	    }
           	    this.defaultMQPushConsumer.setOffsetStore(this.offsetStore);
           	}
            this.offsetStore.load
            //创建顺序消息消费服务
            if (this.getMessageListenerInner() instanceof MessageListenerOrderly) {
                this.consumeOrderly = true;
                this.consumeMessageService =
                    new ConsumeMessageOrderlyService(this, (MessageListenerOrderly) this.getMessageListenerInner());
                //创建并发消息消费服务
            } else if (this.getMessageListenerInner() instanceof MessageListenerConcurrently) {
                this.consumeOrderly = false;
                this.consumeMessageService =
                    new ConsumeMessageConcurrentlyService(this, (MessageListenerConcurrently) this.getMessageListenerInner());
            }
            //消息消费服务启动
            this.consumeMessageService.start();
            //注册消费者实例
            boolean registerOK = mQClientFactory.registerConsumer(this.defaultMQPushConsumer.getConsumerGroup(), this);
            
            if (!registerOK) {
                this.serviceState = ServiceState.CREATE_JUST;
                this.consumeMessageService.shutdown();
                throw new MQClientException("The consumer group[" + this.defaultMQPushConsumer.getConsumerGroup()
                    + "] has been created before, specify another name please." + FAQUrl.suggestTodo(FAQUrl.GROUP_NAME_DUPLICATE_URL),
                    null);
            //启动消费者客户端
            mQClientFactory.start();
            log.info("the consumer [{}] start OK.", this.defaultMQPushConsumer.getConsumerGroup());
            this.serviceState = ServiceState.RUNNING;
            break;
            case RUNNING:
            case START_FAILED:
        case SHUTDOWN_ALREADY:
            throw new MQClientException("The PushConsumer service state not OK, maybe started once, "
                + this.serviceState
                + FAQUrl.suggestTodo(FAQUrl.CLIENT_SERVICE_NOT_OK),
                null);
        default:
            break;
    }

    this.updateTopicSubscribeInfoWhenSubscriptionChanged();
    this.mQClientFactory.checkClientInBroker();
    this.mQClientFactory.sendHeartbeatToAllBrokerWithLock();
    this.mQClientFactory.rebalanceImmediately();
}
```

### 2.5.4 消息拉取

消息消费模式有两种模式：广播模式与集群模式。广播模式比较简单，每一个消费者需要拉取订阅主题下所有队列的消息。本文重点讲解集群模式。在集群模式下，同一个消费者组内有多个消息消费者，同一个主题存在多个消费队列，消费者通过负载均衡的方式消费消息。

消息队列负载均衡，通常的作法是一个消息队列在同一个时间只允许被一个消费消费者消费，一个消息消费者可以同时消费多个消息队列。

#### 1）PullMessageService实现机制

从MQClientInstance的启动流程中可以看出，RocketMQ使用一个单独的线程PullMessageService来负责消息的拉取。

![](/assets/pullMessageService实现机制.nde9t8jw.png)

***代码：PullMessageService#run***

```java
public void run() {
    log.info(this.getServiceName() + " service started");
	//循环拉取消息
    while (!this.isStopped()) {
        try {
            //从请求队列中获取拉取消息请求
            PullRequest pullRequest = this.pullRequestQueue.take();
            //拉取消息
            this.pullMessage(pullRequest);
        } catch (InterruptedException ignored) {
        } catch (Exception e) {
            log.error("Pull Message Service Run Method exception", e);
        }
    }

    log.info(this.getServiceName() + " service end");
}
```

**PullRequest**

![](/assets/PullRequest.DyV1Y_fa.png)

```java
private String consumerGroup;	//消费者组
private MessageQueue messageQueue;	//待拉取消息队列
private ProcessQueue processQueue;	//消息处理队列
private long nextOffset;	//待拉取的MessageQueue偏移量
private boolean lockedFirst = false;	//是否被锁定
```

***代码：PullMessageService#pullMessage***

```java
private void pullMessage(final PullRequest pullRequest) {
    //获得消费者实例
    final MQConsumerInner consumer = this.mQClientFactory.selectConsumer(pullRequest.getConsumerGroup());
    if (consumer != null) {
        //强转为推送模式消费者
        DefaultMQPushConsumerImpl impl = (DefaultMQPushConsumerImpl) consumer;
        //推送消息
        impl.pullMessage(pullRequest);
    } else {
        log.warn("No matched consumer for the PullRequest {}, drop it", pullRequest);
    }
}
```

\####2）ProcessQueue实现机制

ProcessQueue是MessageQueue在消费端的重现、快照。PullMessageService从消息服务器默认每次拉取32条消息，按照消息的队列偏移量顺序存放在ProcessQueue中，PullMessageService然后将消息提交到消费者消费线程池，消息成功消费后从ProcessQueue中移除。

![](/assets/ProcessQueue.DunASEcS.png)

**属性**

```java
//消息容器
private final TreeMap<Long, MessageExt> msgTreeMap = new TreeMap<Long, MessageExt>();
//读写锁
private final ReadWriteLock lockTreeMap = new ReentrantReadWriteLock();
//ProcessQueue总消息树
private final AtomicLong msgCount = new AtomicLong();
//ProcessQueue队列最大偏移量
private volatile long queueOffsetMax = 0L;
//当前ProcessQueue是否被丢弃
private volatile boolean dropped = false;
//上一次拉取时间戳
private volatile long lastPullTimestamp = System.currentTimeMillis();
//上一次消费时间戳
private volatile long lastConsumeTimestamp = System.currentTimeMillis();
```

**方法**

```java
//移除消费超时消息
public void cleanExpiredMsg(DefaultMQPushConsumer pushConsumer)
//添加消息
public boolean putMessage(final List<MessageExt> msgs)
//获取消息最大间隔
public long getMaxSpan()
//移除消息
public long removeMessage(final List<MessageExt> msgs)
//将consumingMsgOrderlyTreeMap中消息重新放在msgTreeMap,并清空consumingMsgOrderlyTreeMap   
public void rollback() 
//将consumingMsgOrderlyTreeMap消息清除,表示成功处理该批消息
public long commit()
//重新处理该批消息
public void makeMessageToCosumeAgain(List<MessageExt> msgs) 
//从processQueue中取出batchSize条消息
public List<MessageExt> takeMessags(final int batchSize)
```

#### 3）消息拉取基本流程

\#####1.客户端发起拉取请求

![](/assets/消息拉取基本流程.CBTJ1Tal.png)

***代码：DefaultMQPushConsumerImpl#pullMessage***

```java
public void pullMessage(final PullRequest pullRequest) {
    //从pullRequest获得ProcessQueue
    final ProcessQueue processQueue = pullRequest.getProcessQueue();
    //如果处理队列被丢弃,直接返回
    if (processQueue.isDropped()) {
        log.info("the pull request[{}] is dropped.", pullRequest.toString());
        return;
    }
	//如果处理队列未被丢弃,更新时间戳
    pullRequest.getProcessQueue().setLastPullTimestamp(System.currentTimeMillis());

    try {
        this.makeSureStateOK();
    } catch (MQClientException e) {
        log.warn("pullMessage exception, consumer state not ok", e);
        this.executePullRequestLater(pullRequest, PULL_TIME_DELAY_MILLS_WHEN_EXCEPTION);
        return;
    }
	//如果处理队列被挂起,延迟1s后再执行
    if (this.isPause()) {
        log.warn("consumer was paused, execute pull request later. instanceName={}, group={}", this.defaultMQPushConsumer.getInstanceName(), this.defaultMQPushConsumer.getConsumerGroup());
        this.executePullRequestLater(pullRequest, PULL_TIME_DELAY_MILLS_WHEN_SUSPEND);
        return;
    }
	//获得最大待处理消息数量
	long cachedMessageCount = processQueue.getMsgCount().get();
    //获得最大待处理消息大小
	long cachedMessageSizeInMiB = processQueue.getMsgSize().get() / (1024 * 1024);
	//从数量进行流控
	if (cachedMessageCount > this.defaultMQPushConsumer.getPullThresholdForQueue()) {
	    this.executePullRequestLater(pullRequest, PULL_TIME_DELAY_MILLS_WHEN_FLOW_CONTROL);
	    if ((queueFlowControlTimes++ % 1000) == 0) {
	        log.warn(
	            "the cached message count exceeds the threshold {}, so do flow control, minOffset={}, maxOffset={}, count={}, size={} MiB, pullRequest={}, flowControlTimes={}",
	            this.defaultMQPushConsumer.getPullThresholdForQueue(), processQueue.getMsgTreeMap().firstKey(), processQueue.getMsgTreeMap().lastKey(), cachedMessageCount, cachedMessageSizeInMiB, pullRequest, queueFlowControlTimes);
	    }
	    return;
	}
	//从消息大小进行流控
	if (cachedMessageSizeInMiB > this.defaultMQPushConsumer.getPullThresholdSizeForQueue()) {
	    this.executePullRequestLater(pullRequest, PULL_TIME_DELAY_MILLS_WHEN_FLOW_CONTROL);
	    if ((queueFlowControlTimes++ % 1000) == 0) {
	        log.warn(
	            "the cached message size exceeds the threshold {} MiB, so do flow control, minOffset={}, maxOffset={}, count={}, size={} MiB, pullRequest={}, flowControlTimes={}",
	            this.defaultMQPushConsumer.getPullThresholdSizeForQueue(), processQueue.getMsgTreeMap().firstKey(), processQueue.getMsgTreeMap().lastKey(), cachedMessageCount, cachedMessageSizeInMiB, pullRequest, queueFlowControlTimes);
	    }
	    return;
    }
    	//获得订阅信息
		 final SubscriptionData subscriptionData = this.rebalanceImpl.getSubscriptionInner().get(pullRequest.getMessageQueue().getTopic());
    	if (null == subscriptionData) {
    	    this.executePullRequestLater(pullRequest, PULL_TIME_DELAY_MILLS_WHEN_EXCEPTION);
    	    log.warn("find the consumer's subscription failed, {}", pullRequest);
    	    return;
		//与服务端交互,获取消息
	    this.pullAPIWrapper.pullKernelImpl(
	    pullRequest.getMessageQueue(),
	    subExpression,
	    subscriptionData.getExpressionType(),
	    subscriptionData.getSubVersion(),
	    pullRequest.getNextOffset(),
	    this.defaultMQPushConsumer.getPullBatchSize(),
	    sysFlag,
	    commitOffsetValue,
	    BROKER_SUSPEND_MAX_TIME_MILLIS,
	    CONSUMER_TIMEOUT_MILLIS_WHEN_SUSPEND,
	    CommunicationMode.ASYNC,
	    pullCallback
	);
            
}
```

\#####2.消息服务端Broker组装消息

![](/assets/消息服务端Broker组装消息.9dAIusDx.png)

***代码：PullMessageProcessor#processRequest***

```java
//构建消息过滤器
MessageFilter messageFilter;
if (this.brokerController.getBrokerConfig().isFilterSupportRetry()) {
    messageFilter = new ExpressionForRetryMessageFilter(subscriptionData, consumerFilterData,
        this.brokerController.getConsumerFilterManager());
} else {
    messageFilter = new ExpressionMessageFilter(subscriptionData, consumerFilterData,
        this.brokerController.getConsumerFilterManager());
}
//调用MessageStore.getMessage查找消息
final GetMessageResult getMessageResult =
    this.brokerController.getMessageStore().getMessage(
    				requestHeader.getConsumerGroup(), //消费组名称								
    				requestHeader.getTopic(),	//主题名称
        			requestHeader.getQueueId(), //队列ID
    				requestHeader.getQueueOffset(), 	//待拉取偏移量
    				requestHeader.getMaxMsgNums(), 	//最大拉取消息条数
    				messageFilter	//消息过滤器
    		);
```

***代码：DefaultMessageStore#getMessage***

```java
GetMessageStatus status = GetMessageStatus.NO_MESSAGE_IN_QUEUE;
long nextBeginOffset = offset;	//查找下一次队列偏移量
long minOffset = 0;		//当前消息队列最小偏移量
long maxOffset = 0;		//当前消息队列最大偏移量
GetMessageResult getResult = new GetMessageResult();
final long maxOffsetPy = this.commitLog.getMaxOffset();	//当前commitLog最大偏移量
//根据主题名称和队列编号获取消息消费队列
ConsumeQueue consumeQueue = findConsumeQueue(topic, queueId);

...
minOffset = consumeQueue.getMinOffsetInQueue();
maxOffset = consumeQueue.getMaxOffsetInQueue();
//消息偏移量异常情况校对下一次拉取偏移量
if (maxOffset == 0) {	//表示当前消息队列中没有消息
    status = GetMessageStatus.NO_MESSAGE_IN_QUEUE;
    nextBeginOffset = nextOffsetCorrection(offset, 0);
} else if (offset < minOffset) {	//待拉取消息的偏移量小于队列的其实偏移量
    status = GetMessageStatus.OFFSET_TOO_SMALL;
    nextBeginOffset = nextOffsetCorrection(offset, minOffset);
} else if (offset == maxOffset) {	//待拉取偏移量为队列最大偏移量
    status = GetMessageStatus.OFFSET_OVERFLOW_ONE;
    nextBeginOffset = nextOffsetCorrection(offset, offset);
} else if (offset > maxOffset) {	//偏移量越界
    status = GetMessageStatus.OFFSET_OVERFLOW_BADLY;
    if (0 == minOffset) {
        nextBeginOffset = nextOffsetCorrection(offset, minOffset);
    } else {
        nextBeginOffset = nextOffsetCorrection(offset, maxOffset);
    }
}
...
//根据偏移量从CommitLog中拉取32条消息
SelectMappedBufferResult selectResult = this.commitLog.getMessage(offsetPy, sizePy);
```

***代码：PullMessageProcessor#processRequest***

```java
//根据拉取结果填充responseHeader
response.setRemark(getMessageResult.getStatus().name());
responseHeader.setNextBeginOffset(getMessageResult.getNextBeginOffset());
responseHeader.setMinOffset(getMessageResult.getMinOffset());
responseHeader.setMaxOffset(getMessageResult.getMaxOffset());

//判断如果存在主从同步慢,设置下一次拉取任务的ID为主节点
switch (this.brokerController.getMessageStoreConfig().getBrokerRole()) {
    case ASYNC_MASTER:
    case SYNC_MASTER:
        break;
    case SLAVE:
        if (!this.brokerController.getBrokerConfig().isSlaveReadEnable()) {
            response.setCode(ResponseCode.PULL_RETRY_IMMEDIATELY);
            responseHeader.setSuggestWhichBrokerId(MixAll.MASTER_ID);
        }
        break;
}
...
//GetMessageResult与Response的Code转换
switch (getMessageResult.getStatus()) {
    case FOUND:			//成功
        response.setCode(ResponseCode.SUCCESS);
        break;
    case MESSAGE_WAS_REMOVING:	//消息存放在下一个commitLog中
        response.setCode(ResponseCode.PULL_RETRY_IMMEDIATELY);	//消息重试
        break;
    case NO_MATCHED_LOGIC_QUEUE:	//未找到队列
    case NO_MESSAGE_IN_QUEUE:	//队列中未包含消息
        if (0 != requestHeader.getQueueOffset()) {
            response.setCode(ResponseCode.PULL_OFFSET_MOVED);
            requestHeader.getQueueOffset(),
            getMessageResult.getNextBeginOffset(),
            requestHeader.getTopic(),
            requestHeader.getQueueId(),
            requestHeader.getConsumerGroup()
            );
        } else {
            response.setCode(ResponseCode.PULL_NOT_FOUND);
        }
        break;
    case NO_MATCHED_MESSAGE:	//未找到消息
        response.setCode(ResponseCode.PULL_RETRY_IMMEDIATELY);
        break;
    case OFFSET_FOUND_NULL:	//消息物理偏移量为空
        response.setCode(ResponseCode.PULL_NOT_FOUND);
        break;
    case OFFSET_OVERFLOW_BADLY:	//offset越界
        response.setCode(ResponseCode.PULL_OFFSET_MOVED);
        // XXX: warn and notify me
        log.info("the request offset: {} over flow badly, broker max offset: {}, consumer: {}",
                requestHeader.getQueueOffset(), getMessageResult.getMaxOffset(), channel.remoteAddress());
        break;
    case OFFSET_OVERFLOW_ONE:	//offset在队列中未找到
        response.setCode(ResponseCode.PULL_NOT_FOUND);
        break;
    case OFFSET_TOO_SMALL:	//offset未在队列中
        response.setCode(ResponseCode.PULL_OFFSET_MOVED);
        requestHeader.getConsumerGroup(), 
        requestHeader.getTopic(), 
        requestHeader.getQueueOffset(),
        getMessageResult.getMinOffset(), channel.remoteAddress());
        break;
    default:
        assert false;
        break;
}
...
//如果CommitLog标记可用,并且当前Broker为主节点,则更新消息消费进度
boolean storeOffsetEnable = brokerAllowSuspend;
storeOffsetEnable = storeOffsetEnable && hasCommitOffsetFlag;
storeOffsetEnable = storeOffsetEnable
    && this.brokerController.getMessageStoreConfig().getBrokerRole() != BrokerRole.SLAVE;
if (storeOffsetEnable) {
    this.brokerController.getConsumerOffsetManager().commitOffset(RemotingHelper.parseChannelRemoteAddr(channel),
        requestHeader.getConsumerGroup(), requestHeader.getTopic(), requestHeader.getQueueId(), requestHeader.getCommitOffset());
}
```

\#####3.消息拉取客户端处理消息

![](/assets/消息拉取客户端处理消息.sw_yk6TG.png)

***代码：MQClientAPIImpl#processPullResponse***

```java
private PullResult processPullResponse(
    final RemotingCommand response) throws MQBrokerException, RemotingCommandException {
    PullStatus pullStatus = PullStatus.NO_NEW_MSG;
   	//判断响应结果
    switch (response.getCode()) {
        case ResponseCode.SUCCESS:
            pullStatus = PullStatus.FOUND;
            break;
        case ResponseCode.PULL_NOT_FOUND:
            pullStatus = PullStatus.NO_NEW_MSG;
            break;
        case ResponseCode.PULL_RETRY_IMMEDIATELY:
            pullStatus = PullStatus.NO_MATCHED_MSG;
            break;
        case ResponseCode.PULL_OFFSET_MOVED:
            pullStatus = PullStatus.OFFSET_ILLEGAL;
            break;

        default:
            throw new MQBrokerException(response.getCode(), response.getRemark());
    }
	//解码响应头
    PullMessageResponseHeader responseHeader =
        (PullMessageResponseHeader) response.decodeCommandCustomHeader(PullMessageResponseHeader.class);
	//封装PullResultExt返回
    return new PullResultExt(pullStatus, responseHeader.getNextBeginOffset(), responseHeader.getMinOffset(),
        responseHeader.getMaxOffset(), null, responseHeader.getSuggestWhichBrokerId(), response.getBody());
}
```

**PullResult类**

```java
private final PullStatus pullStatus;	//拉取结果
private final long nextBeginOffset;	//下次拉取偏移量
private final long minOffset;	//消息队列最小偏移量
private final long maxOffset;	//消息队列最大偏移量
private List<MessageExt> msgFoundList;	//拉取的消息列表
```

![](/assets/PullStatus.BfyZ0EVL.png)

***代码：DefaultMQPushConsumerImpl$PullCallback#OnSuccess***

```java
//将拉取到的消息存入processQueue
boolean dispatchToConsume = processQueue.putMessage(pullResult.getMsgFoundList());
//将processQueue提交到consumeMessageService中供消费者消费
DefaultMQPushConsumerImpl.this.consumeMessageService.submitConsumeRequest(
    pullResult.getMsgFoundList(),
    processQueue,
    pullRequest.getMessageQueue(),
    dispatchToConsume);
//如果pullInterval大于0,则等待pullInterval毫秒后将pullRequest对象放入到PullMessageService中的pullRequestQueue队列中
if (DefaultMQPushConsumerImpl.this.defaultMQPushConsumer.getPullInterval() > 0) {
    DefaultMQPushConsumerImpl.this.executePullRequestLater(pullRequest,
        DefaultMQPushConsumerImpl.this.defaultMQPushConsumer.getPullInterval());
} else {
    DefaultMQPushConsumerImpl.this.executePullRequestImmediately(pullRequest);
}
```

##### 4.消息拉取总结

![](/assets/消息拉取流程总结.CUuhols4.png)

#### 4）消息拉取长轮询机制分析

RocketMQ未真正实现消息推模式，而是消费者主动向消息服务器拉取消息，RocketMQ推模式是循环向消息服务端发起消息拉取请求，如果消息消费者向RocketMQ拉取消息时，消息未到达消费队列时，如果不启用长轮询机制，则会在服务端等待shortPollingTimeMills时间后（挂起）再去判断消息是否已经到达指定消息队列，如果消息仍未到达则提示拉取消息客户端PULL—NOT—FOUND（消息不存在）；如果开启长轮询模式，RocketMQ一方面会每隔5s轮询检查一次消息是否可达，同时一有消息达到后立马通知挂起线程再次验证消息是否是自己感兴趣的消息，如果是则从CommitLog文件中提取消息返回给消息拉取客户端，否则直到挂起超时，超时时间由消息拉取方在消息拉取是封装在请求参数中，PUSH模式为15s，PULL模式通过DefaultMQPullConsumer#setBrokerSuspendMaxTimeMillis设置。RocketMQ通过在Broker客户端配置longPollingEnable为true来开启长轮询模式。

***代码：PullMessageProcessor#processRequest***

```java
//当没有拉取到消息时，通过长轮询方式继续拉取消息
case ResponseCode.PULL_NOT_FOUND:
    if (brokerAllowSuspend && hasSuspendFlag) {
        long pollingTimeMills = suspendTimeoutMillisLong;
        if (!this.brokerController.getBrokerConfig().isLongPollingEnable()) {
            pollingTimeMills = this.brokerController.getBrokerConfig().getShortPollingTimeMills();
        }

        String topic = requestHeader.getTopic();
        long offset = requestHeader.getQueueOffset();
        int queueId = requestHeader.getQueueId();
        //构建拉取请求对象
        PullRequest pullRequest = new PullRequest(request, channel, pollingTimeMills,
            this.brokerController.getMessageStore().now(), offset, subscriptionData, messageFilter);
        //处理拉取请求
        this.brokerController.getPullRequestHoldService().suspendPullRequest(topic, queueId, pullRequest);
        response = null;
        break;
    }
```

**PullRequestHoldService方式实现长轮询**

***代码：PullRequestHoldService#suspendPullRequest***

```java
//将拉取消息请求，放置在ManyPullRequest集合中
public void suspendPullRequest(final String topic, final int queueId, final PullRequest pullRequest) {
    String key = this.buildKey(topic, queueId);
    ManyPullRequest mpr = this.pullRequestTable.get(key);
    if (null == mpr) {
        mpr = new ManyPullRequest();
        ManyPullRequest prev = this.pullRequestTable.putIfAbsent(key, mpr);
        if (prev != null) {
            mpr = prev;
        }
    }

    mpr.addPullRequest(pullRequest);
}
```

***代码：PullRequestHoldService#run***

```java
public void run() {
    log.info("{} service started", this.getServiceName());
    while (!this.isStopped()) {
        try {
            //如果开启长轮询每隔5秒判断消息是否到达
            if (this.brokerController.getBrokerConfig().isLongPollingEnable()) {
                this.waitForRunning(5 * 1000);
            } else {
                //没有开启长轮询,每隔1s再次尝试
              this.waitForRunning(this.brokerController.getBrokerConfig().getShortPollingTimeMills());
            }

            long beginLockTimestamp = this.systemClock.now();
            this.checkHoldRequest();
            long costTime = this.systemClock.now() - beginLockTimestamp;
            if (costTime > 5 * 1000) {
                log.info("[NOTIFYME] check hold request cost {} ms.", costTime);
            }
        } catch (Throwable e) {
            log.warn(this.getServiceName() + " service has exception. ", e);
        }
    }

    log.info("{} service end", this.getServiceName());
}
```

***代码：PullRequestHoldService#checkHoldRequest***

```java
//遍历拉取任务
private void checkHoldRequest() {
    for (String key : this.pullRequestTable.keySet()) {
        String[] kArray = key.split(TOPIC_QUEUEID_SEPARATOR);
        if (2 == kArray.length) {
            String topic = kArray[0];
            int queueId = Integer.parseInt(kArray[1]);
            //获得消息偏移量
            final long offset = this.brokerController.getMessageStore().getMaxOffsetInQueue(topic, queueId);
            try {
                //通知有消息达到
                this.notifyMessageArriving(topic, queueId, offset);
            } catch (Throwable e) {
                log.error("check hold request failed. topic={}, queueId={}", topic, queueId, e);
            }
        }
    }
}
```

***代码：PullRequestHoldService#notifyMessageArriving***

```java
//如果拉取消息偏移大于请求偏移量,如果消息匹配调用executeRequestWhenWakeup处理消息
if (newestOffset > request.getPullFromThisOffset()) {
    boolean match = request.getMessageFilter().isMatchedByConsumeQueue(tagsCode,
        new ConsumeQueueExt.CqExtUnit(tagsCode, msgStoreTime, filterBitMap));
    // match by bit map, need eval again when properties is not null.
    if (match && properties != null) {
        match = request.getMessageFilter().isMatchedByCommitLog(null, properties);
    }

    if (match) {
        try {
            this.brokerController.getPullMessageProcessor().executeRequestWhenWakeup(request.getClientChannel(),
                request.getRequestCommand());
        } catch (Throwable e) {
            log.error("execute request when wakeup failed.", e);
        }
        continue;
    }
}
//如果过期时间超时,则不继续等待将直接返回给客户端消息未找到
if (System.currentTimeMillis() >= (request.getSuspendTimestamp() + request.getTimeoutMillis())) {
    try {
        this.brokerController.getPullMessageProcessor().executeRequestWhenWakeup(request.getClientChannel(),
            request.getRequestCommand());
    } catch (Throwable e) {
        log.error("execute request when wakeup failed.", e);
    }
    continue;
}
```

如果开启了长轮询机制，PullRequestHoldService会每隔5s被唤醒去尝试检测是否有新的消息的到来才给客户端响应，或者直到超时才给客户端进行响应，消息实时性比较差，为了避免这种情况，RocketMQ引入另外一种机制：当消息到达时唤醒挂起线程触发一次检查。

**DefaultMessageStore$ReputMessageService机制**

***代码：DefaultMessageStore#start***

```java
//长轮询入口
this.reputMessageService.setReputFromOffset(maxPhysicalPosInLogicQueue);
this.reputMessageService.start();
```

***代码：DefaultMessageStore$ReputMessageService#run***

```java
public void run() {
    DefaultMessageStore.log.info(this.getServiceName() + " service started");

    while (!this.isStopped()) {
        try {
            Thread.sleep(1);
            //长轮询核心逻辑代码入口
            this.doReput();
        } catch (Exception e) {
            DefaultMessageStore.log.warn(this.getServiceName() + " service has exception. ", e);
        }
    }

    DefaultMessageStore.log.info(this.getServiceName() + " service end");
}
```

***代码：DefaultMessageStore$ReputMessageService#deReput***

```java
//当新消息达到是,进行通知监听器进行处理
if (BrokerRole.SLAVE != DefaultMessageStore.this.getMessageStoreConfig().getBrokerRole()
    && DefaultMessageStore.this.brokerConfig.isLongPollingEnable()) {
    DefaultMessageStore.this.messageArrivingListener.arriving(dispatchRequest.getTopic(),
        dispatchRequest.getQueueId(), dispatchRequest.getConsumeQueueOffset() + 1,
        dispatchRequest.getTagsCode(), dispatchRequest.getStoreTimestamp(),
        dispatchRequest.getBitMap(), dispatchRequest.getPropertiesMap());
}
```

***代码：NotifyMessageArrivingListener#arriving***

```java
public void arriving(String topic, int queueId, long logicOffset, long tagsCode,
    long msgStoreTime, byte[] filterBitMap, Map<String, String> properties) {
    this.pullRequestHoldService.notifyMessageArriving(topic, queueId, logicOffset, tagsCode,
        msgStoreTime, filterBitMap, properties);
}
```

### 2.5.5 消息队列负载与重新分布机制

RocketMQ消息队列重新分配是由RebalanceService线程来实现。一个MQClientInstance持有一个RebalanceService实现，并随着MQClientInstance的启动而启动。

***代码：RebalanceService#run***

```java
public void run() {
    log.info(this.getServiceName() + " service started");
	//RebalanceService线程默认每隔20s执行一次mqClientFactory.doRebalance方法
    while (!this.isStopped()) {
        this.waitForRunning(waitInterval);
        this.mqClientFactory.doRebalance();
    }

    log.info(this.getServiceName() + " service end");
}
```

***代码：MQClientInstance#doRebalance***

```java
public void doRebalance() {
    //MQClientInstance遍历以注册的消费者,对消费者执行doRebalance()方法
    for (Map.Entry<String, MQConsumerInner> entry : this.consumerTable.entrySet()) {
        MQConsumerInner impl = entry.getValue();
        if (impl != null) {
            try {
                impl.doRebalance();
            } catch (Throwable e) {
                log.error("doRebalance exception", e);
            }
        }
    }
}
```

***代码：RebalanceImpl#doRebalance***

```java
//遍历订阅消息对每个主题的订阅的队列进行重新负载
public void doRebalance(final boolean isOrder) {
    Map<String, SubscriptionData> subTable = this.getSubscriptionInner();
    if (subTable != null) {
        for (final Map.Entry<String, SubscriptionData> entry : subTable.entrySet()) {
            final String topic = entry.getKey();
            try {
                this.rebalanceByTopic(topic, isOrder);
            } catch (Throwable e) {
                if (!topic.startsWith(MixAll.RETRY_GROUP_TOPIC_PREFIX)) {
                    log.warn("rebalanceByTopic Exception", e);
                }
            }
        }
    }

    this.truncateMessageQueueNotMyTopic();
}
```

***代码：RebalanceImpl#rebalanceByTopic***

```java
//从主题订阅消息缓存表中获取主题的队列信息
Set<MessageQueue> mqSet = this.topicSubscribeInfoTable.get(topic);
//查找该主题订阅组所有的消费者ID
List<String> cidAll = this.mQClientFactory.findConsumerIdList(topic, consumerGroup);

//给消费者重新分配队列
if (mqSet != null && cidAll != null) {
    List<MessageQueue> mqAll = new ArrayList<MessageQueue>();
    mqAll.addAll(mqSet);

    Collections.sort(mqAll);
    Collections.sort(cidAll);

    AllocateMessageQueueStrategy strategy = this.allocateMessageQueueStrategy;

    List<MessageQueue> allocateResult = null;
    try {
        allocateResult = strategy.allocate(
            this.consumerGroup,
            this.mQClientFactory.getClientId(),
            mqAll,
            cidAll);
    } catch (Throwable e) {
        log.error("AllocateMessageQueueStrategy.allocate Exception. allocateMessageQueueStrategyName={}", strategy.getName(),
            e);
        return;
    }
```

RocketMQ默认提供5中负载均衡分配算法

```java
AllocateMessageQueueAveragely:平均分配
举例:8个队列q1,q2,q3,q4,q5,a6,q7,q8,消费者3个:c1,c2,c3
分配如下:
c1:q1,q2,q3
c2:q4,q5,a6
c3:q7,q8
AllocateMessageQueueAveragelyByCircle:平均轮询分配
举例:8个队列q1,q2,q3,q4,q5,a6,q7,q8,消费者3个:c1,c2,c3
分配如下:
c1:q1,q4,q7
c2:q2,q5,a8
c3:q3,q6
```

注意：消息队列的分配遵循一个消费者可以分配到多个队列，但同一个消息队列只会分配给一个消费者，故如果出现消费者个数大于消息队列数量，则有些消费者无法消费消息。

### 2.5.6 消息消费过程

PullMessageService负责对消息队列进行消息拉取，从远端服务器拉取消息后将消息存储ProcessQueue消息队列处理队列中，然后调用ConsumeMessageService#submitConsumeRequest方法进行消息消费，使用线程池来消费消息，确保了消息拉取与消息消费的解耦。ConsumeMessageService支持顺序消息和并发消息，核心类图如下：

![](/assets/ConsumeMessageService.Dx3kGWJU.png)

**并发消息消费**

***代码：ConsumeMessageConcurrentlyService#submitConsumeRequest***

```java
//消息批次单次
final int consumeBatchSize = this.defaultMQPushConsumer.getConsumeMessageBatchMaxSize();
//msgs.size()默认最多为32条。
//如果msgs.size()小于consumeBatchSize,则直接将拉取到的消息放入到consumeRequest,然后将consumeRequest提交到消费者线程池中
if (msgs.size() <= consumeBatchSize) {
    ConsumeRequest consumeRequest = new ConsumeRequest(msgs, processQueue, messageQueue);
    try {
        this.consumeExecutor.submit(consumeRequest);
    } catch (RejectedExecutionException e) {
        this.submitConsumeRequestLater(consumeRequest);
    }
}else{	//如果拉取的消息条数大于consumeBatchSize,则对拉取消息进行分页
       for (int total = 0; total < msgs.size(); ) {
   		    List<MessageExt> msgThis = new ArrayList<MessageExt>(consumeBatchSize);
   		    for (int i = 0; i < consumeBatchSize; i++, total++) {
   		        if (total < msgs.size()) {
   		            msgThis.add(msgs.get(total));
   		        } else {
   		            break;
   		        }
   		
   		    ConsumeRequest consumeRequest = new ConsumeRequest(msgThis, processQueue, messageQueue);
   		    try {
   		        this.consumeExecutor.submit(consumeRequest);
   		    } catch (RejectedExecutionException e) {
   		        for (; total < msgs.size(); total++) {
   		            msgThis.add(msgs.get(total));
   		 
   		        this.submitConsumeRequestLater(consumeRequest);
   		    }
   		}
}
```

***代码：ConsumeMessageConcurrentlyService$ConsumeRequest#run***

```java
//检查processQueue的dropped,如果为true,则停止该队列消费。
if (this.processQueue.isDropped()) {
    log.info("the message queue not be able to consume, because it's dropped. group={} {}", ConsumeMessageConcurrentlyService.this.consumerGroup, this.messageQueue);
    return;
}

...
//执行消息处理的钩子函数
if (ConsumeMessageConcurrentlyService.this.defaultMQPushConsumerImpl.hasHook()) {
    consumeMessageContext = new ConsumeMessageContext();
    consumeMessageContext.setNamespace(defaultMQPushConsumer.getNamespace());
    consumeMessageContext.setConsumerGroup(defaultMQPushConsumer.getConsumerGroup());
    consumeMessageContext.setProps(new HashMap<String, String>());
    consumeMessageContext.setMq(messageQueue);
    consumeMessageContext.setMsgList(msgs);
    consumeMessageContext.setSuccess(false);
    ConsumeMessageConcurrentlyService.this.defaultMQPushConsumerImpl.executeHookBefore(consumeMessageContext);
}
...
//调用应用程序消息监听器的consumeMessage方法,进入到具体的消息消费业务处理逻辑
status = listener.consumeMessage(Collections.unmodifiableList(msgs), context);

//执行消息处理后的钩子函数
if (ConsumeMessageConcurrentlyService.this.defaultMQPushConsumerImpl.hasHook()) {
    consumeMessageContext.setStatus(status.toString());
    consumeMessageContext.setSuccess(ConsumeConcurrentlyStatus.CONSUME_SUCCESS == status);
    ConsumeMessageConcurrentlyService.this.defaultMQPushConsumerImpl.executeHookAfter(consumeMessageContext);
}
```

### 2.5.7 定时消息机制

定时消息是消息发送到Broker后，并不立即被消费者消费而是要等到特定的时间后才能被消费，RocketMQ并不支持任意的时间精度，如果要支持任意时间精度定时调度，不可避免地需要在Broker层做消息排序，再加上持久化方面的考量，将不可避免的带来巨大的性能消耗，所以RocketMQ只支持特定级别的延迟消息。消息延迟级别在Broker端通过messageDelayLevel配置，默认为“1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h”，delayLevel=1表示延迟消息1s,delayLevel=2表示延迟5s,依次类推。

RocketMQ定时消息实现类为ScheduleMessageService，该类在DefaultMessageStore中创建。通过在DefaultMessageStore中调用load方法加载该类并调用start方法启动。

***代码：ScheduleMessageService#load***

```java
//加载延迟消息消费进度的加载与delayLevelTable的构造。延迟消息的进度默认存储路径为/store/config/delayOffset.json
public boolean load() {
    boolean result = super.load();
    result = result && this.parseDelayLevel();
    return result;
}
```

***代码：ScheduleMessageService#start***

```java
//遍历延迟队列创建定时任务,遍历延迟级别，根据延迟级别level从offsetTable中获取消费队列的消费进度。如果不存在，则使用0
for (Map.Entry<Integer, Long> entry : this.delayLevelTable.entrySet()) {
    Integer level = entry.getKey();
    Long timeDelay = entry.getValue();
    Long offset = this.offsetTable.get(level);
    if (null == offset) {
        offset = 0L;
    }

    if (timeDelay != null) {
        this.timer.schedule(new DeliverDelayedMessageTimerTask(level, offset), FIRST_DELAY_TIME);
    }
}

//每隔10s持久化一次延迟队列的消息消费进度
this.timer.scheduleAtFixedRate(new TimerTask() {

    @Override
    public void run() {
        try {
            if (started.get()) ScheduleMessageService.this.persist();
        } catch (Throwable e) {
            log.error("scheduleAtFixedRate flush exception", e);
        }
    }
}, 10000, this.defaultMessageStore.getMessageStoreConfig().getFlushDelayOffsetInterval());

```

**调度机制**

ScheduleMessageService的start方法启动后，会为每一个延迟级别创建一个调度任务，每一个延迟级别对应SCHEDULE\_TOPIC\_XXXX主题下的一个消息消费队列。定时调度任务的实现类为DeliverDelayedMessageTimerTask，核心实现方法为executeOnTimeup

***代码：ScheduleMessageService$DeliverDelayedMessageTimerTask#executeOnTimeup***

```java
//根据队列ID与延迟主题查找消息消费队列
ConsumeQueue cq =
    ScheduleMessageService.this.defaultMessageStore.findConsumeQueue(SCHEDULE_TOPIC,
        delayLevel2QueueId(delayLevel));
...
//根据偏移量从消息消费队列中获取当前队列中所有有效的消息
SelectMappedBufferResult bufferCQ = cq.getIndexBuffer(this.offset);

...
//遍历ConsumeQueue,解析消息队列中消息
for (; i < bufferCQ.getSize(); i += ConsumeQueue.CQ_STORE_UNIT_SIZE) {
    long offsetPy = bufferCQ.getByteBuffer().getLong();
    int sizePy = bufferCQ.getByteBuffer().getInt();
    long tagsCode = bufferCQ.getByteBuffer().getLong();

    if (cq.isExtAddr(tagsCode)) {
        if (cq.getExt(tagsCode, cqExtUnit)) {
            tagsCode = cqExtUnit.getTagsCode();
        } else {
            //can't find ext content.So re compute tags code.
            log.error("[BUG] can't find consume queue extend file content!addr={}, offsetPy={}, sizePy={}",
                tagsCode, offsetPy, sizePy);
            long msgStoreTime = defaultMessageStore.getCommitLog().pickupStoreTimestamp(offsetPy, sizePy);
            tagsCode = computeDeliverTimestamp(delayLevel, msgStoreTime);
        }
    }

    long now = System.currentTimeMillis();
    long deliverTimestamp = this.correctDeliverTimestamp(now, tagsCode);
    
    ...
    //根据消息偏移量与消息大小,从CommitLog中查找消息.
  	MessageExt msgExt =
   ScheduleMessageService.this.defaultMessageStore.lookMessageByOffset(
       offsetPy, sizePy);
} 
```

### 2.5.8 顺序消息

顺序消息实现类是org.apache.rocketmq.client.impl.consumer.ConsumeMessageOrderlyService

***代码：ConsumeMessageOrderlyService#start***

```java
public void start() {
    //如果消息模式为集群模式，启动定时任务，默认每隔20s执行一次锁定分配给自己的消息消费队列
    if (MessageModel.CLUSTERING.equals(ConsumeMessageOrderlyService.this.defaultMQPushConsumerImpl.messageModel())) {
        this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() {
            @Override
            public void run() {
                ConsumeMessageOrderlyService.this.lockMQPeriodically();
            }
        }, 1000 * 1, ProcessQueue.REBALANCE_LOCK_INTERVAL, TimeUnit.MILLISECONDS);
    }
}
```

***代码：ConsumeMessageOrderlyService#submitConsumeRequest***

```java
//构建消息任务,并提交消费线程池中
public void submitConsumeRequest(
    final List<MessageExt> msgs,
    final ProcessQueue processQueue,
    final MessageQueue messageQueue,
    final boolean dispathToConsume) {
    if (dispathToConsume) {
        ConsumeRequest consumeRequest = new ConsumeRequest(processQueue, messageQueue);
        this.consumeExecutor.submit(consumeRequest);
    }
}
```

***代码：ConsumeMessageOrderlyService$ConsumeRequest#run***

```java
//如果消息队列为丢弃,则停止本次消费任务
if (this.processQueue.isDropped()) {
    log.warn("run, the message queue not be able to consume, because it's dropped. {}", this.messageQueue);
    return;
}
//从消息队列中获取一个对象。然后消费消息时先申请独占objLock锁。顺序消息一个消息消费队列同一时刻只会被一个消费线程池处理
final Object objLock = messageQueueLock.fetchLockObject(this.messageQueue);
synchronized (objLock) {
	...
}
```

### 2.5.9 小结

RocketMQ消息消费方式分别为集群模式、广播模式。

消息队列负载由RebalanceService线程默认每隔20s进行一次消息队列负载，根据当前消费者组内消费者个数与主题队列数量按照某一种负载算法进行队列分配，分配原则为同一个消费者可以分配多个消息消费队列，同一个消息消费队列同一个时间只会分配给一个消费者。

消息拉取由PullMessageService线程根据RebalanceService线程创建的拉取任务进行拉取，默认每次拉取32条消息，提交给消费者消费线程后继续下一次消息拉取。如果消息消费过慢产生消息堆积会触发消息消费拉取流控。

并发消息消费指消费线程池中的线程可以并发对同一个消息队列的消息进行消费，消费成功后，取出消息队列中最小的消息偏移量作为消息消费进度偏移量存储在于消息消费进度存储文件中，集群模式消息消费进度存储在Broker（消息服务器），广播模式消息消费进度存储在消费者端。

RocketMQ不支持任意精度的定时调度消息，只支持自定义的消息延迟级别，例如1s、2s、5s等，可通过在broker配置文件中设置messageDelayLevel。

顺序消息一般使用集群模式，是指对消息消费者内的线程池中的线程对消息消费队列只能串行消费。并并发消息消费最本质的区别是消息消费时必须成功锁定消息消费队列，在Broker端会存储消息消费队列的锁占用情况。

---

---
url: /Java/系统优化/性能优化/1_RxJava响应式编程.md
---

# RxJava响应式编程

## 一、背景

之前一些查询功能是等所有内容全部生成后，再返回给前端，用户可能要等待较长的时间。

有没有办法进行优化呢？

仔细阅读智谱 AI 的官方文档，提供了 **流式** 接口调用方式：

* https://open.bigmodel.cn/dev/api#sdk\_example
* https://open.bigmodel.cn/dev/api#glm-4

通过设置 stream 为 true 来开启流式：

![img](/assets/1716439946613-4cb50ae9-2ca4-43cc-9fee-84c5918cdb37-20240524141752700.BhKlbeUC.png)

官方提供了一段示例代码，如果 stream 设置为 true，需要从返回结果中获取到 `flowable` 对象：

![img](/assets/WAXM3qYn-image.D1DctWjr.png)

映射到代码中来需要这样构造请求参数：

```java
/**
 * 流式请求
 * @param messages
 * @param stream
 * @param temperature
 * @return
 */
public Flowable<ModelData> doRequestFlowable(List<ChatMessage> messages, Boolean stream, Float temperature) {
    // 构造请求
    ChatCompletionRequest chatCompletionRequest = ChatCompletionRequest.builder()
            .model(Constants.ModelChatGLM4)
            .stream(stream)
            .invokeMethod(Constants.invokeMethod)
            .temperature(temperature)
            .messages(messages)
            .build();
    ModelApiResponse invokeModelApiResp = clientV4.invokeModelApi(chatCompletionRequest);
    return invokeModelApiResp.getFlowable();
}
```

这个流式请求是意思？Flowable 又是什么？可以给我们的项目带来哪些优化呢？

实际上 Flowable 是 RxJava 响应式编程库中定义的类，为了更好地进行流式开发，我们要先来了解下响应式编程和 RxJava。

## 二、什么是响应式编程？

响应式编程（Reactive Programming）是一种编程范式，它专注于 **异步数据流** 和 **变化传播**。

响应式编程的核心思想是“数据流是第一等公民”，程序的逻辑建立在数据流的变化之上。

响应式编程的几个核心概念：

1）数据流：响应式编程中，数据以流（Streams）的形式存在。流就像一条河，源源不断、有一个流向（比如从 A 系统到 B 系统再到 C 系统），它可以被过滤、观测、或者跟另一条河流合并成一个新的流。

比如用户输入、网络请求、文件读取都可以是数据流，可以很轻松地对流进行处理。

比如 Java 8 的 Stream API，下列代码中将数组作为一个流，依次进行过滤、转换、汇聚。

```java
list.stream()
    .filter()
    .map()
    .collect()
```

2）异步处理：响应式编程是异步的，即操作不会阻塞线程，而是通过回调或其他机制在未来某个时间点处理结果。这提高了应用的响应性和性能。

3）变化传播：当数据源发生变化时，响应式编程模型会自动将变化传播到依赖这些数据源的地方。这种传播是自动的，不需要显式调用。

举个例子，有一只股票涨了，所有 **订阅** 了这只股的人，都会同时收到 APP 的通知，不用你自己盯着看。

注意，响应式编程更倾向于声明式编程风格，通过定义数据流的转换和组合来实现复杂的逻辑。比如，可以利用 map、filter 等函数来实现数据转换，而不是将一大堆复杂的逻辑混杂在一个代码块中。

## 三、什么是 RxJava？

RxJava 是一个基于事件驱动的、利用可观测序列来实现异步编程的类库，是响应式编程在 Java 语言上的实现。

这个定义中有几个概念，我们分别解释。

### 1、事件驱动

事件可以是任何事情，如用户的点击操作、网络请求的结果、文件的读写等。事件驱动的编程模型是通过事件触发行动。

比如前端开发中，用户点击按钮后会进行弹窗，这就是“点击事件”触发了“弹窗行动”

```javascript
// 前端按钮点击
btn.onClick(()->{
  // 弹窗
  showModal();
})
```

在 RxJava 中，事件可以被看作是数据流中的数据项，称为“事件流”或“数据流”。每当一个事件发生，这个事件就会被推送给那些对它感兴趣的观察者（Observers）。

### 2、可观测序列

可观测序列是指一系列按照时间顺序发出的数据项，可以被观察和处理。可观测序列提供了一种将数据流和异步事件建模为一系列可以订阅和操作的事件的方式。

可以理解为在数据流的基础上封装了一层，多加了一点方法。

## 四、RxJava 应用场景

它的一个核心应用场景就是 UI 场景，像 Android 开发都会用到 RxJava。

UI 场景天然涉及到响应和事件这两点，比如我们在手机 app 上某个按钮，对应 app 就会弹出某个界面，点击按钮其实就是一个事件，那么弹出界面就是对应的响应。

```java
//安卓异步后台执行任务，完成后更新UI
new AsyncTask<Void, Void, ResultType>() {
    protected ResultType doInBackground(Void... params) {
        // 在后台线程中执行的任务
        return result;
    }

    protected void onPostExecute(ResultType result) {
        // doInBackground完成后执行的回调，用于更新UI
    }
}.execute();
```

而 RxJava 给予我们一个统一的异步接口形式，提供链式编程、丰富的操作符让我们在面对复杂的业务场景编写的代码也异常简单。

我在网上找了一个最典型的例子，能直观感受到 RxJava 带来的编码便利性。

```java
Observable.from(folders)
    .flatMap((Func1) (folder) -> { Observable.from(file.listFiles()) })
    .filter((Func1) (file) -> { file.getName().endsWith(".png") })
    .map((Func1) (file) -> { getBitmapFromFile(file) })
    .subscribeOn(Schedulers.io())
    .observeOn(AndroidSchedulers.mainThread())
    .subscribe((Action1) (bitmap) -> { imageCollectorView.addImage(bitmap) });
```

`.subscribeOn(Schedulers.io())` 这行代码设置执行上游 Observable 即 Observable.from(folders)及其后续操作（合并文件夹、查找png图片，转成 bitmap）到这行代码之前的逻辑在一个适用于I/O操作的线程池中执行，即子线程池中。

`.observeOn(AndroidSchedulers.mainThread())` 这行代码指定了下游 Observable（即subscribe方法及其**之前**的操作），在 Android 的主线程上执行更新 UI。

```livescript
subscribe((Action1) (bitmap) -> { imageCollectorView.addImage(bitmap) });` 是绑定观察者的操作，观察者针对被观察者的事件的响应动作是  `imageCollectorView.addImage(bitmap)
```

这是一个在安卓中实现的例子，它在子线程中实现了过滤查找所有文件夹中的 png 图片，转成 bitmap，之后再利用主线程显示图片。

因为安卓的主线程是需要试试响应用户的请求，如果它被利用去执行查找图片和转化 png 的过程，就会让 app 卡顿，影响用户体验，严重可能会导致无响应。

因此耗时的操作需要到子线程中操作，然后最终显示结果才需要利用主线程，上面的演示代码不仅业务逻辑处理便捷，线程的切换也非常简单。

现在我们想象上，上面的这些操作，如果没有 RxJava 要我们来实现的话，需要多少行代码？至少是 7 行代码的几倍吧。

想必我们已经知晓的 RxJava 的便捷之处了，接下来我们再来学学 RxJava 的核心知识点。

## 五、RxJava 的核心知识点

### 观察者模式

RxJava 是基于 **观察者模式** 实现的，分别有观察者和被观察者两个角色，被观察者会实时传输数据流，观察者可以观测到这些数据流。

基于传输和观察的过程，用户可以通过一些操作方法对数据进行转换或其他处理。

在 RxJava 中，观察者就是 Observer，被观察者是 Observable 和 Flowable。

Observable 适合处理相对较小的、可控的、不会迅速产生大量数据的场景。它不具备背压处理能力，也就是说，当数据生产速度超过数据消费速度时，可能会导致内存溢出或其他性能问题。

Flowable 是针对背压（反向压力）问题而设计的可观测类型。背压问题出现于数据生产速度超过数据消费速度的场景。Flowable 提供了多种背压策略来处理这种情况，确保系统在处理大量数据时仍然能够保持稳定。

`被观察者.subscribe(观察者)`，它们之间就建立的订阅关系，被观察者传输的数据或者发出的事件会被观察者观察到。

### 常用操作符

前面提到用户可以通过一些方法对数据进行转换或其他处理，RxJava 提供了很多操作符供我们使用，这块其实和 Java8 的 Stream 类似，概念上都是一样的。

操作符主要可以分为以下几大类：

1）变换类操作符，对数据流进行变换，如 map、flatMap 等。

比如利用 map 将 int 类型转为 string

```java
Flowable<String> flowable = Flowable.range(0, Integer.MAX_VALUE)
        .map(i -> String.valueOf(i))
```

2）聚合类操作符，对数据流进行聚合，如 toList、toMap 等。

将数据转成一个 list

```java
Flowable.range(0, Integer.MAX_VALUE).toList()
```

3）过滤操作符，过滤或者跳过一些数据，如 filter、skip 等。

将大于 10 的数据转成一个 list

```java
Flowable.range(0, Integer.MAX_VALUE).filter(i -> i > 10).toList();
```

4）连接操作符，将多个数据流连接到一起，如 concat、zip 等。

创建两个 Flowable，通过 concat 连接得到一个被观察者，进行统一处理

```java
// 创建两个 Flowable 对象
Flowable<String> flowable1 = Flowable.just("A", "B", "C");
Flowable<String> flowable2 = Flowable.just("D", "E", "F");

// 使用 concat 操作符将两个 Flowable 合并
Flowable<String> flowable = Flowable.concat(flowable1, flowable2);
```

5）排序操作符，对数据流内的数据进行排序，如 sorted

```java
Flowable<String> flowable = Flowable.concat(flowable1, flowable2).sorted();
```

### 事件

RxJava 也是一个基于事件驱动的框架，我们来看看一共有哪些事件，分别在什么时候触发：

1）onNext，被观察者每发送一次数据，就会触发此事件。

2）onError，如果发送数据过程中产生意料之外的错误，那么被观察者可以发送此事件。

3）onComplete，如果没有发生错误，那么被观察者在最后一次调用 onNext 之后发送此事件表示完成数据传输。

对应的观察者得到这些事件后，可以进行一定处理，例如：

```java
flowable.observeOn(Schedulers.io())
    .doOnNext(item -> {
        System.out.println("来数据啦" + item.toString());
    })
    .doOnError(e -> {
        System.out.println("出错啦" + e.getMessage());
    })
    .doOnComplete(() -> {
        System.out.println("数据处理完啦");
    }).subscribe();
```

了解这么多就够用了，更多可以查看官网 https://reactivex.io/，支持多种不同的编程语言。

## 六、DEMO 演示

1）引入依赖

如果要在新项目中使用 RxJava，单独引入 Java 需要的、对应版本的依赖即可：https://github.com/reactive-streams/reactive-streams-jvm

示例代码：

```xml
<dependency>
  <groupId>org.reactivestreams</groupId>
  <artifactId>reactive-streams</artifactId>
  <version>1.0.4</version>
</dependency>
```

2）编写单元测试

```java
@Test
void rxJavaDemo() throws InterruptedException {
    // 创建一个流，每秒发射一个递增的整数（数据流变化）
    Flowable<Long> flowable = Flowable.interval(1, TimeUnit.SECONDS)
            .map(i -> i + 1)
            .subscribeOn(Schedulers.io()); // 指定创建流的线程池

    // 订阅 Flowable 流，并打印每个接受到的数字
    flowable.observeOn(Schedulers.io())
            .doOnNext(item -> System.out.println(item.toString()))
            .subscribe();

    // 让主线程睡眠，以便观察输出
    Thread.sleep(10000L);
}
```

## 七、补充：RxJava 和 WebFlux 的区别

Spring WebFlux 和 RxJava 都是用于构建反应式编程模型的工具和框架，它们有一定的关联，但也有不同的用途和实现方式。以下是两者的关联和区别：

### 关联

1. **反应式编程模型**：

* * **Spring WebFlux** 和 **RxJava** 都支持反应式编程模型，旨在处理异步数据流和事件驱动的编程模式。
  * 反应式编程的核心思想是以非阻塞的方式处理数据流，通过事件驱动的方式响应数据变化。

1. **Reactive Streams 标准**：

* * 两者都遵循 **Reactive Streams** 规范，这是一个旨在提供背压机制的标准，确保在异步处理数据流时不会发生过载。
  * Reactive Streams 规范定义了四个核心接口：`Publisher`、`Subscriber`、`Subscription` 和 `Processor`。这为不同的反应式库和框架提供了互操作的基础。

### Spring WebFlux

* **框架定位**：Spring WebFlux 是 Spring Framework 5 引入的一个反应式 Web 框架，用于构建非阻塞的、事件驱动的 Web 应用。
* **核心依赖**：WebFlux 依赖于 Project Reactor，这是一个用于构建反应式应用的库。
* **编程模型**：使用 Reactor 提供的 `Mono` 和 `Flux` 类型，分别表示 0 或 1 个元素和 0 到 N 个元素的数据流。
* **应用场景**：适用于需要高并发、低延迟的 Web 应用和微服务架构。

### RxJava

* **库定位**：RxJava 是 ReactiveX (Reactive Extensions) 项目的一部分，是一个用于构建基于事件的异步和响应式应用程序的 Java 库。
* **核心概念**：提供 Observable 和 Flowable（以及其他类型）来表示数据流和事件流，支持丰富的操作符来处理和转换数据流。
* **编程模型**：RxJava 提供了大量操作符，如 `map`、`flatMap`、`filter`、`merge` 等，用于流的转换和组合。
* **应用场景**：广泛应用于 Android 开发和服务端应用中，用于处理异步事件和数据流。

### 区别

1. **实现方式**：

* * **WebFlux** 主要基于 Reactor 实现，而不是 RxJava。虽然两者都遵循 Reactive Streams 规范，但它们提供的 API 和操作符有所不同。
  * **RxJava** 独立于 WebFlux，是一个通用的反应式编程库，不仅限于 Web 应用。

1. **核心类型**：

* * **WebFlux** 使用 `Mono` 和 `Flux` 类型。
  * **RxJava** 使用 `Observable`、`Flowable`、`Single`、`Maybe` 和 `Completable` 类型。

1. **适用场景**：

* * **WebFlux** 专注于构建反应式 Web 应用和微服务。
  * **RxJava** 是一个通用的反应式编程库，适用于任何需要处理异步数据流的场景，包括但不限于 Web 应用。

### 结合使用

虽然 WebFlux 主要使用 Reactor，但你也可以在 WebFlux 应用中使用 RxJava。Spring 提供了一些转换工具，可以在 RxJava 和 Reactor 类型之间进行转换。例如，可以使用 `RxJava2Adapter` 将 RxJava 的 `Observable` 转换为 Reactor 的 `Flux`，或者将 RxJava 的 `Single` 转换为 Reactor 的 `Mono`。

### 示例代码

```java
// 使用 RxJava 的 Observable
Observable<String> rxObservable = Observable.just("Hello", "World");
// 转换为 Reactor 的 Flux
Flux<String> reactorFlux = Flux.from(rxObservable.toFlowable(BackpressureStrategy.BUFFER));
// 在 WebFlux 中使用
router.route("/hello")
.get(request -> ServerResponse.ok().body(reactorFlux, String.class));
```

总结来说，Spring WebFlux 和 RxJava 都支持反应式编程，但 WebFlux 是一个框架，主要用于构建反应式 Web 应用，基于 Reactor 实现，而 RxJava 是一个通用的反应式编程库，广泛应用于不同的场景。两者可以结合使用，通过转换工具进行互操作。

---

---
url: /Java/系统优化/系统优化/4_SDK封装.md
---

# SDK封装

---

---
url: /Java/解决方案/数据同步/3_SeaTunnel.md
---

# SeaTunnel

## 一、安装seatunnel

> 官方文档：<https://seatunnel.incubator.apache.org/docs/2.3.3/about/>

### 1、准备工作

所需软件包及版本要求

* JDK >= 1.8.151

* Maven >= 3.6.3

* MySQL >= 5.7.28

下载地址：<https://seatunnel.apache.org/download>

准备安装版本：`apache-seatunnel-2.3.3-bin.tar.gz`

### 2、解压软件

```sh
sudo tar -zxvf apache-seatunnel-2.3.3-bin.tar.gz
# 指定目录 -C ./seatunnel
```

### 3、配置环境变量

在`/etc/profile`中配置环境变量

```sh
SEATUNNEL_HOME=/home/disk2/tools/apache-seatunnel-2.3.3
export PATH=$SEATUNNEL_HOME/bin:$PATH
```

让修改配置立即生效

```sh
source /etc/profile
```

检查生效

```sh
echo $SEATUNNEL_HOME
```

### 4、下载连接器插件

从2.2.0-beta版本开始，二进制包不再默认提供连接器依赖，因此在第一次使用时，需要执行以下命令来安装连接器：(当然，也可以从 [Apache Maven Repository](https://repo.maven.apache.org/maven2/org/apache/seatunnel/) 手动下载连接器，然后将其移动至`connectors/`目录下，如果是2.3.5之前则需要放入`connectors/seatunnel`目录下)。

```sh
./bin/install-plugin.sh
```

如果需要指定的连接器版本，以2.3.12为例

```sh
./bin/install-plugin.sh 2.3.12
```

通常情况下，不需要所有的连接器插件。可以通过配置`config/plugin_config`来指定所需的插件。例如，如果想让示例应用程序正常工作，只需要`connector-console`和`connector-fake`插件。可以修改`plugin_config`配置文件，如下所示：

```sh
--seatunnel-connectors--
connector-fake
connector-console
connector-cdc-mysql
connector-cdc-mongodb
--end--
```

### 5、测试验证

```sh
#进入安装目录
cd /home/disk2/tools/apache-seatunnel-2.3.3/
#启动服务
./bin/seatunnel.sh --config ./config/v2.batch.config.template -e local
```

**查看输出**：当您运行该命令时，可以在控制台中看到它的输出。

SeaTunnel控制台将会打印一些如下日志信息：观察到日志有如下信息表示运行测试成功。

```sh
***********************************************
     CoordinatorService Thread Pool Status
***********************************************
activeCount               :                   1
corePoolSize              :                  10
maximumPoolSize           :          2147483647
poolSize                  :                   1
completedTaskCount        :                   0
taskCount                 :                   1
***********************************************
```

### 6、启动服务

```sh
#进入安装目录
cd /home/disk2/tools/apache-seatunnel-2.3.3/
#启动服务
nohup sh bin/seatunnel-cluster.sh 2>&1 &
```

在seatunnel的安装目录下查看日志

```sh
tail -f logs/seatunnel-engine-server.log
```

有以下类似信息打印出，说明启动成功。

```sh
[] 2025-07-30 17:20:14,956 INFO  [o.a.s.e.s.CoordinatorService  ] [pool-7-thread-1] - [localhost]:5801 [seatunnel] [5.1] 
***********************************************
     CoordinatorService Thread Pool Status
***********************************************
activeCount               :                   1
corePoolSize              :                  10
maximumPoolSize           :          2147483647
poolSize                  :                   2
completedTaskCount        :                   1
taskCount                 :                   2
***********************************************

[] 2025-07-30 17:20:14,957 INFO  [o.a.s.e.s.CoordinatorService  ] [pool-7-thread-1] - [localhost]:5801 [seatunnel] [5.1] 
***********************************************
                Job info detail
***********************************************
createdJobCount           :                   0
scheduledJobCount         :                   0
runningJobCount           :                   0
failingJobCount           :                   0
failedJobCount            :                   0
cancellingJobCount        :                   0
canceledJobCount          :                   0
finishedJobCount          :                   0
```

必须保证Apache SeaTunnel的Server正常运行，Web端服务才能正常运行。

## 二、安装seatunnel-web

> 官方文档：<https://seatunnel.apache.org/zh-CN/seatunnel_web/1.0.0/about>

### 1、准备工作

下载地址：<https://seatunnel.apache.org/download>

准备安装版本：`apache-seatunnel-web-1.0.0-bin.tar.gz`（最新的1.0.2一直没启动成功o(╯□╰)o）

注意：在seatunnel的web端机器上需要先部署seatunnel客户端。

### 2、解压软件

```sh
sudo tar -zxvf apache-seatunnel-web-1.0.0-bin.tar.gz
# 指定目录 -C ./seatunnel-web
```

### 3、初始化数据库

#### 修改配置

将`script/seatunnel_server_env.sh`相关配置改为你的对应的数据库信息

```sh
export HOSTNAME="127.0.0.1"
export PORT="3306"
export USERNAME="root"
export PASSWORD="123456"
```

此处`HOSTNAME,PORT`等名称容易与系统其他名称冲突，加上前缀`STWEB_`

```sh
export STWEB_HOSTNAME="127.0.0.1"
export STWEB_PORT="3306"
export STWEB_USERNAME="root"
export STWEB_PASSWORD="123456"
```

给`script/init_sql.sh`相关变量加上前缀`STWEB_`

```sh
workDir=`dirname $0`
workDir=`cd ${workDir};pwd`

source ${workDir}/seatunnel_server_env.sh

usage="Usage: seatunnel_server_env.sh must contain hostname/port/username/password."

if [[ ! -n "${STWEB_HOSTNAME}" ]]  || [[ ! -n "${STWEB_PORT}" ]] || [[ ! -n "${STWEB_USERNAME}" ]] || [[ ! -n "${STWEB_PASSWORD}" ]]; then
    echo $usage
    exit 1
fi

mysql -h${HOSTNAME} -P${PORT} -u${USERNAME} -p${PASSWORD} < ${workDir}/seatunnel_server_mysql.sql
```

#### 执行初始化数据库命令

执行命令`sh init_sql.sh`，无异常则执行成功。注意需要输入一遍确认密码。

```sh
root@ekroot-b760mds3hddr4:/home/disk2/tools/apache-seatunnel-web-1.0.0/script# ./init_sql.sh
mysql: [Warning] option 'port': value -u adjusted to 0.
Enter password:
```

### 4、配置后端服务

#### 修改端口与数据源

web后端服务的配置文件都在`${web安装目录}/conf`下

`conf/application.yml`修改端口号和数据源信息

```yaml
server:
  port: 8801

spring:
  application:
    name: seatunnel
  jackson:
    date-format: yyyy-MM-dd HH:mm:ss
  datasource:
    driver-class-name: com.mysql.cj.jdbc.Driver
    url: jdbc:mysql://localhost:3306/seatunnel?useSSL=false&useUnicode=true&characterEncoding=utf-8&allowMultiQueries=true&allowPublicKeyRetrieval=true
    username: root
    password: 123456
  mvc:
    pathmatch:
      matching-strategy: ant_path_matcher

jwt:
  expireTime: 86400
  # please add key when deploy
  secretKey:
  algorithm: HS256

---
spring:
  config:
    activate:
      on-profile: h2
  sql:
    init:
      schema-locations: classpath*:script/seatunnel_server_h2.sql
  datasource:
    driver-class-name: org.h2.Driver
    url: jdbc:h2:mem:seatunnel;MODE=MySQL;DB_CLOSE_DELAY=-1;DATABASE_TO_LOWER=true
    username: sa
    password: sa
  h2:
    console:
      enabled: true
      path: /h2
      settings:
        trace: false
        web-allow-others: false
```

开启调试模式

```yaml
logging:
  level:
    root: DEBUG
```

#### 配置client信息

将seatunnel引擎服务节点的安装目录下的config目录下的关于引擎客户端的配置文件拷贝到seatunnel-web安装目录下的conf目录下，同一台机器下部署直接使用以下拷贝命令（注意修改服务的安装目录为你自己的安装目录）

```sh
# 当前目录 /home/disk2/tools/apache-seatunnel-web-1.0.0
sudo cp /home/disk2/tools/apache-seatunnel-2.3.3/config/hazelcast-client.yaml ./conf/
```

#### 配置支持的插件信息

将seatunnel引擎服务节点的安装目录下的connectors目录下的`plugin-mapping.properties`配置文件拷贝到seatunnel-web安装目录下的conf目录下，同一台机器下部署直接使用以下拷贝命令（注意修改服务的安装目录为你自己的安装目录）

```sh
sudo cp /home/disk2/tools/apache-seatunnel-2.3.3/connectors/plugin-mapping.properties ./conf/
```

### 5、下载配置数据源JAR包

配置jar包非常关键，否则会出现各种各样的问题，常见的有：

1. 没有数据源可以进行创建

2. 没有source或者sink进行选择

3. 配置好任务后无法执行

#### 配置元数据mysql的jar包

将`mysql-connector-java-8.0.15.jar`包复制到`/opt/seatunnel/web/apache-seatunnel-web-1.0.0-bin/libs`

#### 配置数据源jar包

脚本在`bin/download_datasource.sh`

修改

```sh
# the datasource default version is 1.0.0, you can also choose a custom version. eg: 1.1.2:  sh install-datasource.sh 2.1.2
version=1.0.0
```

直接执行脚本

```sh
./bin/download_datasource.sh ./libs
```

或者

复制到windows上，使用本地maven加速下载

![image-20250730174138073](/assets/seatunnel_01.ha5oDCNT.png)

使用git自带的ssh工具

```sh
./download_datasource.sh
```

### 6、启动服务

```sh
sudo sh bin/seatunnel-backend-daemon.sh start
```

浏览器访问`服务器IP地址:配置端口/ui`（默认`服务器IP地址:8081/ui`）

![seatunnel\_01](/assets/seatunnel_02.C4cuIB7T.jpg)

默认用户名，密码为admin/admin

![seatunnel\_02](/assets/seatunnel_03.BcDSDLu_.jpg)

设置中可以修改语言。

## 三、遇到问题及解决方法

### 报错1

```sh
yum安装的jdk没有环境变量，直接在脚本中显式设置（更推荐在环境变量中设置）

export JAVA_HOME=/usr/local/jdk-11.0.2/
export PATH=$JAVA_HOME/bin:$PATH

export SEATUNNEL_HOME=/home/tools/apache-seatunnel-2.3.11/
```

### 报错2

```sh
org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'connectorCache' defined in URL [jar:file:/home/disk2/tools/apache-seatunnel-web-1.0.2/libs/seatunnel-app-1.0.2.jar!/org/apache/seatunnel/app/bean/connector/ConnectorCache.class]: Instantiation of bean failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.apache.seatunnel.app.bean.connector.ConnectorCache]: Constructor threw exception; nested exception is java.lang.NoClassDefFoundError: org/apache/seatunnel/api/table/factory/ChangeStreamTableSourceFactory
```

将缺失的 `hadoop3-3.1.4-uber-2.3.2-optional.jar` 包下载并放置到 `$SEATUNNEL_HOME/lib` 目录下

### 报错3

```sh
org.springframework.context.ApplicationContextException: Unable to start web server; nested exception is java.lang.IllegalArgumentException: No selectors
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:163)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:577)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:145)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:745)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:420)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:307)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1317)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1306)
	at org.apache.seatunnel.app.SeatunnelApplication.main(SeatunnelApplication.java:36)
Caused by: java.lang.IllegalArgumentException: No selectors
	at org.eclipse.jetty.io.SelectorManager.<init>(SelectorManager.java:63)
	at org.eclipse.jetty.server.ServerConnector$ServerConnectorManager.<init>(ServerConnector.java:600)
	at org.eclipse.jetty.server.ServerConnector.newSelectorManager(ServerConnector.java:223)
	at org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:216)
	at org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:132)
	at org.springframework.boot.web.embedded.jetty.JettyServletWebServerFactory.createConnector(JettyServletWebServerFactory.java:200)
	at org.springframework.boot.web.embedded.jetty.JettyServletWebServerFactory.createServer(JettyServletWebServerFactory.java:186)
	at org.springframework.boot.web.embedded.jetty.JettyServletWebServerFactory.getWebServer(JettyServletWebServerFactory.java:163)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.createWebServer(ServletWebServerApplicationContext.java:182)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:160)
	... 8 common frames omitted
```

删除hive包

```sh
cd apache-seatunnel-web-1.0.0-bin/libs
rm -rf datasource-hive-1.0.0.jar
```

### 问题4

安装完seatunnel web，新建数据源时，无下拉框和数据源选项。

把后端的connectors和plugins相关jar包放到web下的lib下再试试

## 参考资料

SeaTunnel 与 DataX 、Sqoop、Flume、Flink CDC 对比：<https://cloud.tencent.com/developer/article/2401413>

<https://blog.csdn.net/thc1987/article/details/131240445>

<https://blog.csdn.net/u013995172/article/details/134422053>

<https://blog.csdn.net/qq_41865652/article/details/134574104>

<https://blog.csdn.net/taogumo/article/details/149407864>

同步脚本示例：<https://segmentfault.com/a/1190000045093609>

同步脚本示例：<https://help.primeton.com/dws/7.1.0/product-doc/dws_reference/development/samples/template/sea-MultiTableMigration.html>

---

---
url: /Redis/Redis数据结构/Set结构实战之社交应用中的共同好友.md
---

# Set结构实战之社交应用中的共同好友

在社交应用中，有时需要获取共同好友、全部好友以及专属于某个人的好友，用户推荐算法中肯定会用到这些数据。

利用Redis中Set结构的API能够查询到两个Set结构交集、并集和差集的数据。

最为常见的用处除了用户推荐外，还有查询关注列表、粉丝列表、共同群聊。

案例

```java
@RunWith(SpringRunner.class)
@SpringBootTest
public class SetAction {

    @Autowired
    private RedisTemplate redisTemplate;

    @Test
    public void setAction() {
        // 用户A
        BoundSetOperations operationsA = redisTemplate.boundSetOps("user:a");
        // 关注了用户A的用户
        operationsA.add("A", "B", "C", "D");
        System.out.println("老A的粉丝" + operationsA.members());

        BoundSetOperations operationsB = redisTemplate.boundSetOps("user:b");
        operationsB.add("A", "B", "F", "G", "O", "W");
        System.out.println("老B的粉丝" + operationsB.members());

        // 差集
        Set setA = operationsA.diff("user:b");
        System.out.println("A的专属用户" + setA);

        // 交集
        Set set = operationsA.intersect("user:b");
        System.out.println("共同好友" + set);

        // 并集
        Set union = operationsA.union("user:b");
        System.out.println("全部好友" + union);

        // 是否存在集合中
        Boolean c = operationsA.isMember("C");
        System.out.println("用户c是不是老王的粉丝" + c);
    }
}
```

结果输出

```
老A的粉丝[D, C, A, B]
老B的粉丝[F, B, O, W, A, G]
A的专属用户[D, C]
共同好友[A, B]
全部好友[C, B, F, D, O, G, W, A]
用户c是不是老王的粉丝true
```

---

---
url: /Redis/Redis数据结构/Set结构实战之用户画像标签去重.md
---

# Set结构实战之用户画像标签去重

用户画像是根据用户基本属性、社会属性、行为属性、心理属性等真实信息而抽象出的一个标签化的、虚拟的用户模型。“用户画像”的实质是对 “人”的数字化。

* 应用场景有很多，比如个性化推荐、精准营销、金融风控、精细化运营等等， 举个例子来理解用户画像的实际应用价值，我们经常用手机网购，淘宝里面的千人千面
* 通过“标签 tag”来对用户的多维度特征进行提炼和标识，那每个人的用户画像就需要存储，set集合就适合去重
* 用户画像不止针对某个人，也可以某一人群或行业的画像
* 利用redis可以很好的去重

案例

```java
@Test
public void setAction1() {
    BoundSetOperations operations = redisTemplate.boundSetOps("user:tags:1");
    operations.add("car", "student", "rich", "guangdong", "dog", "rich");

    Set<String> set1 = operations.members();
    System.out.println(set1);
    operations.remove("dog");

    Set<String> set2 = operations.members();
    System.out.println(set2);
}
```

结果

```
[guangdong, student, car, dog, rich]
[car, guangdong, rich, student]
```

---

---
url: /常用框架/ShardingJdbc/0_ShardingJdbc的概述.md
---

# ShardingJdbc的概述

## 一、ShardingJdbc的概述

> 官网：http://shardingsphere.apache.org/index\_zh.html
>
> 下载地址：https://shardingsphere.apache.org/document/current/cn/downloads/
>
> 快速入门：https://shardingsphere.apache.org/document/current/cn/quick-start/shardingsphere-jdbc-quick-start/

以下来自官网的原话：

Apache ShardingSphere 是一套开源的分布式数据库解决方案组成的生态圈，它由 JDBC、Proxy 和 Sidecar（规划中）这 3 款既能够独立部署，又支持混合部署配合使用的产品组成。 它们均提供标准化的数据水平扩展、分布式事务和分布式治理等功能，可适用于如 Java 同构、异构语言、云原生等各种多样化的应用场景。

Apache ShardingSphere 旨在充分合理地在分布式的场景下利用关系型数据库的计算和存储能力，而并非实现一个全新的关系型数据库。 关系型数据库当今依然占有巨大市场份额，是企业核心系统的基石，未来也难于撼动，我们更加注重在原有基础上提供增量，而非颠覆。

Apache ShardingSphere 5.x 版本开始致力于可插拔架构，项目的功能组件能够灵活的以可插拔的方式进行扩展。 目前，数据分片、读写分离、数据加密、影子库压测等功能，以及 MySQL、PostgreSQL、SQLServer、Oracle 等 SQL 与协议的支持，均通过插件的方式织入项目。 开发者能够像使用积木一样定制属于自己的独特系统。Apache ShardingSphere 目前已提供数十个 SPI 作为系统的扩展点，仍在不断增加中。

ShardingSphere 已于2020年4月16日成为 Apache 软件基金会的顶级项目。

## 二、关于改名问题

在3.0以后就更改成了ShardingSphere。

## 三、认识shardingjdbc

![img](/assets/fab03b882c0bcf69b83418b621213b70.vYUUxA-u.png)

定位为轻量级 Java 框架，在 Java 的 JDBC 层提供的额外服务。 它使用客户端直连数据库，以 jar 包形式提供服务，无需额外部署和依赖，可理解为增强版的 JDBC 驱动，完全兼容 JDBC 和各种 ORM 框架。

适用于任何基于 JDBC 的 ORM 框架，如：JPA, Hibernate, Mybatis, Spring JDBC Template 或直接使用 JDBC。

支持任何第三方的数据库连接池，如：DBCP, C3P0, BoneCP, Druid, HikariCP 等。
支持任意实现 JDBC 规范的数据库，目前支持 MySQL，Oracle，SQLServer，PostgreSQL 以及任何遵循 SQL92 标准的数据库。

## 四、认识shardingjdbc功能架构图

![img](/assets/18d3a43a4304b380da2dbd100f2eb7de.CFlp3NBG.png)

## 五、认识Sharding-Proxy

* 向应用程序完全透明，可直接当做 MySQL/PostgreSQL 使用。
* 适用于任何兼容 MySQL/PostgreSQL 协议的的客户端。

## 六、三个组件的比较

|            | Sharding-Jdbc | Sharding-Proxy | Sharding-Sidecar |
| ---------- | ------------- | -------------- | ---------------- |
| 数据库     | 任意          | MYSQL          | MYSQL            |
| 连接消耗数 | 高            | 低             | 低               |
| 异构语言   | 仅Java        | 任意           | 任意             |
| 性能       | 损耗低        | 损耗高         | 损耗低           |
| 中心化     | 是            | 否             | 是               |
| 静态入口   | 无            | 有             | 无               |

## 七、ShardingJdbc混合架构

![img](/assets/8f40f91304567728fe19148a976cb4e6.B3e3Exts.png)

ShardingSphere-JDBC 采用无中心化架构，适用于 Java 开发的高性能的轻量级 OLTP（连接事务处理） 应用；ShardingSphere-Proxy 提供静态入口以及异构语言的支持，适用于 OLAP（连接数据分析） 应用以及对分片数据库进行管理和运维的场景。

Apache ShardingSphere 是多接入端共同组成的生态圈。 通过混合使用 ShardingSphere-JDBC 和 ShardingSphere-Proxy，并采用同一注册中心统一配置分片策略，能够灵活的搭建适用于各种场景的应用系统，使得架构师更加自由地调整适合与当前业务的最佳系统架构。

八、ShardingShpere的功能清单

* 功能列表
  * 数据分片
  * 分库 & 分表
  * 读写分离
  * 分片策略定制化
  * 无中心化分布式主键
* 分布式事务
  * 标准化事务接口
  * XA 强一致事务
  * 柔性事务
  * 数据库治理
* 分布式治理
  * 弹性伸缩
  * 可视化链路追踪
  * 数据加密

## 九、 ShardingSphere数据分片内核剖析

ShardingSphere 的 3 个产品的数据分片主要流程是完全一致的。 核心由 SQL 解析 => 执行器优化 => SQL 路由 => SQL 改写 => SQL 执行 => 结果归并的流程组成。

![img](/assets/eeb4c4ddc2aa41e0e03360b3ddb5d239.llGgIv8D.png)

1. SQL 解析

   分为词法解析和语法解析。 先通过词法解析器将 SQL 拆分为一个个不可再分的单词。再使用语法解析器对 SQL 进行理解，并最终提炼出解析上下文。 解析上下文包括表、选择项、排序项、分组项、聚合函数、分页信息、查询条件以及可能需要修改的占位符的标记。

2. 执行器优化

   合并和优化分片条件，如 OR 等。

3. SQL 路由

   根据解析上下文匹配用户配置的分片策略，并生成路由路径。目前支持分片路由和广播路由。

4. SQL 改写

   将 SQL 改写为在真实数据库中可以正确执行的语句。SQL 改写分为正确性改写和优化改写。

5. SQL 执行

   通过多线程执行器异步执行。

6. 结果归并

   将多个执行结果集归并以便于通过统一的 JDBC 接口输出。结果归并包括流式归并、内存归并和使用装饰者模式的追加归并这几种方式。

### 配置

#### sharding3

分库分表（sharding3版本）：https://blog.csdn.net/zxp2624161989/article/details/107094560/

不分库只分表（sharding3版本）：https://www.bilibili.com/video/BV1id4y1z7St、https://blog.csdn.net/zxp2624161989/article/details/107094560/

水平分片：
创建测试数据局test\_order。分别创建三张表t\_address， t\_user0，t\_user1。
这里假设t\_user这个预计随着系统的运行。
公司发展很好，以后数据量会暴增。所以提前进行水平分片存储。相对于垂直分片，它不再将数据根据业务逻辑分类，
而是通过某个字段（或某几个字段），根据某种规则将数据分散至多个库或表中，
每个分片仅包含数据的一部分。这样单表数据量降下来了，mysql的B+树的检索效率就提高了。

#### sharding4

不分库只分表（sharding4版本）yml配置：https://blog.csdn.net/weixin\_40816738/article/details/126802777
分表配置后加载报错，ShardingParsingRuleRegistry：https://www.codenong.com/cs105364582/
方法一：将 jdk的运行版本降为 1.8。
方法二：将 JAXB 相关jar包重新引入，具体maven。

分库分表（sharding4版本）yml配置：https://blog.csdn.net/akenseren/article/details/127350807
https://www.jianshu.com/p/3b3f7c6fd288

### 参考资料

\[1]. https://www.cnblogs.com/architectforest/p/13537436.html

\[2]. 狂神视频：https://www.bilibili.com/video/BV1ei4y1K7dn
狂神笔记：https://blog.csdn.net/qq\_44866424/article/details/120009099

\[3]. win下配置主从复制（不推荐在win上配置）
https://blog.csdn.net/qq\_27991253/article/details/128017412

### 面试问题

1、与mycat区别：

https://www.zhihu.com/question/64709787

### sharding使用中的一些问题：

1、sharding + MP 启动后无法执行查询：Error querying database？

解决方式：修改版本为4.1.1，使其支持子查询。

https://blog.csdn.net/u014106644/article/details/128335532

4.0.0-RC1版本是有限制支持子查询的 主查询和子查询必须保证相同的分片键;

4.1.1可以支持子查询  子查询判断条件恒为false;

5.0版本 Federation 执行引擎支持子查询；

2、警告：spring boot 集成 sharding jdbc 分库分表 数据库连接健康检查不通过

https://www.cnblogs.com/maohuidong/p/15006724.html

3、服务器配置主从复制报错：Fatal error:The slave I/O thread stops because master and slave have equal MySQL server UUIDs？

原因：使用了VMware克隆了两台虚拟机作为主机和从机导致UUID一样。

解决：找到主机和从机的auto.cnf文件修改uuid值或删除auto.cnf这个文件。

https://blog.csdn.net/cnds123321/article/details/117925881

4、Sharding-JDBC整合Mybatisplus分片键生成策略冲突问题及分析解决

https://blog.csdn.net/weixin\_43584430/article/details/120367418

---

---
url: /常用框架/ShardingJdbc/2_ShardingJdbc配置读写分离.md
---

# ShardingJdbc配置读写分离

## ShardingJdbc配置读写分离

### 1、 新建一个springboot工程

### 2、 引入相关mybatis、数据库驱动、sharding依赖

```xml
 <properties>
     <java.version>11</java.version>
     <sharding-sphere.version>4.0.0-RC1</sharding-sphere.version>
 </properties>
 <!-- 依赖web -->
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-web</artifactId>
</dependency>
<!-- 依赖mybatis和mysql驱动 -->
<dependency>
    <groupId>org.mybatis.spring.boot</groupId>
    <artifactId>mybatis-spring-boot-starter</artifactId>
    <version>2.1.4</version>
</dependency>
<dependency>
    <groupId>mysql</groupId>
    <artifactId>mysql-connector-java</artifactId>
    <scope>runtime</scope>
</dependency>
<!--依赖lombok-->
<dependency>
    <groupId>org.projectlombok</groupId>
    <artifactId>lombok</artifactId>
    <optional>true</optional>
</dependency>
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-test</artifactId>
    <scope>test</scope>
</dependency>
<!--依赖sharding-->
<dependency>
    <groupId>org.apache.shardingsphere</groupId>
    <artifactId>sharding-jdbc-spring-boot-starter</artifactId>
    <version>${sharding-sphere.version}</version>
</dependency>
<dependency>
    <groupId>org.apache.shardingsphere</groupId>
    <artifactId>sharding-core-common</artifactId>
    <version>${sharding-sphere.version}</version>
</dependency>
<!--依赖数据源druid-->
<dependency>
    <groupId>com.alibaba</groupId>
    <artifactId>druid-spring-boot-starter</artifactId>
    <version>1.1.21</version>
</dependency>

```

### 3、 定义配置application.yml

```yaml
server:
  port: 8085
spring:
  main:
    allow-bean-definition-overriding: true
  shardingsphere:
    # 参数配置，显示sql
    props:
      sql:
        show: true
    # 配置数据源
    datasource:
      # 给每个数据源取别名，下面的ds1,ds2,ds3任意取名字
      names: ds1,ds2,ds3
      # 给master-ds1每个数据源配置数据库连接信息
      ds1:
        # 配置druid数据源
        type: com.alibaba.druid.pool.DruidDataSource
        driver-class-name: com.mysql.cj.jdbc.Driver
        url: jdbc:mysql://192.168.233.128:3306/xxl_sharding_db?useUnicode=true&characterEncoding=utf8&tinyInt1isBit=false&useSSL=false&serverTimezone=GMT
        username: root
        password: xxl666
        maxPoolSize: 100
        minPoolSize: 5
      # 配置ds2-slave
      ds2:
        type: com.alibaba.druid.pool.DruidDataSource
        driver-class-name: com.mysql.cj.jdbc.Driver
        url: jdbc:mysql://192.168.233.129:3306/xxl_sharding_db?useUnicode=true&characterEncoding=utf8&tinyInt1isBit=false&useSSL=false&serverTimezone=GMT
        username: root
        password: xxl666
        maxPoolSize: 100
        minPoolSize: 5
      # 配置ds3-slave
      ds3:
        type: com.alibaba.druid.pool.DruidDataSource
        driver-class-name: com.mysql.cj.jdbc.Driver
        url: jdbc:mysql://192.168.233.130:3306/xxl_sharding_db?useUnicode=true&characterEncoding=utf8&tinyInt1isBit=false&useSSL=false&serverTimezone=GMT
        username: root
        password: xxl666
        maxPoolSize: 100
        minPoolSize: 5
    # 配置默认数据源ds1
    sharding:
     # 默认数据源，主要用于写，注意一定要配置读写分离 ,注意：如果不配置，那么就会把三个节点都当做从slave节点，新增，修改和删除会出错。
      default-data-source-name: ds1
    # 配置数据源的读写分离，但是数据库一定要做主从复制
    masterslave:
      # 配置主从名称，可以任意取名字
      name: ms
      # 配置主库master，负责数据的写入
      master-data-source-name: ds1
      # 配置从库slave节点
      slave-data-source-names: ds2,ds3
      # 配置slave节点的负载均衡均衡策略，采用轮询机制
      load-balance-algorithm-type: round_robin
# 整合mybatis的配置XXXXX
mybatis:
  mapper-locations: classpath:mapper/*.xml
  type-aliases-package: com.xxl.shardingjdbc.entity

```

### 4、 定义mapper、controller、entity

**entity**

```java
package com.xxl.shardingjdbc.entity;

import lombok.Data;

/**
 * @Description: 用户表
 * @Author: xxl
 * @Date: 2023/02/10 17:23
 * @Version: 1.0
 */
@Data
public class User {
    // 主键
    private Integer id;
    // 昵称
    private String nickname;
    // 密码
    private String password;
    // 性别
    private Integer sex;
    // 生日
    private String birthday;
    // 分库字段
    private Integer db;
}
```

mapper

```java
package com.xxl.shardingjdbc.mapper;

import com.xxl.shardingjdbc.entity.User;
import org.apache.ibatis.annotations.Insert;
import org.apache.ibatis.annotations.Mapper;
import org.apache.ibatis.annotations.Select;

import java.util.List;

/**
 * @Description:
 * @Author: xxl
 * @Date: 2023/02/10 17:24
 * @Version: 1.0
 */
@Mapper
public interface UserMapper {

    /**
     * @author xxl
     * @description 保存用户
     * @params [user]
     * @date 2023/02/10 17:24
     */
    @Insert("insert into user(nickname,password,sex,birthday,db) values(#{nickname},#{password},#{sex},#{birthday},#{db})")
    void addUser(User user);

    /**
     * @author xxl
     * @description 保存用户
     * @params [user]
     * @date 2023/02/10 17:24
     */
    @Select("select * from user")
    List<User> findUsers();
}
```

controller

```java
package com.xxl.shardingjdbc.controller;

import com.xxl.shardingjdbc.entity.User;
import com.xxl.shardingjdbc.mapper.UserMapper;
import org.apache.tomcat.jni.Address;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RestController;

import java.util.List;
import java.util.Random;

/**
 * @Description:
 * @Author: xxl
 * @Date: 2023/02/10 17:25
 * @Version: 1.0
 */
@RestController
@RequestMapping("/user")
public class UserController {

    @Autowired
    private UserMapper userMapper;

    @GetMapping("/save")
    public String insert() {
        User user = new User();
        user.setNickname("zhangsan" + new Random().nextInt());
        user.setPassword("1234567");
        user.setSex(1);
        user.setBirthday("1988-12-03");
        userMapper.addUser(user);
        return "success";
    }

    @GetMapping("/listuser")
    public List<User> listuser() {
        return userMapper.findUsers();
    }

}
```

### 5、 访问测试查看效果

1：访问 `http://localhost:8085/user/save` 一直进入到ds1主节点

2：访问 `http://localhost:8085/user/listuser` 一直进入到ds2、ds3节点，并且轮询进入。

### 6、 日志查看

---

---
url: /常用框架/ShardingJdbc/4_ShardingJdbc配置分库分表.md
---

# ShardingJdbc配置分库分表

## 逻辑表

逻辑表是指：水平拆分的数据库或者数据表的相同路基和数据结构表的总称。

比如用户数据根据用户id%2拆分为2个表，分别是：ksd\_user0和ksd\_user1。他们的逻辑表名是：ksd\_user

在shardingjdbc中的定义方式如下：

```yaml
spring:
  shardingsphere:
    sharding:
      tables:
        # ksd_user 逻辑表名
        ksd_user:
```

## 分库分表数据节点 - actual-data-nodes

```yaml
 tables:
        # ksd_user 逻辑表名
        ksd_user:
          # 数据节点：多数据源$->{0..N}.逻辑表名$->{0..N} 相同表
          actual-data-nodes: ds$->{0..2}.ksd_user$->{0..1}
           # 数据节点：多数据源$->{0..N}.逻辑表名$->{0..N} 不同表
          actual-data-nodes: ds0.ksd_user$->{0..1},ds1.ksd_user$->{2..4}
          # 指定单数据源的配置方式
          actual-data-nodes: ds0.ksd_user$->{0..4}
          # 全部手动指定
          actual-data-nodes: ds0.ksd_user0,ds1.ksd_user0,ds0.ksd_user1,ds1.ksd_user1,

```

数据分片是最小单元。由数据源名称和数据表组成，比如：ds0.ksd\_user0。

寻找规则如下：

![img](/assets/38bf8afce306037fed0f725894872910.B7U2NCRM.png)

## 分库分表5种分片策略

![img](/assets/a3cff21ef0c4d8a39b6c2053a84a6c24.CF95RVsM.png)

数据源分片分为两种：

* 数据源分片
* 表分片

这两个是不同维度的分片规则，但是它们额能用的分片策略和规则是一样的。它们由两部分构成：

* 分片键
* 分片算法

### 第一种：none

对应NoneShardingStragey,不分片策略，SQL会被发给所有节点去执行，这个规则没有子项目可以配置。

### 第二种：inline 行表达时分片策略(核心，必须要掌握)

对应InlineShardingStragey。使用Groovy的表达时，提供对SQL语句种的=和in的分片操作支持，只支持单分片键。对于简单的分片算法，可以通过简单的配置使用，从而避免繁琐的Java代码开放，如：ksd\_user${分片键（数据表字段）userid % 5} 表示ksd\_user表根据某字段（userid）模 5.从而分为5张表，表名称为：ksd\_user0到ksd\_user4 。如果库也是如此。

```yaml
server:
  port: 8085
spring:
  main:
    allow-bean-definition-overriding: true
  shardingsphere:
    # 参数配置，显示sql
    props:
      sql:
        show: true
    sharding:
      # 默认数据源，主要用于写，注意一定要配置读写分离 ,注意：如果不配置，那么就会把三个节点都当做从slave节点，新增，修改和删除会出错。
      default-data-source-name: ds0
      # 配置分表的规则
      tables:
        # ksd_user 逻辑表名
        ksd_user:
          # 数据节点：数据源$->{0..N}.逻辑表名$->{0..N}
          actual-data-nodes: ds$->{0..1}.ksd_user$->{0..1}
          # 拆分库策略，也就是什么样子的数据放入放到哪个数据库中。
          database-strategy:
            inline:
              sharding-column: sex    # 分片字段（分片键）
              algorithm-expression: ds$->{sex % 2} # 分片算法表达式
          # 拆分表策略，也就是什么样子的数据放入放到哪个数据表中。
          table-strategy:
            inline:
              sharding-column: age    # 分片字段（分片键）
              algorithm-expression: ksd_user$->{age % 2} # 分片算法表达式

```

algorithm-expression行表达式：

* ${begin…end} 表示区间范围。
* ${\[unit1,unit2,….,unitn]} 表示枚举值。
* 行表达式种如果出现连续多个e x p r e s s s i o n 或 {expresssion}或expresssion或->{expression}表达式，整个表达时最终的结果将会根据每个子表达式的结果进行笛卡尔组合。

![img](/assets/0b4f58f74bac6b5baf843683f3213d20.Cd_SvWZo.png)

1、完整案例和配置如下

准备两个数据库ksd\_sharding-db。名字相同，两个数据源ds0和ds1

每个数据库下方ksd\_user0和ksd\_user1即可。

数据库规则，性别为偶数的放入ds0库，奇数的放入ds1库。

数据表规则：年龄为偶数的放入ksd\_user0库，奇数的放入ksd\_user1库。

```yaml
server:
  port: 8085
spring:
  main:
    allow-bean-definition-overriding: true
  shardingsphere:
    # 参数配置，显示sql
    props:
      sql:
        show: true
    # 配置数据源
    datasource:
      # 给每个数据源取别名，下面的ds1,ds1任意取名字
      names: ds0,ds1
      # 给master-ds1每个数据源配置数据库连接信息
      ds0:
        # 配置druid数据源
        type: com.alibaba.druid.pool.DruidDataSource
        driver-class-name: com.mysql.cj.jdbc.Driver
        url: jdbc:mysql://localhost:3306/xxl-sharding-db1?useUnicode=true&characterEncoding=utf8&tinyInt1isBit=false&useSSL=false&serverTimezone=GMT
        username: root
        password: xxl666
        maxPoolSize: 100
        minPoolSize: 5
      # 配置ds1-slave
      ds1:
        type: com.alibaba.druid.pool.DruidDataSource
        driver-class-name: com.mysql.cj.jdbc.Driver
        url: jdbc:mysql://localhost:3306/xxl-sharding-db2?useUnicode=true&characterEncoding=utf8&tinyInt1isBit=false&useSSL=false&serverTimezone=GMT
        username: root
        password: xxl666
        maxPoolSize: 100
        minPoolSize: 5
    # 配置默认数据源ds0
    sharding:
      # 默认数据源，主要用于写，注意一定要配置读写分离 ,注意：如果不配置，那么就会把三个节点都当做从slave节点，新增，修改和删除会出错。
      default-data-source-name: ds0
      # 配置分表的规则
      tables:
        # ksd_user 逻辑表名
        ksd_user:
          # 数据节点：数据源$->{0..N}.逻辑表名$->{0..N}
          actual-data-nodes: ds$->{0..1}.ksd_user$->{0..1}
          # 拆分库策略，也就是什么样子的数据放入放到哪个数据库中。
          database-strategy:
            inline:
              sharding-column: sex    # 分片字段（分片键）
              algorithm-expression: ds$->{sex % 2} # 分片算法表达式
          # 拆分表策略，也就是什么样子的数据放入放到哪个数据表中。
          table-strategy:
            inline:
              sharding-column: age    # 分片字段（分片键）
              algorithm-expression: ksd_user$->{age % 2} # 分片算法表达式
# 整合mybatis的配置XXXXX
mybatis:
  mapper-locations: classpath:mapper/*.xml
  type-aliases-package: com.xxl.shardingjdbc.entity

```

结果如下图：

![img](/assets/32fe2f9390873de9a8296c4a44b6f8b8.BwvO8aKc.png)

### 第三种：根据实时间日期 - 按照标准规则分库分表

1、 标准分片 - Standard(了解)

对应StrandardShardingStrategy.提供对SQL语句中的=，in和恶between and 的分片操作支持。

StrandardShardingStrategy只支持但分片键。提供PreciseShardingAlgorithm和RangeShardingAlgorithm两个分片算法。

PreciseShardingAlgorithm是必选的呃，用于处理=和IN的分片和RangeShardingAlgorithm是可选的，是用于处理Betwwen and分片，如果不配置和RangeShardingAlgorithm,SQL的Between AND 将按照全库路由处理。

2、定义分片的日期规则配置

```yaml
server:
  port: 8085
spring:
  main:
    allow-bean-definition-overriding: true
  shardingsphere:
    # 参数配置，显示sql
    props:
      sql:
        show: true
    # 配置数据源
    datasource:
      # 给每个数据源取别名，下面的ds1,ds1任意取名字
      names: ds0,ds1
      # 给master-ds1每个数据源配置数据库连接信息
      ds0:
        # 配置druid数据源
        type: com.alibaba.druid.pool.DruidDataSource
        driver-class-name: com.mysql.cj.jdbc.Driver
        url: jdbc:mysql://localhost:3306/xxl-sharding-db1?useUnicode=true&characterEncoding=utf8&tinyInt1isBit=false&useSSL=false&serverTimezone=GMT
        username: root
        password: xxl666
        maxPoolSize: 100
        minPoolSize: 5
      # 配置ds1-slave
      ds1:
        type: com.alibaba.druid.pool.DruidDataSource
        driver-class-name: com.mysql.cj.jdbc.Driver
        url: jdbc:mysql://localhost:3306/xxl-sharding-db2?useUnicode=true&characterEncoding=utf8&tinyInt1isBit=false&useSSL=false&serverTimezone=GMT
        username: root
        password: xxl666
        maxPoolSize: 100
        minPoolSize: 5
    # 配置默认数据源ds0
    sharding:
      # 默认数据源，主要用于写，注意一定要配置读写分离 ,注意：如果不配置，那么就会把三个节点都当做从slave节点，新增，修改和删除会出错。
      default-data-source-name: ds0
      # 配置分表的规则
      tables:
        # ksd_user 逻辑表名
        ksd_user:
          # 数据节点：数据源$->{0..N}.逻辑表名$->{0..N}
          actual-data-nodes: ds$->{0..1}.ksd_user$->{0..1}
          # 拆分库策略，也就是什么样子的数据放入放到哪个数据库中。
          database-strategy:
            standard:
              shardingColumn: birthday
              preciseAlgorithmClassName: com.xuexiangban.shardingjdbc.algorithm.BirthdayAlgorithm
          table-strategy:
            inline:
              sharding-column: age    # 分片字段（分片键）
              algorithm-expression: ksd_user$->{age % 2} # 分片算法表达式
# 整合mybatis的配置XXXXX
mybatis:
  mapper-locations: classpath:mapper/*.xml
  type-aliases-package: com.xxl.shardingjdbc.entity

```

3、定义分片的日期规则

```java

public class BirthdayAlgorithm implements PreciseShardingAlgorithm<Date> {
    List<Date> dateList = new ArrayList<>();
    {
        Calendar calendar1 = Calendar.getInstance();
        calendar1.set(2020, 1, 1, 0, 0, 0);
        Calendar calendar2 = Calendar.getInstance();
        calendar2.set(2021, 1, 1, 0, 0, 0);
        Calendar calendar3 = Calendar.getInstance();
        calendar3.set(2022, 1, 1, 0, 0, 0);
        dateList.add(calendar1.getTime());
        dateList.add(calendar2.getTime());
        dateList.add(calendar3.getTime());
    }
    @Override
    public String doSharding(Collection<String> collection, PreciseShardingValue<Date> preciseShardingValue) {
        // 获取属性数据库的值
        Date date = preciseShardingValue.getValue();
        // 获取数据源的名称信息列表
        Iterator<String> iterator = collection.iterator();
        String target = null;
        for (Date s : dateList) {
            target = iterator.next();
            // 如果数据晚于指定的日期直接返回
            if (date.before(s)) {
                break;
            }
        }
        return target;
    }
}

```

4、测试查看结果

http://localhost:8085/user/save?sex=3\&age=3\&birthday=2020-03-09 —- ds1

http://localhost:8085/user/save?sex=3\&age=3\&birthday=2021-03-09 —- ds0

### 第四种：ShardingSphere - 符合分片策略（了解）

对应接口：HintShardingStrategy。通过Hint而非SQL解析的方式分片的策略。

对于分片字段非SQL决定，而是由其他外置条件决定的场景，克使用SQL hint灵活的注入分片字段。例如：按照用户登录的时间，主键等进行分库，而数据库中并无此字段。SQL hint支持通过Java API和SQL注解两种方式使用。让后分库分表更加灵活。

![img](/assets/69613e757df2a6939e92087fde2097ba.C2myu06-.png)

### 第五种：ShardingSphere - hint分片策略（了解）

对应ComplexShardingStrategy。符合分片策略提供对SQL语句中的，in和between and的分片操作支持。
ComplexShardingStrategy支持多分片键，由于多分片键之间的关系复杂，因此并未进行过多的封装，而是直接将分片键组合以及分片操作符透传至分片算法，完全由开发者自己实现，提供最大的灵活度。

![img](/assets/bc9bda0c7c525216b91af7cbf38cae11.DhSVOYj4.png)

---

---
url: /常用框架/ShardingJdbc/1_ShardingJdbc准备-安装MySQL及配置主从复制.md
---

# ShardingJdbc准备-安装MySQL及配置主从复制

---

---
url: /Java/架构设计/分布式/06.分布式监控/SkyWalking.md
---

# SkyWalking链路追踪

> 官网：https://skywalking.apache.org

## 一、简介

* 什么是链路追踪

随着微服务分布式系统变得日趋复杂，越来越多的组件开始走向分布式化，如分布式服务、分布式数据库、分布式缓存等，使得后台服务构成了一种复杂的分布式网络。在服务能力提升的同时，复杂的网络结构也使问题定位更加困难。在一个请求在经过诸多服务过程中，出现了某一个调用失败的情况，查询具体的异常由哪一个服务引起的就变得十分抓狂，问题定位和处理效率是也会非常低。

分布式链路追踪就是将一次分布式请求还原成调用链路，将一次分布式请求的调用情况集中展示，比如各个服务节点上的耗时、请求具体到达哪台机器上、每个服务节点的请求状态等等。

* 为什么要使用链路追踪

链路追踪为分布式应用的开发者提供了完整的调用链路还原、调用请求量统计、链路拓扑、应用依赖分析等工具，可以帮助开发者快速分析和诊断分布式应用架构下的性能瓶颈，提高微服务时代下的开发诊断效率。

* skywalking 链路追踪

`SkyWalking`是一个可观测性分析平台（Observability Analysis Platform 简称OAP）和应用性能管理系统（Application Performance Management 简称 APM）。

提供分布式链路追踪，服务网格（Service Mesh）遥测分析，度量（Metric）聚合和可视化一体化解决方案。

SkyWalking 特点

* 多语言自动探针，java，.Net Code ,Node.Js
* 多监控手段，语言探针和Service Mesh
* 轻量高效，不需要额外搭建大数据平台
* 模块化架构，UI ，存储《集群管理多种机制可选
* 支持警告
* 优秀的可视化效果。

下面是`SkyWalking`的架构图：

![skywalking](/assets/cf05a5a5_1151004.C_vDX2UO.png)

## 二、安装

从地址`https://archive.apache.org/dist/skywalking/`

或skywalking的官网`http://skywalking.apache.org/downloads`

下载包，包的结构如下：

```
skywalking
|-- agent     本地代理模块（探针）
|-- bin       收集启动脚本
|-- config    skywalking数据收集器配置，告警配置等
|-- config-examples
|-- LICENSE
|-- licenses
|-- NOTICE
|-- oap-libs
|-- README.txt
|-- tools
|-- webapp    数据展示UI服务，系统依赖展示，链路追踪
```

### Windows平台安装

可以从下载地址下载`apache-skywalking-apm-$version.tar.gz`包。

Windows下载解压后（.tar.gz），直接点击`bin/startup.bat`就可以了，这个时候实际上是启动了两个项目，一个收集器，一个web页面。

### Linux平台安装

#### 下载

如果数据是存储在elasticsearch，需要下载对应的版本，否则启动报错

```shell
wget https://archive.apache.org/dist/skywalking/8.7.0/apache-skywalking-apm-es7-8.7.0.tar.gz
tar -xzvf apache-skywalking-apm-es7-8.7.0.tar.gz
```

修改配置

```shell
cd apache-skywalking-apm-bin-es7/config
vi application.yml
```

修改存储类型

```yaml
storage:
  selector: ${SW_STORAGE:elasticsearch7}
```

修改elasticsearch配置

```yaml
elasticsearch7:
   clusterNodes: ${SW_STORAGE_ES_CLUSTER_NODES:localhost:9200}  #es7机器地址
   #略，其他配置不用动
   user: ${SW_ES_USER:"es账号"}  #如果es开启认证，需要填账号密码，否则不动即可
   password: ${SW_ES_PASSWORD:"es密码"} #如果es开启认证，需要填账号密码，否则不动不动即可
```

#### 启动收集器

`skywalking`提供了一个可视化的监控平台.

启动skywalking收集器服务，启动脚本为`bin`目录下`startup.sh`.

webapp相关配置修改在`webapp`目录下`webapp.yml`.

安装好之后，在浏览器中访问默认地址：http://IP:8080 就可以访问了。

![image-20240711102104024](/assets/image-20240711102104024.oHwYuNCs.png)

#### 启动项目

拷贝agent目录到所需位置，探针包含整个目录，不要改变目录结构，可修改`agent/config/agent.config`配置`agent.service_name=xxl-job`为自己的应用名，增加JVM启动参数`-javaagent:/.../skywalking-agent/skywalking-agent.jar`。参数值为skywalking-agent.jar的绝对路径

### Docker安装

https://zhuanlan.zhihu.com/p/479291356

## 三、使用

复制安装包目录中的agent目录到java程序所在的机器上，探针包含整个目录，不要改变目录结构，可修改`agent/config/agent.config`配置`agent.service_name=xxl-job`为自己的应用名，增加JVM启动参数

```sh
-javaagent:/skywalking-agent.jar的绝对路径/skywalking-agent.jar -Dskywalking.agent.service_name=服务名 -Dskywalking.collector.backend_service=上边安装的服务器ip:11800 
```

例如

```sh
java -javaagent:/data/apache-skywalking-apm-bin-es7/agent/skywalking-agent.jar -Dskywalking.agent.namespace=dev -Dskywalking.agent.service_name=service-gateway -Dskywalking.collector.backend_service=127.0.0.1:11800 -Dspring.profiles.active=dev -jar /demo.jar
```

参考资料：

\[1]. https://blog.csdn.net/chengqwertyuiop/article/details/125065633

\[2]. http://doc.ruoyi.vip/ruoyi-cloud/cloud/skywalking.html

agent：https://zhuanlan.zhihu.com/p/483352555

简介：https://blog.csdn.net/feiying0canglang/article/details/120319399

https://www.cnblogs.com/swave/p/11347711.html

https://juejin.cn/post/7379416139975589951?searchId=20240711111734E120A0E1DA0C45950289#heading-3

https://juejin.cn/post/7330828921100861478?searchId=20240711111734E120A0E1DA0C45950289#heading-8

---

---
url: /Java/微服务专栏/08.Solon/Solon.md
---

# Solon

Java 新的生态型应用开发框架

> 官方文档：https://solon.noear.org/
>
> 基于Snowy的后台管理框架：https://gitee.com/xiaonuoadmin/snowy-solon

---

---
url: /常用框架/SpringAIAlibaba/1_SpringAIAlibaba概览.md
---

# Spring AI Alibaba 概览

## 一、简介

**Spring AI Alibaba** 是阿里云推出的面向 Java 开发者的开源 AI 应用开发框架，基于 Spring AI 构建，旨在简化生成式 AI 应用的开发。它深度集成了阿里云通义系列大模型及云原生基础设施，提供高层次的 AI API 抽象，帮助开发者快速构建智能化应用。

## 二、项目架构

![Architecture](/assets/architecture-155a1907f1260526a6d192649944300b.BPLf5Dn8.png)

Spring AI Alibaba 项目从架构上包含如下三层：

* **Agent Framework**，是一个以 ReactAgent 设计理念为核心的 Agent 开发框架，使开发者能够构建具备自动上下文工程和人机交互等核心能力的Agent。
* **Graph**，graph 是一个低级别的工作流和多代理协调框架，能够帮助开发者实现复杂的应用程序编排，它具备丰富的预置节点和简化的图状态定义，Graph 是 Agent Framework 的底层运行时基座。
* **Augmented LLM**，以 Spring AI 框架底层原子抽象为基础，为构建大型语言模型（LLM）应用提供基础抽象，例如模型（Model）、工具（Tool）、多模态组件（MCP）、消息（Message）、向量存储（Vector Store）等。

## 三、设计原则

Spring AI Alibaba 推荐使用 Agent Framework 内置的 `ReactAgent` 抽象快速构建 Agent 应用。

如果需要组合 Multi-agent，Agent Framework 还预置了如 `SequentialAgent（顺序代理）` 、`ParallelAgent（并行代理）` 、`RoutingAgent（路由代理）` 和 `LoopAgent（循环代理）` 等基础多智能体工作流模式。

对于一些开发场景而言，直接使用 `Graph API` 也是可行的，它能为应用开发提供更灵活的编排、更直接的状态控制，适用于需要超高可靠性、大量自定义逻辑、需要精确控制延迟时的场景。

## 四、示例

引入JAR包

```xml
<dependency>
    <groupId>com.alibaba.cloud.ai</groupId>
    <artifactId>spring-ai-alibaba-agent-framework</artifactId>
    <version>1.1.0.0-M5</version>
</dependency>

<dependency>
    <groupId>com.alibaba.cloud.ai</groupId>
    <artifactId>spring-ai-alibaba-starter-dashscope</artifactId>
    <version>1.1.0.0-M5</version>
</dependency>
```

代码示例

```java
import com.alibaba.cloud.ai.dashscope.api.DashScopeApi;
import com.alibaba.cloud.ai.dashscope.chat.DashScopeChatModel;
import com.alibaba.cloud.ai.graph.agent.ReactAgent;
import com.alibaba.cloud.ai.graph.CompileConfig;

import org.springframework.ai.chat.model.ChatModel;

public class AgentExample {

    public static void main(String[] args) throws Exception {
        // 创建模型实例
        DashScopeApi dashScopeApi = DashScopeApi.builder()
            .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
            .build();
        ChatModel chatModel = DashScopeChatModel.builder()
            .dashScopeApi(dashScopeApi)
            .build();

        // 创建 Agent
        ReactAgent agent = ReactAgent.builder()
            .name("weather_agent")
            .model(chatModel)
            .instruction("You are a helpful weather forecast assistant.")
            .build();

        // 运行 Agent
        agent.call("what is the weather in Hangzhou?");
    }
}
```

## 五、核心特性

Spring AI Alibaba 提供了丰富的功能，支持开发复杂的 AI 应用。以下是其主要特性：

* **模型交互与适配**：支持文本、图像、语音等多模态输入输出，提供同步、异步及流式通信模式。

* **提示词模板管理**：通过 Prompt Template 简化提示词的定义与动态替换。

* **结构化输出**：将非格式化的模型输出自动转换为 JavaBean，确保数据格式一致性。

* **函数调用**：支持 Function Calling，模型可调用预定义函数辅助完成任务。

* **检索增强（RAG）**：结合向量数据库，实现上下文增强的模型交互。

* **多轮对话支持**：通过 Chat Memory 管理对话上下文，支持连续对话。

* **ReactAgent**：构建具有推理和行动能力的智能代理，遵循 ReAct（推理 + 行动）范式，用于迭代解决问题。

* **多代理编排**：使用内置模式（包括 `SequentialAgent1`、`ParallelAgent`、`LlmRoutingAgent`和`LoopAgent`）组合多个代理，以执行复杂的任务。

* **上下文工程**：内置快速工程、上下文管理和对话流控制的最佳实践，以提高代理的可靠性和性能。

* **人机协同**：将人工反馈和审批步骤无缝集成到代理工作流程中，从而实现关键工具和操作的监督执行。

* **流式传输支持**：代理响应的实时流式传输

* **错误处理**：强大的错误恢复和重试机制

* **基于图的工作流**：基于图的工作流运行时和 API，用于条件路由、嵌套图、并行执行和状态管理。可将工作流导出为 PlantUML 和 Mermaid 格式。

* **A2A 支持**：通过 Nacos 集成支持代理间通信，实现跨服务的分布式代理协调和协作。

* **丰富的模型、工具和 MCP 支持**：利用 Spring AI 的核心概念，支持多种 LLM 提供程序（DashScope、OpenAI 等）、工具调用和模型上下文协议 (MCP)。

---

---
url: /常用框架/SpringBoot/SpringBoot程序开发/20_SpringBoot项目打包发布.md
---

# Spring Boot项目打包发布

Spring Boot 为我们使用、构建和运行 Spring 项目带来了极大的方便，Spring Boot 可以通过 Gradle 或者 Maven 插件将项目构建成可执行的 Jar 包，使得我们写的 Web 项目也可以直接通过 java -jar xxx.jar 方式直接启动，下面我们根据源码来看看，Spring Boot 是如何将代码及依赖的 Jar 包通过插件构建到一个完整的 Jar 包里。

根据 Spring 官方文档和 pom.xml 文件可以发现，Spring Boot 在 Maven 是通过引入 spring-boot-maven-plugin 插件来构建可执行 Jar 包的。

```xml
<build>
    <plugins>
        <plugin>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-maven-plugin</artifactId>
        </plugin>
    </plugins>
</build>
```

我们可以找到 spring-boot-maven-plugin 官方文档[打包可执行档案 ：： Spring Boot](https://docs.spring.io/spring-boot/maven-plugin/packaging.html)，其实 spring-boot-maven-plugin 的真实配置是下面这样的，配置了一个名为 repackage 的 goal，如果项目使用了 spring-boot-starter-parent，它就默认帮我们省去了这个配置。

```xml
<build>
    <plugins>
        <plugin>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-maven-plugin</artifactId>
            <executions>
                <execution>
                    <goals>
                        <goal>repackage</goal>
                    </goals>
                </execution>
            </executions>
        </plugin>
    </plugins>
</build>
```

将当前项目模块进行打包处理（clean package），打包完成后，会在项目的target目录下生成xxx.jar程序文件。

将xxx.jar文件复制到部署目录下，随后通过命令行方式执行此文件。

```sh
java -jar xxx.jar
```

此时，SpringBoot项目将以一个独立的\*.jar文件的方式执行。将此jar文件上传到任何配置有JDK的系统内，可以轻松实现项目的发布。

参考资料：https://blog.csdn.net/ttzommed/article/details/114984341

---

---
url: /常用框架/SpringBoot/SpringBoot程序开发/22_SpringBoot中的异步方法调用.md
---

# Spring Boot中的异步方法调用

## Spring Boot中的异步方法调用

在Spring Boot中，异步方法调用是通过@Async注解实现的。这个注解可以将一个同步方法转换为异步执行，即在不同的线程中运行。这样做的好处是可以提高应用程序的性能，特别是在处理耗时任务时，可以避免阻塞主线程。

### 使用@Async注解

要在Spring Boot中使用 *@Async* 注解，首先需要在启动类上添加 *@EnableAsync* 注解来启用异步支持。然后，可以在需要异步执行的方法上添加 *@Async* 注解。例如：

```java
@Service
public class AsyncService {

    @Async
    public void asyncMethod() {
    	// 异步执行的代码
    }
}
```

这个方法现在会在单独的线程中异步执行。如果方法有返回值，可以使用*Future*接口来处理异步结果。例如：

```java
@Service
public class AsyncService {

    @Async
    public Future<String> asyncMethodWithReturn() {
        // 异步执行的代码
        return new AsyncResult<>("结果");
    }
}
```

### 自定义线程池

默认情况下，*@Async*注解会使用*SimpleAsyncTaskExecutor*作为线程池。但是，这个默认的线程池对于生产环境通常不是最佳选择，因为它会为每个任务创建一个新线程。因此，推荐自定义线程池来优化性能和资源使用。可以通过实现*AsyncConfigurer*接口并重写*getAsyncExecutor*方法来自定义线程池。例如：

```java
@Configuration
@EnableAsync
public class AsyncConfig implements AsyncConfigurer {

    @Override
    public Executor getAsyncExecutor() {
        ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();
        executor.setCorePoolSize(10);
        executor.setMaxPoolSize(50);
        executor.setQueueCapacity(100);
        executor.initialize();
        return executor;
    }
}
```

在这个配置中，定义了一个核心线程数为10，最大线程数为50，队列容量为100的线程池。这样配置后，*@Async*注解就会使用这个自定义的线程池来执行任务。

### 异步方法的限制

使用 *@Async* 注解时，有一些限制需要注意：

1. 异步方法不能是*static*方法。
2. 异步方法所在的类必须被Spring容器管理，即通过 *@Component* 或其他相关注解声明。
3. 异步方法不能和调用它的方法在同一个类中。
4. 类中的依赖必须通过 *@Autowired* 或 *@Resource* 等注解自动注入，不能手动创建对象。
5. 如果使用Spring Boot框架，必须在启动类中添加 *@EnableAsync* 注解。
6. 在异步方法上标注 *@Transactional* 是无效的，应该在调用异步方法的方法上标注。

### 默认线程与自定义线程池

`SimpleAsyncTaskExecutor` 的特点

* **非池化**：`SimpleAsyncTaskExecutor` 不是一个真正的线程池，它每次都会创建一个新的线程来执行任务。
* **无限创建线程**：如果没有限制，它会无限创建新线程，这可能导致资源耗尽。
* **不会自动关闭**：`SimpleAsyncTaskExecutor` 不会自动关闭其创建的线程，因为它是基于简单的线程创建机制，而不是线程池。

测试代码

AynscController

```java
/**
 * @Author: xxl
 * @Date: 2024/11/22 15:17
 */
@RestController
@RequestMapping("/api/aynsc")
public class AynscController {

    @Autowired
    private AynscService aynscService;

    @GetMapping("/testAynsc")
    public String testAynsc() {
        for (int i = 1; i <= 100; i++) {
            aynscService.sendAynscMessage(i);
        }
        return "执行成功";
    }
}
```

AynscService

```java
/**
 * @Author: xxl
 * @Date: 2024/11/22 15:17
 */
@Component
public class AynscService {


    @Async
    public void sendAynscMessage(Integer index) {
        System.out.println("发送异步消息，第" + index + "次！");
    }

}
```

SimpleAsyncTaskExecutor

![image-20241122154418208](/assets/image-20241122154418208.heB3rVX2.png)

添加自定义线程池

```java
/**
 * 自定义线程池
 *
 * @Author: xxl
 * @Date: 2024/11/22 15:38
 */
@Configuration
public class AsyncConfig {

    @Bean
    public ThreadPoolTaskExecutor taskExecutor() {
        ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();
        executor.setCorePoolSize(4); // 核心线程数
        executor.setMaxPoolSize(8); // 最大线程数
        executor.setQueueCapacity(100); // 队列容量
        executor.setThreadNamePrefix("async-thread-"); // 线程名称前缀
        executor.initialize();
        return executor;
    }
}
```

或

```java
/**
 * @Author: xxl
 * @Date: 2024/11/22 16:14
 */
@Configuration
@EnableAsync
public class AsyncConfig2 implements AsyncConfigurer {

    @Override
    public Executor getAsyncExecutor() {
        ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();
        executor.setCorePoolSize(5);
        executor.setMaxPoolSize(10);
        executor.setQueueCapacity(25);
        executor.setThreadNamePrefix("Async-");
        executor.initialize();
        return executor;
    }
}
```

自定义线程池

![image-20241122154028083](/assets/image-20241122154028083.CSE5iHw7.png)

> 获取源码可访问
>
> [springboot\_chowder/springboot\_aynsc at main · Daneliya/springboot\_chowder](https://github.com/Daneliya/springboot_chowder/tree/main/springboot_aynsc)

### 总结

*@Async*注解提供了一种简单而强大的方式来实现异步方法调用，可以显著提高应用程序的性能。通过自定义线程池，可以进一步优化资源使用和控制异步任务的执行。在使用时，需要注意一些限制条件，以确保异步调用能够正确执行。

---

---
url: /常用框架/重试框架/1_Spring-Retry.md
---

# Spring-Retry

`Spring Retry`是`Spring`框架提供的一个模块，它通过提供注解或编程方式的方式，帮助我们实现方法级别的重试机制。

## 一、Spring-Retry的普通使用方式

Spring Retry 为 Spring 应用程序提供了声明性重试支持。它用于Spring批处理、Spring集成、Apache Hadoop(等等)。它主要是针对可能抛出异常的一些调用操作，进行有策略的重试。

### 1、引入依赖

```java
 <dependency>
    <groupId>org.springframework.retry</groupId>
    <artifactId>spring-retry</artifactId>
    <version>1.2.2.RELEASE</version>
 </dependency>
```

### 2、重试方法

准备一个任务方法，采用一个随机整数，根据不同的条件返回不同的值，或者抛出异常

```java
/**
 * spring_retry 普通使用方式
 *
 * @author xxl
 * @date 2022/12/26 22:53
 */
@Slf4j
public class RetryDemoTask {

    public static void main(String[] args) {
        retryTask("参数");
    }

    /**
     * 重试方法：采用一个随机整数，根据不同的条件返回不同的值，或者抛出异常
     *
     * @return 返回结果
     */
    public static boolean retryTask(String param) {
        log.info("收到请求参数:{}", param);

        int i = RandomUtils.nextInt(0, 11);
        log.info("随机生成的数:{}", i);
        if (i == 0) {
            log.info("为0,抛出参数异常.");
            throw new IllegalArgumentException("参数异常");
        } else if (i == 1) {
            log.info("为1,返回true.");
            return true;
        } else if (i == 2) {
            log.info("为2,返回false.");
            return false;
        } else {
            //为其他
            log.info("大于2,抛出自定义异常.");
            throw new RemoteAccessException("大于2,抛出远程访问异常");
        }
    }

}
```

### 3、使用 SpringRetryTemplate

```java
/**
 * spring_retry 测试类
 *
 * @author xxl
 * @date 2022/12/26 22:58
 */
@Slf4j
public class SpringRetryTemplateTest {

    /**
     * 重试间隔时间ms,默认1000ms
     */
    private final static long fixedPeriodTime = 1000L;

    /**
     * 最大重试次数,默认为3
     */
    private final static int maxRetryTimes = 3;

    /**
     * 表示哪些异常需要重试,key表示异常的字节码,value为true表示需要重试
     */
    private final Map<Class<? extends Throwable>, Boolean> exceptionMap = new HashMap<>();


    @Test
    public void test() {
        exceptionMap.put(RemoteAccessException.class, true);

        // 构建重试模板实例
        RetryTemplate retryTemplate = new RetryTemplate();

        // 设置重试回退操作策略，主要设置重试间隔时间
        FixedBackOffPolicy backOffPolicy = new FixedBackOffPolicy();
        backOffPolicy.setBackOffPeriod(fixedPeriodTime);

        // 设置重试策略，主要设置重试次数
        SimpleRetryPolicy retryPolicy = new SimpleRetryPolicy(maxRetryTimes, exceptionMap);

        retryTemplate.setRetryPolicy(retryPolicy);
        retryTemplate.setBackOffPolicy(backOffPolicy);

        Boolean execute = retryTemplate.execute(
                //RetryCallback：重试回调逻辑实例，包装正常的功能操
                retryContext -> {
                    boolean b = RetryDemoTask.retryTask("abc");
                    log.info("调用的结果:{}", b);
                    return b;
                },
                retryContext -> {
                    //RecoveryCallback：整个执行操作结束的恢复操作实例
                    log.info("已达到最大重试次数或抛出了不重试的异常~~~");
                    return false;
                }
        );
        log.info("执行结果:{}", execute);
    }
}
```

代码解析：`RetryTemplate` 承担了重试执行者的角色，它可以设置`SimpleRetryPolicy`(重试策略，设置重试上限，重试的根源实体)，`FixedBackOffPolicy`（固定的回退策略，设置执行重试回退的时间间隔）。

`RetryTemplate`通过`execute`提交执行操作，需要准备`RetryCallback` 和`RecoveryCallback` 两个类实例，前者对应的就是重试回调逻辑实例，包装正常的功能操作，`RecoveryCallback`实现的是整个执行操作结束的恢复操作实例。

只有在调用的时候抛出了异常，并且异常是在`exceptionMap`中配置的异常，才会执行重试操作，否则就调用到`excute`方法的第二个执行方法`RecoveryCallback`中。

重试策略、回退策略还有很多种：

#### **重试策略**

* **NeverRetryPolicy：** 只允许调用`RetryCallback`一次，不允许重试
* **AlwaysRetryPolicy：** 允许无限重试，直到成功，此方式逻辑不当会导致死循环
* **SimpleRetryPolicy：** 固定次数重试策略，默认重试最大次数为3次，`RetryTemplate`默认使用的策略
* **TimeoutRetryPolicy：** 超时时间重试策略，默认超时时间为1秒，在指定的超时时间内允许重试
* **ExceptionClassifierRetryPolicy：** 设置不同异常的重试策略，类似组合重试策略，区别在于这里只区分不同异常的重试
* **CircuitBreakerRetryPolicy：** 有熔断功能的重试策略，需设置3个参数`openTimeout`、`resetTimeout`和`delegate`
* **CompositeRetryPolicy：** 组合重试策略，有两种组合方式，乐观组合重试策略是指只要有一个策略允许即可以重试，悲观组合重试策略是指只要有一个策略不允许即可以重试，但不管哪种组合方式，组合中的每一个策略都会执行

#### **重试回退策略**

重试回退策略，指的是每次重试是立即重试还是等待一段时间后重试。

默认情况下是立即重试，如果需要配置等待一段时间后重试则需要指定回退策略`BackoffRetryPolicy`。

* **NoBackOffPolicy：** 无退避算法策略，每次重试时立即重试
* **FixedBackOffPolicy：** 固定时间的退避策略，需设置参数`sleeper`和`backOffPeriod`，`sleeper`指定等待策略，默认是`Thread.sleep`，即线程休眠，`backOffPeriod`指定休眠时间，默认1秒
* **UniformRandomBackOffPolicy：** 随机时间退避策略，需设置`sleeper`、`minBackOffPeriod`和`maxBackOffPeriod`，该策略在`minBackOffPeriod`,`maxBackOffPeriod`之间取一个随机休眠时间，`minBackOffPeriod`默认500毫秒，`maxBackOffPeriod`默认1500毫秒
* **ExponentialBackOffPolicy：** 指数退避策略，需设置参数`sleeper`、`initialInterval`、`maxInterval`和`multiplie`r，`initialInterval`指定初始休眠时间，默认100毫秒，`maxInterval`指定最大休眠时间，默认30秒，`multiplier`指定乘数，即下一次休眠时间为`当前休眠时间*multiplier`
* **ExponentialRandomBackOffPolicy：** 随机指数退避策略，引入随机乘数可以实现随机乘数回退

### 4、运行结果

```sh
14:26:14.543 [main] DEBUG org.springframework.retry.support.RetryTemplate - Retry: count=0
14:26:14.548 [main] INFO com.xxl.retry.spring_retry.RetryDemoTask - 收到请求参数:abc
14:26:14.554 [main] INFO com.xxl.retry.spring_retry.RetryDemoTask - 随机生成的数:3
14:26:14.554 [main] INFO com.xxl.retry.spring_retry.RetryDemoTask - 大于2,抛出自定义异常.
14:26:15.568 [main] DEBUG org.springframework.retry.support.RetryTemplate - Checking for rethrow: count=1
14:26:15.568 [main] DEBUG org.springframework.retry.support.RetryTemplate - Retry: count=1
14:26:15.568 [main] INFO com.xxl.retry.spring_retry.RetryDemoTask - 收到请求参数:abc
14:26:15.568 [main] INFO com.xxl.retry.spring_retry.RetryDemoTask - 随机生成的数:1
14:26:15.568 [main] INFO com.xxl.retry.spring_retry.RetryDemoTask - 为1,返回true.
14:26:15.568 [main] INFO com.xxl.retry.spring_retry.SpringRetryTemplateTest - 调用的结果:true
14:26:15.569 [main] INFO com.xxl.retry.spring_retry.SpringRetryTemplateTest - 执行结果:true
```

## 二、Spring-Retry的注解使用方式

`Spring-Retry`也支持和`Spring-Boot`整合

### 1、注解介绍

只要在需要重试的方法上加`@Retryable`，在重试失败的回调方法上加`@Recover`，下面是这些注解的属性。

`@EnableRetry`，表示是否开启重试

| 序号 | 属性             | 类型    | 默认值 | 说明                                                         |
| ---- | ---------------- | ------- | ------ | ------------------------------------------------------------ |
| 1    | proxyTargetClass | boolean | false  | 指示是否要创建基于子类的(CGLIB)代理，而不是创建标准的基于Java接口的代理 |

`@Retryable`，标注此注解的方法在发生异常时会进行重试

| 序号 | 属性        | 类型     | 默认值     | 说明                                                         |
| ---- | ----------- | -------- | ---------- | ------------------------------------------------------------ |
| 1    | interceptor | String   | ""         | 将interceptor的bean名称应用到retryable()                     |
| 2    | value       | Class\[]  | {}         | 可重试的异常类型                                             |
| 3    | label       | String   | ""         | 统计报告的唯一标签。如果没有提供，调用这可以选择忽略它，或者提供默认值 |
| 4    | maxAttempts | int      | 3          | 尝试的最大次数(包括第一次失败)，默认为3次                    |
| 5    | backoff     | @Backoff | @Backoff() | 指定用于重试此操作的backoff属性。默认为空                    |
| 6    | exclude     | Class\[]  | {}         | 排除可重试的异常类型                                         |

`@Backoff`

| 序号 | 属性       | 类型    | 默认值 | 说明                                            |
| ---- | ---------- | ------- | ------ | ----------------------------------------------- |
| 1    | delay      | long    | 0      | 如果不设置则默认使用 1000 milliseconds 重试等待 |
| 2    | maxDelay   | long    | 0      | 最大重试等待时间                                |
| 3    | multiplier | long    | 0      | 用于计算下一个延迟的乘数（大于0生效）           |
| 4    | random     | boolean | false  | 随机重试等待时间                                |

### 2、引入依赖

```xml
 <dependency>
    <groupId>org.springframework.retry</groupId>
    <artifactId>spring-retry</artifactId>
    <version>1.2.2.RELEASE</version>
 </dependency>

 <dependency>
    <groupId>org.aspectj</groupId>
    <artifactId>aspectjweaver</artifactId>
    <version>1.9.1</version>
 </dependency>
```

### 3、启动类注解

在application启动类上加上`@EnableRetry`的注解

```java
/**
 * 启动类
 *
 * @author xxl
 * @date 2022/12/26 22:43
 */
@EnableRetry
@SpringBootApplication
public class ReplyApplication {
    
    public static void main(String[] args) {
        SpringApplication.run(ReplyApplication.class, args);
    }
    
}
```

### 4、重试方法

```java
/**
 * spring_retry 注解重试方法
 * 
 * @author xxl
 * @date 2022/12/26 23:12
 */
@Service
@Slf4j
public class SpringRetryDemo {

    /**
     * 重试所调用方法
     *
     * @param param
     * @return
     */
    @Retryable(value = {RemoteAccessException.class, NullPointerException.class}, maxAttempts = 3, backoff = @Backoff(delay = 2000L, multiplier = 2))
    public boolean call(String param) {
        return RetryDemoTask.retryTask(param);
    }

    /**
     * 达到最大重试次数,或抛出了一个没有指定进行重试的异常
     * recover 机制
     *
     * @param e 异常
     */
    @Recover
    public boolean recover(Exception e, String param) {
        log.error("达到最大重试次数,或抛出了一个没有指定进行重试的异常:", e);
        return false;
    }

}
```

`RemoteAccessException`的异常才重试，`@Backoff(delay = 2000L,multiplier = 2))`表示第一次间隔2秒，以后都是次数的2倍,也就是第二次4秒，第三次6秒。

### 5、使用 SpringRetryTemplate

```java
/**
 * spring_retry 注解测试类
 *
 * @author xxl
 * @date 2022/12/26 23:11
 */
@Component
@Slf4j
public class SpringRetryDemoTest extends MyBaseTest {

    @Autowired
    private SpringRetryDemo springRetryDemo;

    /**
     * 只要在需要重试的方法上加@Retryable，在重试失败的回调方法上加@Recover
     */
    @Test
    public void retry() {
        boolean abc = springRetryDemo.call("abc");
        log.info("--结果是:{}--", abc);
    }
}
```

```java
/**
 * 测试基类
 *
 * @author xxl
 * @date 2022/12/26 22:47
 */
@EnableRetry
@RunWith(SpringRunner.class)
@SpringBootTest(classes = ReplyApplication.class)
@Slf4j
public class MyBaseTest {


    @Before
    public void init() {
        log.info("----------------测试开始---------------");
    }

    @After
    public void after() {
        log.info("----------------测试结束---------------");
    }

}
```

### 6、运行结果

```
2025-08-20 14:56:20.911  INFO 23704 --- [           main] com.xxl.test.MyBaseTest                  : ----------------测试开始---------------
2025-08-20 14:56:20.941  INFO 23704 --- [           main] c.xxl.retry.spring_retry.RetryDemoTask   : 收到请求参数:abc
2025-08-20 14:56:20.947  INFO 23704 --- [           main] c.xxl.retry.spring_retry.RetryDemoTask   : 随机生成的数:7
2025-08-20 14:56:20.947  INFO 23704 --- [           main] c.xxl.retry.spring_retry.RetryDemoTask   : 大于2,抛出自定义异常.
2025-08-20 14:56:22.962  INFO 23704 --- [           main] c.xxl.retry.spring_retry.RetryDemoTask   : 收到请求参数:abc
2025-08-20 14:56:22.962  INFO 23704 --- [           main] c.xxl.retry.spring_retry.RetryDemoTask   : 随机生成的数:5
2025-08-20 14:56:22.962  INFO 23704 --- [           main] c.xxl.retry.spring_retry.RetryDemoTask   : 大于2,抛出自定义异常.
2025-08-20 14:56:26.964  INFO 23704 --- [           main] c.xxl.retry.spring_retry.RetryDemoTask   : 收到请求参数:abc
2025-08-20 14:56:26.964  INFO 23704 --- [           main] c.xxl.retry.spring_retry.RetryDemoTask   : 随机生成的数:7
2025-08-20 14:56:26.964  INFO 23704 --- [           main] c.xxl.retry.spring_retry.RetryDemoTask   : 大于2,抛出自定义异常.
2025-08-20 14:56:26.968 ERROR 23704 --- [           main] c.x.r.springboot_retry.SpringRetryDemo   : 达到最大重试次数,或抛出了一个没有指定进行重试的异常:

org.springframework.remoting.RemoteAccessException: 大于2,抛出远程访问异常
	at com.xxl.retry.spring_retry.RetryDemoTask.retryTask(RetryDemoTask.java:43) ~[classes/:na]
	at com.xxl.retry.springboot_retry.SpringRetryDemo.call(SpringRetryDemo.java:29) ~[classes/:na]
	at com.xxl.retry.springboot_retry.SpringRetryDemo$$FastClassBySpringCGLIB$$6b624fcd.invoke(<generated>) ~[classes/:na]
	......

2025-08-20 14:56:26.969  INFO 23704 --- [           main] c.x.t.r.s.SpringRetryDemoTest            : --结果是:false--
2025-08-20 14:56:26.970  INFO 23704 --- [           main] com.xxl.test.MyBaseTest                  : ----------------测试结束---------------
```

## 参考资料

各种重试方案

https://www.cnblogs.com/jingzh/p/17000807.html

https://cloud.tencent.com/developer/article/2101159

https://segmentfault.com/a/1190000022962709

https://www.51cto.com/article/813479.html

---

---
url: /常用框架/Spring/1_Spring容器初始化源码解析.md
---

# Spring容器初始化源码解析

在开始进行源码学习前，首先再回顾一下三种Spring编程风格：

* 基于`Schema`，即通过`xml`标签的配置方式
* 基于`Annotation`的注解技术，使用`@Component`等注解配置bean
* 基于`Java Config`，简单来说就是使用`@Configuration`和`@Bean`进行配置

基于注解的方式需要通过xml或java config来开启。在使用xml时，需要手动开启对注解的支持：

```xml
<context: annotation-config/> 
```

当然，如果在xml中配置了扫描包，现在也可以光添加下面这一行，这行代码中已经包含了注解的开启功能。

```xml
<context: component-sacn base-package="com"/>
```

如果使用的是下面`AnnotationConfigApplicationContext`这种方式，那么就不需要添加任何操作了，其中已经包含了对注解的支持。

```java
AnnotationConfigApplicationContext ctx
	=new AnnotationConfigApplicationContext(SpringConfig.class);
```

在实际使用过程中，三种方式是可以混合使用的，不存在冲突。按照下面这种方式作为`AnnotationConfigApplicationContext`传入的配置文件，即可实现三种风格的统一使用：

```java
@Configuration
@ComponentScan("com")
@ImportResource("classpath:spring.xml") 
public class SpringConfig{
}
```

官方更推荐使用注解的方式。

Spring Boot更多的是基于注解，省略了很多配置的过程，对新手更加友好，降低了劝退率，所以本文将基于注解的方式进行源码解析，另外再说明一下本文基于`spring-framework-5.0.x`源码。

使用注解的方式初始化一个Spring环境，只需要下面一行代码：

```java
AnnotationConfigApplicationContext context
    = new AnnotationConfigApplicationContext(SpringConfig.class);
```

如果看一下它的构造方法，那么可以将它做的工作拆分为三步，为了便于理解可以写成下面的形式，并分为三大模块分别进行说明。

## 构造方法

首先看一下`AnnotationConfigApplicationContext`的继承关系：

![f8a72df38258f4676a9055e58811e8399912c0.png](/assets/f8a72df38258f4676a9055e58811e8399912c0.Dv6xDmnT.png)

`AnnotationConfigApplicationContext`继承了`GenericApplicationContext`，那么我们先看`GenericApplicationContext`的构造方法：

```java
public GenericApplicationContext() {
  this.beanFactory = new DefaultListableBeanFactory();
}
```

在这里初始化了一个`beanFactory`的实现类`DefaultListableBeanFactory`，这就是我们常提到的spring中重要的bean工厂，这里面存放了很多非常重要的数据结构。这里先列出比较重要的`beanDefinitionMap`，会在后面频繁使用：

```java
private final Map<String, BeanDefinition> beanDefinitionMap = new ConcurrentHashMap<>(256);
private volatile List<String> beanDefinitionNames = new ArrayList<>(256);
```

在上面的这个`beanDefinitionMap`中就维护了`beanName`及`BeanDefinition`的对应关系，`beanDefinitionNames`则是一个存放`beanName`的List。

从`AnnotationConfigApplicationContext`的构造方法开始分析：

```java
public AnnotationConfigApplicationContext() {
  this.reader = new AnnotatedBeanDefinitionReader(this);
  this.scanner = new ClassPathBeanDefinitionScanner(this);
}
```

首先实例化了一个`AnnotatedBeanDefinitionReader`对象，看一下`AnnotatedBeanDefinitionReader`的构造函数：

```java
public AnnotatedBeanDefinitionReader(BeanDefinitionRegistry registry) {
  this(registry, getOrCreateEnvironment(registry));
}
```

那么，为什么在这能够将`AnnotationConfigApplicationContext`对象作为`BeanDefinitionRegistry`传入呢？

回头看一下继承关系那张图，`AnnotationConfigApplicationContext`继承了`BeanDefinitionRegistry`，并且最终实现了接口`BeanFactory`，`BeanFactory`可以说是Spring中的顶层类，它是一个工厂，能够产生bean对象，提供了一个非常重要的方法getBean，会在后面讲到。

到这，我们可以得出一个结论：

> `BeanDefinitionRegistry`可以等同于`AnnotationConfigApplicationContext` ，看做spring的上下文环境。

`AnnotatedBeanDefinitionReader`在实例化时，会调用`registerAnnotationConfigProcessors`方法。先看前半段代码：

```java
public static Set<BeanDefinitionHolder> registerAnnotationConfigProcessors(
    BeanDefinitionRegistry registry, @Nullable Object source) {
    DefaultListableBeanFactory beanFactory = unwrapDefaultListableBeanFactory(registry);
    if (beanFactory != null) {
      if (!(beanFactory.getDependencyComparator() instanceof AnnotationAwareOrderComparator)) {
        beanFactory.setDependencyComparator(AnnotationAwareOrderComparator.INSTANCE);
      }
      if (!(beanFactory.getAutowireCandidateResolver() instanceof ContextAnnotationAutowireCandidateResolver)) {
        beanFactory.setAutowireCandidateResolver(new ContextAnnotationAutowireCandidateResolver());
      }
}
```

在这里先获取在父类构造函数中实例好的`beanFactory`，并为它填充一些属性：

* `AnnotationAwareOrderComparator`：主要用于排序，解析`@order`和`@Priority`注解
* `ContextAnnotationAutowireCandidateResolver`：提供处理延迟加载的功能

再看后半段代码，下面生成了6个重要类的`BeanDefinitionHolder`，并存放到一个Set中：

```java
 Set<BeanDefinitionHolder> beanDefs = new LinkedHashSet<>(8);
    if (!registry.containsBeanDefinition(CONFIGURATION_ANNOTATION_PROCESSOR_BEAN_NAME)) {
      RootBeanDefinition def = new RootBeanDefinition(ConfigurationClassPostProcessor.class);
      def.setSource(source);
      beanDefs.add(registerPostProcessor(registry, def, CONFIGURATION_ANNOTATION_PROCESSOR_BEAN_NAME));
    }

    if (!registry.containsBeanDefinition(AUTOWIRED_ANNOTATION_PROCESSOR_BEAN_NAME)) {
      RootBeanDefinition def = new RootBeanDefinition(AutowiredAnnotationBeanPostProcessor.class);
      def.setSource(source);
      beanDefs.add(registerPostProcessor(registry, def, AUTOWIRED_ANNOTATION_PROCESSOR_BEAN_NAME));
    }

    if (!registry.containsBeanDefinition(REQUIRED_ANNOTATION_PROCESSOR_BEAN_NAME)) {
      RootBeanDefinition def = new RootBeanDefinition(RequiredAnnotationBeanPostProcessor.class);
      def.setSource(source);
      beanDefs.add(registerPostProcessor(registry, def, REQUIRED_ANNOTATION_PROCESSOR_BEAN_NAME));
    }

    // Check for JSR-250 support, and if present add the CommonAnnotationBeanPostProcessor.
    if (jsr250Present && !registry.containsBeanDefinition(COMMON_ANNOTATION_PROCESSOR_BEAN_NAME)) {
      RootBeanDefinition def = new RootBeanDefinition(CommonAnnotationBeanPostProcessor.class);
      def.setSource(source);
      beanDefs.add(registerPostProcessor(registry, def, COMMON_ANNOTATION_PROCESSOR_BEAN_NAME));
    }

    // Check for JPA support, and if present add the PersistenceAnnotationBeanPostProcessor.
    if (jpaPresent && !registry.containsBeanDefinition(PERSISTENCE_ANNOTATION_PROCESSOR_BEAN_NAME)) {
      RootBeanDefinition def = new RootBeanDefinition();
      try {
        def.setBeanClass(ClassUtils.forName(PERSISTENCE_ANNOTATION_PROCESSOR_CLASS_NAME,
            AnnotationConfigUtils.class.getClassLoader()));
      }
      catch (ClassNotFoundException ex) {
        throw new IllegalStateException(
            "Cannot load optional framework class: " + PERSISTENCE_ANNOTATION_PROCESSOR_CLASS_NAME, ex);
      }
      def.setSource(source);
      beanDefs.add(registerPostProcessor(registry, def, PERSISTENCE_ANNOTATION_PROCESSOR_BEAN_NAME));
    }

    if (!registry.containsBeanDefinition(EVENT_LISTENER_PROCESSOR_BEAN_NAME)) {
      RootBeanDefinition def = new RootBeanDefinition(EventListenerMethodProcessor.class);
      def.setSource(source);
      beanDefs.add(registerPostProcessor(registry, def, EVENT_LISTENER_PROCESSOR_BEAN_NAME));
    }

    if (!registry.containsBeanDefinition(EVENT_LISTENER_FACTORY_BEAN_NAME)) {
      RootBeanDefinition def = new RootBeanDefinition(DefaultEventListenerFactory.class);
      def.setSource(source);
      beanDefs.add(registerPostProcessor(registry, def, EVENT_LISTENER_FACTORY_BEAN_NAME));
    }

    return beanDefs;
  }
```

这里是使用`RootBeanDefinition`来将普通类转换为`BeanDefinition`，并进一步封装成`BeanDefinitionHolder`。封装成`BeanDefinitionHolder`的操作在`registerPostProcessor`方法中：

```java
 private static BeanDefinitionHolder registerPostProcessor(
      BeanDefinitionRegistry registry, RootBeanDefinition definition, String beanName) {
    definition.setRole(BeanDefinition.ROLE_INFRASTRUCTURE);
    registry.registerBeanDefinition(beanName, definition);
    return new BeanDefinitionHolder(definition, beanName);
  }
```

通过`registerBeanDefinition`方法将`BeanDefinition`注册到spring环境中，这个操作其实就是执行了上面的`beanDefinitionMap`的`put`操作：

```java
this.beanDefinitionMap.put(beanName, beanDefinition);
```

在上面的操作全部完成后，在还没有实例化用户自定义的bean前，已经有了6个spring自己定义的`beanDefinition`，用于实现spring自身的初始化：

![Spring容器初始化源码解析-开源基础软件社区](/assets/f965feb1063d2859eb27756adeed762845a6e9.BY7zyrqc.png)

这里有必要对`BeanDefinition`进行一下说明，它是对具有属性值的`bean`实例的一个说明，或者说是定义。就像是在java类加载的过程，普通java文件要先生成字节码文件，再加载到jvm中生成`class`对象，spring初始化过程中首先要将普通类转化为`BeanDefinition`，然后再实例化为bean。

在实例化`AnnotatedBeanDefinitionReader`完成后，实例化了一个`ClassPathBeanDefinitionScanner`，可以用来扫描包或者类，并将扫描到的类转化为`BeanDefinition`。但是翻阅源码，我们可以看到实际上扫描包的工作不是这个`scanner`对象来完成的，而是在后面spring自己实例化了一个`ClassPathBeanDefinitionScanner`来负责的。

这里的`scanner`仅仅是对外提供一个扩展，可以让我们能够在外部调用`AnnotationConfigApplicationContext`对象的`scan`方法，实现包的扫描，例如：

```java
context.scan("com.hydra");
```

到这里，`AnnotationConfigApplicationContext`的构造函数就执行完了，下面，我们来详细说说接下来被调用的`register`方法。

---

---
url: /常用框架/SpringAI/SpringAI.md
---

# SpringAI Alibaba 整合阿里云百炼 DeepSeek

Spring 官方开源了Spring AI 框架，用来简化 Spring 开发者开发智能体应用的过程。随后阿里巴巴开源了 Spring AI Alibaba，它基于 Spring AI，同时与阿里云百炼大模型服务、通义系列大模型做了深度集成与最佳实践。基于 Spring AI Alibaba，Java 开发者可以非常方便地开发 AI 智能体应用。

开通阿里云百炼账号，获取API-KEY，<https://bailian.console.aliyun.com/>

![image-20250405203607994](/assets/image-20250405203607994.CD2F0MeD.png)

其中deepseek-r1与deepseek-v3分别有 100万的免费 Token，部分蒸馏模型限时免费体验。

![image-20250405203728409](/assets/image-20250405203728409.BZWMcjgP.png)

## SpringBoot接入deepseek实战

使用 Spring AI Alibaba 开发应用与使用普通 Spring Boot 没有什么区别，只需要增加 spring-ai-alibaba-starter 依赖，将 ChatClient Bean 注入就可以实现与模型聊天了。

注意：因为 Spring AI Alibaba 基于 Spring Boot 3.x 开发，因此本地 JDK 版本要求为 17 及以上。

### 1、添加依赖

首先，需要在项目中添加 spring-ai-alibaba-starter 依赖，它将通过 Spring Boot 自动装配机制初始化与阿里云通义大模型通信的 ChatClient、ChatModel 相关实例。

```xml
<dependency>
  <groupId>com.alibaba.cloud.ai</groupId>
  <artifactId>spring-ai-alibaba-starter</artifactId>
  <version>1.0.0-M5.1</version>
</dependency>
```

注意：由于 spring-ai 相关依赖包还没有发布到中央仓库，如出现 spring-ai-core 等相关依赖解析问题，请在您项目的 pom.xml 依赖中加入如下仓库配置。

```text
<repositories>
  <repository>
    <id>spring-milestones</id>
    <name>Spring Milestones</name>
    <url>https://repo.spring.io/milestone</url>
    <snapshots>
      <enabled>false</enabled>
    </snapshots>
  </repository>
</repositories>
```

完整的pom文件如下

```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    <parent>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-parent</artifactId>
        <version>3.4.3</version>
        <relativePath/> <!-- lookup parent from repository -->
    </parent>
    <groupId>com.fox</groupId>
    <artifactId>alibaba-ai-demo</artifactId>
    <version>0.0.1-SNAPSHOT</version>
    <name>alibaba-ai-demo</name>
    <description>alibaba-ai-demo</description>
    <url/>

    <properties>
        <java.version>17</java.version>
        <spring-ai.version>1.0.0-M5</spring-ai.version>
        <spring-ai-alibaba.version>1.0.0-M5.1</spring-ai-alibaba.version>
    </properties>

    <dependencies>

     <dependency>
            <groupId>com.alibaba.cloud.ai</groupId>
            <artifactId>spring-ai-alibaba-starter</artifactId>
            <version>${spring-ai-alibaba.version}</version>
        </dependency>

        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-web</artifactId>
        </dependency>

        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-test</artifactId>
            <scope>test</scope>
        </dependency>
    </dependencies>

    <dependencyManagement>
        <dependencies>
            <dependency>
                <groupId>org.springframework.ai</groupId>
                <artifactId>spring-ai-bom</artifactId>
                <version>${spring-ai.version}</version>
                <type>pom</type>
                <scope>import</scope>
            </dependency>
        </dependencies>
    </dependencyManagement>


    <repositories>
        <!-- spring-ai 相关依赖包还没有发布到中央仓库-->
        <repository>
            <id>spring-milestones</id>
            <name>Spring Milestones</name>
            <url>https://repo.spring.io/milestone</url>
            <snapshots>
                <enabled>false</enabled>
            </snapshots>
        </repository>
    </repositories>

    <build>
        <plugins>
            <plugin>
                <groupId>org.springframework.boot</groupId>
                <artifactId>spring-boot-maven-plugin</artifactId>
            </plugin>
        </plugins>
    </build>

</project>
```

### 2、配置 application.yml

指定 API-KEY（可通过访问阿里云百炼模型服务平台获取，有免费额度可用）

```yaml
spring:
  application:
    name: alibaba-ai-demo

  ai:
    dashscope:
      api-key: ${AI_DASHSCOPE_API_KEY}   # api key
      chat:
        options:
          model: deepseek-r1   # 模型名称
```

### 3、注入智能体代理 ChatClient

接下来，在普通 Controller Bean 中注入 ChatClient 实例，这样你的 Bean 就具备与 AI 大模型智能对话的能力了。

```java
/**
 * @Classname ChatController
 * @Description TODO
 * @Date 2025/4/4 23:38
 * @Created by xxl
 */
@RestController
public class ChatController {

    private final ChatClient chatClient;

    public ChatController(ChatClient.Builder builder) {
        this.chatClient = builder.build();
    }

    @GetMapping("/chat")
    public String chat(@RequestParam(value = "input") String input) {
        return this.chatClient.prompt()
                .user(input)
                .call()
                .content();
    }
}
```

以上示例中，ChatClient 使用默认参数调用大模型，Spring AI Alibaba 还支持通过 DashScopeChatOptions 调整与模型对话时的参数，DashScopeChatOptions 支持两种不同维度的配置方式：

a、全局默认值，即 ChatClient 实例初始化参数。可以在 application.yml 文件中指定

```yaml
spring.ai.dashscope.chat.options.*
```

或调用构造函数

```java
ChatClient.Builder.defaultOptions(options)
```

完成配置初始化。

b、每次 Prompt 调用前动态指定

```java
String result = dashScopeChatClient
  .prompt(query)
  .options(DashScopeChatOptions.builder().withTopP(0.8).build())
  .call()
  .content();
```

### 4、启动服务后测试

![image-20250405204701129](/assets/image-20250405204701129.SWIJJ9qy.png)

### 参考资料

Spring AI：<https://springdoc.cn/spring-ai-intro/>

Spring AI介绍：<https://segmentfault.com/a/1190000045505066#item-1>

Spring AI MCP：<https://springdoc.cn/spring-ai-mcp-announcement/>

spring ai mcp client代码示例：<https://zhuanlan.zhihu.com/p/29313544351>

DeepSeek R1 本地部署：<https://henjihenji.feishu.cn/wiki/MN3Vwl2STigk2qk1r6lcGoY5nYg>

SpringBoot + Spring AI Alibaba 整合阿里云百炼DeepSeek大模型：<https://zhuanlan.zhihu.com/p/26299836051>

https://cloud.tencent.com/developer/article/2531641

https://blog.csdn.net/wanganui/article/details/145593518

---

---
url: /常用框架/Spring/2_SpringAOP面向切面编程.md
---

# SpringAOP面向切面编程

> 官方中文文档：
>
> <https://springdoc.cn/spring/core.html#aop>

### AOP介绍

AOP（Aspect-Oriented Programming，面向切面编程）能够将那些与业务无关，却为业务模块所共同调用的逻辑或责任（例如事务处理、日志管理、权限控制等）封装起来，便于减少系统的重复代码，降低模块间的耦合度，并有利于未来的可扩展性和可维护性。
Spring AOP是基于动态代理的，如果要代理的对象实现了某个接口，那么Spring AOP就会使用JDK动态代理去创建代理对象；而对于没有实现接口的对象，就无法使用JDK动态代理，转而使用CGlib动态代理生成一个被代理对象的子类来作为代理。

![image-20231113170949925](/assets/image-20231113170949925.BiTELKWI.jpg)

图中的implements和extend。即一个是接口，一个是实现类。

当然也可以使用AspectJ，Spring AOP中已经集成了AspectJ，AspectJ应该算得上是Java生态系统中最完整的AOP框架了。使用AOP之后我们可以把一些通用功能抽象出来，在需要用到的地方直接使用即可，这样可以大大简化代码量。我们需要增加新功能也方便，提高了系统的扩展性。日志功能、事务管理和权限管理等场景都用到了AOP。

### Spring AOP和AspectJ AOP的区别

Spring AOP是属于运行时增强，而AspectJ是编译时增强。

Spring AOP基于代理（Proxying），而AspectJ基于字节码操作（Bytecode Manipulation）。

Spring AOP已经集成了AspectJ，AspectJ应该算得上是Java生态系统中最完整的AOP框架了。

AspectJ相比于Spring AOP功能更加强大，但是Spring AOP相对来说更简单。

如果我们的切面比较少，那么两者性能差异不大。但是，当切面太多的话，最好选择AspectJ，它比SpringAOP快很多。

### 在Spring AOP 中，关注点和横切关注的区别

**关注点**是应用中一个模块的行为，一个关注点可能会被定义成一个我们想实现的一个功能。 横切关注点是一个关注点，此关注点是整个应用都会使用的功能，并影响整个应用，比如日志，安全和数据传输，几乎应用的每个模块都需要的功能。因此这些都属于横切关注点。

那什么是**连接点**呢？连接点代表一个应用程序的某个位置，在这个位置我们可以插入一个AOP切面，它实际上是个应用程序执行Spring AOP的位置。

**切入点**是什么？切入点是一个或一组连接点，通知将在这些位置执行。可以通过表达式或匹配的方式指明切入点。

### 通知及通知类型

通知是个在方法执行前或执行后要做的动作，实际上是程序执行时要通过SpringAOP框架触发的代码段。

Spring切面可以应用五种类型的通知：

1. **before**：前置通知，在一个方法执行前被调用。
2. **after**: 在方法执行之后调用的通知，无论方法执行是否成功。
3. **after-returning**: 仅当方法成功完成后执行的通知。
4. **after-throwing**: 在方法抛出异常退出时执行的通知。
5. **around**: 在方法执行之前和之后调用的通知。

### Spring AOP 切点表达式args与@args区别

<https://blog.csdn.net/qq_19922839/article/details/117412231>

### execution表达式

https://cloud.tencent.com/developer/article/1640230

[@Pointcut 的 12 种用法，你知道几种？](https://zhuanlan.zhihu.com/p/153317556)

[@Pointcut语法详解](https://blog.csdn.net/justlpf/article/details/103400452)

概念

Spring AOP——Spring 中面向切面编程]\(https://www.cnblogs.com/joy99/p/10941543.html)

[AOP(面向切面编程)的介绍与作用](https://www.freesion.com/article/7911501010/)

应用场景

[使用AOP给项目添加登录日志](https://blog.csdn.net/w139074301/article/details/121703536)

[spring-AOP（实现登录日志）](https://blog.csdn.net/weixin_48112109/article/details/125672876)

[关于若依管理系统的异步定时任务记录登录日志的分析](https://blog.csdn.net/DreamsArchitects/article/details/117376278)

[五、登录日志、操作日志](https://www.jianshu.com/p/e6a567cc2fa2)

[SpringBoot使用AOP实现简单验证、日志记录、使用NettySocket转发数据](http://www.zuidaima.com/share/5226709711834112.htm) https://www.csdn.net/tags/NtDacg2sMzMwNTgtYmxvZwO0O0OO0O0O.html

---

---
url: /常用框架/Spring/4_SpringAOP实战之日志拦截.md
---

# SpringAOP实战之日志拦截

创建maven项目

### 1. pom.xml文件中加入依赖

```xml
<parent>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-parent</artifactId>
    <version>2.0.5.RELEASE</version>
    <relativePath/>
</parent>
```

### 2. pom.xml文件中加入springboot-starter依赖

```xml
<dependencies>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-web</artifactId>
    </dependency>

    <dependency>
        <groupId>org.projectlombok</groupId>
        <artifactId>lombok</artifactId>
    </dependency>

    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-aop</artifactId>
    </dependency>

    <dependency>
        <groupId>com.alibaba</groupId>
        <artifactId>fastjson</artifactId>
        <version>1.2.72</version>
    </dependency>
</dependencies>
```

### 3. pom.xml文件中加入maven-springboot打包插件

```xml
<build>
    <plugins>
        <plugin>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-maven-plugin</artifactId>
        </plugin>
    </plugins>
</build>
```

### 4. 开发启动类

```java
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

/**
 * 启动类
 *
 * @author xxl
 * @date 2022/5/8 18:54
 */
@SpringBootApplication
public class Application {

    public static void main(String[] args) {
        SpringApplication.run(Application.class, args);
    }

}
```

### 5. 开发用户实体类

```java
import lombok.AllArgsConstructor;
import lombok.Data;

/**
 * 用户实体
 *
 * @author xxl
 * @date 2022/5/8 18:55
 */
@Data
@AllArgsConstructor
public class User {

    private Integer id;
    private String name;
    private Integer age;

}
```

### 6. 开发获取参数工具类

```java
import com.alibaba.fastjson.JSONObject;

import javax.servlet.http.HttpServletRequest;
import java.util.Enumeration;

/**
 * 获取参数工具类
 *
 * @author xxl
 * @date 2022/5/8 18:56
 */
public class ParametersUtils {

    public static String getParameters(HttpServletRequest req) {
        Enumeration<String> enums = req.getParameterNames();
        JSONObject parameter = new JSONObject();
        while (enums.hasMoreElements()) {
            String name = enums.nextElement();
            parameter.put(name, req.getParameter(name));
        }
        return parameter.toJSONString();
    }

}
```

### 7. 开发参数记录aop类

```java
import com.xxl.util.ParametersUtils;
import lombok.extern.slf4j.Slf4j;
import org.aspectj.lang.annotation.AfterReturning;
import org.aspectj.lang.annotation.Aspect;
import org.aspectj.lang.annotation.Before;
import org.aspectj.lang.annotation.Pointcut;
import org.springframework.stereotype.Component;
import org.springframework.web.context.request.RequestContextHolder;
import org.springframework.web.context.request.ServletRequestAttributes;

import javax.servlet.http.HttpServletRequest;

/**
 * 参数记录AOP类
 *
 * @author xxl
 * @date 2022/5/8 19:55
 */
@Slf4j
@Aspect
@Component
public class WebLogAspect {

    @Pointcut("execution(public * com.xxl.controller..*.*(..))")
    public void webLog() {
    }

    @Before("webLog()")
    public void doBefore() {
        // 获取请求
        ServletRequestAttributes attributes = (ServletRequestAttributes) RequestContextHolder.getRequestAttributes();
        if (attributes == null) {
            return;
        }
        HttpServletRequest request = attributes.getRequest();
        // 记录请求内容
        log.info("请求地址：" + request.getRequestURL().toString());
        log.info("请求方法：" + request.getMethod());
        log.info("请求者IP：" + request.getRemoteAddr());
        log.info("请求参数：" + ParametersUtils.getParameters(request));
    }

    @AfterReturning(returning = "ret", pointcut = "webLog()")
    public void doAfterReturning(Object ret) {
        log.info("返回结果：" + ret.toString());
    }

}
```

### 8. 开发测试控制器类

```java
import com.xxl.vo.User;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RestController;

/**
 * 测试控制器
 *
 * @author xxl
 * @date 2022/5/8 18:57
 */
@RestController
public class DemoController {

    @RequestMapping("/test")
    public User test(User user) {
        user.setAge(18);
        return user;
    }

}
```

最后编译打包运行。

> 获取作者源码访问
>
> [Daneliya/springboot\_chowder: springboot大乱炖 (github.com)](https://github.com/Daneliya/springboot_chowder/tree/main/springboot_access_log)

---

---
url: /常用框架/Spring/3_SpringAOP源码分析.md
---

# SpringAOP源码分析

https://www.cnblogs.com/mangoubiubiu/p/16750516.html

https://blog.csdn.net/weixin\_42054506/article/details/130936439

---

---
url: /常用框架/SpringBoot/SpringBoot程序开发/0_SpringBoot编程起步.md
---

# SpringBoot编程起步

### 传统开发中痛的领悟

在Java项目开发中，MVC已经成为了一种深入人心的设计模式，几乎所有正规的项目之中都会使用到MVC设计模式。采用MVC设计模式可以有效地实现显示层、控制层、业务层、数据层的结构分离。

![Image00006](/assets/Image00006.CfMDZmuf.jpg)

虽然MVC开发具有良好的可扩展性，但是在实际的开发过程中，许多开发者依然会感受到如下的问题。

* 采用原生Java程序实现MVC设计模式时，一旦整体项目设计不到位，就会存在大量的重复代码，并且项目维护困难。
* 为了简化MVC各个层的开发，可以引用大量的第三方开发框架，如Spring、Hibernate、MyBatis、Shiro、JPA、SpringSecurity等，但这些框架都需要在Spring中实现整合，其结果就是会存在大量的配置文件。
* 当使用一些第三方的服务组件（如RabbitMQ、Kafka、JavaMail等）时，需要编写大量重复的配置文件，而且还需要根据环境定义不同的profile（如dev、beta、product）。
* 使用Maven作为构建工具时，需要配置大量的依赖关系，且程序需要被打包为\*.war文件并部署到应用服务器上才可以执行。
* Restful作为接口技术应用得越来越广泛，但如果使用Spring来搭建Restful服务，则需要引入大量的Maven依赖库，并且需要编写许多的配置文件。

基于以上种种因素，很多人开始寻求更加简便的开发方式，而遗憾的是，这种简便的开发没有被官方的JavaEE所支持。JavaEE官方支持的技术标准依然只提供最原始的技术支持。

### SpringBoot简介

SpringBoot是Spring开发框架提供的一种扩展支持，其主要目的是希望通过简单的配置实现开发框架的整合，使开发者的注意力可以完全放在程序业务功能的实现上，其核心在于通过“零配置”的方式来实现快速且简单的开发。

Spring Boot开发框架有如下核心功能。

* 独立运行的Spring项目：SpringBoot可以以jar包的形式直接运行在拥有JDK的主机上。
* 内嵌Web容器：SpringBoot内嵌了Tomcat容器与Jetty容器，这样可以不局限于war包的部署形式。
* 简化Maven配置：在实际开发中需要编写大量的Maven依赖，在SpringBoot中会提供一系列使用starter的依赖配置来简化Maven配置文件的定义。
* 自动配置Spring：采用合理的项目组织结构，使Spring的配置注解自动生效。
* 减少XML配置：在SpringBoot中依然支持XML配置，同时也可以利用Bean和自动配置机制减少XML配置文件的定义。

### SpringBoot编程起步

SpringBoot编程需要依赖于Maven或Gradle构建工具完成。

### 参考资料

\[1]. W3shhool教程：https://www.w3ccoo.com/spring\_boot/index.html

\[2]. Java微服务架构实战-周兴华代码示例：https://github.com/CoderDream/SpringBoot-SpringCloud-Docker-RabbitMQ

\[3]. Springboot一些相关问题：https://sloving.top/SpringBoot/%E8%BF%88%E5%85%A5%E5%89%8D%E5%90%8E%E7%AB%AF%E5%88%86%E7%A6%BB%E6%97%B6%E4%BB%A3.html

---

---
url: /常用框架/SpringBoot/SpringBoot程序开发/2_SpringBoot程序测试.md
---

# SpringBoot程序测试

SpringBoot程序开发完成之后，需要对程序的功能进行测试，这时需要启动Spring容器。开发者可以直接利用SpringBoot提供的依赖包来实现控制层方法测试。

## 一、引入依赖

修改pom.xml配置文件，引入测试相关依赖包。

```xml
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-test</artifactId>
    <scope>test</scope>
</dependency>
<dependency>
    <groupId>junit</groupId>
    <artifactId>junit</artifactId>
    <scope>test</scope>
</dependency>
```

## 二、编写一个测试程序类

```java
package com.xxl.springboot.init.controller;

import org.springframework.boot.autoconfigure.EnableAutoConfiguration;
import org.springframework.stereotype.Controller;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.ResponseBody;

/**
 * @author xxl
 * @date 2024/12/29 0:21
 */
@Controller
@EnableAutoConfiguration
public class SampleController {

    @RequestMapping("/")
    @ResponseBody
    public String home() {
        return "www.xxl.cn";
    }

}

```

```java
package com.xxl.springboot.init.controller;

import junit.framework.TestCase;
import org.junit.Test;
import org.junit.runner.RunWith;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.test.context.SpringBootTest;
import org.springframework.test.context.junit4.SpringJUnit4ClassRunner;
import org.springframework.test.context.web.WebAppConfiguration;

/**
 * @author xxl
 * @date 2024/12/29 0:22
 */
@SpringBootTest(classes = SampleController.class)
@RunWith(SpringJUnit4ClassRunner.class)
@WebAppConfiguration
public class TestSampleController {

    @Autowired
    private SampleController sampleController;

    @Test
    public void testHome() {
        TestCase.assertEquals(this.sampleController.home(), "www.xxl.cnn");
    }
}

```

## 三、启动测试

测试程序编写完成之后，就可以启动测试了。

失败示例

```cmd
junit.framework.ComparisonFailure: 
预期:www.xxl.cn
实际:www.xxl.cnn
<点击以查看差异>


	at junit.framework.Assert.assertEquals(Assert.java:100)
	at junit.framework.Assert.assertEquals(Assert.java:107)
	at junit.framework.TestCase.assertEquals(TestCase.java:260)
	at com.xxl.springboot.init.controller.TestSampleController.testHome(TestSampleController.java:25)
...
...
```

> 补充
>
> 利用MockMvc编写测试用例：https://developer.aliyun.com/article/1641912

---

---
url: /常用框架/SpringBoot/SpringBoot程序开发/5_SpringBoot调试.md
---

# SpringBoot调试

在项目开发的过程中经常需要对代码进行反复修改，这样就会导致SpringBoot运行容器反复启动。为了解决这种频繁重启问题，SpringBoot提供了自动加载配置的依赖库，以实现代码的动态加载。

修改pom.xml配置文件，追加自动加载依赖库配置。

```xml
<!-- SpringBoot调试 -->
<dependency>
    <groupId>org.springframework</groupId>
    <artifactId>springloaded</artifactId>
    <version>1.2.8.RELEASE</version>
</dependency>
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-devtools</artifactId>
</dependency>
```

每当用户修改项目中程序类的时候都会由SpringBoot自动加载更新后的程序代码

但是需要注意的是

如果报错：SpringbootInitApplication: delete method not implemented SpringbootInitApplication: 虚拟机不支持的操作

**是因为HotSwap只支持对方法body的修改，不支持对类和方法签名的修改（比如修改类名，方法名，方法参数，添加或者删除一个方法，增加、删除类文件等，是不能够热部署到服务上的。这时候需要停止服务器重新部署后再启动，就不会出现上面的提示了等**

https://blog.csdn.net/Maxiao1204/article/details/103203515

[Idea环境实现SpringBoot实现两种热部署方式（亲测有效）-腾讯云开发者社区-腾讯云 (tencent.com)](https://cloud.tencent.com/developer/article/1542394)

---

---
url: /常用框架/Caffeine/1_SpringBoot集成Caffeine.md
---

# SpringBoot炖Caffeine

### 1. 先睹为快

### 2. 实现原理

#### 2.1 新建项目

#### 2.2 创建maven目录结构，以及pom.xml文件

#### 2.3 pom.xml文件中加入依赖

```xml
<parent>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-parent</artifactId>
    <version>2.0.5.RELEASE</version>
    <relativePath/>
</parent>
```

#### 2.4 pom.xml文件中加入springboot-starter依赖

```xml
<dependencies>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-web</artifactId>
    </dependency>

    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-cache</artifactId>
    </dependency>

    <dependency>
        <groupId>com.github.ben-manes.caffeine</groupId>
        <artifactId>caffeine</artifactId>
        <version>2.6.0</version>
    </dependency>

    <dependency>
        <groupId>org.projectlombok</groupId>
        <artifactId>lombok</artifactId>
    </dependency>
</dependencies>
```

#### 2.5 pom.xml文件中加入maven-springboot打包插件

```xml
<build>
    <plugins>
        <plugin>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-maven-plugin</artifactId>
        </plugin>
    </plugins>
</build>
```

#### 2.6 开发启动类

```java
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.cache.annotation.EnableCaching;

@EnableCaching
@SpringBootApplication
public class Application {

    public static void main(String[] args) {
        SpringApplication.run(Application.class, args);
    }

}
```

#### 2.7 开发用户实体类

```java
import lombok.AllArgsConstructor;
import lombok.Data;

@Data
@AllArgsConstructor
public class User {

    private Integer id;
    private String uname;
    private String pwd;
    private Integer age;

}
```

#### 2.8 开发用户服务层

主要基于Spring缓存注解@Cacheable、@CacheEvict、@CachePut的方式使用

@Cacheable ：改注解修饰的方法，若不存在缓存，则执行方法并将结果写入缓存；若存在缓存，则不执行方法，直接返回缓存结果。
@CachePut ：执行方法，更新缓存；该注解下的方法始终会被执行。
@CacheEvict ：删除缓存
@Caching 将多个缓存组合在一个方法上（该注解可以允许一个方法同时设置多个注解）
@CacheConfig 在类级别设置一些缓存相关的共同配置（与其它缓存配合使用）

```java
import com.oven.vo.User;
import org.springframework.cache.annotation.CacheEvict;
import org.springframework.cache.annotation.Cacheable;
import org.springframework.stereotype.Service;

@Service
public class UserService {

    @CacheEvict(value = "FIVE", key = "#id")
    public void delete(Integer id) {
        System.out.println("删除key为[" + id + "]的缓存");
    }

    @Cacheable(value = "FIVE", key = "#id", sync = true)
    public User getById(Integer id) {
        System.out.println("操作数据库，进行通过ID查询，ID: " + id);
        return new User(id, "admin", "123", 18);
    }

}
```

#### 2.9 开发用户控制层

```java
import com.oven.service.UserService;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RestController;

import javax.annotation.Resource;

@RestController
public class UserController {

    @Resource
    private UserService userService;

    @RequestMapping("/getById")
    public Object getById(Integer id) {
        return userService.getById(id);
    }

    @RequestMapping("/delete")
    public Object delete(Integer id) {
        userService.delete(id);
        return "删除成功！";
    }

}
```

#### 2.10 开发缓存枚举类

```java
public enum CacheType {

    TEN(10),

    FIVE(5);

    private final int expires;

    CacheType(int expires) {
        this.expires = expires;
    }

    public int getExpires() {
        return expires;
    }

}
```

#### 2.11 开发caffeine缓存配置类

```java
import com.github.benmanes.caffeine.cache.Caffeine;
import org.springframework.cache.CacheManager;
import org.springframework.cache.caffeine.CaffeineCache;
import org.springframework.cache.support.SimpleCacheManager;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

import java.util.ArrayList;
import java.util.List;
import java.util.concurrent.TimeUnit;

@Configuration
public class CaffeineConfig {

    @Bean
    public CacheManager cacheManager() {
        SimpleCacheManager cacheManager = new SimpleCacheManager();
        List<CaffeineCache> caffeineCaches = new ArrayList<>();
        for (CacheType cacheType : CacheType.values()) {
            caffeineCaches.add(new CaffeineCache(cacheType.name(),
                    Caffeine.newBuilder()
                            .expireAfterWrite(cacheType.getExpires(), TimeUnit.SECONDS)
                            .build()));
        }
        cacheManager.setCaches(caffeineCaches);
        return cacheManager;
    }

}
```

#### 2.12 编译打包运行

### 3. 应用场景

---

---
url: /Java/架构设计/分布式/07.分布式日志收集/5_SpringBoot集成Graylog.md
---

# SpringBoot集成Graylog

https://gitee.com/xu\_xiaolong/docker-compose/blob/master/compose/graylog/docker-compose-graylog.yml

https://zhuanlan.zhihu.com/p/113936683

https://juejin.cn/post/7483145055239225378

https://blog.csdn.net/xiaoye319/article/details/124023907

https://blog.csdn.net/w1014074794/article/details/120738822

https://blog.51cto.com/u\_12205/12635181

https://zhuanlan.zhihu.com/p/25937785

logback出现大量XXX\_IS\_UNDEFINED日志文件的问题

---

---
url: /数据库/05.Neo4j/3_SpringBoot集成Neo4j.md
---

# SpringBoot集成Neo4j

## 集成原生Neo4J

```xml
<dependency>
    <groupId>org.neo4j.driver</groupId>
    <artifactId>neo4j-java-driver</artifactId>
    <version>4.4.12</version>
</dependency>
```

代码

```java
public static void main(String[] args) {
    Driver driver = GraphDatabase.driver("bolt://localhost:7687", AuthTokens.basic("neo4j", "Yinlidong1995."));
    Session session = driver.session();
    session.run("CREATE (n:Part {name: {name},title: {title}})",
                parameters("name", "Arthur001", "title", "King001"));
    Result result = session.run("MATCH (a:Part) WHERE a.name = {name} " +
                                "RETURN a.name AS name, a.title AS title",
                                parameters("name", "Arthur001"));
    while (result.hasNext()) {
        Record record = result.next();
        System.out.println(record.get("title").asString() + " " + record.get("name").asString());
    }
    session.close();
    driver.close();
}
```

## SpringBoot集成Neo4J

## 参考资料

https://zhuanlan.zhihu.com/p/450327256

---

---
url: /常用框架/SpringBoot/SpringBoot源码分析/SpringBoot异步实现方式.md
---
# SpringBoot异步实现方式

https://www.jb51.net/program/299085b9l.htm

---

---
url: /常用框架/SpringBoot/SpringBoot服务整合/5_SpringBoot整合安全框架.md
---

# SpringBoot整合安全框架

## SpringBoot整合安全框架

Shiro是Apache推出的新一代认证与授权管理开发框架，可以方便地与第三方的认证机构进行整合。下面将直接采用自定义缓存类来实现多个Redis数据库信息的保存。

![Image00233](/assets/Image00233.B8pqeQ3a.jpg)

### SpringBoot整合Shiro开发框架

SpringBoot与Shiro的整合处理，本质上和Spring与Shiro的整合区别不大，但开发者需要注意以下3点：

1. SpringBoot可以自动导入一系列的开发包，但是这些开发包里面不包含对Shiro的支持，所以还需要配置shiro的开发依赖库。
2. SpringBoot不提倡使用spring-shiro.xml文件进行配置，需要将配置文件转为Bean的形式（需要考虑缓存的调度时间问题）。
3. Shiro在进行一些Session管理以及缓存配置时要用到shiro-quartz依赖包，该依赖包使用的是QuartZ-1.X版本，而现在能找到的都是QuartZ-2.x版本。因此，如果不使用SpringBoot，那么这样的使用差别不大；如果使用了SpringBoot集成，就会产生后台的异常信息。

#### 引入依赖

修改pom.xml配置文件，追加Shiro的相关依赖包。

定义版本属性

```xml
<druid.version>1.1.1</druid.version>
<shiro.version>1.3.2</shiro.version>
<thymeleaf-extras-shiro.version>1.2.1</thymeleaf-extras-shiro.version>
<quartz.version>2.3.0</quartz.version>
```

定义配置依赖管理

```xml
<dependency>
    <groupId>com.github.theborakompanioni</groupId>
    <artifactId>thymeleaf-extras-shiro</artifactId>
    <version>${thymeleaf-extras-shiro.version}</version>
</dependency>
<dependency>
    <groupId>org.apache.shiro</groupId>
    <artifactId>shiro-spring</artifactId>
    <version>${shiro.version}</version>
</dependency>
<dependency>
    <groupId>org.apache.shiro</groupId>
    <artifactId>shiro-core</artifactId>
    <version>${shiro.version}</version>
</dependency>
<dependency>
    <groupId>org.apache.shiro</groupId>
    <artifactId>shiro-ehcache</artifactId>
    <version>${shiro.version}</version>
</dependency>
<dependency>
    <groupId>org.apache.shiro</groupId>
    <artifactId>shiro-web</artifactId>
    <version>${shiro.version}</version>
</dependency>
<dependency>
    <groupId>org.quartz-scheduler</groupId>
    <artifactId>quartz</artifactId>
    <version>${quartz.version}</version>
</dependency>
```

修改pom.xml配置文件，追加依赖库配置。

```xml
<dependency>
    <groupId>org.apache.shiro</groupId>
    <artifactId>shiro-spring</artifactId>
</dependency>
<dependency>
    <groupId>org.apache.shiro</groupId>
    <artifactId>shiro-core</artifactId>
</dependency>
<dependency>
    <groupId>org.apache.shiro</groupId>
    <artifactId>shiro-ehcache</artifactId>
</dependency> 
<dependency>
    <groupId>org.quartz-scheduler</groupId>
    <artifactId>quartz</artifactId>
</dependency>
```

#### 创建配置类

建立ShiroConfig的配置程序类，将所有Shiro的配置项都写在此配置类中。

```java
@Configuration
public class ShiroConfig {
	public static final String LOGOUT_URL = "/logout.action" ;			// 退出路径
	public static final String LOGIN_URL = "/loginPage" ;				// 登录路径
	public static final String UNAUTHORIZED_URL = "/unauth" ;			// 未授权错误页
	public static final String SUCCESS_URL = "/pages/back/welcome" ;	// 登录成功页
	
	@Resource(name = "redisConnectionFactory")
	private RedisConnectionFactory redisConnectionFactoryAuthentication;
	@Resource(name = "redisConnectionFactoryAuthorization")
	private RedisConnectionFactory redisConnectionFactoryAuthorization;
	@Resource(name = "redisConnectionFactoryActiveSessionCache")
	private RedisConnectionFactory redisConnectionFactoryActiveSessionCache; 
	
	
	@Bean
	public MemberRealm getRealm() {										// 定义Realm
		MemberRealm realm = new MemberRealm();
		realm.setCredentialsMatcher(new DefaultCredentialsMatcher());	// 配置缓存
		realm.setAuthenticationCachingEnabled(true);
		realm.setAuthenticationCacheName("authenticationCache");
		realm.setAuthorizationCachingEnabled(true);
		realm.setAuthorizationCacheName("authorizationCache");
		return realm;
	}
	@Bean(name = "lifecycleBeanPostProcessor")
	public LifecycleBeanPostProcessor getLifecycleBeanPostProcessor() {	// Shiro实现控制器处理
		return new LifecycleBeanPostProcessor();
	}
	@Bean
	@DependsOn("lifecycleBeanPostProcessor")
	public DefaultAdvisorAutoProxyCreator getDefaultAdvisorAutoProxyCreator() {
		DefaultAdvisorAutoProxyCreator daap = new DefaultAdvisorAutoProxyCreator();
		daap.setProxyTargetClass(true);
		return daap;
	}
	@Bean
	public CacheManager getCacheManager(
			@Qualifier("redisConnectionFactory")
			RedisConnectionFactory redisConnectionFactoryAuthentication ,
			@Qualifier("redisConnectionFactoryAuthorization")
			RedisConnectionFactory redisConnectionFactoryAuthorization ,
			@Qualifier("redisConnectionFactoryActiveSessionCache")
			RedisConnectionFactory redisConnectionFactoryActiveSessionCache
			) {															// 缓存配置
		RedisCacheManager cacheManager = new RedisCacheManager();		// 缓存集合
		Map<String,RedisConnectionFactory> map = new HashMap<>() ;
		map.put("authenticationCache", redisConnectionFactoryAuthentication) ;
		map.put("authorizationCache", redisConnectionFactoryAuthorization) ;
		map.put("activeSessionCache", redisConnectionFactoryActiveSessionCache) ;
		cacheManager.setConnectionFactoryMap(map);
		return cacheManager;
	}
	@Bean
	public SessionIdGenerator getSessionIdGenerator() { 				// SessionID生成
		return new JavaUuidSessionIdGenerator();
	}
	@Bean
	public SessionDAO getSessionDAO(SessionIdGenerator sessionIdGenerator) {
		EnterpriseCacheSessionDAO sessionDAO = new EnterpriseCacheSessionDAO();
		sessionDAO.setActiveSessionsCacheName("activeSessionCache");
		sessionDAO.setSessionIdGenerator(sessionIdGenerator);
		return sessionDAO;
	}
	@Bean
	public RememberMeManager getRememberManager() {						// 记住我
		CookieRememberMeManager rememberMeManager = new CookieRememberMeManager();
		SimpleCookie cookie = new SimpleCookie("MLDNJAVA-RememberMe");
		cookie.setHttpOnly(true); 
		cookie.setMaxAge(3600);
		rememberMeManager.setCookie(cookie);
		return rememberMeManager;
	}
	@Bean
	public DefaultQuartzSessionValidationScheduler getQuartzSessionValidationScheduler(
			DefaultWebSessionManager sessionManager) {
		DefaultQuartzSessionValidationScheduler sessionValidationScheduler = new DefaultQuartzSessionValidationScheduler();
		sessionValidationScheduler.setSessionValidationInterval(100000);
		sessionValidationScheduler.setSessionManager(sessionManager);
		return sessionValidationScheduler;
	}
	
	@Bean
	public AuthorizationAttributeSourceAdvisor getAuthorizationAttributeSourceAdvisor(
			DefaultWebSecurityManager securityManager) {
		AuthorizationAttributeSourceAdvisor aasa = new AuthorizationAttributeSourceAdvisor();
		aasa.setSecurityManager(securityManager);
		return aasa; 
	}

	@Bean
	public ShiroDialect shiroDialect() { 							// 追加配置，启动Thymeleaf模版支持
		return new ShiroDialect();
	}
	@Bean
	public DefaultWebSessionManager getSessionManager(SessionDAO sessionDAO) { // Session管理
		DefaultWebSessionManager sessionManager = new DefaultWebSessionManager();
		sessionManager.setDeleteInvalidSessions(true);
		sessionManager.setSessionValidationSchedulerEnabled(true);
		sessionManager.setSessionDAO(sessionDAO);
		SimpleCookie sessionIdCookie = new SimpleCookie("mldn-session-id");
		sessionIdCookie.setHttpOnly(true);
		sessionIdCookie.setMaxAge(-1);
		sessionManager.setSessionIdCookie(sessionIdCookie);
		sessionManager.setSessionIdCookieEnabled(true);
		return sessionManager;
	}
	@Bean
	public DefaultWebSecurityManager getSecurityManager(Realm memberRealm, CacheManager cacheManager,
			SessionManager sessionManager, RememberMeManager rememberMeManager) {// 缓存管理
		DefaultWebSecurityManager securityManager = new DefaultWebSecurityManager();
		securityManager.setRealm(memberRealm);
		securityManager.setCacheManager(cacheManager);
		securityManager.setSessionManager(sessionManager);
		securityManager.setRememberMeManager(rememberMeManager);
		return securityManager;
	}
	public FormAuthenticationFilter getLoginFilter() { 		// 在ShiroFilterFactoryBean中使用
		FormAuthenticationFilter filter = new FormAuthenticationFilter();
		filter.setUsernameParam("mid");
		filter.setPasswordParam("password");
		filter.setRememberMeParam("rememberMe");
		filter.setLoginUrl(LOGIN_URL);						// 登录提交页面
		filter.setFailureKeyAttribute("error");
		return filter;
	}
	public LogoutFilter getLogoutFilter() { 				// 在ShiroFilterFactoryBean中使用
		LogoutFilter logoutFilter = new LogoutFilter();
		logoutFilter.setRedirectUrl("/");					// 首页路径，登录注销后回到的页面
		return logoutFilter;
	}
	@Bean
	public ShiroFilterFactoryBean getShiroFilterFactoryBean(DefaultWebSecurityManager securityManager) {
		ShiroFilterFactoryBean shiroFilterFactoryBean = new ShiroFilterFactoryBean();
		shiroFilterFactoryBean.setSecurityManager(securityManager);	// 设置 SecurityManager
		shiroFilterFactoryBean.setLoginUrl(LOGIN_URL);		// 设置登录页路径
		shiroFilterFactoryBean.setSuccessUrl(SUCCESS_URL);	// 设置跳转成功页
		shiroFilterFactoryBean.setUnauthorizedUrl(UNAUTHORIZED_URL);	// 授权错误页
		Map<String, Filter> filters = new HashMap<String, Filter>();
		filters.put("authc", this.getLoginFilter());
		filters.put("logout", this.getLogoutFilter());
		shiroFilterFactoryBean.setFilters(filters);
		Map<String, String> filterChainDefinitionMap = new HashMap<String, String>();
		filterChainDefinitionMap.put("/logout.page", "logout");
		filterChainDefinitionMap.put("/loginPage", "authc");	// 定义内置登录处理
		filterChainDefinitionMap.put("/pages/**", "authc");
		filterChainDefinitionMap.put("/*", "anon");
		shiroFilterFactoryBean.setFilterChainDefinitionMap(filterChainDefinitionMap);
		return shiroFilterFactoryBean;
	}
}
```

在本配置程序之中，最为重要的一个配置方法就是`getQuartzSessionValidationScheduler()`，这也是SpringBoot整合Shiro中最为重要的一点。之所以重新配置，主要原因是SpringBoot整合Shiro时的定时调度组件版本落后，所以才需要由用户自定义一个`SessionValidationScheduler`接口子类。

#### 页面使用

在使用Shiro的过程中，除了需要对控制层与业务层的拦截过滤之外，对于页面也需要有所支持，而SpringBoot本身不提倡使用JSP页面，所以就需要引入一个支持Shiro处理的Thymeleaf命名空间。

```
<html xmlns:th="http://www.thymeleaf.org" xmlns:shiro="http://www.pollix.at/thymeleaf/shiro">
```

配置完命名空间之后，Shiro就可以使用`\<shiro:hasRole/>`、`\<shiro:principal/>`这样的标签来进行Shiro操作。

### SpringBoot基于Shiro整合OAuth统一认证

在实际项目开发过程中，随着项目功能不断推出，会出现越来越多的子系统。这样就需要使用统一的登录认证处理。在一个良好的系统设计中一般都会存在有一个单点登录，而OAuth正是现在最流行的单点登录协议。

![Image00240](/assets/Image00240.3I-u8h1r.jpg)

#### 引入依赖

修改pom.xml配置文件，引入oltu相关依赖包。

```xml
<oltu.version>1.0.2</oltu.version>
...
<dependency>
    <groupId>org.apache.oltu.oauth2</groupId>
    <artifactId>org.apache.oltu.oauth2.client</artifactId>
    <version>${oltu.version}</version>
</dependency>
<dependency>
    <groupId>org.apache.oltu.oauth2</groupId>
    <artifactId>org.apache.oltu.oauth2.authzserver</artifactId>
    <version>${oltu.version}</version>
</dependency>
<dependency>
    <groupId>org.apache.oltu.oauth2</groupId>
    <artifactId>org.apache.oltu.oauth2.resourceserver</artifactId>
    <version>${oltu.version}</version>
</dependency>
```

修改pom.xml配置文件，在SpringBoot项目中引入相关依赖。

```xml
<dependency>
    <groupId>org.apache.oltu.oauth2</groupId>
    <artifactId>org.apache.oltu.oauth2.client</artifactId>
</dependency>
<dependency>
    <groupId>org.apache.oltu.oauth2</groupId>
    <artifactId>org.apache.oltu.oauth2.authzserver</artifactId>
</dependency>
<dependency>
    <groupId>org.apache.oltu.oauth2</groupId>
    <artifactId>org.apache.oltu.oauth2.resourceserver</artifactId>
</dependency>
```

#### 修改配置文件

对于OAuth整合的处理里面，最为重要的就是为项目指明OAuth的相关处理路径，修改application.yml信息，配置OAuth相关属性。

```yaml
oauth: 
  client:
    id: d0fde52c-538f-4e06-9c2f-363fe4321c7e               # 保存client_id的信息
    secret: 902be4ff-9a36-331d-9f71-afb604d07787      # 保存client_secret的信息
  token:        # 保存token访问地址
    url: http://www.server.com:80/enterpriseauth-oauth-server/accessToken.action
  memberinfo:     # 获得用户信息的访问地址（此地址要在accessToken获取之后获得）
    url: http://www.server.com:80/enterpriseauth-oauth-server/memberInfo.action
  redirect:   # 保存返回的地址（此地址要与之前的OAuthFilter对应上）
    uri: http://www.client.com:9090/shiro-oauth
  login:     # 定义登录访问路径地址
    url: http://www.server.com:80/enterpriseauth-oauth-server/authorize.action?client_id=d0fde52c-538f-4e06-9c2f-363fe4321c7e&response_type=code&redirect_uri=http://www.client.com:9090/shiro-oauth
```

#### 创建配置类

一旦项目中引入OAuth处理，则Realm一定会发生更改，定义一个新的OAuthRealm类（代替之前的MemberRealm程序类）。

```java
public class OAuthRealm extends AuthorizingRealm {
	@Resource
	private IMemberService memberService;
	private String clientId; 			// 应该由客户服务器申请获得
	private String clientSecret; 		// 应该由客户服务器申请获得
	private String redirectUri; 		// 返回地址
	private String accessTokenUrl; 		// 进行Token操作的地址定义
	private String memberInfoUrl; 		// 获得用户信息的路径

	@Override
	public boolean supports(AuthenticationToken token) {
		return token instanceof OAuthToken; // 只有该类型的Token可以执行此Realm
	}

	@Override
	protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken token) throws AuthenticationException {
		// 此方法主要是实现用户的认证处理操作
		System.err.println("=========== 1、进行用户认证处理操作（doGetAuthenticationInfo()） ===========");
		OAuthToken oAuthToken = (OAuthToken) token; // 强制转型为自定义的OAuthToken，里面有code
		String authCode = (String) oAuthToken.getCredentials(); // 获取OAuth返回的Code数据
		String mid = this.getMemberInfo(authCode);
		return new SimpleAuthenticationInfo(mid, authCode, "memberRealm");
	}

	@Override
	protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principals) {
		// 此方法主要用于用户的授权处理操作，授权一定要在认证之后进行
		System.err.println("=========== 2、进行用户授权处理操作（doGetAuthorizationInfo()） ===========");
		SimpleAuthorizationInfo info = new SimpleAuthorizationInfo(); // 返回授权的信息
		String mid = (String) principals.getPrimaryPrincipal(); // 获得用户名
		Map<String, Set<String>> map = this.memberService.getRoleAndActionByMember(mid);
		info.setRoles(map.get("allRoles")); // 将所有的角色信息保存在授权信息中
		info.setStringPermissions(map.get("allActions")); // 保存所有的权限
		return info;
	}
	private String getMemberInfo(String code) { // 获取用户的信息
		String mid = null;
		try {
			OAuthClient oauthClient = new OAuthClient(new URLConnectionClient());
			OAuthClientRequest accessTokenRequest = OAuthClientRequest.tokenLocation(this.accessTokenUrl) // 设置Token的访问地址
					.setGrantType(GrantType.AUTHORIZATION_CODE).setClientId(this.clientId)
					.setClientSecret(this.clientSecret).setRedirectURI(this.redirectUri).setCode(code)
					.buildQueryMessage();
			// 构建了一个专门用于进行Token数据回应处理的操作类对象，获得Token的请求是POST
			OAuthJSONAccessTokenResponse oauthResponse = oauthClient.accessToken(accessTokenRequest,
					OAuth.HttpMethod.POST);
			String accessToken = oauthResponse.getAccessToken(); // 获得Token
			// 获得AccessToken设计目的是为了能够通过此Token获得mid的信息，所以此时应该继续构建第二次请求
			// 如果要想获得请求操作一定要设置有accessToken处理信息
			OAuthClientRequest memberInfoRequest = new OAuthBearerClientRequest(this.memberInfoUrl)
					.setAccessToken(accessToken).buildQueryMessage(); // 创建一个请求操作
			// 要进行指定用户信息请求的回应处理项
			OAuthResourceResponse resouceResponse = oauthClient.resource(memberInfoRequest, OAuth.HttpMethod.GET,
					OAuthResourceResponse.class); 
			mid = resouceResponse.getBody(); // 获取mid的信息
		} catch (Exception e) {
			e.printStackTrace();
		}
		return mid; 
	}
	public void setMemberInfoUrl(String memberInfoUrl) {
		this.memberInfoUrl = memberInfoUrl;
	}
	public void setClientId(String clientId) {
		this.clientId = clientId;
	}
	public void setClientSecret(String clientSecret) {
		this.clientSecret = clientSecret;
	}
	public void setRedirectUri(String redirectUri) {
		this.redirectUri = redirectUri;
	}
	public void setAccessTokenUrl(String accessTokenUrl) {
		this.accessTokenUrl = accessTokenUrl;
	}
}
```

#### 创建认证过滤器

此时基本的OAuth整合环境已经配置成功，随后还需要建立一个执行OAuth认证的过滤器，在这个过滤器中主要是要获取一个OAuth-Token信息（建立一个OAuthToken类，该类继承UsernamePasswordToken父类，里面保存有principal、authcode两个属性信息）。

```java
public class OAuthAuthenticatingFilter extends AuthenticatingFilter {
	private String authcodeParam = "code" ; // 由OAuth返回的地址上提供有参数
	private String failureUrl ;  // 定义一个失败的跳转页面
	@Override
	protected boolean onAccessDenied(ServletRequest request, ServletResponse response) throws Exception {
		// 随后需要在这个程序之中进行关于oauth登录处理的相关配置操作
		String error = request.getParameter("error") ; // 此处要求获得错误的提示信息
		if (!(error == null || "".equals(error))) {	// 现在出现有错误提示信息
			String errorDesc = request.getParameter("error_description") ; // 错误信息
			// 如果此时出现有错误信息，则直接跳转到错误页面
			WebUtils.issueRedirect(request, response,
					this.failureUrl + "?error=" + error + "&error_description" + errorDesc);
			return false ; // 后续的操作不再执行，直接跳转
		}
		Subject subject = super.getSubject(request, response) ; // 获得Subject
		if (!subject.isAuthenticated()) { // 用户现在未进行登录认证
			String code = request.getParameter(this.authcodeParam) ; // 需要接收返回的code数据
			if (code == null || "".equals(code)) {	// 此时一定是一个错误的处理操作
				super.saveRequestAndRedirectToLogin(request, response); // 跳转到登录页
				return false ;
			}
		}
		return super.executeLogin(request, response); // 执行登录处理逻辑
	}
	@Override
	protected boolean onLoginSuccess(AuthenticationToken token, Subject subject, ServletRequest request,
			ServletResponse response) throws Exception {			// 登录成功之后应该跳转到成功页面
		super.issueSuccessRedirect(request, response);				// 跳转到登录成功页面
		return false ;
	}
	@Override
	protected boolean onLoginFailure(AuthenticationToken token, AuthenticationException e, ServletRequest request,
			ServletResponse response) {								// 登录失败
		Subject subject = super.getSubject(request, response) ; 	// 获得当前用户Subject
		if (subject.isAuthenticated() || subject.isRemembered()) {	// 认证判断
			try { 													// 已经登录成功了就返回到首页上
				super.issueSuccessRedirect(request, response);
			} catch (Exception e1) {}
		} else { 													// 如果没有成功则直接跳转到失败页面
			try {
				WebUtils.issueRedirect(request, response, this.failureUrl);
			} catch (IOException e1) {}
		}
		return false ;
	} 
	public void setAuthcodeParam(String authcodeParam) {
		this.authcodeParam = authcodeParam;
	}
	public void setFailureUrl(String failureUrl) {
		this.failureUrl = failureUrl;
	}
	@Override
	protected AuthenticationToken createToken(ServletRequest request, ServletResponse response) throws Exception {
		OAuthToken token = new OAuthToken(request.getParameter(this.authcodeParam)) ;	// 要传入一个自定义的Token信息
		token.setRememberMe(true); 									// 设置记住我的功能
		return token ; 
	}
}
```

此时成功地实现了SpringBoot + Shiro + OAuth的整合处理，而这样的整合模式也是实际项目开发中的最佳组合。

---

---
url: /常用框架/SpringBoot/SpringBoot服务整合/1_SpringBoot整合数据源.md
---

# SpringBoot整合数据源

在实际项目开发中任何项目都很难脱离数据库而单独存在，所以为了提高数据库的操作性能，开发者往往会借助于数据库连接池来进行处理，同时在项目中利用DataSource进行数据源的连接。常用的有C3P0和Druid两类数据库连接池。

## SpringBoot整合C3P0数据库连接池

C3P0是一个开源的JDBC连接池，它实现了数据源和JNDI绑定，支持JDBC3规范和JDBC2的标准扩展，同时在Hibernate、Spring项目开发中被广泛应用。

### 1、引入依赖

修改父pom.xml配置文件，追加C3P0依赖支持管理。

```xml
<c3p0.version>0.9.1.2</c3p0.version>
<mysql-connector-java.version>5.1.21</mysql-connector-java.version>
...
<dependency>
    <groupId>mysql</groupId>
    <artifactId>mysql-connector-java</artifactId>
    <version>${mysql-connector-java.version}</version>
</dependency>
<dependency>
    <groupId>c3p0</groupId>
    <artifactId>c3p0</artifactId>
    <version>${c3p0.version}</version>
</dependency>
```

修改pom.xml配置文件，引入C3P0的相关依赖支持库。

```xml
<dependency>
    <groupId>mysql</groupId>
    <artifactId>mysql-connector-java</artifactId>
</dependency>

<dependency>
    <groupId>com.mchange</groupId>
    <artifactId>c3p0</artifactId>
</dependency>
```

### 2、修改配置文件

修改application.yml配置文件，追加C3P0数据库连接池配置信息。

```yaml
c3p0: # 定义C3P0配置
  jdbcUrl: jdbc:mysql://localhost:3306/xxl_springboot_action?serverTimezone=UTC # 数据库连接地址
  user: root                                # 数据库用户名
  password: 123456                           # 数据库密码
#  driverClass: org.gjt.mm.mysql.Driver      # 数据库驱动程序
  driverClass: com.mysql.cj.jdbc.Driver      # 数据库驱动程序
  minPoolSize: 1                            # 最小连接数
  maxPoolSize: 1                            # 最大连接数
  maxIdleTime: 3000                         # 最大等待时间
  initialPoolSize: 1                        # 初始化连接数
```

### 3、创建数据源连接池配置类

建立C3P0数据源连接池配置类，此时设置的Bean名称为dataSource。

```java
@Configuration
public class C3P0DatasourceConfig {
    @Bean(name = "dataSource")
    @ConfigurationProperties(prefix = "c3p0")    // 定义资源导入前导标记
    public DataSource dataSource() {
        return DataSourceBuilder.create().type(
                com.mchange.v2.c3p0.ComboPooledDataSource.class).build();
    }
}
```

### 4、测试

编写测试类，测试当前DataSource配置是否正确。

```java
@SpringBootTest(classes = SpringbootIntegrationC3p0Application.class)        // 定义要测试的SpringBoot类
@RunWith(SpringJUnit4ClassRunner.class)                            // 使用Junit进行测试
@WebAppConfiguration                                            // 进行Web应用配置
public class TestDataSource {
    @Autowired
    private DataSource dataSource;                                // 注入DataSource对象

    @Test
    public void testConnection() throws Exception {
        System.out.println(this.dataSource.getConnection());    // 获取连接
    }
}
```

如果有以下报错：

```sh
java.sql.SQLException: SSL connection required for plugin 'sha256_password'. Check if "useSSL" is set to "true".
```

在数据库连接后面追加`?useSSL=false`

```sh
java.sql.SQLException: The server time zone value 'ÖÐ¹ú±ê×¼Ê±¼ä' is unrecognized or represents more than one time zone. You must configure either the server or JDBC driver (via the serverTimezone configuration property) to use a more specifc time zone value if you want to utilize time zone support.
```

在数据库连接后面追加`?serverTimezone=UTC`

## SpringBoot整合Druid数据库连接池

Druid是阿里巴巴推出的一款数据库连接池组件（可以理解为C3P0的下一代产品），也是一个用于大数据实时查询和分析的高容错、高性能开源分布式系统，可高效处理大规模的数据并实现快速查询和分析。

### 1、引入依赖

修改父pom.xml文件，引入Druid的相关依赖库。

```xml
<druid.version>1.1.1</druid.version>
...
<dependency>
    <groupId>com.alibaba</groupId>
    <artifactId>druid</artifactId>
    <version>${druid.version}</version>
</dependency>
```

修改pom.xml配置文件，追加Druid依赖配置。

```xml
<dependency>
    <groupId>com.alibaba</groupId>
    <artifactId>druid</artifactId>
</dependency>
```

### 2、修改配置文件

修改application.yml配置文件，追加Druid的连接配置。

```yaml
spring:
  datasource:
    type: com.alibaba.druid.pool.DruidDataSource    # 配置当前要使用的数据源的操作类型
    driver-class-name: com.mysql.cj.jdbc.Driver     # 配置MySQL的驱动程序类
    url: jdbc:mysql://localhost:3306/xxl_springboot_action?serverTimezone=UTC           # 数据库连接地址
    username: root                                  # 数据库用户名
    password: 123456                                # 数据库连接密码
    dbcp2: # 进行数据库连接池的配置
      min-idle: 1                                   # 数据库连接池的最小维持连接数
      initial-size: 1                               # 初始化提供的连接数
      max-total: 1                                  # 最大的连接数
      max-wait-millis: 3000                         # 等待连接获取的最大超时时间
```

此时，项目中就可以采用Druid数据库连接池来进行数据库操作了。

> **提示：进行连接测试前需要导入相应ORM框架的依赖支持包。**
>
> 在本程序中，如果要进行DataSource连接测试，则需要导入ORM依赖关联包。例如，可以在本程序中导入MyBatis的ORM开发包。
>
> ```xml
> <dependency>
>     <groupId>org.mybatis.spring.boot</groupId>
>     <artifactId>mybatis-spring-boot-starter</artifactId>
>     <version>1.3.1</version>
> </dependency>
> ```
>
> 导入以上开发包之后，可以正常测试。如果未导入，则程序测试时会出现Unsatisfied DependencyException异常信息。

---

---
url: /常用框架/SpringBoot/SpringBoot服务整合/3_SpringBoot整合消息服务组件.md
---

# SpringBoot整合消息服务组件

在进行分布式系统设计时，经常会使用消息服务组件进行系统整合与异步服务通信，其基本结构为生产者与消费者处理。常用的消息组件主要包括两类：JMS标准（ActiveMQ）和AMQP标准（RabbitMQ、Kafka）。

![Image00200](/assets/Image00200.DhnWlART.jpg)

## SpringBoot整合ActiveMQ消息组件

ActiveMQ是Apache提供的开源组件，是基于JMS标准的实现组件。下面将利用SpringBoot整合ActiveMQ组件，实现队列消息的发送与接收。

### 1、引入依赖

修改pom.xml配置文件，追加spring-boot-starter-activemq依赖库。

```xml
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-activemq</artifactId>
</dependency>
```

### 2、修改配置文件

修改application.yml配置文件，进行ActiveMQ的配置。

```yaml
spring: 
  jms:
    pub-sub-domain: false   # 配置消息类型，如果是true则表示为topic消息，如果为false表示Queue消息
  activemq:
    user: admin             # 连接用户名
    password: admin         # 连接密码
    broker-url: tcp://activemq-server:61616   # 消息组件的连接主机信息
```

### 3、编写消息者代码

定义消息消费监听类。

```java
@Service
public class MessageConsumer {
	@JmsListener(destination="mldn.msg.queue")	// 定义消息监听队列
	public void receiveMessage(String text) {	// 进行消息接收处理
		System.err.println("【*** 接收消息 ***】" + text);
	}
}
```

### 4、编写生产者代码

定义消息生产者业务接口。

```java
public interface IMessageProducer {
	public void send(String msg) ;	// 消息发送
}
```

定义消息业务实现类。

```java
@Service
public class MessageProducerImpl implements IMessageProducer {
	@Autowired
	private JmsMessagingTemplate jmsMessagingTemplate;				// 消息发送模版
	@Autowired
	private Queue queue;											// 注入队列
	@Override
	public void send(String msg) {
		this.jmsMessagingTemplate.convertAndSend(this.queue, msg);	// 消息发送

	}
}
```

### 5、创建队列配置类

定义JMS消息发送配置类，该类主要用于配置队列信息。

```java

@Configuration
@EnableJms
public class ActiveMQConfig {
	@Bean
	public Queue queue() {
		return new ActiveMQQueue("mldn.msg.queue") ;	// 定义队列名称
	}
}
```

每当有消息接收到时，都会自动执行MessageConsumer类，进行消息消费。

## SpringBoot整合RabbitMQ消息组件

RabbitMQ是一个在AMQP基础上构建的新一代企业级消息系统，该组件由Pivotal公司提供，使用ErLang语言开发。

> **提示：RabbitMQ与SpringCloud整合之中意义重大。**
>
> SpringCloud是在SpringBoot基础上构建的微架构技术开发框架，其中的SpringCloudConfig自动刷新机制就基于消息组件完成，并且推荐使用RabbitMQ消息组件（同属于Pivotal公司产品）。在SpringCloudStream中也使用RabbitMQ作为服务组件。

### 1、引入依赖

修改pom.xml配置文件，追加`spring-boot-starter-amqp`依赖包。

```xml
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-amqp</artifactId>
</dependency>
```

### 2、修改配置文件

修改`application.yml`配置文件，进行RabbitMQ的相关配置。

```yaml
spring: 
  rabbitmq:
    addresses: rabbitmq-server      # rabbitmq服务主机名称
    username: admin                 # 用户名
    password: admin                 # 密码
    virtual-host: /                 # 虚拟主机
```

### 3、消息生产配置类

为了可以正常使用RabbitMQ进行消息处理，还需要做一个消息生产配置类。

```java
@Configuration
public class ProducerConfig {
	public static final String EXCHANGE = "xxl.microboot.exchange"; 	// 交换空间名称
	public static final String ROUTINGKEY = "xxl.microboot.routingkey"; // 设置路由key
	public static final String QUEUE_NAME = "xxl.microboot.queue"; 	// 队列名称
	@Bean
	public Binding bindingExchangeQueue(DirectExchange exchange,Queue queue) {
		return BindingBuilder.bind(queue).to(exchange).with(ROUTINGKEY) ;
	}
	@Bean
	public DirectExchange getDirectExchange() { 						// 使用直连的模式
		return new DirectExchange(EXCHANGE, true, true);
	}
	@Bean
	public Queue queue() { 											// 队列信息
		return new Queue(QUEUE_NAME);
	}
}
```

### 4、编写生产者代码

建立消息发送接口。

```java
public interface IMessageProducer {
	public void send(String msg) ;	// 消息发送
}
```

建立消息业务实现子类。

```java
@Service
public class MessageProducerImpl implements IMessageProducer {
	@Resource
	private RabbitTemplate rabbitTemplate; 
	@Override
	public void send(String msg) {
		this.rabbitTemplate.convertAndSend(ProducerConfig.EXCHANGE,
				ProducerConfig.ROUTINGKEY, msg);
	}
}
```

### 5、消息消费配置类

建立一个消息消费端的配置程序类。

```java
@Configuration
public class ConsumerConfig {
	public static final String EXCHANGE = "xxl.microboot.exchange"; 		// 交换空间名称
	public static final String ROUTINGKEY = "xxl.microboot.routingkey"; 	// 设置路由key
	public static final String QUEUE_NAME = "xxl.microboot.queue"; 		// 队列名称
	@Bean
	public Queue queue() { 													// 队列信息
		return new Queue(QUEUE_NAME);
	}
	@Bean
	public DirectExchange getDirectExchange() { 							// 使用直连的模式
		return new DirectExchange(EXCHANGE, true, true);
	}
	@Bean
	public Binding bindingExchangeQueue(DirectExchange exchange,Queue queue) {
		return BindingBuilder.bind(queue).to(exchange).with(ROUTINGKEY) ;
	}
}
```

### 6、编写消费者代码

定义监听处理类。

```java

@Service
public class MessageConsumer {
	@RabbitListener(queues="xxl.microboot.queue")
	public void receiveMessage(String text) {	// 进行消息接收处理
		System.err.println("【*** 接收消息 ***】" + text);
	}
}
```

此时程序实现了与RabbitMQ消息组件的整合，同时在整个程序中只需要调用IMessageProducer接口中的send()方法就可以正常发送，而后会找到设置同样ROUTINGKEY的消费者进行消息消费。

## SpringBoot整合Kafka消息组件

Kafka是新一代的消息系统，也是目前性能最好的消息组件，在数据采集业务中被广泛应用。本程序中配置的Kafka将基于Kerberos认证实现消息组件处理。

### 1、定义客户端文件

【操作系统-Windows】定义一个Kerberos客户端文件，路径为`d:\kafka_client_jaas.conf`。

```conf
KafkaClient {  
        org.apache.kafka.common.security.plain.PlainLoginModule required  
        username="bob"  
        password="bob-pwd";  
};  
```

### 2、引入kafka依赖

修改pom.xml配置文件，追加依赖库配置。

```xml
<dependency>
    <groupId>org.springframework.kafka</groupId>
    <artifactId>spring-kafka</artifactId>
</dependency>
```

### 3、修改配置文件

修改application.yml配置文件，进行Kafka配置项编写。

```yaml
spring: 
  kafka:
    bootstrap-servers:                  # 定义主机列表
    - kafka-single:9095
    template:
      default-topic: xxl-microboot     # 定义主题名称
    producer:                           # 定义生产者配置
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
    consumer:                           # 定义消费者配置
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      group-id: group-1                 # 数据分组
    properties:
      sasl.mechanism: PLAIN              # 安全机制
      security.protocol: SASL_PLAINTEXT   # 安全协议
```

### 4、编写消息发送代码

定义消息业务发送接口。

```java
public interface IMessageProducer {
	public void send(String msg) ;	// 消息发送
}
```

使用Kafka消息机制实现消息发送接口。

```java
@Service 
public class MessageProducerImpl implements IMessageProducer {
	@Resource
	private KafkaTemplate<String, String> kafkaTemplate;	// Kafka消息模版
	@Override
	public void send(String msg) {
		this.kafkaTemplate.sendDefault("mldn-key", msg);	// 发送消息
	}
}
```

### 5、编写消息消费代码

建立一个Kafka消息的消费程序类。

```java
@Service
public class MessageConsumer {
	@KafkaListener(topics = {"mldn-microboot"})
	public void receiveMessage(ConsumerRecord<String, String> record) { 	// 进行消息接收处理
		System.err.println("【*** 接收消息 ***】key = " + record.key() + "、value = "
				+ record.value());
	}
}
```

### 6、修改启动类

由于此时Kafka采用Kerberos认证，因此需要修改程序启动主类。

```java
@SpringBootApplication	// 启动SpringBoot程序，而后自带子包扫描
public class SpringBootStartApplication { // 必须继承指定的父类
	static {
		System.setProperty("java.security.auth.login.config",
				"d:/kafka_client_jaas.conf"); // 表示系统环境属性
	}
	public static void main(String[] args) throws Exception {
		SpringApplication.run(SpringBootStartApplication.class, args);	// 启动SpringBoot程序
	}
} 
```

此时，可以通过测试程序调用IMessageProducer接口进行消息发送，由于Kafka已经配置了自动创建主题，所以即使现在主题不存在，也不影响程序执行。

---

---
url: /常用框架/SpringBoot/SpringBoot服务整合/6_SpringBoot整合邮件服务器.md
---

# SpringBoot整合邮件服务器

## SpringBoot整合邮件服务器

Java本身提供了JavaMail标准以实现邮件的处理，同时用户也可以搭建属于自己的邮件服务器或者直接使用各个邮箱系统实现邮件的发送处理。下面将利用SpringBoot整合邮件服务，同时使用QQ邮箱系统进行服务整合。

### 邮件服务配置

登录QQ邮箱，进入邮箱设置页面，找到邮件服务配置项。

![image-20250223183814634](/assets/image-20250223183814634.DxRARpHO.png)

开启邮箱的邮件服务后，将得到一个唯一的授权码。

![image-20250223184050198](/assets/image-20250223184050198.BAikSNSY.png)

### 引入依赖

修改pom.xml配置文件，引入依赖库。

```xml
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-mail</artifactId>
</dependency>
```

### 修改配置文件

修改application.yml配置文件，实现邮件配置。

```yaml
spring:
  mail:
    host: smtp.qq.com                       # 邮箱服务器
    username: xxx@qq.com                    # 用户名
    password: clxcgqofubazbgcj              # 授权码
    properties:
      mail.smtp.auth: true                  # stmp授权开启 
      mail.smtp.starttls.enable: true       # 启动tls服务
      mail.smtp.starttls.required: true     # 启动tls支持
```

### 测试

编写测试类，进行邮件发送测试。

```java
@SpringBootTest(classes = SpringBootIntegrationMailApplication.class)
@RunWith(SpringJUnit4ClassRunner.class)
@WebAppConfiguration
public class TestMail {
    @Resource
    private JavaMailSender javaMailSender;                    // 注入JavaMailSender对象

    @Test
    public void testSendMail() {
        SimpleMailMessage message = new SimpleMailMessage();    // 要发送的消息内容
        message.setFrom("xxx@qq.com");                // 发送者
        message.setTo("xxx@126.com");                // 接收者
        message.setSubject("测试邮件");             // 邮件主题
        message.setText("好好学习，天天向上。");     // 邮件内容
        this.javaMailSender.send(message);        // 发送邮件
    }
}
```

由于SpringBoot中已经进行了大量的简化配置，所以此时的程序只需要注入JavaMailSender对象，并设置好邮件内容，就可以实现邮件信息的发送。

结果

![image-20250223185738513](/assets/image-20250223185738513.BMvXIES5.png)

---

---
url: /常用框架/SpringBoot/SpringBoot服务整合/2_SpringBoot整合ORM开发框架.md
---

# SpringBoot整合ORM开发框架

## SpringBoot整合Mybatis开发框架

MyBatis是一款常用并且配置极为简单的ORM开发框架。其与Spring结合后，可以利用Spring的特征实现DAO接口的自动配置。在SpringBoot中，又对MyBatis框架的整合进行了进一步简化。想实现这种配置，需要在项目中引入mybatis-spring-boot- starter依赖支持库。

提示：需要数据库连接池支持。（开发中一般使用Druid作为数据库连接池）

```sql
DROP
DATABASE IF EXISTS xxl_springboot_action ;
CREATE
DATABASE xxl_springboot_action CHARACTER SET UTF8 ;
USE
xxl_springboot_action ;
CREATE TABLE dept
(
    deptno BIGINT AUTO_INCREMENT,
    dname  VARCHAR(50),
    CONSTRAINT pk_deptno PRIMARY KEY (deptno)
);
INSERT INTO dept(dname)
VALUES ('开发部');
INSERT INTO dept(dname)
VALUES ('财务部');
INSERT INTO dept(dname)
VALUES ('市场部');
INSERT INTO dept(dname)
VALUES ('后勤部');
INSERT INTO dept(dname)
VALUES ('公关部');
```

### 1、引入mybatis依赖包

修改pom.xml配置文件，引入相关依赖包。

```xml
<dependency>
    <groupId>org.mybatis.spring.boot</groupId>
    <artifactId>mybatis-spring-boot-starter</artifactId>
    <version>1.3.1</version>
</dependency>
```

### 2、创建VO实体类

```java
@SuppressWarnings("serial")
public class Dept implements Serializable {
    private Long deptno;
    private String dname;

    // setter、getter略
    public Long getDeptno() {
        return deptno;
    }

    public void setDeptno(Long deptno) {
        this.deptno = deptno;
    }

    public String getDname() {
        return dname;
    }

    public void setDname(String dname) {
        this.dname = dname;
    }

    @Override
    public String toString() {
        return "Dept [deptno=" + deptno + ", dname=" + dname + "]";
    }
}
```

### 3、添加mybatis配置文件

在`src/main/resources`目录中创建`mybatis/mybatis.cfg.xml`配置文件

```xml
<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE configuration
        PUBLIC "-//mybatis.org//DTD Config 3.0//EN"
        "http://mybatis.org/dtd/mybatis-3-config.dtd">
<configuration>        <!-- 进行Mybatis的相应的环境的属性定义 -->
    <settings>        <!-- 在本项目之中开启二级缓存 -->
        <setting name="cacheEnabled" value="true"/>
    </settings>
</configuration>
```

### 4、修改application.yml配置文件

修改`application.yml`配置文件，追加MyBatis配置

```yaml
mybatis:
  config-location: classpath:mybatis/mybatis.cfg.xml    # mybatis配置文件所在路径
  type-aliases-package: com.xxl.springboot.vo             # 定义所有操作类的别名所在包
```

### 5、创建数据层接口

建立IDeptDAO接口，该接口将由Spring自动实现。

```java
@Mapper
public interface IDeptDAO {

    /**
     * 查询全部部门信息
     *
     * @return
     */
    @Select("SELECT deptno,dname FROM dept")
    public List<Dept> findAll();

}
```

### 6、定义业务层接口及实现类

定义IDeptService业务层接口。

```javascript
public interface IDeptService {
    public List<Dept> list() ;
}
```

定义DeptServiceImpl业务层实现子类。

```java
@Service
public class DeptServiceImpl implements IDeptService {

    @Autowired
    private IDeptDAO deptDAO;

    @Override
    public List<Dept> list() {
        return this.deptDAO.findAll();
    }

}
```

### 7、测试

编写测试类，测试IDeptService业务方法。

```java
@SpringBootTest(classes = SpringBootIntegrationMybatisApplication.class)        // 定义要测试的SpringBoot类
@RunWith(SpringJUnit4ClassRunner.class)                            // 使用Junit进行测试
@WebAppConfiguration                                            // 进行Web应用配置
public class TestDeptService {

    @Autowired
    private IDeptService deptService;                            // 注入业务接口对象

    @Test
    public void testList() {
        List<Dept> allDepts = this.deptService.list();
        for (Dept dept : allDepts) {
            System.out.println("部门编号：" + dept.getDeptno() + "、部门名称：" + dept.getDname());
        }
    }
}
```

打印结果

```
部门编号：1、部门名称：开发部
部门编号：2、部门名称：财务部
部门编号：3、部门名称：市场部
部门编号：4、部门名称：后勤部
部门编号：5、部门名称：公关部
```

> 完整代码参考：[springboot-action/springboot-integration/springboot-integration-mybatis-parent at main · Daneliya/springboot-action (github.com)](https://github.com/Daneliya/springboot-action/tree/main/springboot-integration/springboot-integration-mybatis-parent)

## SpringBoot整合JPA开发框架

JPA是官方推出的Java持久层操作标准（现主要使用Hibernate实现），使用SpringData技术和JpaRepository接口技术，也可以达到简化数据层的目的。要在SpringBoot中使用SpringDataJPA，需要spring-boot-starter-data-jpa依赖库的支持。

### 1、引入jpa依赖包

修改pom.xml配置文件，引入相关依赖包。

```xml
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-data-jpa</artifactId>
</dependency>
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-cache</artifactId>
</dependency>
<dependency>
    <groupId>org.hibernate</groupId>
    <artifactId>hibernate-ehcache</artifactId>
</dependency>
```

jdk11中不再支持javaxb，因此需要添加以下依赖项

```xml
<dependency>
    <groupId>javax.xml.bind</groupId>
    <artifactId>jaxb-api</artifactId>
    <version>2.2.11</version>
</dependency>
<dependency>
    <groupId>com.sun.xml.bind</groupId>
    <artifactId>jaxb-core</artifactId>
    <version>2.2.11</version>
</dependency>
<dependency>
    <groupId>com.sun.xml.bind</groupId>
    <artifactId>jaxb-impl</artifactId>
    <version>2.2.11</version>
</dependency>
<dependency>
    <groupId>javax.activation</groupId>
    <artifactId>activation</artifactId>
    <version>1.1.1</version>
</dependency>
```

否则报错

```
org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'entityManagerFactory' defined in class path resource [org/springframework/boot/autoconfigure/orm/jpa/HibernateJpaConfiguration.class]: Invocation of init method failed; nested exception is javax.persistence.PersistenceException: [PersistenceUnit: default] Unable to build Hibernate SessionFactory; nested exception is org.hibernate.MappingException: Could not get constructor for org.hibernate.persister.entity.SingleTableEntityPersister
```

还需要增加

```xml
<!-- https://mvnrepository.com/artifact/org.javassist/javassist -->
<dependency>
    <groupId>org.javassist</groupId>
    <artifactId>javassist</artifactId>
    <version>3.15.0-GA</version>
</dependency>
```

否则报错

```
Unable to instantiate default tuplizer [org.hibernate.tuple.entity.PojoEntityTuplizer]
```

### 2、修改application.yml配置文件

```yaml
spring:
  datasource:
    type: com.alibaba.druid.pool.DruidDataSource    # 配置当前要使用的数据源的操作类型
    driver-class-name: com.mysql.cj.jdbc.Driver     # 配置MySQL的驱动程序类
    url: jdbc:mysql://localhost:3306/xxl_springboot_action?serverTimezone=UTC&useSSL=false           # 数据库连接地址
    username: xxx                                  # 数据库用户名
    password: xxx                                  # 数据库连接密码
  jpa:
    show-sql: true # 控制台显示SQL
    hibernate:
      ddl-auto: update # 新或者创建数据表结构
```

> closing inbound before receiving peer's close\_notify
> 在url添加：useSSL=false

### 3、创建PO实体类

建立持久化类Dept。

```java
import java.io.Serializable;

import javax.persistence.Cacheable;
import javax.persistence.Entity;
import javax.persistence.GeneratedValue;
import javax.persistence.GenerationType;
import javax.persistence.Id;

@Table(name = "dept")
@SuppressWarnings("serial")
@Cacheable(true) 
@Entity
public class Dept implements Serializable {
    
	@Id
	@GeneratedValue(strategy = GenerationType.IDENTITY) // 根据名称引用配置的主键生成器
	private Long deptno; // 字段的映射（属性名称=字段名称）
    
    @Column()
	private String dname;

	// setter、getter略
	public Dept() {
	}

	public Long getDeptno() {
		return this.deptno;
	}

	public void setDeptno(Long deptno) {
		this.deptno = deptno;
	}


	public String getDname() {
		return this.dname;
	}

	public void setDname(String dname) {
		this.dname = dname;
	}
	
}
```

### 4、创建数据层接口

定义IDeptDAO接口，此接口继承JpaRepository父接口。

```java
public interface IDeptDAO extends JpaRepository<Dept, Long> { // 包含有全部的基础Crud支持
}
```

### 5、定义业务层接口及实现类

定义IDeptService业务层接口。

```java
public interface IDeptService {
	public List<Dept> list() ; 	
}
```

定义DeptServiceImpl业务层实现子类。

```java
@Service
public class DeptServiceImpl implements IDeptService {
	@Autowired
	private IDeptDAO deptDAO ;
	@Override
	public List<Dept> list() {
		return this.deptDAO.findAll() ; 
	}

}
```

### 6、启动类增加扫描

修改程序启动主类，追加Repository扫描配置。

```java
@SpringBootApplication	// 启动SpringBoot程序，而后自带子包扫描
@EnableJpaRepositories(basePackages="com.xxl.mldnboot.dao")
public class SpringBootStartApplication { // 必须继承指定的父类
	public static void main(String[] args) throws Exception {
		SpringApplication.run(SpringBootStartApplication.class, args);	// 启动SpringBoot程序
	}
}
```

### 7、测试

编写测试类，测试IDeptService业务方法。

```java
@SpringBootTest(classes = SpringBootStartApplication.class)		// 定义要测试的SpringBoot类
@RunWith(SpringJUnit4ClassRunner.class)							// 使用Junit进行测试
@WebAppConfiguration											// 进行Web应用配置
public class TestDeptService {
	@Autowired
	private IDeptService deptService ; 							// 注入业务接口对象
	@Test
	public void testList() {
		List<Dept> allDepts = this.deptService.list() ;
		for (Dept dept : allDepts) {
			System.out.println("部门编号：" + dept.getDeptno() + "、部门名称：" + dept.getDname());
		}
	}
}
```

打印结果

```sh
Hibernate: select dept0_.deptno as deptno1_0_, dept0_.dname as dname2_0_ from dept dept0_
部门编号：1、部门名称：开发部
部门编号：2、部门名称：财务部
部门编号：3、部门名称：市场部
部门编号：4、部门名称：后勤部
部门编号：5、部门名称：公关部
```

> 注意的是，如果想启用Repository配置，则需要在程序启动主类时使用@EnableJpaRepositories注解配置扫描包，而后才可以正常使用。

## 事务处理

SpringBoot中可以使用PlatformTransactionManager接口来实现事务的统一控制，而进行控制的时候也可以采用注解或者AOP切面配置形式来完成。

### 1、方法上启用事务注解

在业务层的方法上启用事务控制。

```java
public interface IDeptService {

    @Transactional(propagation = Propagation.REQUIRED, readOnly = true)
    public List<Dept> list();

    @Transactional(rollbackFor = Exception.class)
    public boolean addDept(String deptName);
}
```

方法实现类

```java
@Service
public class DeptServiceImpl implements IDeptService {

    @Autowired
    private IDeptDAO deptDAO;

    @Override
    public List<Dept> list() {
        return this.deptDAO.findAll();
    }

    @Override
    public boolean addDept(String deptName) {
        int result = this.deptDAO.addDept(deptName);
        if (result > 0) {
            // 自定义异常测试
            int i = 1 / 0;
            return true;
        }
        return false;
    }

}

```

数据层代码

```java
@Mapper
public interface IDeptDAO {

    /**
     * 查询全部部门信息
     *
     * @return
     */
    @Select("SELECT deptno,dname FROM dept")
    public List<Dept> findAll();

    /**
     * 添加
     *
     * @param deptName
     * @return
     */
    @Insert("INSERT INTO dept(dname) VALUES (#{deptName})")
    int addDept(String deptName);
}
```

### 2、启动类开启事务配置

在程序主类中还需要配置事务管理注解。

```java
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.transaction.annotation.EnableTransactionManagement;

@SpringBootApplication
@EnableTransactionManagement
public class SpringBootIntegrationTransactionalApplication {

    public static void main(String[] args) {
        SpringApplication.run(SpringBootIntegrationTransactionalApplication.class, args);
    }

}
```

项目中利用业务层中定义的@Transactional注解就可以实现事务的控制，但是这样的事务控制过于复杂。在一个大型项目中可能存在成百上千的业务接口，全部使用注解控制必然会造成代码的大量重复。在实际工作中，SpringBoot与事务结合最好的控制方法就是定义一个事务的配置类。

### 3、配置类方式

引入切面依赖

```xml
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-aop</artifactId>
</dependency>
```

取消事务注解配置，并定义TransactionConfig配置类。

```java
@Configuration                        // 定义配置Bean
@Aspect                                // 采用AOP切面处理
public class TransactionConfig {

    private static final int TRANSACTION_METHOD_TIMEOUT = 5;    // 事务超时时间为5秒

    private static final String AOP_POINTCUT_EXPRESSION = "execution (* om.xxl.springboot.integration.transactional.service.*.*(..))";    // 定义切面表达式

    @Autowired
    private PlatformTransactionManager transactionManager;        // 注入事务管理对象

    @Bean("txAdvice")                                            // Bean名称必须为txAdvice
    public TransactionInterceptor transactionConfig() {            // 定义事务控制切面
        // 定义只读事务控制，该事务不需要启动事务支持
        RuleBasedTransactionAttribute readOnly = new RuleBasedTransactionAttribute();
        readOnly.setReadOnly(true);
        readOnly.setPropagationBehavior(TransactionDefinition.PROPAGATION_NOT_SUPPORTED);
        // 定义更新事务，同时设置事务操作的超时时间
        RuleBasedTransactionAttribute required = new RuleBasedTransactionAttribute();
        required.setPropagationBehavior(TransactionDefinition.PROPAGATION_REQUIRED);
        required.setTimeout(TRANSACTION_METHOD_TIMEOUT);
        Map<String, TransactionAttribute> transactionMap = new HashMap<>();            // 定义业务切面
        transactionMap.put("add*", required);
        transactionMap.put("edit*", required);
        transactionMap.put("delete*", required);
        transactionMap.put("get*", readOnly);
        transactionMap.put("list*", readOnly);
        NameMatchTransactionAttributeSource source = new NameMatchTransactionAttributeSource();
        source.setNameMap(transactionMap);
        TransactionInterceptor transactionInterceptor = new TransactionInterceptor(transactionManager, source);
        return transactionInterceptor;
    }

    @Bean
    public Advisor transactionAdviceAdvisor() {
        AspectJExpressionPointcut pointcut = new AspectJExpressionPointcut();
        pointcut.setExpression(AOP_POINTCUT_EXPRESSION);                                    // 定义切面
        return new DefaultPointcutAdvisor(pointcut, transactionConfig());
    }
}
```

此时程序中的事务控制可以利用TransactionConfig类结合AspectJ切面与业务层中的方法匹配，而后就不再需要业务方法使用@Transactional注解重复定义了。

### 4、测试

```java
@SpringBootTest(classes = SpringBootIntegrationTransactionalApplication.class)        // 定义要测试的SpringBoot类
@RunWith(SpringJUnit4ClassRunner.class)                            // 使用Junit进行测试
@WebAppConfiguration                                            // 进行Web应用配置
public class TestDeptService {

    @Autowired
    private IDeptService deptService;                            // 注入业务接口对象

    @Test
    public void testAdd() {
        this.deptService.addDept("科室1");
    }
}
```

---

---
url: /常用框架/SpringBoot/SpringBoot服务整合/4_SpringBoot整合Redis数据库.md
---

# SpringBoot整合Redis数据库

## SpringBoot整合Redis数据库

Redis是当下最流行的用于实现缓存机制的NoSQL数据库，其主要通过key-value存储，支持高并发访问。在实际工作中，Redis结合SpringData技术后可以方便地实现序列化对象的存储。SpringBoot很好地支持了Redis，可以在项目中使用SpringData进行Redis数据操作。

### 一、SpringBoot整合RedisTemplate操作Redis

RedisTemplate是SpringData提供的Redis操作模板，该操作模板主要以Jedis驱动程序为实现基础，进行数据操作封装，所以可以直接调用Redis中的各种数据处理命令进行数据库操作。

#### 1、引入依赖

修改项目中的pom.xml配置文件，追加Redis的依赖引用。

```xml
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-data-redis</artifactId>
</dependency>
```

#### 2、修改配置文件

修改application.yml配置文件，引入Redis相关配置项。

```yaml
spring: 
  redis:                    # Redis相关配置
    host: redis-server      # 主机名称
    port: 6379              # 端口号
    password: admin       # 认证密码
    timeout: 1000         # 连接超时时间
    database: 0         # 默认数据库
    pool:               # 连接池配置
      max-active: 10    # 最大连接数
      max-idle: 8       # 最大维持连接数
      min-idle: 2       # 最小维持连接数
      max-wait: 100     # 最大等待连接超时时间
```

#### 3、测试

在application.yml配置文件中定义完Redis的相关配置后，就可以通过程序来利用RedisTemplate模板进行数据处理了。下面直接编写一个测试类进行测试。

```java
@SpringBootTest(classes = SpringBootStartApplication.class)
@RunWith(SpringJUnit4ClassRunner.class)
@WebAppConfiguration
public class TestRedisTemplate {
	@Autowired
	private RedisTemplate<String, String> redisTemplate;					// 引入RedisTemplate
	@Test
	public void testSet() {
		this.redisTemplate.opsForValue().set("皮卡丘", "java");				// 设置字符串信息
		System.out.println(this.redisTemplate.opsForValue().get("皮卡丘"));	// 根据key获取value
	}
}
```

本程序在测试类中直接注入了RedisTemplate模板对象，并且利用模板对象中提供的方法实现了key-value数据的保存与获取。

### 二、Redis对象序列化操作

在实际项目开发中，使用RedisTemplate操作Redis数据库不仅可以方便地进行命令的操作，还可以结合对象序列化操作，实现对象的保存。

#### 1、定义序列化配置类

定义对象的序列化配置类，以实现RedisSerializer接口。

```java
public class RedisObjectSerializer implements RedisSerializer<Object> {
	// 为了方便进行对象与字节数组的转换，所以应该首先准备出两个转换器
	private Converter<Object, byte[]> serializingConverter = new SerializingConverter();
	private Converter<byte[], Object> deserializingConverter = new DeserializingConverter();
	private static final byte[] EMPTY_BYTE_ARRAY = new byte[0]; // 做一个空数组，不是null

	@Override
	public byte[] serialize(Object obj) throws SerializationException {
		if (obj == null) { // 这个时候没有要序列化的对象出现，所以返回的字节数组应该就是一个空数组
			return EMPTY_BYTE_ARRAY;
		}
		return this.serializingConverter.convert(obj); // 将对象变为字节数组
	}

	@Override
	public Object deserialize(byte[] data) throws SerializationException {
		if (data == null || data.length == 0) { // 此时没有对象的内容信息
			return null;
		}
		return this.deserializingConverter.convert(data);
	}
}
```

#### 2、创建RedisTemplate配置类

要让建立的对象序列化管理类生效，还需要建立一个RedisTemplate的配置类。

```java
@Configuration
public class RedisConfig {
	@Bean
	public RedisTemplate<String, Object> getRedisTemplate(
			RedisConnectionFactory factory) {
		RedisTemplate<String, Object> redisTemplate = new RedisTemplate<String, Object>();
		redisTemplate.setConnectionFactory(factory);
		redisTemplate.setKeySerializer(new StringRedisSerializer()); 	// key的序列化类型
		redisTemplate.setValueSerializer(new RedisObjectSerializer()); 	// value的序列化类型
		return redisTemplate;
	}
}
```

#### 3、创建序列化对象

建立一个待序列化的VO类对象。

```java
@SuppressWarnings("serial")
public class Member implements Serializable {
	private String mid;
	private String name ;
	private Integer age;
	// setter、getter略
	public String getMid() {
		return mid;
	}
	public void setMid(String mid) {
		this.mid = mid;
	}
	public String getName() {
		return name;
	}
	public void setName(String name) {
		this.name = name;
	}
	public Integer getAge() {
		return age;
	}
	public void setAge(Integer age) {
		this.age = age;
	}
}
```

#### 4、测试

建立测试类，实现对象信息保存。

```java
@SpringBootTest(classes = SpringBootStartApplication.class)
@RunWith(SpringJUnit4ClassRunner.class)
@WebAppConfiguration
public class TestRedisTemplate {
	@Autowired
	private RedisTemplate<String, Object> redisTemplate;	// 引入RedisTemplate
	@Test 
	public void testGet() {									// 根据key取得数据
		System.out.println(this.redisTemplate.opsForValue().get("皮卡丘"));
	}
	@Test
	public void testSet() {
		Member vo = new Member() ;							// 实例化VO对象
		vo.setMid("pikaqiu");
		vo.setName("皮卡丘");
		vo.setAge(19);
		this.redisTemplate.opsForValue().set("皮卡丘", vo);	// 保存数据
	}
}

```

此时的程序可以使用String作为key类型，Object作为value类型，直接利用RedisTemplate可以将对象序列化保存在Redis数据库中，也可以利用指定的key通过Redis获取对应信息。

### 三、配置多个RedisTemplate

SpringBoot通过配置application.yml，只能够注入一个RedisTemplate对象。从事过实际开发的读者应该清楚，在实际的使用中有可能会在项目中连接多个Redis数据源，这时将无法依靠SpringBoot的自动配置实现，只能够由用户自己来创建RedisTemplate对象。

#### 1、修改配置文件

为了规范配置，需要在application.yml中进行两个Redis数据库连接的配置。

```yaml
myredis:                    # 自定义Redis连接配置
  redis-one:                # 第一个Redis连接
    host: redis-server-1    # Redis主机
    port: 6379              # 连接端口
    password: admin     # 认证信息
    timeout: 1000       # 连接超时时间
    database: 0         # 默认数据库
    pool:               # 连接池配置
      max-active: 10    # 最大连接数
      max-idle: 8       # 最大维持连接数
      min-idle: 2       # 最小维持连接数
      max-wait: 100     # 最大等待连接超时时间
  redis-two:  # 第二个Redis连接
    host: redis-server-2  # Redis主机
    port: 6379            # 连接端口
    password: admin       # 认证信息
    timeout: 1000         # 连接超时时间
    database: 1           # 默认数据库
    pool:                 # 连接池配置
      max-active: 10      # 最大连接数
      max-idle: 8         # 最大维持连接数
      min-idle: 2         # 最小维持连接数
      max-wait: 100       # 最大等待连接超时时间
```

#### 2、修改依赖

修改pom.xml配置文件。

```xml
<!--	<dependency>		// 取消掉spring-boot-starter-data-redis依赖库
  <groupId>org.springframework.boot</groupId>
  <artifactId>spring-boot-starter-data-redis</artifactId>
 </dependency> -->
<dependency>
    <groupId>org.springframework.data</groupId>
    <artifactId>spring-data-redis</artifactId>
</dependency>
<dependency>
    <groupId>redis.clients</groupId>
    <artifactId>jedis</artifactId>
</dependency>
```

#### 3、自定义的Redis配置类

编写自定义的Redis配置类。

```java
@Configuration
public class RedisConfig {	// 表示定义一个配置类
    
	@Resource(name="redisConnectionFactory") 
	private RedisConnectionFactory redisConnectionFactoryOne;
    
	@Resource(name="redisConnectionFactoryTwo") 
	private RedisConnectionFactory redisConnectionFactoryTwo;
    
	@Bean("redisConnectionFactoryTwo")
	public RedisConnectionFactory getRedisConnectionFactoryTwo(
			@Value("${myredis.redis-two.host}") String hostName,
			@Value("${myredis.redis-two.password}") String password,
			@Value("${myredis.redis-two.port}") int port,
			@Value("${myredis.redis-two.database}") int database,
			@Value("${myredis.redis-two.pool.max-active}") int maxActive,
			@Value("${myredis.redis-two.pool.max-idle}") int maxIdle,
			@Value("${myredis.redis-two.pool.min-idle}") int minIdle,
			@Value("${myredis.redis-two.pool.max-wait}") long maxWait) { // 负责建立Factory的连接工厂类
		JedisConnectionFactory jedisFactory = new JedisConnectionFactory();
		jedisFactory.setHostName(hostName);
		jedisFactory.setPort(port);
		jedisFactory.setPassword(password);
		jedisFactory.setDatabase(database); 
		JedisPoolConfig poolConfig = new JedisPoolConfig(); // 进行连接池配置
		poolConfig.setMaxTotal(maxActive);
		poolConfig.setMaxIdle(maxIdle);
		poolConfig.setMinIdle(minIdle);
		poolConfig.setMaxWaitMillis(maxWait);
		jedisFactory.setPoolConfig(poolConfig);
		jedisFactory.afterPropertiesSet(); // 初始化连接池配置
		return jedisFactory;
	}
    
	@Bean("redisConnectionFactory")	// 如果要与SpringBoot整合一定要提供一个指定名字的RedisConnectionFactory
	public RedisConnectionFactory getRedisConnectionFactoryOne(
			@Value("${myredis.redis-one.host}") String hostName,
			@Value("${myredis.redis-one.password}") String password,
			@Value("${myredis.redis-one.port}") int port,
			@Value("${myredis.redis-one.database}") int database,
			@Value("${myredis.redis-one.pool.max-active}") int maxActive,
			@Value("${myredis.redis-one.pool.max-idle}") int maxIdle,
			@Value("${myredis.redis-one.pool.min-idle}") int minIdle,
			@Value("${myredis.redis-one.pool.max-wait}") long maxWait) { // 建立Factory的连接工厂类
		JedisConnectionFactory jedisFactory = new JedisConnectionFactory();
		jedisFactory.setHostName(hostName);
		jedisFactory.setPort(port);
		jedisFactory.setPassword(password);
		jedisFactory.setDatabase(database);
		JedisPoolConfig poolConfig = new JedisPoolConfig(); // 进行连接池配置
		poolConfig.setMaxTotal(maxActive);
		poolConfig.setMaxIdle(maxIdle);
		poolConfig.setMinIdle(minIdle);
		poolConfig.setMaxWaitMillis(maxWait);
		jedisFactory.setPoolConfig(poolConfig);
		jedisFactory.afterPropertiesSet(); 								// 初始化连接池配置
		return jedisFactory;
	}
	@Bean("redisOne")
	public RedisTemplate<String, String> getRedisTemplateOne() { 
		RedisTemplate<String, String> redisTemplate = new RedisTemplate<String, String>();
		redisTemplate.setKeySerializer(new StringRedisSerializer()); 	// key的序列化类型
		redisTemplate.setValueSerializer(new RedisObjectSerializer()); 	// value的序列化类型
		redisTemplate.setConnectionFactory(this.redisConnectionFactoryOne);
		return redisTemplate; 
	}
	@Bean("redisTwo")
	public RedisTemplate<String, String> getRedisTemplateTwo() { 
		RedisTemplate<String, String> redisTemplate = new RedisTemplate<String, String>();
		redisTemplate.setKeySerializer(new StringRedisSerializer()); 	// key的序列化类型
		redisTemplate.setValueSerializer(new RedisObjectSerializer()); 	// value的序列化类型
		redisTemplate.setConnectionFactory(this.redisConnectionFactoryTwo);
		return redisTemplate; 
	}
}

```

#### 4、测试

编写测试类，使用两个RedisTemplate进行数据操作。

```java
@SpringBootTest(classes = SpringBootStartApplication.class)
@RunWith(SpringJUnit4ClassRunner.class)
@WebAppConfiguration
public class TestRedisTemplate {
    
	@Resource(name="redisOne")
	private RedisTemplate<String,String> redisOne;
    
	@Resource(name="redisTwo")
	private RedisTemplate<String,String> redisTwo;
    
	@Test
	public void testSet() {
		this.redisOne.opsForValue().set("皮卡丘", "hello");	// 保存数据
		this.redisTwo.opsForValue().set("杰尼龟", "hello");	// 保存数据
	}
}
```

本程序利用RedisConfig程序类注入了两个RedisTemplate对象，因此该程序具备了两个Redis数据库的操作能力。

---

---
url: /Redis/Redis基础/4_SpringBoot整合Redisson.md
---

# SpringBoot整合Redisson

## 分布式锁

[redisson使用全解——redisson官方文档+注释（上篇）\_redisson官网中文-CSDN博客](https://blog.csdn.net/A_art_xiang/article/details/125525864)

[redis分布式锁实现---基于redisson封装自己的分布式锁\_redsson实现了分布式令牌桶的封装-CSDN博客](https://blog.csdn.net/Hellowenpan/article/details/119320317)

https://blog.csdn.net/heihei\_linlin/article/details/126529597

## SpringBoot集成Redisson实现延迟队列

在单机环境中，JDK已经自带了很多能够实现延时队列功能的组件，比如DelayQueue, Timer, ScheduledExecutorService等组件，都可以较为简便地创建延时任务，但上述组件使用一般需要把任务存储在内存中，服务重启存在任务丢失风险，且任务规模体量受内存限制，同时也造成长时间内存占用，并不灵活，通常适用于单进程客服端程序中或对任务要求不高的项目中。

在分布式环境下，仅使用JDK自带组件并不能可靠高效地实现延时队列，通常需要引入第三方中间件或框架。

比如常见的经典任务调度框架Quartz或基于此框架的xxl-job等其它框架，这些框架的主要功能是实现定时任务或周期性任务，在Redis、RabbitMQ还未广泛应用时，譬如常见的超时未支付取消订单等功能都是由定时任务实现的，通过定时轮询来判断是否已到达触发执行的时间点。但由于定时任务需要一定的周期性，周期扫描的间隔时间不好控制，太短会造成很多无意义的扫描，且增大系统压力，太长又会造成执行时间误差太大，且可能造成单次扫描所处理的堆积记录数量过大。

此外，利用MQ做延时队列也是一种常见的方式，比如通过RabbitMQ的TTL和死信队列实现消息的延迟投递，但是投递出去的MQ消息无法方便地实现删除或修改,即无法实现任务的取消或任务执行时间点的更改，同时也不能方便地对消息进行去重。

### 方式一：redis的过期key监控

**1，开启过期key监听**

在redis的配置里把这个注释去掉

```
notify-keyspace-events Ex
```

然后重启redis

**2，使用redis过期监听实现延迟队列**

继承KeyExpirationEventMessageListener类，实现父类的方法，就可以监听key过期时间了。当有key过期，就会执行这里。这里就把需要的key过滤出来，然后发送给kafka队列。

```java
@Component
@Slf4j
public class RedisKeyExpirationListener extends KeyExpirationEventMessageListener  {
    
    @Autowired
    private KafkaProducerService kafkaProducerService;

    public RedisKeyExpirationListener(RedisMessageListenerContainer listenerContainer) {
        super(listenerContainer);
    }

    /**
     * 针对 redis 数据失效事件，进行数据处理
     * @param message
     * @param pattern
     */
    @Override
    public void onMessage(Message message, byte[] pattern){
        if(message == null || StringUtils.isEmpty(message.toString())){
            return;
        }
        String content = message.toString();
        //key的格式为   flag:时效类型:运单号  示例如下
        try {
            if(content.startsWith(AbnConstant.EMS)){
                kafkaProducerService.sendMessageSync(TopicConstant.EMS_WAYBILL_ABN_QUEUE,content);
            }else if(content.startsWith(AbnConstant.YUNDA)){
                kafkaProducerService.sendMessageSync(TopicConstant.YUNDA_WAYBILL_ABN_QUEUE,content);
            }
        } catch (Exception e) {
            log.error("监控过期key，发送kafka异常，",e);
        }
    }
}
```

可以看的出来，这种方式其实是很简单的，但是有几个问题需要注意，一是，这个尽量单机运行，因为多台机器都会执行，浪费cpu，增加数据库负担。二是，机器频繁部署的时候，如果有时间间隔，会出现数据的漏处理。三是，能不用就别用，有坑。

### 方式二：redis的zset实现延迟队列

Redis的数据结构zset，同样可以实现延迟队列的效果，且更加灵活，可以实现MQ无法做到的一些特性，采用Redis实现延时队列，对其进行优化与封装。

1，生产者实现

可以看到生产者很简单，其实就是利用zset的特性（利用`zset`的`score`属性，`redis`会将`zset`集合中的元素按照`score`进行从小到大排序），给一个zset添加元素而已，而时间就是它的score。

```java
public void produce(Integer taskId, long exeTime) {
    System.out.println("加入任务， taskId: " + taskId + ", exeTime: " + exeTime + ", 当前时间：" + LocalDateTime.now());
    RedisOps.getJedis().zadd(RedisOps.key, exeTime, String.valueOf(taskId));
}
```

2，消费者实现

消费者的代码也不难，就是把已经过期的zset中的元素给删除掉，然后处理数据。

```java
public void consumer() {
    Executors.newSingleThreadExecutor().submit(new Runnable() {
        @Override
        public void run() {
            while (true) {
                Set<String> taskIdSet = RedisOps.getJedis().zrangeByScore(RedisOps.key, 0, System.currentTimeMillis(), 0, 1);
                if (taskIdSet == null || taskIdSet.isEmpty()) {
                    System.out.println("没有任务");
} else {
                taskIdSet.forEach(id -> {
                    long result = RedisOps.getJedis().zrem(RedisOps.key, id);
                    if (result == 1L) {
                        System.out.println("从延时队列中获取到任务，taskId:" + id + " , 当前时间：" + LocalDateTime.now());
                    }
                });
            }
            try {
                TimeUnit.MILLISECONDS.sleep(100);
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
        }
    }
});

```

这种方式其实是比上个方式要好的。因为，他的两个缺点都被克服掉了。多台机器也没事儿，也不用再担心部署时间间隔长的问题。

### 方式三：封装 Redis 延迟队列工具类（RBlockingDeque+RDelayedQueue）

### 方式四：LUA脚本

参考资料：

\[1]. [基于Redis实现延时队列的优化方案](https://blog.csdn.net/u012791490/article/details/125243933)

\[2]. [redis实现延时队列的两种方式](https://blog.csdn.net/qq_36268452/article/details/113392170)

\[3]. [SpringBoot集成Redisson实现延迟队列\_redssion延时队列订阅](https://blog.csdn.net/qq_40087415/article/details/115940092)

https://blog.51cto.com/u\_16099257/7262841

https://cloud.tencent.com/developer/article/2379783

https://cloud.tencent.com/developer/article/2223087

https://blog.csdn.net/z1ztai/article/details/129669032

https://blog.csdn.net/BASK2311/article/details/129223472

https://blog.csdn.net/ZuiChuDeQiDian/article/details/104374110

https://blog.csdn.net/LBWNB\_Java/article/details/129055830

https://blog.51cto.com/u\_13270529/5960644

https://www.163.com/dy/article/HO0M5GA60518R7MO.html

https://www.zhangshengrong.com/p/Ap1ZrDlK10/

---

---
url: /常用框架/SpringBoot/SpringBoot程序开发/3_SpringBoot注解分析.md
---

# SpringBoot注解分析

---

---
url: /Java/架构设计/分布式/06.分布式监控/SpringBootAdmin.md
---

# SpringBootAdmin系统监控

## 概述

SpringBoot提供了actuator服务，用于系统监控，但是只提供json数据，而SpringBootAdmin 能够将Actuator中的信息进行界面化的展示，也可以监控所有SpringBoot应用的健康状况，提供实时警报功能。

SpringBootAdmin的主要功能有：

* 显示应用程序的监控状态
* 应用程序上下线监控
* 查看 JVM，线程信息
* 可视化的查看日志以及下载日志文件
* 动态切换日志级别
* Http 请求信息跟踪

## SpringBoot集成

admin分为server端和client端

> 排坑：如果Sprigboot依赖是2.1.x 引入的admin也是2.1.x，如果是2.3.x引入的就是2.3.x

### Server端服务搭建

新建一个工程，名为springboot-admin-server。

依赖引入

```xml
<dependencies>
    <!--项目中添加 spring-boot-starter-->
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-web</artifactId>
    </dependency>
    <!-- SpringBootAdmin -->
    <dependency>
        <groupId>de.codecentric</groupId>
        <artifactId>spring-boot-admin-starter-server</artifactId>
        <version>2.3.1</version>
    </dependency>
    <!--Spring boot 安全监控-->
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-actuator</artifactId>
    </dependency>
</dependencies>
```

启动类加注解

```java
@EnableAdminServer
@SpringBootApplication
public class SpringbootAdminServerApplication {

	public static void main(String[] args) {
		SpringApplication.run(SpringbootAdminServerApplication.class, args);
	}

}
```

服务端配置

```yaml
server:
  port: 13006
#  servlet:
#    context-path: /xxl-springboot-admin

spring:
  application:
    name: xxl-admin # 应用名

# 暴露端点
management:
  endpoints:
    web:
      exposure:
        include: '*'  # 需要开放的端点。默认值只打开 health 和 info 两个端点。通过设置 *, 可以开放所有端点
  endpoint:
    health:
      show-details: always
```

### client端依赖引入

新建一个工程，名为springboot-admin-client，工程中引入如下依赖。或直接在common中引入，多个client中引入common模块，则代表所有client端都引入了。

```xml
<!-- SpringBoot Admin 客户端依赖 -->
<dependency>
    <groupId>de.codecentric</groupId>
    <artifactId>spring-boot-admin-starter-client</artifactId>
    <version>2.3.1</version>
</dependency>
<!--Spring boot 安全监控-->
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-actuator</artifactId>
</dependency>
```

所有微服务（客户端）配置：

```yaml
management:
  endpoints:
    web:
      exposure:
        include: '*'
```

配置成功后访问：http://localhost:13006

> 获取本文档所有代码可访问
>
> [springboot\_chowder/springboot\_admin\_client at main · Daneliya/springboot\_chowder (github.com)](https://github.com/Daneliya/springboot_chowder/tree/main/springboot_admin_client)
>
> [springboot\_chowder/springboot\_admin\_server at main · Daneliya/springboot\_chowder (github.com)](https://github.com/Daneliya/springboot_chowder/tree/main/springboot_admin_server)

参考资料：

\[1]. [若依中关于SpringBootAdmin的使用](http://doc.ruoyi.vip/ruoyi-cloud/cloud/monitor.html#%E7%99%BB%E5%BD%95%E8%AE%A4%E8%AF%81)

\[2]. [Spring Boot Admin 介绍及使用\_bootadmin-CSDN博客](https://blog.csdn.net/zouliping123456/article/details/121977792)

\[3]. [Spring Boot Admin 监控指标接入Grafana可视化的实例详解(IT技术) (qb5200.com)](https://www.qb5200.com/article/589139.html)

\[4]. [springboot基础(54):Spring Boot Admin的metrics端点指标控制\_springboot admin metrics-CSDN博客](https://blog.csdn.net/u011628753/article/details/125922904)

---

---
url: /Java/微服务专栏/01.SpringCloud/4_SpringCloud服务组件.md
---

# SpringCloud服务组件

SpringCloud微架构开发中存在着众多的微服务，这些微服务之间也会存在互相的调用关联，为了防止某一个微服务不可用时关联微服务出现问题，需要引入Hystrix熔断处理机制。同时，微服务的调用形式在消费端应该以远程接口的形式出现，为此SpringCloud家族提供了Feign转换技术。为了保证微服务的安全访问，还提供了类似网关的Zuul组件支持。

## 一、Ribbon负载均衡组件

所有的微服务都需要注册到Eureka服务中，因此可以通过Eureka对所有微服务进行管理。消费端应该通过Eureka来进行微服务接口调用，这种调用可以利用Ribbon技术来实现。

### 1、Ribbon基本使用

Ribbon是一个与Eureka结合的组件，其主要作用是进行Eureka中的服务调用。要使用Ribbon，需要在项目中配置spring-cloud-starter-ribbon依赖库。同时对于所有注册到Eureka中的微服务也要求有微服务的名称，在消费端将通过微服务的名称进行微服务调用。

修改pom.xml配置文件，引入Ribbon依赖库。

```xml
<!--Spring Cloud Eureka 客户端依赖-->
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-netflix-eureka-client</artifactId>
</dependency>
<!--Spring Cloud Ribbon 依赖-->
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-netflix-ribbon</artifactId>
</dependency>
```

修改RestfulConfig配置类，追加Ribbon注解。

```java
@Configuration
public class RestfulConfig {

    @Bean
    public HttpHeaders getHeaders() {                // Http头信息配置
        HttpHeaders headers = new HttpHeaders();     // 定义HTTP的头信息
        String auth = "xxl:hello";                   // 认证的原始信息
        byte[] encodedAuth = Base64.getEncoder()
                .encode(auth.getBytes(Charset.forName("US-ASCII"))); // 进行加密的处理
        // 在进行授权的头信息内容配置的时候加密的信息一定要与“Basic”之间有一个空格
        String authHeader = "Basic " + new String(encodedAuth);
        headers.set("Authorization", authHeader);    // 保存头信息
        return headers;
    }

    @Bean
    @LoadBalanced                            // Ribbon提供的负载均衡注解
    public RestTemplate getRestTemplate() {
        return new RestTemplate();            // 实例化RestTemplate对象
    }
}
```

修改application.yml配置文件，追加Eureka访问地址。

```yaml
server:
  port: 80          # 服务端口
spring: 
  application:
    name: springcloud-ribbon-server
eureka:
  client:             # 客户端进行Eureka注册的配置
    service-url:
      defaultZone: http://127.0.0.1:7001/eureka
    register-with-eureka: false    # 当前的微服务不注册到eureka之中
```

在DeptController控制器类中通过微服务名称调用微服务，此时不再需要知道微服务的具体主机信息。

```java
@RestController                            // 为方便起见使用Restful风格展示
public class DeptController {
    public static final String DEPT_ADD_URL = "http://springcloud-dept-service/dept/add";
    public static final String DEPT_GET_URL = "http://springcloud-dept-service/dept/get";
    public static final String DEPT_LIST_URL = "http://springcloud-dept-service/dept/list";
    @Resource
    private RestTemplate restTemplate;        // 注入RestTemplate对象
    @Resource
    private HttpHeaders headers;            // 注入Http头信息对象

    @SuppressWarnings("unchecked")
    @GetMapping("/consumer/dept/list")
    public Object listDeptRest() {
        List<DeptDTO> allDepts = this.restTemplate
                .exchange(DEPT_LIST_URL, HttpMethod.GET,
                        new HttpEntity<Object>(this.headers), List.class)
                .getBody();                    // 访问服务设置头信息
        return allDepts;

    }

    @GetMapping("/consumer/dept/get")
    public Object getDeptRest(long deptno) {
        DeptDTO dept = this.restTemplate
                .exchange(DEPT_GET_URL + "/" + deptno, HttpMethod.GET,
                        new HttpEntity<Object>(this.headers), DeptDTO.class)
                .getBody();                    // 访问服务设置头信息
        return dept;
    }

    @GetMapping("/consumer/dept/add")
    public Object addDeptRest(DeptDTO dept) {    // 传输DeptDTO对象
        DeptDTO result = this.restTemplate.exchange(DEPT_ADD_URL, HttpMethod.POST,
                        new HttpEntity<Object>(dept, this.headers), DeptDTO.class)
                .getBody();                // 访问服务设置头信息
        return result;
    }

}
```

修改程序启动类，追加Eureka客户端注解。

```java
@SpringBootApplication
@EnableEurekaClient
public class StartRibbonApplication {
    public static void main(String[] args) {
        SpringApplication.run(StartRibbonApplication.class, args);
    }
}
```

此时，消费端就可以实现Eureka中注册微服务的调用，并且在消费端通过名称实现微服务调用。

![image-20250316210608327](/assets/image-20250316210608327.D6ROxBX1.png)

### 2、Ribbon负载均衡

微服务搭建的业务中心可以通过多台业务功能相同的微服务构建微服务集群，所有的微服务为了可以动态维护，都需要将其注册到Eureka之中，这样消费端就可以利用Ribbon与Eureka的服务主机列表实现微服务轮询调用，以实现负载均衡。需要注意的是，Ribbon提供的是一种客户端的负载均衡配置。

![Image00372](/assets/Image00372.edxBCpd6.jpg)

参考 springcloud-dept-8001，再创建两个微服务 Moudle ：springcloud-dept-8002 和 springcloud-dept-8003。

| NO   | 项目名称              | 运行端口 | 数据库名称          | host主机名称  |
| ---- | --------------------- | -------- | ------------------- | ------------- |
| 1    | springcloud-dept-8001 | 8001     | xxl\_springcloud\_db1 | dept-8001.com |
| 2    | springcloud-dept-8002 | 8002     | xxl\_springcloud\_db2 | dept-8002.com |
| 3    | springcloud-dept-8003 | 8003     | xxl\_springcloud\_db3 | dept-8003.com |

在MySQL数据库中执行以下 SQL 语句，准备测试数据

```sql
DROP DATABASE IF EXISTS xxl_springcloud_db1;

CREATE DATABASE xxl_springcloud_db1 CHARACTER SET UTF8;

USE xxl_springcloud_db1;

DROP TABLE IF EXISTS `dept`;
CREATE TABLE `dept` (
  `dept_no` int NOT NULL AUTO_INCREMENT,
  `dept_name` varchar(255) DEFAULT NULL,
  `db_source` varchar(255) DEFAULT NULL,
  PRIMARY KEY (`dept_no`)
) ENGINE=InnoDB AUTO_INCREMENT=6 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;

INSERT INTO `dept` VALUES ('1', '开发部', DATABASE());
INSERT INTO `dept` VALUES ('2', '人事部', DATABASE());
INSERT INTO `dept` VALUES ('3', '财务部', DATABASE());
INSERT INTO `dept` VALUES ('4', '市场部', DATABASE());
INSERT INTO `dept` VALUES ('5', '运维部', DATABASE());

#############################################################################################
DROP DATABASE IF EXISTS xxl_springcloud_db2;

CREATE DATABASE xxl_springcloud_db2 CHARACTER SET UTF8;

USE xxl_springcloud_db2;

DROP TABLE IF EXISTS `dept`;
CREATE TABLE `dept` (
  `dept_no` int NOT NULL AUTO_INCREMENT,
  `dept_name` varchar(255) DEFAULT NULL,
  `db_source` varchar(255) DEFAULT NULL,
  PRIMARY KEY (`dept_no`)
) ENGINE=InnoDB AUTO_INCREMENT=6 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;

INSERT INTO `dept` VALUES ('1', '开发部', DATABASE());
INSERT INTO `dept` VALUES ('2', '人事部', DATABASE());
INSERT INTO `dept` VALUES ('3', '财务部', DATABASE());
INSERT INTO `dept` VALUES ('4', '市场部', DATABASE());
INSERT INTO `dept` VALUES ('5', '运维部', DATABASE());

#############################################################################################
DROP DATABASE IF EXISTS xxl_springcloud_db3;

CREATE DATABASE xxl_springcloud_db3 CHARACTER SET UTF8;

USE xxl_springcloud_db3;

DROP TABLE IF EXISTS `dept`;
CREATE TABLE `dept` (
  `dept_no` int NOT NULL AUTO_INCREMENT,
  `dept_name` varchar(255) DEFAULT NULL,
  `db_source` varchar(255) DEFAULT NULL,
  PRIMARY KEY (`dept_no`)
) ENGINE=InnoDB AUTO_INCREMENT=6 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;

INSERT INTO `dept` VALUES ('1', '开发部', DATABASE());
INSERT INTO `dept` VALUES ('2', '人事部', DATABASE());
INSERT INTO `dept` VALUES ('3', '财务部', DATABASE());
INSERT INTO `dept` VALUES ('4', '市场部', DATABASE());
INSERT INTO `dept` VALUES ('5', '运维部', DATABASE());
```

要实现负载均衡，首先要保证注册到Eureka中的所有微服务的**名称相同**。

修改application.yml配置文件，实现服务名称定义。

```yaml
spring:
  application:
    name: springcloud-dept-service                  # 定义微服务名称
```

启动所有的微服务，并且同时向Eureka中进行注册。

![image-20250316215122737](/assets/image-20250316215122737.D1lk1d3A.png)

可以发现，针对这些相同名称的微服务，会有3台主机提供服务支持。

此时的程序就实现了部门业务的集群配置。由于在消费端已经配置了@LoadBalanced注解，因此会采用自动轮询的模式实现不同业务主机的服务调用。读者运行程序后会发现，每一次都会通过不同的微服务主机执行业务。

**提问：能否不使用Eureka而直接通过Ribbon调用微服务？**

所有的微服务都在Eureka中注册，如果不通过Eureka，能否直接使用Ribbon进行微服务调用？

**回答：可以禁用Eureka，而直接利用Ribbon调用微服务，但是不推荐。**

在Ribbon中有一个服务器信息列表，开发者可以利用它配置所要访问的微服务列表，以实现微服务的调用。

![Image00376](/assets/Image00376.CI8BeXSE.jpg)

然后需要在消费端的application.yml配置文件中进行如下配置。

**范例：** 直接使用Ribbon访问微服务。

```yaml
ribbon:
  eureka:
    enabled: false
springcloud-dept-service:
  ribbon:
    listOfService:
      http://127.0.0.1:8001,
      http://127.0.0.1:8002,
      http://127.0.0.1:8003
```

此时直接在消费端配置了所有的微服务信息列表，而调用的形式也必须通过Ribbon特定的LoadBalancerClient类才可以完成访问。这里面最麻烦的问题在于：使用过程中如果有某台微服务主机出现宕机现象，Ribbon会自动将其剔除，但是在其恢复之后，开发者需要手动将其添加到Ribbon服务器列表之中才可以继续使用。从这一点来讲，并不如Eureka智能。在进行微服务的开发中，强烈建议使用Eureka来负责所有微服务的注册，这样可以实现服务端列表的动态更新。

### 3、Ribbon负载均衡策略

默认情况下，Ribbon中采用服务列表的顺序模式实现负载均衡处理，开发者也可以根据自身的情况实现自定义的负载均衡配置。Ribbon中，有如下3种核心配置策略（接口和类都在com.netflix.loadbalancer包中）。

1. **IRule：** Ribbon的负载均衡策略（所有的负载均衡策略均继承自IRule接口，常用子类如下表所示），默认采用ZoneAvoidanceRule实现，该策略能够在多区域环境下选出最佳区域的实例进行访问。

   ![Image00378](/assets/Image00378-1742132070367.B1V3zloG.jpg)

2. **IPing：** Ribbon的实例检查策略，默认采用NoOpPing子类实现。该检查策略是一个特殊的实现，实际上它并不会检查实例是否可用，而是始终返回true，默认认为所有服务实例都是可用的。如果用户有需要，也可以更换为PingUrl子类。

3. **ILoadBalancer：** 负载均衡器，默认采用ZoneAwareLoadBalancer实现，具备区域感知的能力。

由于Ribbon是工作在消费端的程序，所以进行负载均衡策略配置时，只需要在消费端进行处理。注意，不要将配置类放在SpringBoot程序启动时可以扫描到的子包中。

【springcloud-ribbon】追加一个LoadBalance的配置类，此类要放在Spring启动时无法扫描到的包中。

```java
public class RibbonLoadBalanceConfig {
	@Bean
	public IRule ribbonRule() { 			// 其中IRule就是所有规则的标准
		return new com.netflix.loadbalancer.RandomRule(); // 随机的访问策略
	}
	@Bean
	public IPing ribbonPing() {				// 定义Ping策略
		return new com.netflix.loadbalancer.PingUrl() ;
	}
}
```

【springcloud-ribbon】在程序启动类中使用@RibbonClient注解引入配置。

```java
@SpringBootApplication 
@EnableEurekaClient
@RibbonClient(name="ribbonClient", configuration=RibbonLoadBalanceConfig.class)
public class StartRibbonApplication {
	public static void main(String[] args) {
		SpringApplication.run(StartWebConsumerApplication80.class, args);
	}
}
 
```

这里采用自定义的负载均衡策略与主机检测策略实现了微服务调用。

在通过Ribbon调用微服务过程中，还可以利用LoadBalancerClient获取要调用的微服务端信息。修改DeptController控制器程序类，追加资源注入。

```java
@Autowired
private LoadBalancerClient loadBalancerClient ;	// 客户端信息

@GetMapping("/consumer/client") 
public Object client() {
    // 获取指定名称的微服务实例对象
    ServiceInstance serviceInstance = this.loadBalancerClient.choose("MLDNCLOUD-DEPT-SERVICE") ;
    Map<String,Object> info = new HashMap<String,Object>() ;
    info.put("host", serviceInstance.getHost()) ;
    info.put("port", serviceInstance.getPort()) ;
    info.put("serviceId", serviceInstance.getServiceId()) ;
    return info ;
}
```

Restful信息（http://127.0.0.1/consumer/client）

```json
{
    "port": 8003,
    "host": "192.168.226.1",
    "serviceId": "springcloud-dept-service"
}
```

本程序使用了LoadBalancerClient类进行客户端信息的注入，并且利用此对象根据微服务的名称MICROCLOUD-DEPT-SERVICE来获取客户端要调用的微服务的基础信息。

## 二、Feign远程接口映射

SpringCloud是以Restful为基础实现的开发框架，在整体调用过程中，即使引入了Eureka，也需要消费端使用完整的路径才可以正常访问远程接口，同时还需要开发者手动利用RestTemplate进行调用与返回结果的转换。为了解决这种复杂的调用逻辑，在SpringCloud中提供Feign技术（依赖于Ribbon技术支持），利用此技术可以将远程的Restful服务映射为远程接口，消费端可通过远程接口实现远程方法调用。

![Image00383](/assets/Image00383.BWm4gtwt.jpg)

### 1、Feign接口转换

Feign接口转换技术是针对Restful访问地址的封装，即同一组Restful访问地址应该变为一个远程接口中的业务方法，提供给消费端使用。

【springcloud-api】修改pom.xml配置文件，引入Feign依赖库（会自动引入Ribbon依赖）。

```xml
<!--添加 OpenFeign 依赖-->
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-openfeign</artifactId>
</dependency>
```

【springcloud-api】由于此时服务端需要通过认证访问，因此需要建立一个Feign的配置类，配置认证请求。

```java
@Configuration
public class FeignClientConfig {

    @Bean
    public BasicAuthRequestInterceptor getBasicAuthRequestInterceptor() {
        return new BasicAuthRequestInterceptor("xxl", "123456");
    }
    
}
```

【springcloud-api】修改IDeptService接口定义，追加Feign处理相关注解。

```java
@FeignClient(value = "springcloud-dept-service", configuration = FeignClientConfig.class)
public interface IDeptService {

    @PostMapping("/dept/add")
    public DeptDTO add(DeptDTO dto);                            // 增加新部门

    @GetMapping("/dept/get/{deptno}")
    public DeptDTO get(@PathVariable("deptno") long deptno);    // 根据部门编号获取部门信息

    @GetMapping("/dept/list")
    public List<DeptDTO> list();                                // 部门信息列表
}
```

【springcloud-feign】此时消费端不再需要通过RestTemplate来进行Restful服务访问。直接在控制器中注入IDeptService，即可实现微服务调用。

```java
@RestController
public class DeptController {

    @Autowired
    private IDeptService deptService;            // 注入远程业务接口对象


    @GetMapping("/consumer/dept/list")
    public Object listDeptRest() {
        return this.deptService.list();        // 调用Restful业务方法
    }

    @GetMapping("/consumer/dept/get")
    public Object getDeptRest(long deptno) {
        return this.deptService.get(deptno);    // 调用Restful业务方法
    }

    @GetMapping("/consumer/dept/add")
    public Object addDeptRest(DeptDTO dept) {    // 传输DeptDTO对象
        return this.deptService.add(dept);        // 调用Restful业务方法
    }

}
```

【springcloud-feign】修改启动类，配置Feign转换接口扫描包。

```java
@SpringBootApplication
@EnableEurekaClient
@EnableFeignClients(basePackages = {"com.xxl.api.service"})    // 定义Feign接口扫描包
public class StartFeignApplication {

    public static void main(String[] args) {
        SpringApplication.run(StartFeignApplication.class, args);
    }

}
```

在消费端进行远程业务调用时，所有的访问地址都与IDeptService接口中的方法对应，这样消费端调用远程操作中就感觉像在本地调用一样。

### 2、Feign相关配置

Feign的核心作用是将Restful服务的信息转换为接口，在整体的处理过程中依然需要进行JSON（或者XML、文本传输）数据的传递。为了避免长时间占用网络带宽，提升数据传输效率，往往需要对数据进行压缩。

【springcloud-feign】修改application.yml，进行数据压缩配置。

```yaml
feign:
  compression:
    request:
      mime-types:             # 可以被压缩的类型
      - text/xml
      - application/xml
      - application/json
      min-request-size: 2048  # 超过2048的字节进行压缩
```

【springcloud-feign】在SpringBoot项目启动过程中，对于Feign接口与远程Restful地址的映射也可以通过日志信息进行详细显示，修改application.yml进行日志级别变更。

```yaml
logging:
  level:
    com.xxl.api.service: DEBUG  # 定义显示转换信息的开发包与日志级别
```

修改FeignClientConfig配置类，追加日志配置。

```java
@Bean
public feign.Logger.Level getFeignLoggerLevel() {
    return feign.Logger.Level.FULL;
}
```

配置完成后重新启动消费端项目【springcloud-feign】，在第一次进行接口调用时，可以通过控制台看到如下的重要提示信息：

```sh
2025-03-17 00:41:04.437 DEBUG 1220 --- [p-nio-80-exec-1] com.xxl.api.service.IDeptService         : [IDeptService#list] <--- HTTP/1.1 200 (636ms)
2025-03-17 00:41:04.437 DEBUG 1220 --- [p-nio-80-exec-1] com.xxl.api.service.IDeptService         : [IDeptService#list] content-type: application/json;charset=UTF-8
2025-03-17 00:41:04.437 DEBUG 1220 --- [p-nio-80-exec-1] com.xxl.api.service.IDeptService         : [IDeptService#list] date: Sun, 16 Mar 2025 16:41:04 GMT
2025-03-17 00:41:04.437 DEBUG 1220 --- [p-nio-80-exec-1] com.xxl.api.service.IDeptService         : [IDeptService#list] transfer-encoding: chunked
2025-03-17 00:41:04.437 DEBUG 1220 --- [p-nio-80-exec-1] com.xxl.api.service.IDeptService         : [IDeptService#list] 
2025-03-17 00:41:04.444 DEBUG 1220 --- [p-nio-80-exec-1] com.xxl.api.service.IDeptService         : [IDeptService#list] [{"deptNo":1,"deptName":"开发部","loc":null},{"deptNo":2,"deptName":"人事部","loc":null},{"deptNo":3,"deptName":"财务部","loc":null},{"deptNo":4,"deptName":"市场部","loc":null},{"deptNo":5,"deptName":"运维部","loc":null}]
2025-03-17 00:41:04.444 DEBUG 1220 --- [p-nio-80-exec-1] com.xxl.api.service.IDeptService         : [IDeptService#list] <--- END HTTP (236-byte body)
2025-03-17 00:41:05.160  INFO 1220 --- [erListUpdater-0] c.netflix.config.ChainedDynamicProperty  : Flipping property: springcloud-dept-service.ribbon.ActiveConnectionsLimit to use NEXT property: niws.loadbalancer.availabilityFilteringRule.activeConnectionsLimit = 2147483647
```

通过上述提示信息可以发现，Feign在进行接口转换时集成了Ribbon负载均衡机制，微服务消费端和提供端之间的信息采用JSON结构进行传递，并且可以自动将相应的返回数据变为目标类型。

## 三、Hystrix熔断机制

在实际项目中，由于业务功能的不断扩充，会出现大量的微服务互相调用的情况。

![Image00394](/assets/Image00394.DYlm207i.jpg)

如图所示，微服务1要想完成功能，需要调用微服务2、微服务3、微服务4，一旦这个时候微服务4出现问题（其他微服务没有问题），则微服务1、2、3就有可能出现错误。这样的问题在微服务开发中称为雪崩效应。

为了防止这种雪崩效应的出现，在SpringCloud中引入了Hystrix熔断机制。在大部分开发状态下，开发者可以直接使用Hystrix的默认配置。如果有需要，开发者也可以使用如下几类常用配置项。

:one:微服务执行相关配置项。

* hystrix.command.default.execution.isolation.strategy（默认为thread）：隔离策略，可选用thread或semaphore。
* hystrix.command.default.execution.isolation.thread.timeoutInMilliseconds（默认为1000ms）：命令执行超时时间。
* hystrix.command.default.execution.timeout.enabled（默认为true）：执行是否启用超时配置。
* hystrix.command.default.execution.isolation.thread.interruptOnTimeout（默认为true）：发生超时时是否中断。
* hystrix.command.default.execution.isolation.semaphore.maxConcurrentRequests（默认为10）：最大并发请求数，该参数在使用ExecutionIsolationStrategy.SEMAPHORE策略时才有效。如果达到最大并发请求数，请求会被拒绝。理论上选择semaphore size和选择thread size一致，但选用semaphore时每次执行的单元要比较小且执行速度较快（ms量级），否则应该选用thread。semaphore一般占整个容器（Tomcat或Jetty）线程池的一小部分。

2️⃣失败回退（fallback）相关配置项。

* hystrix.command.default.fallback.isolation.semaphore.maxConcurrentRequests（默认为10）：如果并发数达到该设置值，请求会被拒绝，抛出异常，并且失败回退，不会被调用。
* hystrix.command.default.fallback.enabled（默认为true）：当执行失败或者请求被拒绝时，是否会调用fallback方法。

3️⃣熔断处理相关的配置项。

* hystrix.command.default.circuitBreaker.enabled（默认为true）：跟踪circuit的健康性，如果出现问题，则请求熔断。
* hystrix.command.default.circuitBreaker.sleepWindowInMilliseconds（默认为5000）：触发熔断时间。
* hystrix.command.default.circuitBreaker.errorThresholdPercentage（默认为50）：错误比率阀值，如果错误率≥该值，circuit会被打开，并短路所有请求触发失败回退。
* hystrix.command.default.circuitBreaker.forceOpen（默认为false）：强制打开熔断器。如果打开这个开关，将拒绝所有用户请求。
* hystrix.command.default.circuitBreaker.forceClosed（默认为false）：强制关闭熔断器。

4️⃣线程池（ThreadPool）相关配置项。

* hystrix.threadpool.default.coreSize（默认为10）：并发执行的最大线程数。
* hystrix.threadpool.default.maxQueueSize（默认为−1）：BlockingQueue的最大队列数。值为−1时，使用同步队列（SynchronousQueue）；值为正数时，使用LinkedBlcokingQueue。

### 1、Hystrix基本使用

Hystrix的主要功能是对出现问题的微服务调用采用熔断处理，可以直接在微服务提供方上进行配置。

修改pom.xml配置文件，追加Hystrix依赖配置。

修改程序启动主类，增加熔断注解配置。

此时的程序配置了熔断机制，这样即使有更多层级的微服务调用，也不会因为某一个微服务出现问题而导致所有的微服务均不可用。

### 2、失败回退

失败回退（fallback）也被称为服务降级，指的是当某个服务不可用时默认执行的处理操作。Hystrix中的失败回退是在客户端实现的一种处理机制。

如果要定义失败回退处理，建议通过FallbackFactory接口来进行实现。

修改IDeptService接口定义，追加fallbackFactory处理。

在进行Feign接口转换中，使用fallback设置当微服务不可用时的返回处理执行类。这样调用失败后，会返回DeptServiceFallback子类中所实现的方法内容。

> **提示：也可以单独定义为一个接口定义Fallback处理类。**
>
> 在进行Fallback类定义时，用户还可以直接创建IDeptService的失败回退子类（DeptServiceFallback），而后在通过@FeignClient注解中的fallback属性（fallback=DeptServiceFallback.class）进行配置。
>
> 这种子类配置有可能造成业务接口对象的注入混淆，所以不建议使用。

修改程序启动主类，追加扫描包配置，需要将配置的Fallback处理类进行配置。

这样当微服务关闭之后，由于服务提供方不再可用，所以此时会自动调用DeptServiceFallback类中的相应方法进行处理，返回的都是固定的“失败”信息，如图9-8所示。

### 3、HystrixDashboard

Hystrix提供了监控功能，这个功能就是Hystrix Dashboard，可以利用它来进行某一个微服务的监控操作。

修改pom.xml配置文件，追加依赖库。

微服务如果需要被监控，则要引入actuator依赖库。

在需要进行监控的控制器方法上追加@HystrixCommand注解。

本程序在控制器中的add()、get()、list()3个方法上使用了@HystrixCommand注解，这样只有这3个方法的状态可以被监控到。

> **提示：@HystrixCommand也可以配置失败回退处理。**
>
> 对于失败回退，也可以直接在控制层进行定义，此时只需要在控制层的相应方法上使用@HystrixCommand注解中的fallbackMethod属性定义。
>
> **范例：** 在控制层上定义失败回退。
>
> 此时，当list()方法执行有问题时，会自动调用listFallback()方法进行失败处理。

修改微服务启动类，追加Hystrix支持。

修改application.yml配置文件，修改运行端口。

定义程序启动主类。

修改hosts配置文件，追加主机信息。

程序配置完成后分别启用所需要的Eureka微服务（mldncloud-eureka-7001）、部门微服务（mldncloud-dept-service-8001）、HystrixDashboard微服务（mldncloud-hystrix-dashboard）、消费端微服务（mldncloud-consumer-feign）。服务启动后通过Dashboard访问地址http://dashboard.com:9001/hystrix，并且输入监控地址http://mldnjava:hello@dept-8001.com:8001/hystrix.stream，界面如图9-9所示。当通过消费端访问微服务之后，会针对微服务的状态进行跟踪，如图9-10所示。

### 4、Turbine聚合监控

HystrixDashboard只能够针对某一个微服务进行监控，如果项目中有许多个微服务，且需要对所有微服务统一监控的时候，就可以使用Turbine来实现聚合监控。

修改pom.xml配置文件，引入Turbine依赖库。

修改application.yml，进行Turbine聚合配置。

如果要对所有的微服务进行监控，则在定义微服务时需要配置认证信息。由于这种认证信息只能够在访问微服务的路径中进行配置，所以需要修改安全配置类，取消对监控路径的安全限制。

在本配置中，忽略了Web安全访问（WebSecurity）下/hystrix.stream、/turbine.stream两个路径的认证，所以对这两个路径不再进行安全认证处理。

定义程序启动类，使用Turbine注解。

本程序在启动主类上使用@EnableTurbine注解，这样就可以启动Turebine聚合监控了。

修改hosts主机配置，增加新的主机名称。

启动所有相关的微服务，随后通过HystrixDashboard启动监控程序，如图9-11所示，输入Turbine的监控路径（http://turbine.com:9101/turbine.stream）并且利用消费端访问相应的微服务信息，就可以得到如图9-12所示的监控结果。

## 四、Zuul路由网关

微服务创建是一个庞大的系统工程，在一个整体项目中往往会存在着若干类的微服务。例如，要开发一个企业管理程序，有可能会用到3类微服务集群：内部员工操作微服务集群、外部客户操作微服务集群和网站管理操作微服务集群。

![Image00416](/assets/Image00416.Bckh5oh1.jpg)

### 1、Zuul整合微服务

Zuul实现的是路由网关微服务，为了方便Zuul的统一管理，所有的Zuul微服务需要向Eureka注册，然后Zuul才可以利用Eureka获取所有的微服务信息，而后客户端再通过Zuul调用微服务，整体流程如图所示。

![Image00417](/assets/Image00417.BC_fqIWo.jpg)

新建【springcloud-zuul-gateway】微服务

修改pom.xml配置文件，追加Zuul相关依赖包。

```
```

Zuul需要向Eureka中注册，同时也需要通过Eureka获取微服务信息。修改application.yml配置文件如下：

```
```

修改程序启动主类，追加Zuul注解配置。

```
```

启动相应微服务。通过Eureka控制中心可以发现，Zuul微服务信息依然会保存到Eureka注册中心。

微服务可以通过Zuul代理访问。由于此时没有进行任何配置，所以可以直接通过Eureka注册的微服务代理http://gateway-9501.com:9501/mldncloud-dept-service/dept/list进行访问，提示信息如下：

```
```

可以发现，当进行Zuul代理访问时，默认情况下采用的就是Eureka的注册名称。

### 2、Zuul访问过滤

Zuul本质上就属于一个网关代理操作。在实际使用中，所有的微服务都要有自己的认证信息，因此，如果用户当前所代理的微服务具有认证信息，就必须在其访问前追加认证的头部操作。这样的功能需要通过Zuul的过滤操作完成。

**提示：关于Zuul网关代理认证设计。**

在本例讲解过程中将首先恢复部门微服务中的认证管理，同时将通过Zuul进行微服务认证的配置。程序进行认证信息定义与处理的过程中采用的基本流程为：Zuul通过过滤器配置微服务的认证信息，而后Zuul再通过SpringSecurity定义Zuul的认证信息，如图9-16所示。

这样的设计结构，密码管理会非常混乱。如果是小型开发，则可以使用。如果是大型开发，则整体项目一旦出现问题，维护成本是相当高的，所以这种认证机制将在后续通过OAuth代替。

建立认证请求过滤器，该过滤器必须继承ZuulFilter父类。

### 3、Zuul路由配置

### 4、Zuul服务降级

### 5、上传微服务

微服务除了可以进行业务处理之外，也可以针对上传功能进行创建，所有的上传微服务依然需要向Eureka中注册，这样就可以在Zuul中进行微服务代理操作。

> **注意：不建议构建上传微服务。**
>
> 在实际的开发过程中，利用微服务端实现上传业务并不是合理做法，从实际开发来讲，上传的功能一般都在Web消费端完成，最好的做法是直接利用Web消费端将上传文件保存到文件服务器中（如FastDFS）。即使现在使用了微服务做上传，那么一般情况下也会将其保存到文件服务器中。所以本节只针对Zuul的功能进行技术性的讨论。

修改application.yml配置文件，追加上传配置。

定义上传Rest微服务。

启动上传微服务，然后可以利用curl命令进行上传测试，测试成功会返回相应的上传信息。

如果需要Zuul进行上传微服务代理，还需要修改application.yml，追加微服务的代理配置项。

## 五、参考资料

\[1]. Java微服务架构实战：SpringBoot+SpringCloud+Docker+RabbitMQ／李兴华

\[2]. https://c.biancheng.net/springcloud/ribbon.html

---

---
url: /Java/微服务专栏/01.SpringCloud/1_SpringCloud简介.md
---

# SpringCloud简介

## 一、RPC分布式开发技术

RPC（Remote Procedure Call，远程过程调用）技术是进行项目业务拆分的重要技术手段，SpringCloud是新一代的RPC技术，其在SpringBoot技术上进行构建，基于Restful架构，采用标准数据结构（JSON）进行数据交互。

目的设计与开发实质上是业务层设计与业务功能的完善，在传统的单主机项目中，业务层的变化一般很少，所以可以直接将业务层定义在Web之中。

![Image00267](/assets/Image00267.BSzNy3Y4.jpg)

在互联网时代，为了应对业务需求的变更以及用户访问量的迅猛增加，同时也为了程序的可维护性，RPC技术的应用非常广泛。利用RPC技术可以方便地帮助企业搭建业务中心，这样所有的Web端就可以利用远程接口技术实现业务中心方法的调用，从而单独进行业务中心的维护。

![Image00268](/assets/Image00268.BPWEa4s2.jpg)

利用RPC技术除了可以实现业务中心的创建之外，最重要的作用是可以为一个业务实现多个RPC服务端的创建。当多个RPC服务端实现同一个业务时，可以利用代理软件实现负载均衡。这样的设计不仅可以提升性能，也方便了高峰时期的业务端扩展，从而实现高并发、高可用、分布式的项目结构设计。

![Image00269](/assets/Image00269.DGRBgC6P.jpg)

> **提示：关于高并发、高可用和分布式设计。**
>
> 在实际开发中，业务层的调用除了可发生于Web端，还可能存在于移动端。为了满足大量的并发访问，必须尽可能提升业务层的处理能力。由于单主机的性能是有限的，所以往往会建立业务层设计集群，使用多台主机共同实现业务层的高性能处理。这种设计中，即使业务集群中的某一台主机出现问题，也不会影响到整体业务的执行，从而实现高可用的处理机制。同时，分布式的业务中心也更便于维护以及业务设计人员进行业务完善。

## 二、RPC实现技术

RPC技术采用客户端与服务端的处理模式实现分布式开发调用，其本身是一个标准，并没有定义任何传输协议，所以用户可以使用各种技术来实现。但随着技术的发展，目前也出现了一些RPC的实现技术。

### CORBA

CORBA（Common Object Request Broker Architecture，公共对象请求代理架构）是由OMG组织制订的一种标准的面向对象应用程序体系规范。该标准可以使用任何语言实现，同时需要编写IDL接口描述文件。

### RMI

RMI（Remote Method Invocation，远程方法调用）是在JDK 1.1版本中提供的分布式开发技术，在实现过程中需要创建骨架与存根后才可以使用。随着JDK版本的提升，存根也可以自动生成。

RMI的基础架构图

![Image00270](/assets/Image00270.BP6tK8ej.jpg)

### EJB

EJB（Enterprise JavaBean）是SUN公司（已被Oracle公司收购）推出的基于RMI技术的分布式开发技术，是一个开发基于组件的企业多重应用程序的标准。在EJB技术中主要分为3种Bean：会话Bean（业务层）、实体Bean（数据层）和消息驱动Bean（消息队列中间件）。

3种Bean的对应关系图

![Image00271](/assets/Image00271.pXliY6Hx.jpg)

> **提示：EJB最后只剩下了理论价值。**
>
> EJB技术是SUN公司当年的重头戏，它提出了完善的分布式业务中心设计理念，但由于在实现上存在偏差以及EJB容器过于昂贵，EJB技术并没有真正“火”起来。其后来推出的EJB 3.x标准多数情况下还需要与WebService相结合，不得不说这是一个失败的实现。但EJB并非毫无用处，其设计理念造就了今天开源框架的发展，所以它依然是一个里程碑一样的技术。

### WebService

WebService（Web服务）技术使得运行在不同机器上的应用无需借助附加的、专门的第三方软件或硬件，就可相互交换数据或集成。其主要使用XML作为接口描述，同时利用SOAP（Simple Object Access Protocol，简单对象访问协议）进行通信，基本结构如图6-6所示。WebService是一个开发标准，可以直接运行在Web容器中，而后陆续出现了许多开发组件，如Axis、XFire、CXF。除了这些技术之外，还需要使用者利用工具生成一系列的伪代码，而后才可以正常实现远程接口调用。

![Image00272](/assets/Image00272.Cuk6PJsK.jpg)

### RPC开发框架

RPC开发框架：WebService是一个大型的重量级标准，可以方便地实现异构系统架构的整合，但是其性能却经常被开发者所诟病。在其之后，许多技术开发公司陆续推出了属于自己的RPC开发框架（如阿里巴巴-Dubbo、阿里巴巴-High Speed Framework、Facebook-thrift、Google-grpc、Twitter-finagle等），这些RPC开发框架在项目中使用得也非常广泛。

### Restful

Restful（也可简称为Rest，指Representational State Transfer，表现层状态转换）是目前最流行的一种互联网软件架构。它结构清晰，符合标准，易于理解，方便扩展，所以逐渐得到越来越多网站的采用。SpringBoot与SpringCloud都可以方便地利用Restful标准进行构建。除了SpringCloud技术外，开发者还可以利用Jersey框架构建基于Restful风格的WebService服务。

> **提示：Rest提出者Roy Thomas Fielding。**
>
> Rest是Roy Thomas Fielding在其2000年的博士论文中提出的。他是HTTP（1.0版和1.1版）的主要设计者，Apache服务器软件的作者之一，Apache基金会的第一任主席。

## 三、SpringCloud技术架构

SpringCloud是一套技术架构（主要使用的是netflix技术产品），其整体的架构核心是围绕Restful展开的，并且以SpringBoot技术为核心基础进行构建的RPC分布式技术。在SpringCloud技术中，对于Restful处理过程中一定要有两个端：服务的提供者（Provider）和服务的消费者（Consumer）。

基本架构图

![Image00274](/assets/Image00274.BD60qJus.jpg)

> **提示：SpringCloud以SpringBoot为实现基础。**
>
> 在SpringBoot中如果要将控制层的处理方法以Restful形式返回，那么往往需要使用@Controller + @ResponseBody或者@RestController注解才可以实现。而SpringCloud技术是在SpringBoot的基础上构建的RPC应用，主要以Restful设计架构进行异构系统的数据交互，即SpringCloud中是不存在控制层跳转到显示页面处理操作的。

SpringCloud的本质虽然是基于Restful的一种应用，但它依然属于RPC的一种技术实现。与传统RPC不一样的地方在于，SpringCloud使用了一系列的开源组件进行整合应用，其中包含有如下5个基本组件。

### 微服务注册中心

微服务的核心意义在于将一个总体的业务端拆分到不同的业务主机上，所有微服务的Restful访问地址非常多。为了统一管理这些地址信息，可以即时地告诉用户哪些服务不可用，应该准备一个分布式的注册中心，并且该注册中心支持HA（High Available，高可用性集群）机制。为了高速并且方便地进行所有服务的注册操作，在SpringCloud里面提供了一个Eureka的注册中心，所有的微服务都可以在此注册中心中进行注册。

> **提示：HA机制。**
>
> HA是保证业务连续性的有效解决方案，一般有两个或两个以上的节点，且分为活动节点及备用节点。可以简单地理解为：如果班长在，则班长负责安排同学；如果班长不在，则副班长顶替班长完成任务。

![Image00275](/assets/Image00275.BYjy92KI.jpg)

### Ribbon负载均衡

单台主机的性能是有限的，如果要处理并发访问量高的微服务，就必须创建多个相同的微服务，同时采用负载均衡设计，使每一个微服务都可以为项目提供服务支持。SpringCloud中引入了Ribbon，在客户端实现了负载均衡。

![Image00276](/assets/Image00276.G7DyMe2A.jpg)

### Feign接口映射

RPC开发技术遵循了客户端与服务端开发模式，且客户端使用远程接口来实现远程业务调用是最为合理的。SpringCloud技术依赖于Restful架构，开发者可以使用Feign技术实现远程接口以及远程Restful服务的映射处理。

![Image00277](/assets/Image00277.CdOIaVgZ.jpg)

### Zuul网关代理

为了保证微服务调用的安全性以及统一性，所有的微服务都可以采用统一的服务网关技术，通过映射名称访问相应的微服务资源。这样更好地体现了Java中key=value的设计思想。而且所有的微服务通过Zuul进行代理后，也可以更加合理地进行名称的隐藏。

![Image00278](/assets/Image00278.MHQzcnKT.jpg)

### SpringCloudConfig远程配置

在SpringCloud中通常会存在成百上千个微服务（有些大型项目中微服务甚至更多），为保持高效运转，其配置文件需要进行统一管理。在SpringCloud技术中实现SpringCloudConfig微服务定义，该微服务可以直接注册到Eureka中，以实现HA机制，而所有提供业务支持的微服务都通过SpringCloudConfig从GIT或SVN服务器上抓取用户配置信息，因此可以将配置资源进行统一管理，而后可以利用SpringCloudBus技术实现动态配置更新。

![Image00279](/assets/Image00279.Coo2cl-E.jpg)

以上几项是SpringCloud构建RPC微服务的重要实现技术。除了这些之外，在SpringCloud中充分考虑到了HA处理机制。同时SpringCloud也可以基于SpringSecurity技术实现安全访问，或者集成OAuth实现统一认证授权管理。

## 四、总结

1．RPC是实现远程过程调用的技术标准，可以使用各种语言实现。SpringCloud是基于Restful架构实现的RPC技术。

2．SpringCloud在实现微服务的定义时，主要使用Netflix公司的产品（如Eureka、Zuul、Feign、Ribbon等）实现架构整合。

3．SpringCloud可以结合SpringSecurity技术进行安全访问。

---

---
url: /Java/微服务专栏/01.SpringCloud/2_SpringCloud与Restful.md
---

# SpringCloud与Restful

## 一、搭建SpringCloud项目开发环境

SpringCloud是建立在SpringBoot基础上的，所以开发者必须掌握SpringBoot开发框架。由于SpringCloud是基于Restful架构的RPC开发实现，所以微架构设计中往往在客户端利用RestTemplate来实现远程Restful业务调用。为了保证系统安全，也可以使用SpringSecurity进行安全访问控制。

SpringCloud技术与SpringBoot技术一样，都提供了统一的pom.xml配置项，配置好相应的版本之后就可以在各个Maven子模块中进行依赖支持库的简单引用。

![Image00280](/assets/Image00280.CYLKBSaM.jpg)

SpringCloud技术与传统开发不一样的地方在于，其版本号并未采用数字，而是使用了一系列英国的地名作为标注。

![Image00281](/assets/Image00281.CZq5kwRp.jpg)

### 1、搭建项目，引入依赖

创建一个新的Springboot项目，修改pom.xml配置文件。如果要开发SpringCloud，则一定要引入SpringBoot依赖支持。

```xml
<!-- 属性定义 -->
<spring-boot-dependencies.version>2.0.5.RELEASE</spring-boot-dependencies.version>
<spring-cloud-dependencies.version>Finchley.SR2</spring-cloud-dependencies.version>



<!-- 依赖库配置 -->
<dependency>				<!-- 定义SpringBoot依赖管理 -->
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-dependencies</artifactId>
    <version>${spring-boot-dependencies.version}</version>
    <type>pom</type>
    <scope>import</scope>
</dependency>
<dependency>				<!-- 进行SpringCloud依赖包的导入处理 -->
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-dependencies</artifactId>
    <version>${spring-cloud-dependencies.version}</version>
    <type>pom</type>
    <scope>import</scope>
</dependency>
```

### 2、SpringBoot与SpringCloud对应版本及官方查询方法

SpringCloud 与 SpringBoot各版本的对应关系

| SpringCloud         | SpringBoot                                     |
| ------------------- | ---------------------------------------------- |
| Finchley            | 2.0.x                                          |
| Finchley.SR1        | Spring Boot >=2.0.3.RELEASE and <=2.0.9RELEASE |
| Finchley.SR4        | Spring Boot >=2.0.3.RELEASE and <=2.0.9RELEASE |
| Greenwich           | 2.1.x                                          |
| Hoxton              | 2.2.x，2.3.x (Starting with SR5)               |
| 2020.0.x aka Ilford | 2.4.x                                          |

方式一：通过官网查询

https://spring.io/projects/spring-cloud#learn

![image-20250407000038978](/assets/image-20250407000038978.BlWgyvgn.png)

对应版本号后，点击 Reference Doc. 。能看到很明显的 Supported Boot Version 字样。

![image-20250407000107973](/assets/image-20250407000107973.BCkLOxNP.png)

方式二：

https://start.spring.io/actuator/info

![image-20250407000247690](2_SpringCloud与Restful.assets/image-20250407000247690.png)

可以得到一串json，其中对应关系一目了然，此网站返回内容，除了包含springCloud和springBoot的版本对应关系，还有其他组件和springBoot之间的版本对应关系。

————————————————

```
                        版权声明：本文为博主原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接和本声明。
```

原文链接：https://blog.csdn.net/xt314159/article/details/123372353

## 二、Restful基础实现

不管使用何种技术实现的RPC项目开发，采用的均为服务端与客户端结构。为了保证服务端定义与客户端访问的标准性，可以单独创建一个远程接口的描述项目。

## 三、Restful接口描述

利用SpringCloud开发技术可以方便地实现Restful技术标准。从另外一个方面来说，这些接口如果要给其他消费端程序使用，就需要有良好的接口说明信息，可以明确地将服务接口以及参数的作用告诉使用者，此时就可以利用Swagger技术实现。

修改pom.xml配置文件，引入Swagger相关依赖库。

建立Swagger配置类，定义接口描述基础信息。

修改DeptRest程序类，使用Swagger进行接口描述。

本程序为Restful控制器中追加了接口的注解描述，当程序启动后可以通过swagger-ui.html地址进行访问，页面运行后的效果如图

## 四、SpringSecurity安全访问

---

---
url: /Java/微服务专栏/01.SpringCloud/5_SpringCloudConfig.md
---

# SpringCloudConfig

---

---
url: /Java/微服务专栏/01.SpringCloud/7_SpringCloudSleuth.md
---

# SpringCloudSleuth

微服务的开发与调用是一个周期很长并且非常繁杂的处理过程，为了可以监控各个微服务之间的调用情况，在SpringCloud里面提供Sleuth跟踪技术，可以针对微服务的调用实现信息采集处理。

## 一、SpringCloudSleuth简介

微服务是一种子业务的拆分处理机制，在微服务处理架构过程中经常会出现若干个微服务互相调用的情况。
![Image00517](/assets/Image00517.CU7zcUTe.jpg)

上图给出的是一种实际开发中可能存在的调用过程，有可能是几个微服务之间互相调用，也有可能为完成某一个大型的业务需要几十个微服务之间互相调用。在这样的场景中，就有可能出现如下几个问题：

* 当业务处理执行速度变慢时，有可能是某一个或某几个微服务处理性能不高。该如何去追踪这些处理速度较慢的微服务，从而实现性能的整体提升？

* 如果某一个微服务出现问题，应该如何快速找到出现问题的微服务并且加以修复？

* 如果现在微服务变为环形调用，那么这些关系该如何描述出来？

所以，一个完善的微服务并不只是简单地进行RPC的功能实现，还应该对整体的微服务执行进行监控。SpringCloud中提供的Sleuth技术就可以实现微服务的调用跟踪，它可以自动形成一个调用连接线，通过这个连接线开发者可以轻松找到所有微服务间的关系。所有微服务的调用信息都自动发送到Sleuth中，如图所示。

![Image00518](/assets/Image00518.DX66SMvl.jpg)

这样不仅可以采集到微服务调用的关系，也可以获取微服务所耗费的时间，从而进行整体微服务状态的监控以及相应的数据分析。

所有微服务发送到Sleuth采集微服务上的信息都是以Span描述的，每一个Span包含4个组成部分。

1. cs-Client Sent：客户端发出一个请求，描述的是一个Span开始。
2. sr-Server Received：服务端接收请求，sr-cs表示发送的网络延迟。
3. ss-Server Sent：服务端发送请求（回应处理），ss-sr表示服务端的消耗时间。
4. cr-Client Received：客户端接收到服务端数据，cr-ss表示回复所需要的时间。

## 二、搭建SpringCloudSleuth微服务

> 官网：<https://zipkin.io/>

Zipkin 服务搭建有多种方式：使用官方提供了可直接启动的 Jar 包，通过 Docker 镜像运行，或者自己手动添加依赖创建 Zipkin 服务器应用。 不过到了 spring-boot 2.0 后官方就不推荐自己通过手动添加依赖创建 Zipkin 服务器应用了。

### 1、自建Zipkin Server的方式（< spring-boot 2.0）

SpringCloudSleuth使用的核心组件是Twitter推出的Zipkin监控组件，所以这里配置的模块需要包含Zipkin相关配置依赖。

新建【springcloud-sleuth】服务。

修改pom.xml配置文件，追加如下依赖库。

```xml
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-sleuth</artifactId>
</dependency>
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-zipkin</artifactId>
</dependency>
<dependency>
    <groupId>io.zipkin.java</groupId>
    <artifactId>zipkin-server</artifactId>
</dependency>
<dependency>
    <groupId>io.zipkin.java</groupId>
    <artifactId>zipkin-autoconfigure-ui</artifactId>
</dependency>
```

修改application.yml配置文件。

```yaml
server:
  port: 8601          # 配置监听端口号
spring: 
  application:
    name: springcloud-zipkin-server
```

修改程序启动类。

```java
@SpringBootApplication
@EnableZipkinServer                    // 启用Zipkin服务
public class StartSleuthApplication8601 {
    public static void main(String[] args) {
        SpringApplication.run(StartSleuthApplication8601.class, args);
    }
} 
```

### 2、通过 Jar 包运行（> spring-boot 2.0）

下载：https://repo1.maven.org/maven2/io/zipkin/zipkin-server/

由于我是JDK11，选择 zipkin-server-2.27.0-exec.jar 下载，3.0以上需要JDK11。

在命令行输入 `java -jar zipkin-server-2.27.0-exec.jar` 启动 Zipkin Server。

也可以执行`nohup java -jar zipkin-server-2.27.0-exec.jar &`命令将 Zipkin 服务作为守护进程后台运行。

* 默认Zipkin Server的请求端口为 9411
* Zipkin Server的启动参数可以通过官方提供的yml配置文件查找
* 在浏览器输入http://127.0.0.1:9411即可进入到Zipkin Server的管理后台

### 3、客户端配置

在需要监控的微服务中引入spring-cloud-starter-zipkin依赖库。

```xml
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-zipkin</artifactId>
</dependency>
```

修改application.yml配置文件，配置Sleuth连接信息。

```yaml
spring:
  zipkin:
    base-url: http://127.0.0.1:9411/    #zipkin server的请求地址
    sender:
      type: web                         #请求方式,默认以http的方式向zipkin server发送追踪数据
  sleuth:
    sampler:
      probability: 1.0                  #采样的百分比
```

需采样的百分比，默认为0.1，即10%，这里配置1，是记录全部 的sleuth信息，是为了收集到更多的数据（仅供测试用）。在分布式系统中，过于频繁的采样会影响系 统性能，所以这里配置需要采用一个合适的值。

依次启动所有相关的微服务，并且通过消费端进行微服务的调用，随后就可以通过Zipkin地址检测到访问信息。

## 三、Sleuth数据采集

搭建完Sleuth后，此时需要考虑一个实际的问题：当前所有发送到Sleuth服务端的统计汇总操作都是记录在内存中的，也就是说，如果开发者关闭了Zipkin服务端，那么这些统计信息将消失。这样的设计明显是不合理的，应该将这些统计的数据记录保存下来。同时，有可能一个项目中存在许多微服务，这样就需要发送大量的数据信息进入，为了解决这种高并发的问题，可以结合消息组件（Stream）进行缓存处理。

![image-20250315171948234](/assets/image-20250315171948234.CQf09bo0.png)

### 1、以MySQL作为数据源

#### 数据库初始化

在 MySQL 中创建一个名为 zipkin 的数据库，编码使用 utf8。

```sql
DROP DATABASE IF EXISTS zipkin ;
CREATE DATABASE zipkin CHARACTER SET UTF8 ;
```

接着从官方的 GitHub 仓库中下载初始化 sql 语句，然后执行创建表结构（一共三张表）。下载地址如下：

<https://github.com/openzipkin/zipkin/blob/master/zipkin-storage/mysql-v1/src/main/resources/mysql.sql>

```sql
--
-- Copyright The OpenZipkin Authors
-- SPDX-License-Identifier: Apache-2.0
--
USE zipkin ;
CREATE TABLE IF NOT EXISTS zipkin_spans (
  `trace_id_high` BIGINT NOT NULL DEFAULT 0 COMMENT 'If non zero, this means the trace uses 128 bit traceIds instead of 64 bit',
  `trace_id` BIGINT NOT NULL,
  `id` BIGINT NOT NULL,
  `name` VARCHAR(255) NOT NULL,
  `remote_service_name` VARCHAR(255),
  `parent_id` BIGINT,
  `debug` BIT(1),
  `start_ts` BIGINT COMMENT 'Span.timestamp(): epoch micros used for endTs query and to implement TTL',
  `duration` BIGINT COMMENT 'Span.duration(): micros used for minDuration and maxDuration query',
  PRIMARY KEY (`trace_id_high`, `trace_id`, `id`)
) ENGINE=InnoDB ROW_FORMAT=COMPRESSED CHARACTER SET=utf8 COLLATE utf8_general_ci;

ALTER TABLE zipkin_spans ADD INDEX(`trace_id_high`, `trace_id`) COMMENT 'for getTracesByIds';
ALTER TABLE zipkin_spans ADD INDEX(`name`) COMMENT 'for getTraces and getSpanNames';
ALTER TABLE zipkin_spans ADD INDEX(`remote_service_name`) COMMENT 'for getTraces and getRemoteServiceNames';
ALTER TABLE zipkin_spans ADD INDEX(`start_ts`) COMMENT 'for getTraces ordering and range';

CREATE TABLE IF NOT EXISTS zipkin_annotations (
  `trace_id_high` BIGINT NOT NULL DEFAULT 0 COMMENT 'If non zero, this means the trace uses 128 bit traceIds instead of 64 bit',
  `trace_id` BIGINT NOT NULL COMMENT 'coincides with zipkin_spans.trace_id',
  `span_id` BIGINT NOT NULL COMMENT 'coincides with zipkin_spans.id',
  `a_key` VARCHAR(255) NOT NULL COMMENT 'BinaryAnnotation.key or Annotation.value if type == -1',
  `a_value` BLOB COMMENT 'BinaryAnnotation.value(), which must be smaller than 64KB',
  `a_type` INT NOT NULL COMMENT 'BinaryAnnotation.type() or -1 if Annotation',
  `a_timestamp` BIGINT COMMENT 'Used to implement TTL; Annotation.timestamp or zipkin_spans.timestamp',
  `endpoint_ipv4` INT COMMENT 'Null when Binary/Annotation.endpoint is null',
  `endpoint_ipv6` BINARY(16) COMMENT 'Null when Binary/Annotation.endpoint is null, or no IPv6 address',
  `endpoint_port` SMALLINT COMMENT 'Null when Binary/Annotation.endpoint is null',
  `endpoint_service_name` VARCHAR(255) COMMENT 'Null when Binary/Annotation.endpoint is null'
) ENGINE=InnoDB ROW_FORMAT=COMPRESSED CHARACTER SET=utf8 COLLATE utf8_general_ci;

ALTER TABLE zipkin_annotations ADD UNIQUE KEY(`trace_id_high`, `trace_id`, `span_id`, `a_key`, `a_timestamp`) COMMENT 'Ignore insert on duplicate';
ALTER TABLE zipkin_annotations ADD INDEX(`trace_id_high`, `trace_id`, `span_id`) COMMENT 'for joining with zipkin_spans';
ALTER TABLE zipkin_annotations ADD INDEX(`trace_id_high`, `trace_id`) COMMENT 'for getTraces/ByIds';
ALTER TABLE zipkin_annotations ADD INDEX(`endpoint_service_name`) COMMENT 'for getTraces and getServiceNames';
ALTER TABLE zipkin_annotations ADD INDEX(`a_type`) COMMENT 'for getTraces and autocomplete values';
ALTER TABLE zipkin_annotations ADD INDEX(`a_key`) COMMENT 'for getTraces and autocomplete values';
ALTER TABLE zipkin_annotations ADD INDEX(`trace_id`, `span_id`, `a_key`) COMMENT 'for dependencies job';

CREATE TABLE IF NOT EXISTS zipkin_dependencies (
  `day` DATE NOT NULL,
  `parent` VARCHAR(255) NOT NULL,
  `child` VARCHAR(255) NOT NULL,
  `call_count` BIGINT,
  `error_count` BIGINT,
  PRIMARY KEY (`day`, `parent`, `child`)
) ENGINE=InnoDB ROW_FORMAT=COMPRESSED CHARACTER SET=utf8 COLLATE utf8_general_ci;
```

#### 启动 Zipkin 服务—自建方式

【sleuth服务】修改pom.xml配置文件，追加相关依赖库。

```xml
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-sleuth-zipkin-stream</artifactId>
</dependency>
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-stream-rabbit</artifactId>
</dependency>
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-jdbc</artifactId>
</dependency>
<dependency>
    <groupId>mysql</groupId>
    <artifactId>mysql-connector-java</artifactId>
</dependency>
```

需要注意的是，此时需要删除spring-cloud-starter-zipkin依赖库。

【sleuth服务】修改application.yml配置文件，追加MySQL连接与RabbitMQ消息组件配置。

```yaml
server:
  port: 8601          # 配置监听端口号
spring:
  rabbitmq:
    host: rabbitmq-single   # 消息主机
    port: 5672      # 连接端口
    username: xxl   # 用户名
    password: 123456  # 密码
    virtual-host: /   # 虚拟主机
  datasource:
    type: com.alibaba.druid.pool.DruidDataSource    # 配置当前要使用的数据源的操作类型
    driver-class-name: com.mysql.cj.jdbc.Driver     # 配置MySQL的驱动程序类
    url: jdbc:mysql://localhost:3306/xxl_springboot_action?serverTimezone=UTC&useSSL=false           # 数据库连接地址
    username: root                                  # 数据库用户名
    password: xxl666                                # 数据库连接密码
  application:
    name: springcloud-zipkin-server
zipkin: 
  storage:  # 设置zipkin收集的信息通过mysql进行存储
    type: mysql # 数据库存储

```

【客户端】修改pom.xml配置文件，追加依赖配置。

```xml
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-sleuth-zipkin-stream</artifactId>
</dependency>
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-stream-rabbit</artifactId>
</dependency>
```

【客户端】修改application.yml配置文件，由于此时是通过消息组件进行采集信息的发送，所以删除zipkin.base-url配置，追加RabbitMQ相关配置，这几个微服务将作为消息生产者存在。

```yaml
spring:
  rabbitmq:
    host: rabbitmq-single       # 消息主机
    port: 5672                  # 连接端口
    username: xxl               # 用户名
    password: 123456            # 密码
    virtual-host: /             # 虚拟主机
```

【sleuth服务】修改程序，启动注解。

```java
@SpringBootApplication
@EnableZipkinStreamServer					// 启用Zipkin服务
public class StartSleuthApplication8601 {
	public static void main(String[] args) {
		SpringApplication.run(StartSleuthApplication8601.class, args);
	}
} 
```

此时启动各个微服务，这样所有被监听的微服务都将成为消息的生产者，然后Sleuth将作为消息消费者，将收到的消息保存到数据库中存储。

#### 启动 Zipkin 服务—Jar方式

（1）Zipkin 启动时只需要指定好 MySql 连接信息即可。如果是通过 jar 包运行，则执行如下命令：

```sh
java -jar zipkin-server-2.27.0-exec.jar --STORAGE_TYPE=mysql --MYSQL_HOST=192.168.1.1 --MYSQL_TCP_PORT=3306 --MYSQL_USER=xxl --MYSQL_PASS=123456 --MYSQL_DB=zipkin
```

（2）如果通过 Docker 镜像运行，则执行如下命令：

```sh
docker run --name zipkin -d -p 9411:9411 -e STORAGE_TYPE=mysql -e MYSQL_HOST=192.168.1.1 -e MYSQL_TCP_PORT=3306 -e MYSQL_USER=xxl -e MYSQL_PASS=123456 -e MYSQL_DB=zipkin openzipkin/zipkin`
```

（3）如果每次都要使用 docker 命令来分别启动 zipkin 容器还是略显繁琐，我们也可以通过 Docker Compose 进行启动，docker-compose.yml 文件内容如下：

```yaml
version: '2'

services:
  zipkin:
    image: openzipkin/zipkin
    container_name: zipkin
    environment:
      - STORAGE_TYPE=mysql
      - MYSQL_HOST=192.168.1.1
      - MYSQL_TCP_PORT=3306
      - MYSQL_USER=xxl
      - MYSQL_PASS=123456
      - MYSQL_DB=zipkin
      #- RABBIT_ADDRESSES=192.168.1.1:5672
      #- RABBIT_USER=hangge
      #- RABBIT_PASSWORD=123
    ports:
      - 9411:9411
```

### 2、以Elasticsearch作为数据源

#### 启动 Zipkin 服务

（1）Zipkin 启动时只需要指定好 Elasticsearch 连接信息即可。如果是通过 jar 包运行，则执行如下命令：

```sh
java -jar zipkin-server-2.27.0-exec.jar --STORAGE_TYPE=elasticsearch --ES_HOSTS=192.168.1.1:9200
```

（2）如果通过 Docker 镜像运行，则执行如下命令：

```sh
docker run --name zipkin -d -p 9411:9411 -e STORAGE_TYPE=elasticsearch -e ES_HOSTS=192.168.1.1:9200 openzipkin/zipkin
```

（3）如果每次都要使用 docker 命令来分别启动 zipkin 容器还是略显繁琐，我们也可以通过 Docker Compose 进行启动，docker-compose.yml 文件内容如下：

```yaml
version: '2'

services:
  zipkin:
    image: openzipkin/zipkin
    container_name: zipkin
    environment:
      - STORAGE_TYPE=elasticsearch
      - ES_HOSTS=192.168.1.1:9200
      #- RABBIT_ADDRESSES=192.168.1.1:5672
      #- RABBIT_USER=hangge
      #- RABBIT_PASSWORD=123
    ports:
      - 9411:9411
```

## 四、API 接口介绍与使用

### 1、文档

Zipkin Server 提供的 API 接口都是以 /api/v2 路径作为前缀，详细的接口请求参数和请求返回格式可以访问 Zipkin 官网的 API 页面来查看：<https://zipkin.io/zipkin-api/>

### 2、常用接口说明

常用接口说明如下：

| 接口路径      | 请求方式 | 接口描述                                    |
| ------------- | -------- | ------------------------------------------- |
| /dependencies | GET      | 用来获取通过收集到的 Span 分析出的依赖关系  |
| /services     | GET      | 用来获取服务列表                            |
| /spans        | GET      | 根据服务名来获取所有的 Span 名              |
| /spans        | POST     | 向 Zipkin Server 上传 Span                  |
| /dependencies | GET      | 根据 Trace ID 获取指定跟踪信息的 Span 列表  |
| /dependencies | GET      | 根据指定条件查询并返回符合条件的 trace 清单 |

## 五、参考资料

<https://juejin.cn/post/6997243493458378765>

<https://blog.csdn.net/kenkao/article/details/127844321>

<https://zhuanlan.zhihu.com/p/457555795>

---

---
url: /Java/微服务专栏/01.SpringCloud/6_SpringCloudStream.md
---

# SpringCloudStream

---

---
url: /StableDiffusion/StableDiffusion/0_StableDiffusion介绍.md
---

# StableDiffusion介绍

## 介绍

> 项目网址：<https://stablediffusionweb.com/zh-cn>

项目主页

![image-20240227222534659](/assets/image-20240227222534659.3Hg_KQ-P.png)

### 关于Stable Diffusion

**Stable Diffusion（简称SD）** 是2022年发布的一个深度学习文本到图像生成模型，由慕尼黑大学的CompVis研究团体首先提出，并与初创公司Stability AI、Runway合作开发，同时得到了EleutherAI和LAION的支持。

它可以实现的功能有很多，可以根据文本的描述生成指定内容的图片（图生图），也可以用于已有图片内容的转绘（图生图），还可以用作图像的局部重绘、外补扩充、高清修复，甚至是视频的“动画化”生成。

SD的源代码是**开源**发布在网上的，这意味着任何人都可以**免费、不限量**地使用它进行AI绘画生成操作。有开发者使用它的源代码制作了易于用户使用的图形化界面（GUI），于是便有了今天我们大多数人手里可以使用的**Stable Diffusion WebUI（SD Web UI）**。

### 配置需求：我的电脑跑得动SD吗？

目前，Stable Diffusion可以在一台搭载有民用级显卡的电脑上运行。它的配置要求不高但具有一定针对性，最主要的要求是**显卡性能与显存大小**。以下是配置参考：

#### **最低配置**

| 操作系统 | 无硬性要求              |
| -------- | ----------------------- |
| CPU      | 无硬性要求              |
| 显卡     | RTX 2060 及同等性能显卡 |
| 显存     | 6GB                     |
| 内存     | 8GB                     |
| 硬盘空间 | 20GB的可用硬盘空间      |

在此配置条件下，约1-2分钟一张图，可绘制分辨率512 \* 512像素。

#### 推荐**配置**

| 操作系统 | Windows 10/11 64 位       |
| -------- | ------------------------- |
| CPU      | 支持 64 位的多核处理器    |
| 显卡     | RTX 3060Ti 及同等性能显卡 |
| 显存     | 8GB                       |
| 内存     | 16GB                      |
| 硬盘空间 | 100~150GB的可用硬盘空间   |

在此配置条件下，约10-30秒一张图，可绘制分辨率1024 \* 1024像素。

**Q：Mac能用Stable Diffusion吗？显卡用N卡好还是A卡好？**

**A：问就是都能用，但目前，Nvidia（英伟达、N卡）显卡是AIGC应用公认的最优解。**

* 使用其他显卡/系统的电脑也可以运行Stable Diffusion，但同等性能下的速度、效率较N卡均有一定差距。

**Q：我想买一台新电脑来跑图，应该怎么选？**

**A：非要这么问的话，购置一台N卡台式机是最佳的选择。**

* **请将大部分预算花在显卡上**，并购置足量硬盘（最好是固态）来放置可能会比较占地方的模型文件，其他硬件（CPU、内存等）不存在严格限制。
* 单就显卡而言，同等预算下，**可以优先照顾显存**，因为它影响的东西比性能多。

## 原理介绍

Jay Alammar大佬讲解：<https://jalammar.github.io/illustrated-stable-diffusion>

Jay Alammar大佬中文讲解：<https://zhuanlan.zhihu.com/p/618862789>

电子羊资料包：<https://vc4k1knlxj.feishu.cn/docx/RgLJdwEnRokZRTxCK33cqRhPnzd>

Nenly同学笔记：<https://nenly.notion.site/7d86299aa00f4ddbb8ef4496cdb54278>

---

---
url: /Java/Java开发技巧/02.函数式编程/2_Stream流.md
---

# Stream流

## 一、概述

Java8的Stream使用的是函数式编程模式，如同它的名字一样，它可以被用来对集合或数组进行链状流式的操作。可以更方便的让我们对集合或数组操作。

## 二、案例数据准备

依赖

```xml
<dependencies>
    <dependency>
        <groupId>org.projectlombok</groupId>
        <artifactId>lombok</artifactId>
        <version>1.18.16</version>
    </dependency>
</dependencies>
```

```java
@Data
@NoArgsConstructor
@AllArgsConstructor
@EqualsAndHashCode//用于后期的去重使用
public class Author {
    //id
    private Long id;
    //姓名
    private String name;
    //年龄
    private Integer age;
    //简介
    private String intro;
    //作品
    private List<Book> books;
}
```

```java
@Data
@AllArgsConstructor
@NoArgsConstructor
@EqualsAndHashCode//用于后期的去重使用
public class Book {
    //id
    private Long id;
    //书名
    private String name;

    //分类
    private String category;

    //评分
    private Integer score;

    //简介
    private String intro;

}
```

```java
    private static List<Author> getAuthors() {
        //数据初始化
        Author author = new Author(1L,"蒙多",33,"一个从菜刀中明悟哲理的祖安人",null);
        Author author2 = new Author(2L,"亚拉索",15,"狂风也追逐不上他的思考速度",null);
        Author author3 = new Author(3L,"易",14,"是这个世界在限制他的思维",null);
        Author author4 = new Author(3L,"易",14,"是这个世界在限制他的思维",null);

        //书籍列表
        List<Book> books1 = new ArrayList<>();
        List<Book> books2 = new ArrayList<>();
        List<Book> books3 = new ArrayList<>();

        books1.add(new Book(1L,"刀的两侧是光明与黑暗","哲学,爱情",88,"用一把刀划分了爱恨"));
        books1.add(new Book(2L,"一个人不能死在同一把刀下","个人成长,爱情",99,"讲述如何从失败中明悟真理"));

        books2.add(new Book(3L,"那风吹不到的地方","哲学",85,"带你用思维去领略世界的尽头"));
        books2.add(new Book(3L,"那风吹不到的地方","哲学",85,"带你用思维去领略世界的尽头"));
        books2.add(new Book(4L,"吹或不吹","爱情,个人传记",56,"一个哲学家的恋爱观注定很难把他所在的时代理解"));

        books3.add(new Book(5L,"你的剑就是我的剑","爱情",56,"无法想象一个武者能对他的伴侣这么的宽容"));
        books3.add(new Book(6L,"风与剑","个人传记",100,"两个哲学家灵魂和肉体的碰撞会激起怎么样的火花呢？"));
        books3.add(new Book(6L,"风与剑","个人传记",100,"两个哲学家灵魂和肉体的碰撞会激起怎么样的火花呢？"));

        author.setBooks(books1);
        author2.setBooks(books2);
        author3.setBooks(books3);
        author4.setBooks(books3);

        List<Author> authorList = new ArrayList<>(Arrays.asList(author,author2,author3,author4));
        return authorList;
    }
```

## 三、快速入门

### 3.1、需求

我们可以调用getAuthors方法获取到作家的集合。现在需要打印所有年龄小于18的作家的名字，并且要注意去重。

### 3.2、实现

```java
//打印所有年龄小于18的作家的名字，并且要注意去重
List<Author> authors = getAuthors();
authors.
    stream()//把集合转换成流
    .distinct()//先去除重复的作家
    .filter(author -> author.getAge()<18)//筛选年龄小于18的
    .forEach(author -> System.out.println(author.getName()));//遍历打印名字
```

### 3.3、常用操作

#### 3.3.1、Stream流创建操作

|        | 相关方法                               |
| :----- | :------------------------------------- |
| 集合   | Collection.stream/parallelStream       |
| 数组   | Arrays.stream                          |
| 数字   | lntStream/LongStream.range/rangeClosed |
| 数字   | Random.ints/longs/doubles              |
| 自定义 | Stream.generate/iterate                |

单列集合： `集合对象.stream()`

```java
List<Author> authors = getAuthors();
Stream<Author> stream = authors.stream();
```

双列集合：转换成单列集合后再创建

```java
Map<String,Integer> map = new HashMap<>();
map.put("蜡笔小新",19);
map.put("黑子",17);
map.put("日向翔阳",16);

Stream<Map.Entry<String, Integer>> stream = map.entrySet().stream();
```

数组：`Arrays.stream(数组) `或者使用`Stream.of`来创建

```JAVA
Integer[] arr = {1,2,3,4,5};
Stream<Integer> stream = Arrays.stream(arr);
Stream<Integer> stream2 = Stream.of(arr);
```

数字流

```java
// 创建数字流
IntStream.of(1,2,3);
IntStream.range(1,10);

// 创建一个无限流
new Random().ints().limit(10);

// 自定义
Random random = new Random();
Stream.generate(() -> random.nextInt()).limit(20);
```

#### 3.3.2、Stream流中间操作

![image-20210717171216038](/assets/05image-20210717171216038.Czfqx69t.png)

##### filter

可以对流中的元素进行条件过滤，符合过滤条件的才能继续留在流中。

例如：

```java
// 打印所有姓名长度大于1的作家的姓名       
List<Author> authors = getAuthors();
authors.stream()
    .filter(author -> author.getName().length()>1)
    .forEach(author -> System.out.println(author.getName()));
```

##### map

可以把对流中的元素进行计算或转换。

例如：

```java
List<Author> authors = getAuthors();

// 打印所有作家的姓名
// authors.stream()
//     .map(author -> author.getName())
//     .forEach(s -> System.out.println(s));

// 所有作家年龄加10岁
authors.stream()
    .map(author -> author.getAge())
    .map(age -> age + 10)
    .forEach(age -> System.out.println(age));
```

##### distinct

可以去除流中的重复元素。

例如：

```java
// 打印所有作家的姓名，并且要求其中不能有重复元素。
List<Author> authors = getAuthors();
authors.stream()
    .distinct()
    .forEach(author -> System.out.println(author.getName()));
```

**注意：distinct方法是依赖Object的equals方法来判断是否是相同对象的。所以需要注意重写equals方法。**

##### sorted

可以对流中的元素进行排序。

例如：

```java
List<Author> authors = getAuthors();
// 对流中的元素按照年龄进行降序排序，并且要求不能有重复的元素。
authors.stream()
    .distinct()
    .sorted()
    .forEach(author -> System.out.println(author.getAge()));
// 对流中的元素按照年龄进行降序排序，并且要求不能有重复的元素。
authors.stream()
    .distinct()
    .sorted((o1, o2) -> o2.getAge()-o1.getAge())
    .forEach(author -> System.out.println(author.getAge()));
```

**注意：如果调用空参的sorted()方法，需要流中的元素是实现了Comparable。**

##### limit

可以设置流的最大长度，超出的部分将被抛弃。

例如：

```java
// 对流中的元素按照年龄进行降序排序，并且要求不能有重复的元素,然后打印其中年龄最大的两个作家的姓名。
List<Author> authors = getAuthors();
authors.stream()
    .distinct()
    .sorted()
    .limit(2)
    .forEach(author -> System.out.println(author.getName()));
```

##### skip

跳过流中的前n个元素，返回剩下的元素

例如：

```java
// 打印除了年龄最大的作家外的其他作家，要求不能有重复元素，并且按照年龄降序排序。
List<Author> authors = getAuthors();
authors.stream()
    .distinct()
    .sorted()
    .skip(1)
    .forEach(author -> System.out.println(author.getName()));
```

##### flatMap

map只能把一个对象转换成另一个对象来作为流中的元素。而flatMap可以把一个对象转换成多个对象作为流中的元素。

例一：

```java
// 打印所有书籍的名字。要求对重复的元素进行去重。
List<Author> authors = getAuthors();
authors.stream()
    .flatMap(author -> author.getBooks().stream())
    .distinct()
    .forEach(book -> System.out.println(book.getName()));
```

例二：

```java
// 打印现有数据的所有分类。要求对分类进行去重。不能出现这种格式：哲学,爱情     爱情
List<Author> authors = getAuthors();
authors.stream()
    .flatMap(author -> author.getBooks().stream())
    .distinct()
    .flatMap(book -> Arrays.stream(book.getCategory().split(",")))
    .distinct()
    .forEach(category-> System.out.println(category));
```

#### 3.3.3、Stream终止操作

![image-20210717211443754](/assets/05image-20210717211443754.BVSDZa3C.png)

##### forEach

对流中的元素进行遍历操作，我们通过传入的参数去指定对遍历到的元素进行什么具体操作。

例子：

```java
// 输出所有作家的名字
List<Author> authors = getAuthors();
authors.stream()
    .map(author -> author.getName())
    .distinct()
    .forEach(name-> System.out.println(name));
```

##### count

可以用来获取当前流中元素的个数。

例子：

```java
// 打印这些作家的所出书籍的数目，注意删除重复元素。
List<Author> authors = getAuthors();
long count = authors.stream()
    .flatMap(author -> author.getBooks().stream())
    .distinct()
    .count();
System.out.println(count);
```

##### max\&min

可以用来或者流中的最值。

例子：

```java
// 分别获取这些作家的所出书籍的最高分和最低分并打印。
//Stream<Author>  -> Stream<Book> ->Stream<Integer>  ->求值

List<Author> authors = getAuthors();
Optional<Integer> max = authors.stream()
    .flatMap(author -> author.getBooks().stream())
    .map(book -> book.getScore())
    .max((score1, score2) -> score1 - score2);

Optional<Integer> min = authors.stream()
    .flatMap(author -> author.getBooks().stream())
    .map(book -> book.getScore())
    .min((score1, score2) -> score1 - score2);
System.out.println(max.get());
System.out.println(min.get());
```

##### collect

把当前流转换成一个集合。

例子：

```java
// 获取一个存放所有作者名字的List集合。
List<Author> authors = getAuthors();
List<String> nameList = authors.stream()
    .map(author -> author.getName())
    .collect(Collectors.toList());
System.out.println(nameList);
```

```java
// 获取一个所有书名的Set集合。
List<Author> authors = getAuthors();
Set<Book> books = authors.stream()
    .flatMap(author -> author.getBooks().stream())
    .collect(Collectors.toSet());

System.out.println(books);
```

```java
// 获取一个Map集合，map的key为作者名，value为List<Book>
List<Author> authors = getAuthors();

Map<String, List<Book>> map = authors.stream()
    .distinct()
    .collect(Collectors.toMap(author -> author.getName(), author -> author.getBooks()));

System.out.println(map);
```

##### anyMatch（查找与匹配）

可以用来判断是否有任意符合匹配条件的元素，结果为boolean类型。

例子：

```java
// 判断是否有年龄在29以上的作家
List<Author> authors = getAuthors();
boolean flag = authors.stream()
    .anyMatch(author -> author.getAge() > 29);
System.out.println(flag);
```

##### allMatch（查找与匹配）

可以用来判断是否都符合匹配条件，结果为boolean类型。如果都符合结果为true，否则结果为false。

例子：

```java
// 判断是否所有的作家都是成年人
List<Author> authors = getAuthors();
boolean flag = authors.stream()
    .allMatch(author -> author.getAge() >= 18);
System.out.println(flag);
```

##### noneMatch（查找与匹配）

可以判断流中的元素是否都不符合匹配条件。如果都不符合结果为true，否则结果为false

例子：

```java
// 判断作家是否都没有超过100岁的。
List<Author> authors = getAuthors();

boolean b = authors.stream()
    .noneMatch(author -> author.getAge() > 100);

System.out.println(b);
```

##### findAny（查找与匹配）

```
获取流中的任意一个元素。该方法没有办法保证获取的一定是流中的第一个元素。
```

例子：

```java
// 获取任意一个年龄大于18的作家，如果存在就输出他的名字
List<Author> authors = getAuthors();
Optional<Author> optionalAuthor = authors.stream()
    .filter(author -> author.getAge()>18)
    .findAny();

optionalAuthor.ifPresent(author -> System.out.println(author.getName()));
```

##### findFirst（查找与匹配）

获取流中的第一个元素。

例子：

```java
// 获取一个年龄最小的作家，并输出他的姓名。
List<Author> authors = getAuthors();
Optional<Author> first = authors.stream()
    .sorted((o1, o2) -> o1.getAge() - o2.getAge())
    .findFirst();

first.ifPresent(author -> System.out.println(author.getName()));
```

##### reduce归并

对流中的数据按照你指定的计算方式计算出一个结果。（缩减操作）

reduce的作用是把stream中的元素给组合起来，我们可以传入一个初始值，它会按照我们的计算方式依次拿流中的元素和初始化值进行计算，计算结果再和后面的元素计算。

reduce两个参数的重载形式内部的计算方式如下：

```java
T result = identity;
for (T element : this stream)
	result = accumulator.apply(result, element)
return result;
```

其中identity就是我们可以通过方法参数传入的初始值，accumulator的apply具体进行什么计算也是我们通过方法参数来确定的。

例子：

```java
// 使用reduce求所有作者年龄的和
List<Author> authors = getAuthors();
Integer sum = authors.stream()
    .distinct()
    .map(author -> author.getAge())
    .reduce(0, (result, element) -> result + element);
System.out.println(sum);
```

```java
// 使用reduce求所有作者中年龄的最大值
List<Author> authors = getAuthors();
Integer max = authors.stream()
    .map(author -> author.getAge())
    .reduce(Integer.MIN_VALUE, (result, element) -> result < element ? element : result);

System.out.println(max);
```

```java
// 使用reduce求所有作者中年龄的最小值
List<Author> authors = getAuthors();
Integer min = authors.stream()
    .map(author -> author.getAge())
    .reduce(Integer.MAX_VALUE, (result, element) -> result > element ? element : result);
System.out.println(min);
```

```
reduce一个参数的重载形式内部的计算
```

```java
 	 boolean foundAny = false;
     T result = null;
     for (T element : this stream) {
         if (!foundAny) {
             foundAny = true;
             result = element;
         }
         else
             result = accumulator.apply(result, element);
     }
     return foundAny ? Optional.of(result) : Optional.empty();
```

```
如果用一个参数的重载方法去求最小值代码如下：
```

```java
        //        使用reduce求所有作者中年龄的最小值
        List<Author> authors = getAuthors();
        Optional<Integer> minOptional = authors.stream()
                .map(author -> author.getAge())
                .reduce((result, element) -> result > element ? element : result);
        minOptional.ifPresent(age-> System.out.println(age));
```

#### 3.3.4、Steam并行流

`parallel`效果演示

```java
public class Test {
    public static void debug(int i){
        System.out.println("debug" + i);
        try {
            // 睡眠3秒，方便查看效果
            TimeUnit.SECONDS.sleep(3);
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
    }
}


public static void main(String[] args) {
    // 每3秒打印一次
    IntStream.range(1,100).forEach(Test::debug);

    // 每3秒打印8次
    IntStream.range(1,100).parallel().forEach(Test::debug);
}
```

效果查看

![lv\_0\_20210717213822](/assets/05lv_0_20210717213822.D-hOgtZw.gif)

> 问题一：面改成并行流后，我们新的需求出现了，先并行执行，然后串行执行。
>
> ```java
> IntStream.range(1,100).parallel().peek(Test::debug).sequential().peek(Test::debug).count();
> ```
>
> `sequential` 代表串行的意思，最终代码以串行执行。结论：多次调用串行并行，以最后一次调用为准

> 问题二：串行执行的线程数可以改变吗？
>
> ```java
> //使用这个属性可以修改默认的线程数
> System.setProperty("java.util.concurrent.ForkJoinPool.common.parallelism","20");
> ```

#### 3.3.5、Steam流收集器

创建性别枚举类

```java
public enum Gender {

    /**
     * 男
     */
    MALE,

    /**
     * 女
     */
    FEMALE,
}
```

创建班级枚举类

```java
public enum Grade {

    /**
     * 一班
     */
    ONE,

    /**
     * 二班
     */
    TWO,

    /**
     * 三班
     */
    THREE,

    /**
     * 四班
     */
    FOUR
}
```

创建学生类

```java
@Data
@AllArgsConstructor
public class Student {

    /**
     * 姓名
     */
    private String name;

    /**
     * 年龄
     */
    private int age;

    /**
     * 性别
     */
    private Gender gender;

    /**
     * 班级
     */
    private Grade grade;

}
```

初始化数据

```java
public static void main(String[] args) {
    List<Student> students = Arrays.asList(
        new Student("小明", 10, Gender.MALE, Grade.ONE),
        new Student("大明", 9, Gender.MALE, Grade.THREE),
        new Student("小白", 8, Gender.FEMALE, Grade.TWO),
        new Student("小黑", 13, Gender.FEMALE, Grade.FOUR),
        new Student("小红", 7, Gender.FEMALE, Grade.THREE),
        new Student("小黄", 13, Gender.MALE, Grade.ONE),
        new Student("小青", 13, Gender.FEMALE, Grade.THREE),
        new Student("小紫", 9, Gender.FEMALE, Grade.TWO),
        new Student("小王", 6, Gender.MALE, Grade.ONE),
        new Student("小李", 6, Gender.MALE, Grade.ONE),
        new Student("小马", 14, Gender.FEMALE, Grade.FOUR),
        new Student("小刘", 13, Gender.MALE, Grade.FOUR)
    );
}
```

实战演练

```java
public static void main(String[] args) {
    List<Student> students = Arrays.asList(
        new Student("小明", 10, Gender.MALE, Grade.ONE),
        new Student("大明", 9, Gender.MALE, Grade.THREE),
        new Student("小白", 8, Gender.FEMALE, Grade.TWO),
        new Student("小黑", 13, Gender.FEMALE, Grade.FOUR),
        new Student("小红", 7, Gender.FEMALE, Grade.THREE),
        new Student("小黄", 13, Gender.MALE, Grade.ONE),
        new Student("小青", 13, Gender.FEMALE, Grade.THREE),
        new Student("小紫", 9, Gender.FEMALE, Grade.TWO),
        new Student("小王", 6, Gender.MALE, Grade.ONE),
        new Student("小李", 6, Gender.MALE, Grade.ONE),
        new Student("小马", 14, Gender.FEMALE, Grade.FOUR),
        new Student("小刘", 13, Gender.MALE, Grade.FOUR)
    );

    // 获取所有学生的年龄列表
    List<Integer> ages = students.stream().map(Student::getAge).collect(Collectors.toList());

    // 获取所有学生的年龄列表(去重)
    Set<Integer> agesSet = students.stream().map(Student::getAge).collect(Collectors.toSet());

    // 统计汇总信息
    IntSummaryStatistics collect = students.stream()
        .collect(Collectors.summarizingInt(Student::getAge));
    System.out.println(collect); // count=12, sum=121, min=6, average=10.083333, max=14

    // 按性别分块-> 根据boolean分块，true为男生，false为女生
    Map<Boolean, List<Student>> genders = students.stream()
        .collect(Collectors.partitioningBy(student -> student.getGender() == Gender.MALE));

    // 按班级分组
    Map<Grade, List<Student>> grades = students.stream().
        collect(Collectors.groupingBy(Student::getGrade));
    
    // 按班级分组，查询每个班级的人数
    Map<Grade, Long> collect1 = students.stream()
            .collect(Collectors.groupingBy(Student::getGrade, Collectors.counting()));
}
```

### 3.4、注意事项

* 惰性求值（如果没有终结操作，没有中间操作是不会得到执行的）
* 流是一次性的（一旦一个流对象经过一个终结操作后。这个流就不能再被使用）
* 不会影响原数据（我们在流中可以多数据做很多处理。但是正常情况下是不会影响原来集合中的元素的。这往往也是我们期望的）

## 四、使用Optional操作数据

### 4.1、概述

我们在编写代码的时候出现最多的就是空指针异常。所以在很多情况下我们需要做各种非空的判断。

例如：

```java
Author author = getAuthor();
if(author!=null){
    System.out.println(author.getName());
}
```

尤其是对象中的属性还是一个对象的情况下。这种判断会更多。

而过多的判断语句会让我们的代码显得臃肿不堪。

所以在JDK8中引入了Optional，养成使用Optional的习惯后你可以写出更优雅的代码来避免空指针异常。

并且在很多函数式编程相关的API中也都用到了Optional，如果不会使用Optional也会对函数式编程的学习造成影响。

### 4.2、使用

#### 4.2.1、创建对象

Optional就好像是包装类，可以把我们的具体数据封装Optional对象内部。然后我们去使用Optional中封装好的方法操作封装进去的数据就可以非常优雅的避免空指针异常。

我们一般使用**Optional**的**静态方法ofNullable**来把数据封装成一个Optional对象。无论传入的参数是否为null都不会出现问题。

```java
Author author = getAuthor();
Optional<Author> authorOptional = Optional.ofNullable(author);
```

你可能会觉得还要加一行代码来封装数据比较麻烦。但是如果改造下getAuthor方法，让其的返回值就是封装好的Optional的话，我们在使用时就会方便很多。

而且在实际开发中我们的数据很多是从数据库获取的。Mybatis从3.5版本可以也已经支持Optional了。我们可以直接把dao方法的返回值类型定义成Optional类型，MyBastis会自己把数据封装成Optional对象返回。封装的过程也不需要我们自己操作。

如果你**确定一个对象不是空**的则可以使用**Optional**的**静态方法of**来把数据封装成Optional对象。

```java
Author author = new Author();
Optional<Author> authorOptional = Optional.of(author);
```

但是一定要注意，如果使用of的时候传入的参数必须不为null。（尝试下传入null会出现什么结果）

如果一个方法的返回值类型是Optional类型。而如果我们经判断发现某次计算得到的返回值为null，这个时候就需要把null封装成Optional对象返回。这时则可以使用**Optional**的**静态方法empty**来进行封装。

```java
Optional.empty()
```

#### 4.2.2、安全消费值

我们获取到一个Optional对象后肯定需要对其中的数据进行使用。这时候我们可以使用其**ifPresent**方法对来消费其中的值。

这个方法会判断其内封装的数据是否为空，不为空时才会执行具体的消费代码。这样使用起来就更加安全了。

例如,以下写法就优雅的避免了空指针异常。

```java
Optional<Author> authorOptional = Optional.ofNullable(getAuthor());

authorOptional.ifPresent(author -> System.out.println(author.getName()));
```

#### 4.2.3、获取值

如果我们想获取值自己进行处理可以使用get方法获取，但是不推荐。因为当Optional内部的数据为空的时候会出现异常。

#### 4.2.4、安全获取值

如果我们期望安全的获取值。我们不推荐使用get方法，而是使用Optional提供的以下方法。

* orElseGet

  获取数据并且设置数据为空时的默认值。如果数据不为空就能获取到该数据。如果为空则根据你传入的参数来创建对象作为默认值返回。

  ```java
  Optional<Author> authorOptional = Optional.ofNullable(getAuthor());
  Author author1 = authorOptional.orElseGet(() -> new Author());
  ```

* orElseThrow

  获取数据，如果数据不为空就能获取到该数据。如果为空则根据你传入的参数来创建异常抛出。

  ```java
  Optional<Author> authorOptional = Optional.ofNullable(getAuthor());
  try {
      Author author = authorOptional.orElseThrow((Supplier<Throwable>) () -> new RuntimeException("author为空"));
      System.out.println(author.getName());
  } catch (Throwable throwable) {
      throwable.printStackTrace();
  }
  ```

#### 4.2.5、过滤

我们可以使用filter方法对数据进行过滤。如果原本是有数据的，但是不符合判断，也会变成一个无数据的Optional对象。

```java
Optional<Author> authorOptional = Optional.ofNullable(getAuthor());
authorOptional.filter(author -> author.getAge()>100).ifPresent(author -> System.out.println(author.getName()));
```

#### 4.2.6、判断

我们可以使用isPresent方法进行是否存在数据的判断。如果为空返回值为false,如果不为空，返回值为true。但是这种方式并不能体现Optional的好处，**更推荐使用ifPresent方法**。

```java
Optional<Author> authorOptional = Optional.ofNullable(getAuthor());

if (authorOptional.isPresent()) {
    System.out.println(authorOptional.get().getName());
}
```

#### 4.2.7、数据转换

Optional还提供了map可以让我们的对数据进行转换，并且转换得到的数据也还是被Optional包装好的，保证了我们的使用安全。

例如我们想获取作家的书籍集合。

```java
private static void testMap() {
    Optional<Author> authorOptional = getAuthorOptional();
    Optional<List<Book>> optionalBooks = authorOptional.map(author -> author.getBooks());
    optionalBooks.ifPresent(books -> System.out.println(books));
}
```

---

---
url: /Python/AI大模型应用开发/16_Streamlit入门.md
---

# Streamlit入门

## 一、超易上手的网站框架

对于想做网站却被前后端技术劝退的人来说，Streamlit 是一个理想选择。它能让你用纯 Python 代码在几分钟内搭建出可分享的网站，无需前端经验，甚至支持免费部署，堪称 “一站式网站解决方案”。

### 1、Streamlit 的核心优势

* **简单易上手**：全程使用 Python 编写，无需学习 HTML、CSS、JavaScript 等前端技术。
* **快速开发**：几行代码即可实现基础网站功能，官方宣称 “几分钟内将 Python 脚本转为网站”。
* **功能强大**：支持文本、表格、图表、交互组件（按钮、输入框等），满足数据展示、工具类网站需求。
* **便于分享**：支持免费部署，生成可公开访问的链接，让他人轻松使用你的网站。
* **一站式解决方案**：集 “后端逻辑 + 前端展示 + 简易部署” 于一体，无需额外配置服务器或框架。

### 2、入门步骤：安装与测试

#### 2.1. 安装 Streamlit

在项目文件夹的终端中执行以下命令：

```sh
pip install streamlit
```

#### 2.2. 验证安装

安装完成后，通过官方示例测试是否可用：

```sh
streamlit hello
```

* 若成功，浏览器会自动打开一个本地网站（地址以`localhost`开头），展示 Streamlit 的功能示例。
* 示例包含交互演示（如数据可视化、机器学习模型展示），可通过侧边栏切换不同页面。

#### 2.3. 终止服务器

若需停止网站运行，在终端按`Ctrl+C`即可终止 Streamlit 服务器，此时刷新网页会显示 “无法访问”。

### 3、适用场景

* 快速搭建数据可视化工具（如 CSV 分析器、图表生成器）；
* 开发 AI 交互应用（如简易 ChatGPT、问答工具）；
* 制作个人作品集、项目演示网站；
* 开发内部工具（如数据报表系统、流程简化工具）。

## 二、网站上添加文本 图片 表格

Streamlit 通过简单的 Python 代码即可在网页上展示文本、图片、表格等内容，且支持实时更新。以下是常用组件的使用方法，帮助你快速上手：

### 1、基础准备

1. **新建文件**：创建一个 Python 文件（如`page1.py`），导入 Streamlit 并简写为`st`：

   ```python
   import streamlit as st
   ```

2. **运行网页**：在终端执行以下命令，启动本地服务器（默认地址：`http://localhost:8501`）：

   ```sh
   streamlit run page1.py
   ```

3. **实时更新**：网页右上角点击 “Always rerun”，保存代码后自动刷新页面，无需手动重启。

### 2、文本与格式展示

#### 2.1. 基本文本输出

* 使用`st.write()`函数展示文本，支持直接输出变量：

  ```python
  # 输出字符串
  st.write("早上好！")

  # 输出变量
  a = 100
  st.write(a)  # 网页显示：100

  # 输出列表/字典
  st.write([1, 2, 3])  # 列表
  st.write({"姓名": "小明", "年龄": 18})  # 字典
  ```

* **简化写法**：直接写变量或字符串，Streamlit 会自动调用`st.write()`：

  ```python
  "这行文本会自动显示"  # 等价于 st.write("这行文本会自动显示")
  ```

#### 2.2. 标题与 Markdown

* `st.title()`：大标题（最醒目）

* 支持 Markdown 语法（如标题、加粗、列表）：

  ```python
  st.title("📚 我的第一个Streamlit网页")  # 标题+emoji
  st.write("## 二级标题")  # Markdown二级标题
  st.write("**加粗文本**")  # 加粗
  ```

### 3、图片展示

使用`st.image()`函数展示本地图片，需指定图片路径：

```python
# 展示图片（width参数设置宽度）
st.image("./头像.png", width=300)  # 假设图片与代码同目录
```

### 4、表格数据展示

Streamlit 对数据科学友好，可快速展示`pandas.DataFrame`（表格数据）。

#### 4.1. 安装 pandas

```sh
pip install pandas
```

#### 4.2. 展示交互式表格

使用`st.dataframe()`，支持排序、搜索、下载等交互功能：

```python
import pandas as pd

# 创建表格数据
data = {
    "学号": [101, 102, 103],
    "班级": ["一班", "二班", "一班"],
    "成绩": [90, 85, 95]
}
df = pd.DataFrame(data)

# 展示交互式表格
st.dataframe(df)
```

#### 4.3. 展示静态表格

使用`st.table()`，无交互功能，仅用于展示：

```python
st.table(df)  # 静态表格，不可排序/搜索
```

### 5、分隔线

用`st.divider()`添加分隔线，区分不同内容块：

```python
st.write("第一部分内容")
st.divider()  # 分隔线
st.write("第二部分内容")
```

### 6、核心优势

* **零前端知识**：全程用 Python 实现，无需 HTML/CSS。
* **实时反馈**：保存代码后自动更新网页，调试高效。
* **功能丰富**：一行代码实现表格交互、图片展示等复杂功能。

## 三、网站上添加输入组件

Streamlit 提供了丰富的输入组件，可轻松实现文本输入、数字选择、勾选框、按钮等交互功能。这些组件能实时响应用户操作，并触发代码重新运行以更新页面内容。以下是常用输入组件的使用方法：

### 1、文本输入组件

#### 1.1. 单行文本输入（`st.text_input`）

用于接收短文本（如姓名、邮箱），返回用户输入的字符串：

```python
import streamlit as st

# 文本输入框（标签为“请输入你的名字”）
name = st.text_input("请输入你的名字")

# 当用户输入内容后，显示问候语
if name:
    st.write(f"你好，{name}！")
```

* **特性**：用户输入后按回车，变量`name`会更新，页面自动刷新。

#### 1.2. 密码输入（`st.text_input`+`type="password"`）

隐藏输入内容，适合用于密码、密钥等敏感信息：

```python
# 密码输入框（输入内容默认隐藏）
password = st.text_input("请输入密码", type="password")

if password:
    st.write("密码已输入（为保护隐私不显示内容）")
```

#### 1.3. 多行文本输入（`st.text_area`）

用于接收长文本（如备注、问题描述）：

```python
# 多行文本框（可手动调整高度）
feedback = st.text_area("请输入你的反馈")

if feedback:
    st.write("你的反馈：")
    st.write(feedback)
```

### 2、数字输入组件（`st.number_input`）

用于接收数字，支持限制范围、步长等参数：

```python
# 数字输入框（标签为“请输入你的年龄”）
age = st.number_input(
    "请输入你的年龄",
    value=20,  # 默认值
    min_value=0,  # 最小值
    max_value=120,  # 最大值
    step=1  # 增减步长
)

st.write(f"你的年龄是：{age}")
```

* **特性**：用户可通过输入框直接输入数字，或点击加减号 / 减号调整，超出范围会提示错误。

### 3、勾选框（`st.checkbox`）

返回 / 勾选状态，返回布尔值（`True`/`False`）：

```python
# 勾选框（标签为“同意条款”）
agree = st.checkbox("同意以上条款")

# 勾选后显示确认信息
if agree:
    st.write("你已同意条款")
```

### 4、按钮（`st.button`）

点击后触发动作，返回布尔值（`True`表示已点击）：

```python
# 按钮（显示“提交”）
submit = st.button("提交")

# 点击后显示成功信息
if submit:
    st.write("提交成功！")
```

### 5、Streamlit 的运行机制

当用户与组件交互（如输入文本、点击按钮）或修改代码并保存时，Streamlit 会**重新运行整个 Python 脚本**，更新变量值并刷新页面。这一机制确保页面内容始终与用户操作同步。

### 6、总结

Streamlit 的输入组件简单易用，无需复杂逻辑即可实现交互功能：

* 文本输入：`st.text_input`（单行）、`st.text_area`（多行）
* 数字输入：`st.number_input`（支持范围和步长）
* 状态选择：`st.checkbox`（勾选 / 未勾选）
* 动作触发：`st.button`（点击后执行操作）

这些组件为构建交互式工具（如表单、数据分析界面）提供了基础。

## 四、网站上添加更多输入组件

除了基础输入组件，Streamlit 还提供了单选按钮、下拉框、滑块、文件上传器等丰富组件，支持更灵活的用户输入。以下是这些组件的具体用法：

### 1、单选组件

#### 1.1. 单选按钮（`st.radio`）

用于从多个选项中选择**唯一**答案（如性别、学历）：

```python
import streamlit as st

# 单选按钮：选择性别
gender = st.radio(
    "你的性别是？",  # 提示文本
    ["男", "女", "保密"],  # 选项列表
    index=0  # 默认选中第1个选项（索引从0开始）
)

st.write(f"你选择了：{gender}")
```

* **特性**：默认选中第一个选项，可通过`index`修改默认值；若无需默认选项，设`index=None`。

#### 1.2. 单选下拉框（`st.selectbox`）

节省空间的单选组件，适合选项较多的场景（如下拉菜单选择城市）：

```python
# 单选下拉框：选择联系方式
contact = st.selectbox(
    "你希望通过什么方式联系？",  # 提示文本
    ["电话", "邮件", "微信", "QQ", "其它"]  # 选项列表
)

st.write(f"好的，我们会通过{contact}联系你")
```

* **特性**：用户点击下拉箭头选择选项，返回选中的单个值。

### 2、多选组件（`st.multiselect`）

允许用户选择**多个选项**，返回包含所有选中值的列表（如选择兴趣爱好）：

```python
# 多选下拉框：选择喜欢的水果
fruits = st.multiselect(
    "你喜欢的水果是什么？",  # 提示文本
    ["苹果", "香蕉", "橙子", "梨", "西瓜", "葡萄", "其它"]  # 选项列表
)

if fruits:  # 若有选择，展示结果
    st.write("你选择的水果是：")
    for fruit in fruits:
        st.write(f"- {fruit}")
```

### 3、滑块组件（`st.slider`）

通过拖动选择数字，适合需要直观调整数值的场景（如年龄、评分）：

```python
# 滑块：选择年龄
age = st.slider(
    "请选择你的年龄",  # 提示文本
    min_value=0,  # 最小值
    max_value=120,  # 最大值
    value=25,  # 默认值
    step=1  # 拖动步长（每次增减1）
)

st.write(f"你选择的年龄是：{age}")
```

* **扩展**：支持浮点型数值（如`step=0.5`），或范围选择（`value=(18, 30)`返回元组）。

### 4、文件上传器（`st.file_uploader`）

接收用户上传的文件（如图片、文档、CSV），支持限制文件格式：

```python
# 文件上传器：仅允许上传Python文件（.py）
uploaded_file = st.file_uploader(
    "上传你的Python代码文件",  # 提示文本
    type=["py"]  # 允许的文件格式（可传入多个，如["png", "jpg"]）
)

if uploaded_file:  # 若有文件上传
    st.write(f"文件名：{uploaded_file.name}")  # 展示文件名
    # 读取文件内容（文本文件适用）
    content = uploaded_file.read()
    st.text("文件内容：")
    st.code(content.decode("utf-8", errors="ignore"))  # 以代码格式展示
```

* **说明**：`uploaded_file`是文件对象，包含`name`（文件名）、`read()`（读取内容）等方法；非文本文件（如图片）需用对应库处理（如`PIL`处理图片）。

### 5、组件的共性与使用技巧

1. **返回值类型**：根据组件功能不同，返回值可能是字符串（单选）、列表（多选）、数字（滑块）或文件对象（上传器）。
2. **实时更新**：用户操作后，Streamlit 会重新运行脚本，变量值自动更新，页面同步刷新。
3. **条件展示**：结合`if`语句，可在用户输入后展示对应内容（如 “选择后才显示结果”）。

## 五、调整网站布局和增加容器

Streamlit 不仅支持基础组件，还提供了丰富的布局工具，可实现侧边栏、多列展示、选项卡切换、折叠展开等复杂布局，让网页更美观、交互更友好。以下是具体实现方法：

### 1、侧边栏（Sidebar）

侧边栏适合放置次要信息（如设置、辅助输入），不占用主页面空间，且可折叠。

#### 实现方式

使用`with st.sidebar:`语句，缩进部分的内容会显示在侧边栏：

```python
import streamlit as st

# 侧边栏内容
with st.sidebar:
    st.write("### 侧边栏输入")
    name = st.text_input("请输入你的名字")
    if name:
        st.write(f"你好，{name}！")

# 主页面内容
st.write("### 主页面内容")
st.write("这里是主要展示区域")
```

#### 效果

* 侧边栏默认显示在页面左侧，可通过点击箭头折叠 / 展开。
* 侧边栏中的组件功能与主页面一致（如输入、交互等）。

### 2、多列布局（Columns）

通过多列布局可将内容横向分割，适合对比展示或分类排列组件。

#### 实现方式

1. 使用`st.columns(n)`创建`n`个列，返回列对象列表；
2. 通过`with 列对象:`语句，在对应列中添加内容。

```python
column1, column2, column3 = st.columns([1, 2, 3])
with column1:
    password = st.text_input("请输入你的密码：", type="password")

with column2:
    paragraph = st.text_area("请输入一段自我介绍：")

with column3:
    age = st.number_input("请输入你的年龄：", value=20, min_value=0, max_value=150, step=1)
    st.write(f"你的年龄是：{age}岁")
```

#### 说明

* 若传入整数`st.columns(3)`，则列宽均分；
* 传入列表`st.columns([1, 2, 3])`可自定义宽度比例（数值越大，占比越宽）。

### 3、选项卡（Tabs）

选项卡允许用户通过切换标签页查看不同内容，适合分类展示（如 “基本信息”“详细数据”）。

#### 实现方式

1. 使用`st.tabs(标签列表)`创建选项卡，返回标签对象列表；
2. 通过`with 标签对象:`语句，在对应标签页中添加内容。

```python
tab1, tab2, tab3 = st.tabs(["性别", "联系方式", "喜好水果"])
with tab1:
    gender = st.radio("你的性别是什么？", ["男性", "女性", "跨性别"], index=None)
    if gender:
        st.write(f"你选择的性别是{gender}")

with tab2:
    contact = st.selectbox("你希望通过什么方式联系？",
                 ["电话", "邮件", "微信", "QQ", "其它"])
    st.write(f"好的，我们会通过{contact}联系你")

with tab3:
    fruits = st.multiselect("你喜欢的水果是什么？",
                   ["苹果", "香蕉", "橙子", "梨", "西瓜", "葡萄", "其它"])
    for fruit in fruits:
        st.write(f"你选择的水果是{fruit}")
```

#### 特性

* 切换选项卡时，已输入的内容会保留（变量值不丢失）；
* 适合内容较多但可分类的场景，减少页面滚动。

### 4、折叠展开组件（Expander）

折叠组件允许用户自主选择是否查看详情，避免页面信息过载。

#### 实现方式

使用`with st.expander(标题):`语句，缩进部分的内容默认折叠，点击可展开。

```python
with st.expander("身高信息"):
    height = st.slider("你的身高是多少厘米？", value=170, min_value=100, max_value=230, step=1)
    st.write(f"你的身高是{height}厘米")
```

#### 适用场景

* 展示次要信息（如 “备注”“高级设置”）；
* 隐藏冗长内容（如 “数据来源说明”“详细日志”）。

## 六、管理用户会话状态

在 Streamlit 中，用户与组件交互（如点击按钮）会触发整个脚本重新运行，导致普通变量被重置。通过**会话状态（Session State）** 可保存变量值，确保在多次运行中保留数据。以下是具体原理与实现：

### 1、问题场景：变量被重置的困境

```python
import streamlit as st

# 初始值设为0
a = 0

# 点击按钮后a加1
if st.button("加1"):
    a += 1

# 展示a的值
st.write(f"当前值：{a}")
```

#### 问题表现

* 首次点击按钮，`a`变为 1；
* 再次点击，`a`始终保持 1，无法继续增加。

#### 原因

Streamlit 在用户点击按钮后会**重新运行整个脚本**，`a`被重新初始化为 0，加 1 后仍为 1。

### 2、解决方案：使用会话状态（Session State）

会话状态可在用户会话（浏览器标签页打开期间）中保存变量，即使脚本重新运行也不会丢失数据。

#### 2.1. 会话状态的基本用法

* 通过`st.session_state`访问会话状态（字典形式）；
* 首次使用时初始化变量，后续直接更新。

#### 2.2. 修复后的代码

```python
import streamlit as st

# 初始化会话状态中的变量a（仅首次运行时执行）
if "a" not in st.session_state:
    st.session_state["a"] = 0  # 或 st.session_state.a = 0（属性式访问）

# 点击按钮后，更新会话状态中的a
clicked = st.button("加1")
if clicked:
    st.session_state["a"] += 1  # 或 st.session_state.a += 1

# 展示会话状态中的a的值
st.write(f"当前值：{st.session_state['a']}")
print(st.session_state)
```

#### 2.3. 效果

* 每次点击按钮，`a`的值会持续增加（1→2→3...）；
* 变量值保存在会话状态中，不受脚本重新运行影响。

### 3、会话状态的核心特性

* **会话隔离**：每个浏览器标签页是独立会话，会话状态不共享（如用户 A 和用户 B 的`a`值互不影响）。
* **生命周期**：会话状态在浏览器标签页关闭后失效。
* **灵活访问**：支持字典式（`st.session_state["key"]`）或属性式（`st.session_state.key`）访问。

### 4、适用场景

* 保存用户输入历史（如多次点击的累计值）；
* 保留页面切换时的临时数据（如选项卡间共享变量）；
* 实现复杂交互逻辑（如多步骤表单、游戏分数记录）。

通过会话状态，Streamlit 可突破 “脚本重新运行导致变量重置” 的限制，实现更复杂的交互功能。这是构建动态网页（如工具类应用、多步骤流程）的核心技术之一。

## 七、创建多页面网站

当 Streamlit 应用内容增多时，单页面难以承载，此时可通过**多页面功能**将内容拆分到不同页面，用户可通过侧边栏自由跳转。实现方式简单，无需复杂路由配置，以下是具体步骤：

### 1、多页面实现步骤

#### 1.1. 项目结构调整

* **指定主页**：选择一个 Python 文件作为网站主页（如`app.py`），直接放在项目根目录。
* **创建页面文件夹**：在项目根目录新建名为`pages`的文件夹（**必须小写且命名为 pages**）。
* **存放子页面**：将其他页面的 Python 文件（如`page1.py`、`page2.py`）全部放入`pages`文件夹。

示例结构：

```markdown
my_streamlit_app/
├─ app.py  # 主页
└─ pages/
   ├─ page1.py  # 子页面1
   ├─ page2.py  # 子页面2
   └─ page3.py  # 子页面3
```

#### 1.2. 运行多页面网站

在终端运行主页文件：

```python
streamlit run app.py
```

### 2、多页面效果与特性

#### 2.1. 页面跳转

* 运行后，侧边栏会自动显示所有页面的入口（以文件名作为标签）。
* 点击侧边栏的页面名称，即可切换到对应页面（如点击 “page1” 跳转到`page1.py`的内容）。

#### 2.2. 会话状态共享

* 不同页面间可通过`st.session_state`共享变量（如用户输入、累计值等）。
* 只要不关闭浏览器标签页，会话状态会一直保留，跳转页面后数据不丢失。

#### 2.3. 灵活性

* 可随时在`pages`文件夹中添加 / 删除子页面，无需修改配置，Streamlit 会自动识别。
* 主页与子页面的编写方式完全一致，支持所有 Streamlit 组件和布局。

### 3、优势

* **内容拆分**：将庞大的应用按功能拆分，每个页面聚焦一个主题，提升用户体验。
* **简单易用**：无需学习路由规则，仅通过文件夹结构即可实现多页面，对新手友好。
* **状态共享**：通过会话状态实现页面间数据传递，满足复杂交互需求（如多步骤流程）。

通过这种方式，可轻松构建包含多个页面的 Streamlit 网站，适合内容较多的应用（如数据分析平台、多功能工具集等）。只需调整文件结构，即可实现页面管理与跳转，极大降低了多页面开发的门槛。

---

---
url: /常用框架/SpringAIAlibaba/Agent Framework/7_StructuredOutput结构化输出.md
---

# Structured Output 结构化输出

结构化输出允许 Agent 以特定的、可预测的格式返回数据。相比于解析自然语言响应，您可以直接获得 JSON 对象或 Java POJO 形式的结构化数据，应用程序可以直接使用。

Spring AI Alibaba 的 `ReactAgent.Builder` 通过 `outputSchema` 和 `outputType` 方法处理结构化输出。当您设置所需的结构化输出模式时，Agent 会自动在用户消息中增加模式指令，模型会根据指定的格式生成数据。

```java
Builder outputSchema(String outputSchema)
```

## 一、输出格式选项

Spring AI Alibaba 支持两种方式控制结构化输出：

* **`outputSchema(String schema)`**: 提供自定义的 JSON schema 字符串
* **`outputType(Class type)`**: 提供 Java 类 - 使用 `BeanOutputConverter` 自动转换为 JSON schema
* **不指定**: 返回非结构化的自然语言响应

结构化响应在 Agent 的 `AssistantMessage` 中作为 JSON 文本返回，可以解析为您需要的格式。

## 二、输出 Schema 策略

您可以使用 JSON schema 字符串显式定义输出格式。这使您可以完全控制结构，并可以为每个字段包含详细的描述。

### 方法签名

```java
Builder outputSchema(String outputSchema)
```

**参数:**

* `outputSchema` (String, 必需): 定义结构化输出格式的 JSON schema 字符串。Schema 应包含字段名称、类型、描述和要求，以指导模型。

### 示例

**基本 JSON Schema:**

```java
/**
 * 基本 JSON Schema 示例
 */
@SneakyThrows
public static void jsonConfiguration() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();
    // 模型配置
    DashScopeChatOptions options = DashScopeChatOptions.builder()
                .withModel("deepseek-v3.2")           // 模型名称
                .withTemperature(0.3)                 // 温度参数
                .withMaxToken(500)                    // 最大令牌数
                .withTopP(0.9)                        // Top-P 采样
                .build();
    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();
    
    // [!code ++:9]
    String contactInfoSchema = """
            请按照以下JSON格式输出：
            {
                "name": "人名",
                "email": "电子邮箱地址",
                "phone": "电话号码"
            }
            """;
    // 创建 Agent
    ReactAgent agent = ReactAgent.builder()
        .name("contact_extractor")
        .model(chatModel)
        .outputSchema(contactInfoSchema) // [!code ++]
        .build();
    // 调用并获取响应
    AssistantMessage result = agent.call("从以下信息提取联系方式：张三，zhangsan@example.com，(555) 123-4567");
    System.out.println(result.getText());
    // 输出:
    //        {
    //            "name": "张三",
    //            "email": "zhangsan@example.com",
    //            "phone": "(555) 123-4567"
    //        }
}
```

**复杂嵌套 Schema:**

````java
/**
 * 复杂嵌套 Schema 示例
 */
@SneakyThrows
public static void productReviewSchemaConfiguration() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();
    // 模型配置
    DashScopeChatOptions options = DashScopeChatOptions.builder()
                .withModel("deepseek-v3.2")           // 模型名称
                .withTemperature(0.3)                 // 温度参数
                .withMaxToken(500)                    // 最大令牌数
                .withTopP(0.9)                        // Top-P 采样
                .build();
    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();
    
    // [!code ++:12]
    String productReviewSchema = """
            请严格按照以下JSON格式返回产品评价分析：
            {
                "rating": 1-5之间的整数评分,
                "sentiment": "情感倾向（正面/负面/中性）",
                "keyPoints": ["关键点1", "关键点2", "关键点3"],
                "details": {
                    "pros": ["优点1", "优点2"],
                    "cons": ["缺点1", "缺点2"]
                }
            }
            """;
    // 创建 Agent
    ReactAgent agent = ReactAgent.builder()
        .name("review_analyzer")
        .model(chatModel)
        .outputSchema(productReviewSchema) // [!code ++]
        .build();
    // 调用并获取响应
    AssistantMessage result = agent.call("分析评价：这个产品很棒，5星好评。配送快速，但价格稍贵。");
    System.out.println(result.getText());
    // 输出:
    //```json
    //        {
    //            "rating": 5,
    //            "sentiment": "正面",
    //            "keyPoints": ["产品很棒", "配送快速", "价格稍贵"],
    //            "details": {
    //                "pros": ["产品品质好", "配送速度快"],
    //                "cons": ["价格偏高"]
    //            }
    //        }
    //```
}
````

**结构化分析 Schema:**

````java
/**
 * 结构化分析 Schema 示例
 */
@SneakyThrows
public static void analysisSchemaConfiguration() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();
    // 模型配置
    DashScopeChatOptions options = DashScopeChatOptions.builder()
                .withModel("deepseek-v3.2")           // 模型名称
                .withTemperature(0.3)                 // 温度参数
                .withMaxToken(500)                    // 最大令牌数
                .withTopP(0.9)                        // Top-P 采样
                .build();
    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();
    
    // [!code ++:13]
    String analysisSchema = """
            请按照以下JSON格式返回文本分析结果：
            {
                "summary": "内容摘要（50字以内）",
                "keywords": ["关键词1", "关键词2", "关键词3"],
                "sentiment": "情感倾向（正面/负面/中性）",
                "entities": {
                    "persons": ["人名1", "人名2"],
                    "locations": ["地点1", "地点2"],
                    "organizations": ["组织1", "组织2"]
                }
            }
            """;
    // 创建 Agent
    ReactAgent agent = ReactAgent.builder()
        .name("text_analyzer")
        .model(chatModel)
        .outputSchema(analysisSchema) // [!code ++]
        .build();
    // 调用并获取响应
    AssistantMessage result = agent.call("分析这段文字：昨天，李明在北京参加了阿里巴巴公司的技术大会，感受到了创新的力量。");
    System.out.println(result.getText());
    // 输出:
    //```json
    //        {
    //            "summary": "李明在北京参加阿里巴巴技术大会，感受到创新力量。",
    //            "keywords": ["李明", "阿里巴巴", "技术大会", "创新"],
    //            "sentiment": "正面",
    //            "entities": {
    //                "persons": ["李明"],
    //                "locations": ["北京"],
    //                "organizations": ["阿里巴巴公司"]
    //            }
    //        }
    //```
}
````

`outputSchema` 方法提供了最大的灵活性，您可以定义任何 JSON 结构，并提供详细的中文或英文指令来指导模型的输出格式。

## 三、输出类型策略

对于类型安全的结构化输出，您可以提供 Java 类，Spring AI Alibaba 将使用 `BeanOutputConverter` 自动将其转换为 JSON schema。这种方法确保了编译时类型安全和自动 schema 生成。

### 方法签名

```java
Builder outputType(Class<?> outputType)
```

**参数:**

* `outputType` (`Class`, 必需): 定义输出结构的 Java 类。该类应该是带有标准 getter 和 setter 的 POJO。

## 四、工作原理

当 outputFormat 或 outputType 被指定时，Spring AI Alibaba 会自动选择：

* 当大模型服务支持 “原生结构化输出” 时（目前支持 OpenAiChatModel、DashScopeChatModel），自动使用模型内置的结构化输出能力（这也是目前最稳定、可靠的方式，因为模型服务会自动提供校验支持）。
* 针对其他没有 “原生结构化输出” 的模型，Spring AI Alibaba 会使用内置的 ToolCall策略，通过一个动态的 ToolCall 来格式化模型输出。

结构化响应将在 Agent 的状态对象 OverAllState 中返回，可通过 `structured_output` 读取。

### 模型原生结构化输出

比如，针对 DashScopeChatModel 模型，在配置 outputSchema 或 outputType 后，Spring AI Alibaba 会自动设置如下参数，以启用模型原生结构化输出能力。

```java
ChatOptions options = DashScopeChatOptions.builder()
.withResponseFormat(
    DashScopeResponseFormat.builder()
        .type(DashScopeResponseFormat.Type.JSON_OBJECT)
        .build())
.build();
```

同时，Spring AI Alibaba 框架会增强系统 Prompt，引导模型输出格式化内容

```java
// In AgentLlmNode.augmentUserMessage() method
public void augmentUserMessage(List<Message> messages, String outputSchema) {
    if (!StringUtils.hasText(outputSchema)) {
        return;
    }

    for (int i = messages.size() - 1; i >= 0; i--) {
        Message message = messages.get(i);
        if (message instanceof UserMessage userMessage) {
            messages.set(i, userMessage.mutate()
                .text(userMessage.getText() + System.lineSeparator() + outputSchema)
                .build());
            break;
        }
    }
}
```

> 注意，相比于 DashScope 模型是通过增强 Prompt 提示词实现最终的 JSON 格式，实现的是一个尽最大努力的效果，OpenAI 模型则是在模型 API 层面支持 Json 格式，提供格式的严格保证支持。

### ToolCall 结构化输出

对于不支持原生结构化输出的模型，Spring AI Alibaba 支持通过调用工具来实现相同效果。此方法适用于所有支持工具调用的模型，即大多数现代模型。

## 五、错误处理

模型可能不总是返回格式完美的 JSON。以下是处理潜在问题的策略:

### Try-Catch 模式

```java
/**
 * Try-Catch 错误处理示例
 */
@SneakyThrows
public static void tryCatchConfiguration() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();
    // 模型配置
    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();
    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();

    // 创建 Agent
    ReactAgent agent = ReactAgent.builder()
        .name("data_extractor")
        .model(chatModel)
        .outputType(DataOutput.class)
        .build();
    // 调用并获取响应
    AssistantMessage result = agent.call("提取数据");
    try { // [!code ++:9]
        ObjectMapper mapper = new ObjectMapper();
        DataOutput data = mapper.readValue(result.getText(), DataOutput.class);
        // 处理数据
    } catch (JsonProcessingException e) {
        System.err.println("JSON解析失败: " + e.getMessage());
        System.err.println("原始输出: " + result.getText());
        // 回退处理
    }
}
```

### 验证模式

```java
package com.xxl.ai.framework.output;

import lombok.Data;

/**
 * @Classname ValidatedOutput
 * @Description 结构化输出验证
 * @Date 2025/12/7 23:55
 * @Created by xxl
 */
@Data
public class ValidatedOutput {

    private String title;
    private Integer rating;

    public void validate() throws IllegalArgumentException {
        if (title == null || title.isEmpty()) {
            throw new IllegalArgumentException("标题不能为空");
        }
        if (rating != null && (rating < 1 || rating > 5)) {
            throw new IllegalArgumentException("评分必须在1-5之间");
        }
    }

}
```

验证模式示例

```java
/**
 * 验证模式示例
 */
@SneakyThrows
public static void validationPatternConfiguration() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();
    // 模型配置
    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)                    // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();
    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();
    // 创建 Agent
    ReactAgent agent = ReactAgent.builder()
        .name("validated_agent")
        .model(chatModel)
        .outputType(ValidatedOutput.class) // [!code ++]
        .build();
    // 调用并获取响应
    try { // [!code ++:9]
        AssistantMessage result = agent.call("生成评价");
        ObjectMapper mapper = new ObjectMapper();
        ValidatedOutput output = mapper.readValue(result.getText(), ValidatedOutput.class);
        output.validate();  // 如果无效则抛出异常
        System.out.println("Valid output: " + output.getTitle());
    } catch (Exception e) {
        System.err.println("Validation failed: " + e.getMessage());
    }
}
```

### 重试模式

```java
package com.xxl.ai.framework.output;

import lombok.Data;

/**
 * @Classname ContactOutput
 * @Description 联系信息输出类
 * @Date 2025/12/8 00:01
 * @Created by xxl
 */
@Data
public class ContactOutput {

    private String name;

    private String email;

    private String phone;
}
```

重试模式示例

```java
/**
 * 重试模式示例
 */
public static void retryPatternConfiguration() {
    // 创建 DashScope API 实例
    DashScopeApi dashScopeApi = DashScopeApi.builder()
        .apiKey(System.getenv("AI_DASHSCOPE_API_KEY"))
        .build();
    // 模型配置
    DashScopeChatOptions options = DashScopeChatOptions.builder()
        .withModel("deepseek-v3.2")           // 模型名称
        .withTemperature(0.3)                 // 温度参数
        .withMaxToken(500)          // 最大令牌数
        .withTopP(0.9)                        // Top-P 采样
        .build();
    // 创建 ChatModel
    ChatModel chatModel = DashScopeChatModel.builder()
        .dashScopeApi(dashScopeApi)
        .defaultOptions(options)
        .build();

    ReactAgent agent = ReactAgent.builder()
        .name("retry_agent")
        .model(chatModel)
        .outputType(ContactOutput.class)
        .build();

    int maxRetries = 3;
    ContactOutput data = null;
    ObjectMapper mapper = new ObjectMapper();

    for (int i = 0; i < maxRetries; i++) { // [!code ++:15]
        try {
            AssistantMessage result = agent.call("提取数据");
            data = mapper.readValue(result.getText(), ContactOutput.class);
            break;  // 成功
        } catch (Exception e) {
            if (i == maxRetries - 1) {
                throw new RuntimeException("多次尝试后仍然失败", e);
            }
            System.out.println("第" + (i + 1) + "次尝试失败，重试中...");
        }
    }
    if (data != null) {
        System.out.println("Successfully extracted: " + data.getName());
    }
}
```

Spring AI Alibaba 专注于简单性和灵活性，允许开发者在显式 schema 字符串（最大控制）和 Java 类（类型安全）之间进行选择。

---

---
url: /daily/博客文档/VitPress/Teek主题美化.md
---

# Teek主题美化

> 文档地址：[vitepress-theme-teek](https://vp.teek.top/)

## VitePress 安装

```
{
  "teekHome": true,
  "vpHome": false,
  "wallpaper": {
    "enabled": true
  },
  "footerInfo": {
    "customHtml": "<span id=\"runtime\"></span>"
  },
  "docAnalysis": {
    "createTime": "2025-03-23",
    "statistics": {
      "provider": "busuanzi"
    }
  },
  "friendLink": {
    "list": [
      {
        "name": "Teeker",
        "desc": "朝圣的使徒，正在走向编程的至高殿堂！",
        "avatar": "https://testingcf.jsdelivr.net/gh/Kele-Bingtang/static/user/avatar2.png",
        "link": "http://notes.teek.top/"
      },
      {
        "name": "vuepress-theme-vdoing",
        "desc": "🚀一款简洁高效的VuePress 知识管理&博客 主题",
        "avatar": "https://doc.xugaoyi.com/img/logo.png",
        "link": "https://doc.xugaoyi.com/"
      },
      {
        "name": "One",
        "desc": "明心静性，爱自己",
        "avatar": "https://onedayxyy.cn/img/xyy-touxiang.png",
        "link": "https://onedayxyy.cn/"
      },
      {
        "name": "Hyde Blog",
        "desc": "人心中的成见是一座大山",
        "avatar": "https://teek.seasir.top/avatar/avatar.webp",
        "link": "https://teek.seasir.top/"
      },
      {
        "name": "二丫讲梵",
        "desc": "💻学习📝记录🔗分享",
        "avatar": "https://wiki.eryajf.net/img/logo.png",
        "link": " https://wiki.eryajf.net/"
      },
      {
        "name": "粥里有勺糖",
        "desc": "简约风的 VitePress 博客主题",
        "avatar": "https://theme.sugarat.top/logo.png",
        "link": "https://theme.sugarat.top/"
      },
      {
        "name": "VitePress 快速上手中文教程",
        "desc": "如果你也想搭建它，那跟我一起做吧",
        "avatar": "https://avatars.githubusercontent.com/u/90893790?v=4",
        "link": "https://vitepress.yiov.top/"
      },
      {
        "name": "友人A",
        "desc": "おとといは兎をみたの，昨日は鹿，今日はあなた",
        "avatar": "http://niubin.site/logo.jpg",
        "link": "http://niubin.site/"
      }
    ],
    "autoScroll": true
  },
  "social": [
    {
      "icon": "mdi:github",
      "name": "GitHub",
      "link": "https://github.com/kele-bingtang"
    },
    {
      "icon": "simple-icons:gitee",
      "name": "Gitee",
      "link": "https://gitee.com/kele-bingtang"
    }
  ],
  "banner": {
    "name": "🎉 Teek Blog",
    "description": "故事由我书写，旅程由你见证，传奇由她聆听 —— 来自 Young Kbt",
    "bgStyle": "partImg"
  }
}


{
  "teekHome": true,
  "vpHome": false,
  "wallpaper": {
    "enabled": true
  },
  "footerInfo": {
    "customHtml": "<span id=\"runtime\"></span>"
  },
  "docAnalysis": {
    "createTime": "2025-03-23",
    "statistics": {
      "provider": "busuanzi"
    }
  },
  "friendLink": {
    "list": [
      {
        "name": "Teeker",
        "desc": "朝圣的使徒，正在走向编程的至高殿堂！",
        "avatar": "https://testingcf.jsdelivr.net/gh/Kele-Bingtang/static/user/avatar2.png",
        "link": "http://notes.teek.top/"
      },
      {
        "name": "vuepress-theme-vdoing",
        "desc": "🚀一款简洁高效的VuePress 知识管理&博客 主题",
        "avatar": "https://doc.xugaoyi.com/img/logo.png",
        "link": "https://doc.xugaoyi.com/"
      },
      {
        "name": "One",
        "desc": "明心静性，爱自己",
        "avatar": "https://onedayxyy.cn/img/xyy-touxiang.png",
        "link": "https://onedayxyy.cn/"
      },
      {
        "name": "Hyde Blog",
        "desc": "人心中的成见是一座大山",
        "avatar": "https://teek.seasir.top/avatar/avatar.webp",
        "link": "https://teek.seasir.top/"
      },
      {
        "name": "二丫讲梵",
        "desc": "💻学习📝记录🔗分享",
        "avatar": "https://wiki.eryajf.net/img/logo.png",
        "link": " https://wiki.eryajf.net/"
      },
      {
        "name": "粥里有勺糖",
        "desc": "简约风的 VitePress 博客主题",
        "avatar": "https://theme.sugarat.top/logo.png",
        "link": "https://theme.sugarat.top/"
      },
      {
        "name": "VitePress 快速上手中文教程",
        "desc": "如果你也想搭建它，那跟我一起做吧",
        "avatar": "https://avatars.githubusercontent.com/u/90893790?v=4",
        "link": "https://vitepress.yiov.top/"
      },
      {
        "name": "友人A",
        "desc": "おとといは兎をみたの，昨日は鹿，今日はあなた",
        "avatar": "http://niubin.site/logo.jpg",
        "link": "http://niubin.site/"
      }
    ],
    "autoScroll": true
  },
  "social": [
    {
      "icon": "mdi:github",
      "name": "GitHub",
      "link": "https://github.com/kele-bingtang"
    },
    {
      "icon": "simple-icons:gitee",
      "name": "Gitee",
      "link": "https://gitee.com/kele-bingtang"
    }
  ],
  "pageStyle": "segment-nav",
  "bodyBgImg": {
    "imgSrc": [
      "/blog/bg1.webp",
      "/blog/bg2.webp",
      "/blog/bg3.webp"
    ]
  },
  "banner": {
    "name": "🎉 Teek Blog",
    "description": [
      "故事由我书写，旅程由你见证，传奇由她聆听 —— 来自 Young Kbt",
      "积跬步以至千里，致敬每个爱学习的你 —— 来自 Evan Xu",
      "这一生波澜壮阔或是不惊都没问题 —— 来自 Weibw"
    ],
    "descStyle": "types"
  },
  "themeEnhance": {
    "layoutSwitch": {
      "defaultMode": "original"
    }
  }
}
```

### Teek案例

https://teek.seasir.top

https://onedayxyy.cn

---

---
url: /常用框架/Thymeleaf/1_基本语法.md
---

# Thymeleaf基本语法

> **Thymeleaf**官方网站：https://www.thymeleaf.org/index.html

## 一、介绍

Thymeleaf是用来开发Web和独立环境项目的现代服务器端Java模板引擎，既适用于 web 环境，也适用于独立环境，比较适合当前的人员分工问题。其能够处理HTML、XML、JavaScript、CSS 甚至纯文本。提供了一种优雅且高度可维护的模板创建方法，可以直接在浏览器中正确显示，也可以作为静态原型方便开发团队协作。

补充：

Spring官方支持的服务的渲染模板中，并不包含jsp。但是支持一些模板引擎技术，目前官方比较流行的有：Thymeleaf，Freemarker，Mustache。

## 二、特点

1. 动静结合： Thymeleaf 在有网络和无网络的环境下皆可运行，即它可以让美工在浏览器查看页面的静态效果，也可以让程序员在服务器查看带数据的动态页面效果。
2. Thymeleaf支持 html 原型，然后在 html 标签里增加额外的属性来达到模板+数据的展示方式。浏览器解释 html 时会忽略未定义的标签属性，所以 thymeleaf 的模板可以静态地运行；当有数据返回到页面时，Thymeleaf 标签会动态地替换掉静态内容，使页面动态显示。
3. 开箱即用： Thymeleaf提供标准和spring标准两种方言，可以直接套用模板实现JSTL、 OGNL表达式效果，避免每天套模板、改jstl、改标签的困扰。同时开发人员也可以扩展和创建自定义的方言。
4. 多方言支持： Thymeleaf 提供spring标准方言和一个与 SpringMVC 完美集成的可选模块，可以快速的实现表单绑定、属性编辑器、国际化等功能。
5. 与SpringBoot完美整合，SpringBoot提供了Thymeleaf的默认配置，并且为Thymeleaf设置了视图解析器，我们可以像操作jsp一样来操作Thymeleaf。代码几乎没有任何区别，就是在模板语法上有区别。

## 三、SpringBoot集成Thymeleaf

### 1、引入依赖

```xml
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-thymeleaf</artifactId>
</dependency>
```

### 2、可选配置

虽然Springboot官方对Thymeleaf做了很多默认配置，但咱们引入Thymeleaf的jar包依赖后很可能根据自己特定需求进行更细化的配置，例如页面缓存、字体格式设置等等。

Springboot官方提供的配置内容有以下：

```yaml
# THYMELEAF (ThymeleafAutoConfiguration)
spring.thymeleaf.cache=true # Whether to enable template caching.
spring.thymeleaf.check-template=true # Whether to check that the template exists before rendering it.
spring.thymeleaf.check-template-location=true # Whether to check that the templates location exists.
spring.thymeleaf.enabled=true # Whether to enable Thymeleaf view resolution for Web frameworks.
spring.thymeleaf.enable-spring-el-compiler=false # Enable the SpringEL compiler in SpringEL expressions.
spring.thymeleaf.encoding=UTF-8 # Template files encoding.
spring.thymeleaf.excluded-view-names= # Comma-separated list of view names (patterns allowed) that should be excluded from resolution.
spring.thymeleaf.mode=HTML # Template mode to be applied to templates. See also Thymeleaf's TemplateMode enum.
spring.thymeleaf.prefix=classpath:/templates/ # Prefix that gets prepended to view names when building a URL.
spring.thymeleaf.reactive.chunked-mode-view-names= # Comma-separated list of view names (patterns allowed) that should be the only ones executed in CHUNKED mode when a max chunk size is set.
spring.thymeleaf.reactive.full-mode-view-names= # Comma-separated list of view names (patterns allowed) that should be executed in FULL mode even if a max chunk size is set.
spring.thymeleaf.reactive.max-chunk-size=0 # Maximum size of data buffers used for writing to the response, in bytes.
spring.thymeleaf.reactive.media-types= # Media types supported by the view technology.
spring.thymeleaf.servlet.content-type=text/html # Content-Type value written to HTTP responses.
spring.thymeleaf.suffix=.html # Suffix that gets appended to view names when building a URL.
spring.thymeleaf.template-resolver-order= # Order of the template resolver in the chain.
spring.thymeleaf.view-names= # Comma-separated list of view names (patterns allowed) that can be resolved.
```

上面的配置有些我们可能不常使用，因为Springboot官方做了默认配置大部分能够满足我们的使用需求，但如果你的项目有特殊需求也需要妥善使用这些配置。

比如`spring.thymeleaf.cache=false`是否允许页面缓存的配置，我们在开发时候要确保页面是最新的所以需要禁用缓存；而在上线运营时可能页面不常改动为了减少服务端压力以及提升客户端响应速度会允许页面缓存的使用。

再比如在开发虽然我们大部分使用UTF-8多一些，我们可以使用`spring.thymeleaf.encoding=UTF-8`来确定页面的编码，但如果你的项目是GBK编码就需要将它改成GBK。

另外Springboot默认模板引擎文件是放在templates目录下：`spring.thymeleaf.prefix=classpath:/templates/`,如果有需求将模板引擎也可修改配置，将templates改为自己需要的目录。同理其他的配置如果需要自定义化也可参照上面配置进行修改。

## 四、快速入门

### 1、第一个Thymeleaf程序

#### 使用IDEA创建项目

![image-20241128223042690](/assets/image-20241128223042690.BtqsiyQG.png)

目录结构如下

![image-20241128223210342](/assets/image-20241128223210342.ErSvLZBp.png)

如果创建项目时不勾选Thymeleaf，可以自己引入

```xml
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-web</artifactId>
</dependency>
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-thymeleaf</artifactId>
</dependency>
```

#### 修改配置

```yaml
# 关闭Thymeleaf的缓存
spring:
  thymeleaf:
    cache: false
```

#### 编写controller

```java
/**
 * @author xxl
 * @date 2024/11/28 21:57
 */
@Controller
public class StudyController {
    @GetMapping("/showHello")
    public String showHello(Model model) {
        model.addAttribute("msg", "Hello, Thymeleaf!");
        return "index";
    }
}
```

注意

* `controller`上使用 `@Controller` 不要使用`@RestController`。
* **model.addAttribute("msg", "Hello, Thymeleaf!")** 就是Model存入数据的书写方式，Model是一个特殊的类，相当于维护一个Map一样，而Model中的数据通过controller层的关联绑定在view层（即Thymeleaf中）可以直接使用。
* **return "index"**：这个index就是在templates目录下对应模板的名称，即应该对应hello.html这个Thymeleaf文件（与页面关联默认规则为：templates目录下`返回字符串.html`）。

#### 编写Thymeleaf页面

在项目的resources目录下的templates文件夹下面创建一个叫`index.html`的文件，在这个html文件中的标签修改为`<html xmlns:th="http://www.thymeleaf.org">`在Thymeleaf中就可以使用Thymeleaf的语法和规范啦。

```html
<!DOCTYPE html>
<!-- 把html 的名称空间，改成：xmlns:th="http://www.thymeleaf.org" 会有语法提示-->
<html lang="en" xmlns:th="http://www.thymeleaf.org">
<head>
    <meta charset="UTF-8">
    <title>hello</title>
</head>
<body>
<h1 th:text="${msg}">大家好</h1>
</body>
</html>
```

#### 启动程序

启动SpringBoot程序， 通过浏览器访问：<http://localhost:8080/index>

### 2、常用标签

动静结合：

Thymeleaf崇尚自然模板，意思就是模板是纯正的html代码，脱离模板引擎，在纯静态环境也可以直接运行。现在如果我们直接在html中编写 `${}` 这样的表达式，显然在静态环境下就会出错，这不符合Thymeleaf的理念。

Thymeleaf中所有的表达式都需要写在指令中，指令是HTML5中的自定义属性，在Thymeleaf中所有指令都是以th:开头。因为表达式`${user.name}`是写在自定义属性中，因此在静态环境下，表达式的内容会被当做是普通字符串，浏览器会自动忽略这些指令，这样就不会报错了！

th常用标签

| 标签      | 作用               | 示例                                                         |
| --------- | ------------------ | ------------------------------------------------------------ |
| th:id     | 替换id             | \<input th:id="${user.id}"/>                                 |
| th:text   | 文本替换           | \<p text:="${user.name}">bigsai\</p>                         |
| th:utext  | 支持html的文本替换 | \<p utext:="${htmlcontent}">content\</p>                     |
| th:object | 替换对象           | \<div th:object="${user}">\</div>                            |
| th:value  | 替换值             | \<input th:value="${user.name}" >                            |
| th:each   | 迭代               | \<tr th:each="student:${user}" >                             |
| th:href   | 替换超链接         | \<a th:href="@{index.html}">超链接\</a>                      |
| th:src    | 替换资源           | \<script type="text/javascript" th:src="@{index.js}">\</script> |

## 报错

报错1

```
simsun.ttc' with 'Identity-H' is not recognized
```

解决：<https://blog.csdn.net/zhangtongpeng/article/details/100173633>

## 参考资料

模板引擎是什么：<https://blog.csdn.net/blanceage/article/details/125714796>

Themleaf语法：<https://developer.aliyun.com/article/769977>

Themleaf语法：<https://blog.csdn.net/Lzy410992/article/details/115371017>

Html 语法：<https://www.runoob.com/html/html-tables.html>

IText 语法：<https://www.cnblogs.com/antLaddie/p/18263471>

图片：<https://juejin.cn/post/7111712403082969096>

图片：<https://blog.csdn.net/dzydzy7/article/details/105313967>

实战：<https://juejin.cn/post/7195779037033513017>

实战：<https://blog.csdn.net/qq_36981760/article/details/107240792>

实战：<https://www.cnblogs.com/yunfeiyang-88/p/10984740.html>

实战：<https://blog.csdn.net/Zereao/article/details/90378802>

---

---
url: /数据库/04.分布式数据库TIDB/1_TIDB简介.md
---

# TIDB简介

## 一、需求分析

随着业务规‍模持续扩大，数据越来越‌多，我们会发现系统面临‍着几个关键挑战：

* 传统MySQL单机部署的存储容量和性能已经难以满足不断增长的用户和内容需求，而数据库分库分表又有较高的研发和运维成本。
* 业务中经常需要跨表查询和修改，如点赞后内容计数表，分库分表后事务一致性难以保证。
* 数据库扩容、分片再平衡、备份恢复等运维工作复杂，系统的水平扩展能力也受到严重限制。

为解决上述‍问题，我们需要引入一‌种既能保持关系型数据‍库的 ACID 特性‍，又能实现水平扩展的‍分布式数据库解决方案。

## 二、TiDB 介绍

我们使用 TiDB 来替代 MySQL。

[TiDB](https://github.com/pingcap/tidb) 是 [PingCAP](https://pingcap.com/about-cn/) 公司自主设计、研发的 **开源** 分布式关系型数据库，是一款同时支持在线事务处理与在线分析处理 (Hybrid Transactional and Analytical Processing, HTAP) 的融合型分布式数据库产品，具备水平扩容或者缩容、金融级高可用、实时 HTAP、云原生的分布式数据库、兼容 MySQL 协议和 MySQL 生态等重要特性。目标是为用户提供一站式 OLTP (Online Transactional Processing)、OLAP (Online Analytical Processing)、HTAP 解决方案。TiDB 适合高可用、强一致要求较高、数据规模较大等各种应用场景。

![tidb\_img1](/assets/tidb_img1.CyGAgnBy.png)

![tidb\_img2](/assets/tidb_img2.C2GiAuJs.png)

五大核心特性（简单了解即可）：

* 一键水平扩缩容：得益于 TiDB 存储计算分离的架构的设计，可按需对计算、存储分别进行在线扩容或者缩容，扩容或者缩容过程中对应用运维人员透明。
* 金融级高可用：数据采用多副本存储，数据副本通过 Multi-Raft 协议同步事务日志，多数派写入成功事务才能提交，确保数据强一致性且少数副本发生故障时不影响数据的可用性。可按需配置副本地理位置、副本数量等策略，满足不同容灾级别的要求。
* 实时 HTAP：提供行存储引擎 [TiKV](https://docs.pingcap.com/zh/tidb/stable/tikv-overview/)、列存储引擎 [TiFlash](https://docs.pingcap.com/zh/tidb/stable/tiflash-overview/) 两款存储引擎，TiFlash 通过 Multi-Raft Learner 协议实时从 TiKV 复制数据，确保行存储引擎 TiKV 和列存储引擎 TiFlash 之间的数据强一致。TiKV、TiFlash 可按需部署在不同的机器，解决 HTAP 资源隔离的问题。
* 云原生的分布式数据库：专为云而设计的分布式数据库，通过 [TiDB Operator](https://docs.pingcap.com/zh/tidb-in-kubernetes/stable/tidb-operator-overview) 可在公有云、私有云、混合云中实现部署工具化、自动化。
* 兼容 MySQL 协议和 MySQL 生态：兼容 MySQL 协议、MySQL 常用的功能、MySQL 生态，应用无需或者修改少量代码即可从 MySQL 迁移到 TiDB。提供丰富的[数据迁移工具](https://docs.pingcap.com/zh/tidb/stable/ecosystem-tool-user-guide/)帮助应用便捷完成数据迁移。

它很适合本项目的海量数据及高并发的 OLTP 场景，因为传统的单机数据库无法满足因数据爆炸性的增长对数据库的容量要求。TiDB 是一种性价比高的解决方案，采用计算、存储分离的架构，可对计算、存储分别进行扩缩容，计算最大支持 512 节点，每个节点最大支持 1000 并发，集群容量最大支持 PB 级别。

TiDB 核心组件，对存储架构感兴趣的同学了解下：

1）[TiDB Server](https://docs.pingcap.com/zh/tidb/stable/tidb-computing/)：SQL 层，对外暴露 MySQL 协议的连接 endpoint，负责接受客户端的连接，执行 SQL 解析和优化，最终生成分布式执行计划。TiDB 层本身是无状态的，实践中可以启动多个 TiDB 实例，通过负载均衡组件（如 TiProxy、LVS、HAProxy、ProxySQL 或 F5）对外提供统一的接入地址，客户端的连接可以均匀地分摊在多个 TiDB 实例上以达到负载均衡的效果。TiDB Server 本身并不存储数据，只是解析 SQL，将实际的数据读取请求转发给底层的存储节点 TiKV（或 TiFlash）。

2）[PD (Placement Driver) Server](https://docs.pingcap.com/zh/tidb/stable/tidb-scheduling/)：整个 TiDB 集群的元信息管理模块，负责存储每个 TiKV 节点实时的数据分布情况和集群的整体拓扑结构，提供 TiDB Dashboard 管控界面，并为分布式事务分配事务 ID。PD 不仅存储元信息，同时还会根据 TiKV 节点实时上报的数据分布状态，下发数据调度命令给具体的 TiKV 节点，可以说是整个集群的“大脑”。此外，PD 本身也是由至少 3 个节点构成，拥有高可用的能力。建议部署奇数个 PD 节点。

3）存储节点

* [TiKV Server](https://docs.pingcap.com/zh/tidb/stable/tidb-storage/)：负责存储数据，从外部看 TiKV 是一个分布式的提供事务的 Key-Value 存储引擎。存储数据的基本单位是 Region，每个 Region 负责存储一个 Key Range（从 StartKey 到 EndKey 的左闭右开区间）的数据，每个 TiKV 节点会负责多个 Region。TiKV 的 API 在 KV 键值对层面提供对分布式事务的原生支持，默认提供了 SI (Snapshot Isolation) 的隔离级别，这也是 TiDB 在 SQL 层面支持分布式事务的核心。TiDB 的 SQL 层做完 SQL 解析后，会将 SQL 的执行计划转换为对 TiKV API 的实际调用。所以，数据都存储在 TiKV 中。另外，TiKV 中的数据都会自动维护多副本（默认为三副本），天然支持高可用和自动故障转移。
* [TiFlash](https://docs.pingcap.com/zh/tidb/stable/tiflash-overview/)：TiFlash 是一类特殊的存储节点。和普通 TiKV 节点不一样的是，在 TiFlash 内部，数据是以列式的形式进行存储，主要的功能是为分析型的场景加速。

架构图如下：

![tidb\_img3](/assets/tidb_img3.BTjvgP1u.png)

## 三、参考资料

\[1]. https://blog.csdn.net/wt334502157/article/details/126468893

\[2]. https://www.cnblogs.com/jiamiing/p/18891454

---

---
url: /常用框架/SpringAIAlibaba/Agent Framework/3_Tools.md
---

# Tools

许多 AI 应用程序通过自然语言与用户交互。然而，某些业务场景需要模型使用结构化输入直接与外部系统（如 API、数据库或文件系统）进行交互。

Tools 是 [agents](https://java2ai.com/docs/frameworks/agent-framework/tutorials/agents) 调用来执行操作的组件。它们通过定义良好的输入和输出让模型与外部世界交互，从而扩展模型的能力。Tools 封装了一个可调用的函数及其输入模式。我们可以把工具定义传递给兼容的 [models](https://java2ai.com/docs/frameworks/agent-framework/tutorials/models)，允许模型决定是否调用工具以及使用什么参数。在这些场景中，工具调用使模型能够生成符合指定输入模式的请求。

> **注意：服务器端工具使用**
>
> 某些聊天模型（例如 OpenAI、Anthropic 和 Gemini）具有在服务器端执行的内置工具，如 Web 搜索和代码解释器。请参阅提供商概述以了解如何使用特定聊天模型访问这些工具。

## 一、创建工具

### 1.1、基础工具定义

Spring AI 提供了对从函数指定工具的内置支持，可以通过编程方式使用低级 `FunctionToolCallback` 实现，也可以动态地作为在运行时解析的 `@Bean`。

#### 编程方式规范：FunctionToolCallback

可以通过编程方式构建 `FunctionToolCallback`，将函数类型（`Function`、`Supplier`、`Consumer` 或 `BiFunction`）转换为工具。

```java
import java.util.function.Function;

public class WeatherService implements Function<WeatherRequest, WeatherResponse> {
    public WeatherResponse apply(WeatherRequest request) {
        return new WeatherResponse(30.0, Unit.C);
    }
}

public enum Unit { C, F }
public record WeatherRequest(String location, Unit unit) {}
public record WeatherResponse(double temp, Unit unit) {}
```

`FunctionToolCallback.Builder` 允许你构建 `FunctionToolCallback` 实例并提供有关工具的关键信息：

* **name**: 工具的名称。AI 模型使用此名称在调用时识别工具。因此，在同一上下文中不允许有两个同名的工具。对于特定的聊天请求，名称在模型可用的所有工具中必须是唯一的。**必需**。
* **toolFunction**: 表示工具方法的函数对象（`Function`、`Supplier`、`Consumer` 或 `BiFunction`）。**必需**。
* **description**: 工具的描述，模型可以使用它来了解何时以及如何调用工具。如果未提供，将使用方法名称作为工具描述。但是，强烈建议提供详细描述，因为这对于模型理解工具的目的和使用方式至关重要。如果未提供良好的描述，可能导致模型在应该使用工具时不使用，或者使用不正确。
* **inputType**: 函数输入的类型。**必需**。
* **inputSchema**: 工具输入参数的 JSON schema。如果未提供，将根据 `inputType` 自动生成 schema。你可以使用 `@ToolParam` 注解提供有关输入参数的附加信息，例如描述或参数是必需还是可选。默认情况下，所有输入参数都被视为必需。
* **toolMetadata**: 定义附加设置的 `ToolMetadata` 实例，例如是否应将结果直接返回给客户端，以及要使用的结果转换器。你可以使用 `ToolMetadata.Builder` 类构建它。
* **toolCallResultConverter**: 用于将工具调用结果转换为字符串对象以发送回 AI 模型的 `ToolCallResultConverter` 实例。如果未提供，将使用默认转换器（`DefaultToolCallResultConverter`）。

```java
import org.springframework.ai.tool.ToolCallback;
import org.springframework.ai.tool.function.FunctionToolCallback;

/**
 * 编程方式规范 - FunctionToolCallback 构建示例
 */
public static void programmaticToolSpecification() {
    ToolCallback toolCallback = FunctionToolCallback
        .builder("currentWeather", new WeatherService())
        .description("Get the weather in location")
        .inputType(WeatherRequest.class)
        .build();
}
```

函数的输入和输出可以是 `Void` 或 POJO。输入和输出 POJO 必须是可序列化的，因为结果将被序列化并发送回模型。函数以及输入和输出类型必须是公共的。

**重要提示**：某些类型不受支持。有关更多详细信息，请参阅函数工具限制。

**添加工具到 ChatClient**：

使用编程规范方法时，可以将 `FunctionToolCallback` 实例传递给 `ChatClient` 的 `toolCallbacks()` 方法。该工具仅对添加到的特定聊天请求可用。

```java
import org.springframework.ai.chat.client.ChatClient;

ToolCallback toolCallback = ...;

ChatClient.create(chatModel)
    .prompt("What's the weather like in Copenhagen?")
    .toolCallbacks(toolCallback)
    .call()
    .content();
```

**添加默认工具到 ChatClient**：

使用编程规范方法时，你可以通过将 `FunctionToolCallback` 实例传递给 `defaultToolCallbacks()` 方法将默认工具添加到 `ChatClient.Builder`。如果同时提供默认工具和运行时工具，运行时工具将完全覆盖默认工具。

```java
ChatModel chatModel = ...;
ToolCallback toolCallback = ...;

ChatClient chatClient = ChatClient.builder(chatModel)
    .defaultToolCallbacks(toolCallback)
    .build();
```

**注意**：默认工具在从同一 `ChatClient.Builder` 构建的所有 `ChatClient` 实例执行的所有聊天请求中共享。它们对于跨不同聊天请求常用的工具很有用，但如果不小心使用也可能很危险，可能在不应该使用时使它们可用。

**添加工具到 ChatModel**：

使用编程规范方法时，你可以将 `FunctionToolCallback` 实例传递给 `ToolCallingChatOptions` 的 `toolCallbacks()` 方法。该工具仅对添加到的特定聊天请求可用。

```java
import org.springframework.ai.chat.model.ChatModel;
import org.springframework.ai.chat.prompt.Prompt;
import org.springframework.ai.model.tool.ToolCallingChatOptions;

ChatModel chatModel = ...;
ToolCallback toolCallback = ...;

ChatOptions chatOptions = ToolCallingChatOptions.builder()
    .toolCallbacks(toolCallback)
    .build();

Prompt prompt = new Prompt("What's the weather like in Copenhagen?", chatOptions);
chatModel.call(prompt);
```

#### 动态规范：@Bean

你可以将工具定义为 Spring beans，让 Spring AI 使用 `ToolCallbackResolver` 接口（通过 `SpringBeanToolCallbackResolver` 实现）在运行时动态解析它们，而不是以编程方式指定工具。此选项使你可以将任何 `Function`、`Supplier`、`Consumer` 或 `BiFunction` bean 用作工具。bean 名称将用作工具名称，Spring Framework 的 `@Description` 注解可用于为工具提供描述，供模型用来了解何时以及如何调用工具。

```java
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.context.annotation.Description;
import java.util.function.Function;

@Configuration(proxyBeanMethods = false)
class WeatherTools {

    WeatherService weatherService = new WeatherService();

    @Bean
    @Description("Get the weather in location")
    Function<WeatherRequest, WeatherResponse> currentWeather() {
        return weatherService;
    }
}
```

**重要提示**：某些类型不受支持。有关更多详细信息，请参阅函数工具限制。

工具输入参数的 JSON schema 将自动生成。你可以使用 `@ToolParam` 注解提供有关输入参数的附加信息，例如描述或参数是必需还是可选。默认情况下，所有输入参数都被视为必需。

```java
import org.springframework.ai.tool.annotation.ToolParam;

record WeatherRequest(
@ToolParam(description = "The name of a city or a country") String location,
Unit unit
) {}
```

此工具规范方法的缺点是不保证类型安全，因为工具解析是在运行时完成的。为了缓解这一问题，你可以使用 `@Bean` 注解显式指定工具名称并将值存储在常量中，以便你可以在聊天请求中使用它而不是硬编码工具名称。

```java
@Configuration(proxyBeanMethods = false)
class WeatherTools {

public static final String CURRENT_WEATHER_TOOL = "currentWeather";

    @Bean(CURRENT_WEATHER_TOOL)
    @Description("Get the weather in location")
    Function<WeatherRequest, WeatherResponse> currentWeather() {
        // ...
    }
}
```

**添加工具到 ChatClient**（使用动态规范）：

```java
ChatClient.create(chatModel)
    .prompt("What's the weather like in Copenhagen?")
    .toolNames("currentWeather")
    .call()
    .content();
```

**添加默认工具到 ChatClient**（使用动态规范）：

```java
ChatModel chatModel = ...;

ChatClient chatClient = ChatClient.builder(chatModel)
    .defaultToolNames("currentWeather")
    .build();
```

#### 函数工具限制

以下类型目前不支持作为用作工具的函数的输入或输出类型：

* 原始类型
* `Optional`
* 集合类型（例如 `List`、`Map`、`Array`、`Set`）
* 异步类型（例如 `CompletableFuture`、`Future`）
* 响应式类型（例如 `Flow`、`Mono`、`Flux`）

使用基于方法的工具规范方法支持原始类型和集合。

### 1.2、自定义工具属性

#### 自定义工具名称

默认情况下，工具名称来自函数名称。当你需要更具描述性的内容时，可以覆盖它：

```java
ToolCallback searchTool = FunctionToolCallback
    .builder("web_search", new SearchFunction()) // 自定义名称
    .description("Search the web for information")
    .inputType(String.class)
    .build();

System.out.println(searchTool.getName()); // web_search
// 推荐：使用 ToolDefinition 提取名称
System.out.println(searchTool.getToolDefinition().name()); // web_search
```

#### 自定义工具描述

覆盖自动生成的工具描述以提供更清晰的模型指导：

```java
ToolCallback calculatorTool = FunctionToolCallback
    .builder("calculator", new CalculatorFunction())
    .description("Performs arithmetic calculations. Use this for any math problems.")
    .inputType(String.class)
    .build();
```

### 1.3、高级模式定义

使用 Java 类或 JSON schemas 定义复杂的输入：

**使用 Java 记录类（Record）**：

```java
import org.springframework.ai.tool.annotation.ToolParam;

public record WeatherInput(
@ToolParam(description = "City name or coordinates") String location,
@ToolParam(description = "Temperature unit preference") Unit units,
@ToolParam(description = "Include 5-day forecast") boolean includeForecast
) {}

public enum Unit { CELSIUS, FAHRENHEIT }

public class WeatherFunction implements Function<WeatherInput, String> {
@Override
public String apply(WeatherInput input) {
double temp = input.units() == Unit.CELSIUS ? 22 : 72;
String result = String.format(
"Current weather in %s: %.0f degrees %s",
input.location(),
temp,
input.units().toString().substring(0, 1).toUpperCase()
);

if (input.includeForecast()) {
result += "
Next 5 days: Sunny";
}

return result;
}
}

ToolCallback weatherTool = FunctionToolCallback
.builder("get_weather", new WeatherFunction())
.description("Get current weather and optional forecast")
.inputType(WeatherInput.class)
.build();
```

## 二、访问上下文

**为什么这很重要**：当工具可以访问 Agent 状态、运行时上下文和长期记忆时，它们最强大。这使工具能够做出上下文感知的决策、个性化响应并在对话中维护信息。

工具可以通过 `ToolContext` 参数访问运行时信息，该参数提供：

* **State（状态）** - 通过执行流动的可变数据（消息、计数器、自定义字段）
* **Context（上下文）** - 不可变配置，如用户 ID、会话详细信息或应用程序特定配置
* **Store（存储）** - 跨对话的持久长期记忆
* **Config（配置）** - 执行的 RunnableConfig
* **Tool Call ID** - 当前工具调用的 ID

### 2.1、ToolContext

使用 `ToolContext` 在单个参数中访问所有运行时信息。只需将 `ToolContext` 添加到你的工具签名中，它将自动注入而不会暴露给 LLM。

**访问状态**：

工具可以使用 `ToolContext` 访问当前的 Graph 状态：

```java
```

**警告**：`toolContext` 参数对模型是隐藏的。对于上面的示例，模型只看到 `input` 在工具模式中 - `toolContext` **不**包含在请求中。

**更新状态**：

在 Spring AI Alibaba 中，你可以通过 Hook 或在工具执行后返回的信息来更新 Agent 的状态。

```java
```

### 2.2、Context（上下文）

通过 `ToolContext` 访问不可变配置和上下文数据，如用户 ID、会话详细信息或应用程序特定配置。

```java
```

### 2.3、Memory（存储）

使用存储访问跨对话的持久数据。在 Spring AI Alibaba 中，你可以使用 checkpointer 来实现长期记忆。

```java
```

## 三、内置工具

## 四、在 ReactAgent 中使用工具

在 ReactAgent 中使用工具非常简单：

```java
```

---

---
url: /Linux/Ubuntu/ubuntu安装应用.md
---

# ubuntu安装应用

Ubuntu20.04安装VMware tools一直失败，通过命令行进行安装

https://blog.csdn.net/weixin\_47827320/article/details/123033545
虚拟机安装ubuntu和VMware Tool
注意选64位

https://blog.csdn.net/weixin\_44193315/article/details/115112607

---

---
url: /Linux/Shell命令/2_Ubuntu桌面版禁止root登录解决方法.md
---

# Ubuntu桌面版禁止root登录解决方法

## 描述

虚拟机安装了ubuntu系统，通过下列命令设置好了root密码：

sudo passwd root

然后注销用户，用root登录，结果一直显示认证失败。

密码我确定没问题，因为普通用户终端切换root可以登录，所以密码无误

Su root  可以正常切换登录

网上搜索了一下，才知道是因为系统禁止了root账户登录图形界面。

## 解决方法

### 1.修改gdm配置

输入命令，修改gdm-autologin

```bash
sudo gedit /etc/pam.d/gdm-autologin
```

命令成功后打开窗口界面：

注释第三行（加#号）：

```bash
#auth required pam_succeed_if.so user != root quiet_success
```

修改好后保存关闭即可。

### 2.输入命令，修改gdm-password

```bash
sudo gedit /etc/pam.d/gdm-password
```

同样注释第三行：

```bash
#auth required pam_succeed_if.so user != root quiet_success
```

修改好后保存关闭即可。

### 3.修改profile

输入命令，修改root的profile

```bash
sudo gedit /root/.profile
```

注释屏蔽掉最后一行，并添加一行：

```bash
tty -s && mesg n || true
```

文件修改后保存退出。

### 4.重启

注销系统，然后就可以用root用户名密码登录了。

---

---
url: /@pages/loginPage.md
---


---

---
url: /@pages/riskLinkPage.md
---


---

---
url: /index_default.md
---

## 风格切换

Teek 可以通过配置搭配出各种风格，而当前文档站默认仅演示较简约的文档风格。

在右上角  图标的 配置切换 区域可以切换其他风格。因为配置较多，很多的功能并没有完全演示，需要安装 Teek，然后自行探索出自己喜欢的风格。

## 💡 反馈交流

在使用过程中有任何问题和想法，请给我提 [Issue](https://github.com/Kele-Bingtang/vitepress-theme-teek/issues)。 你也可以在 Issue 查看别人提的问题和解决方案。

或者加入我们的交流群（添加我的微信并备注 进群）：

如果图片链接失效，可以在微信右上角 -> 添加朋友，然后搜索 `teekers` 来添加我的微信。

---

---
url: /Redis/Redis基础/Valkey.md
---

# Valkey简介

## 前言

Redis 作为最受欢迎的内存型 NoSQL 键值存储系统之一，一直都很受广大用户的推崇。然而，由于 Redis 调整了许可政策，这一举措在开源社区引起了广泛争议。

[前因后果](https://www.bilibili.com/video/BV1FhjBzeEt2)

## Valkey：开源社区的新希望

为了应对 Redis 的许可变化，Linux 基金会适时推出了 [Valkey 项目](https://github.com/valkey-io/valkey)。该项目旨在打造一个强大的开源替代方案，延续 Redis 的开源精神。

Valkey 基于 Redis 7.2.4 版本进行开发，并采用宽松的 BSD 三条款许可，为项目的可持续发展和社区协作打下了坚实的基础。目前，该项目已得到多家科技巨头的支持，包括 Amazon Web Services、Google Cloud、Oracle、Ericsson 和 Snap Inc. 等。

在 Linux 基金会的引领下，Valkey 将建立开放透明的治理机制，积极吸纳社区力量，共同推动项目的发展。

[Valkey 官网](https://valkey.io/)

## Valkey vs. Redis：关键区别

### Valkey 与 Redis 对比速查

| 特性          | Valkey                                    | Redis                            |
| ------------- | ----------------------------------------- | -------------------------------- |
| **许可证**    | 开源（BSD 三条款）                        | 源码可用（非完全开源）           |
| **社区支持**  | 社区驱动，有 AWS、Oracle 等巨头支持       | 由 Redis Inc. 提供商业支持       |
| **性能表现**  | 新 I/O 线程模型，支持每秒 119 万请求      | 以高性能著称，具体表现因版本而异 |
| **多线程**    | 优化的多线程架构，支持 I/O 与命令执行分离 | 大多数操作仍为单线程             |
| **复制机制**  | 双通道复制技术                            | 主从复制，支持 Redis Cluster     |
| **扩展能力**  | 自动集群故障转移，优化的扩展机制          | 支持集群化和分片                 |
| **内存效率**  | 改进的字典结构，提高内存使用效率          | 高效内存管理，支持内存淘汰策略   |
| **可观测性**  | 支持每个 slot 的精细化监控指标            | 基础监控和指标收集               |
| **RDMA 支持** | 提供实验性 RDMA 支持                      | 暂无原生 RDMA 支持               |
| **平台支持**  | Linux、macOS、OpenBSD、NetBSD、FreeBSD    | Windows、Linux、macOS            |
| **发展重点**  | 追求高吞吐量和低延迟                      | 注重高性能和数据持久性           |
| **应用场景**  | 缓存、实时数据处理、消息代理              | 缓存、会话管理、实时分析         |

### 概述

Redis 和 Valkey 都是强大的键值存储系统，在缓存、实时数据处理等场景中表现出色。随着 Valkey 作为社区驱动的 Redis 替代方案迅速崛起，了解两者的关键差异变得尤为重要，以便在技术选型时做出明智决策。

* **Redis**：作为一款成熟的内存数据结构存储系统，Redis 广泛用于数据库、缓存和消息代理等场景。它支持多种数据结构，如字符串、哈希表、列表和集合。Redis 以其卓越的性能、易用性和丰富的功能特性而备受青睐。
* **Valkey**：作为 Redis 的开源替代方案，Valkey 得到了多家知名科技公司的支持。它致力于打造一个完全开源且高性能的数据结构服务器，并在可扩展性、多线程支持和功能特性上寻求突破和创新。

### 许可证与社区生态

* **Redis**：近期 Redis 修改了其许可政策，虽说源代码依然允许查看，但商业使用会受到一定限制。这引发了开源社区对其未来发展的担忧。目前，Redis 由 Redis Inc. 进行维护，并提供商业支持服务。
* **Valkey**：坚定地采用 BSD 三条款许可，确保了项目的开源性质和社区主导的开发模式。在 AWS、Google Cloud 等科技巨头的支持下，Valkey 的持续开发和维护得到了强有力的保障。

### 性能与扩展性

* **Redis**：长期以来，Redis 都以卓越的性能在业界享有盛誉。它的单线程架构设计简单高效，尤其适合处理小数据量、高并发的场景。
* **Valkey**：在继承 Redis 高性能的基础上，进一步优化了多线程支持，并显著提升了可扩展性。通过引入新的 I/O 线程模型，Valkey 的性能得到了大幅提升，每秒可处理高达 119 万个请求。此外，Valkey 还增强了自动集群故障转移机制，使得分布式环境下的扩展变得更加容易，集群运维也更加顺畅和高可靠。

### 可观测性与监控能力

* **Redis**：提供了基础的监控功能，包括对内存使用、请求速率和连接数等指标的监控，能够满足常规应用的需求。不过，在面对更复杂的运维场景或需要精细化分析时，Redis 的监控能力稍显不足。
* **Valkey**：相比之下，Valkey 提供了更加细粒度的可观测性功能。它能够精确追踪每个 slot（槽位）的性能指标，帮助用户深入了解系统的使用模式和性能瓶颈。这种细粒度的监控使得运维工作更加高效、便捷，特别适用于需要高精度控制和分析的大规模分布式系统。

### 内存效率与 RDMA 支持

* **Redis**：目前尚未提供原生的 RDMA支持，这在一些对延迟和吞吐量极为敏感的高性能场景下可能存在一定局限。此外，Redis 的内存使用效率虽然已经较高，但在某些特定应用中仍有优化空间。
* **Valkey**：在内存效率方面进行了创新改进，采用了全新的字典结构，显著提升了内存使用效率，使得在相同硬件条件下能够存储更多数据。此外，Valkey 还引入了实验性的 RDMA 支持，这一特性在特定的超高性能网络环境中，如大规模分布式缓存系统，能够显著降低延迟和 CPU 负载，从而带来明显的性能提升。

### 生态伙伴与发展动力

Valkey 项目背后得到了多家科技巨头的鼎力支持，这不仅增强了项目的可信度，还为未来的发展注入了强劲动力：

* **Amazon Web Services (AWS)**：提供资源和专业技术，助力 Valkey 保持在开源社区的领先地位。
* **Google Cloud**：深度参与开源治理，积极推动行业的创新与协作。
* **Oracle**：投入开发力量，促进 Valkey 在各种应用场景中的落地和普及。
* **Ericsson**：贡献专业的技术经验，帮助全面提升 Valkey 的性能和功能特性。
* **Snap Inc.**：专注于优化系统的扩展性与处理能力，以满足大规模应用的需求。

这着些科技公司的参与，不仅为 Valkey 提供了丰富的资源和多样化的技术支持，还推动了项目在不同行业和应用场景中的广泛使用。

相比之下，Redis 主要依靠 Redis Inc. 进行维护和商业化运作，重点发展 Redis Enterprise 等商业产品。虽然 Redis 仍然拥有强大的社区支持和成熟的生态系统，但在开源协作的广度和资源投入上，相较于获得众多科技巨头支持的 Valkey，可能略显不足。

## 选型建议与展望

Redis 和 Valkey 都是卓越的键值存储解决方案，各自具备独特的优势。在进行技术选型时，需要根据实际需求权衡以下几个关键因素：

* **开源许可偏好**：Valkey 坚持完全开源，采用 BSD 三条款许可证，而 Redis 已经转变许可模式。如果开源合规和自由使用是优先考虑的因素，Valkey 可能更符合需求。
* **性能需求**：对于有极致性能要求、特别是多线程和大规模扩展需求的场景，Valkey 提供了更为先进的多线程支持和 RDMA 等实验性功能，可能具备一定的优势。
* **社区支持力度**：Redis 拥有更成熟的生态和社区支持，对于寻求稳定、经过长时间验证的解决方案的团队，Redis 仍然是一个值得信赖的选择。而 Valkey 则背靠多家科技巨头，拥有强大的资源支持，但作为一个较新的项目，其社区生态仍在快速发展中。
* **长期发展规划**：如果你的团队重视技术创新并希望拥抱未来的新特性与改进，Valkey 凭借其快速的迭代和创新能力，可能是一个更具前瞻性的选择。

从发展趋势来看，Valkey 正快速追赶 Redis，并在某些方面展现出超越的潜力。例如，Valkey 在多线程性能、内存效率以及 RDMA 支持等特性上已领先一步。随着更多创新功能的引入和持续优化，Valkey 的竞争力将进一步增强。

***

无论是选择 Redis 还是 Valkey，二者都能有效应对现代数据处理的挑战。从实时数据处理到复杂的数据分析，它们均提供了强有力的技术支撑。你的最终选择应该基于项目需求、团队的技术路线以及对未来发展的规划。无论如何，两者的技术实力都足以胜任各类高性能键值存储任务。

## 参考资料

https://www.sysgeek.cn/valkey-vs-redis

https://www.bilibili.com/video/BV1FhjBzeEt2

---

---
url: /daily/日常笔记/Vim操作技巧.md
---

# Vim

https://zhuanlan.zhihu.com/p/402606980

---

---
url: /01.指南/10.使用/30.Vite 插件.md
---

# Vite 插件

VitePress 是基于 Vite 进行搭建，因此可以编写 Vite 插件来辅助完成一些在无法在浏览器环境完成的动作，比如在 VitePress 启动后，扫描文档目录下的 Markdown 文件，提取 `frontmatter` 的信息进行分析，或在渲染 Markdown 内容前，对其进行加工。

得益于 Vite 环境，Teek 内置了一些 Vite 插件来执行在 Node 环境才能完成的事情，这些插件分别为：

* [vitepress-plugin-permalink](https://github.com/Kele-Bingtang/vitepress-theme-teek/blob/master/plugins/vitepress-plugin-permalink/README.md)
* [vitepress-plugin-sidebar-resolve](https://github.com/Kele-Bingtang/vitepress-theme-teek/blob/master/plugins/vitepress-plugin-sidebar-resolve/README.md)
* [vitepress-plugin-md-h1](https://github.com/Kele-Bingtang/vitepress-theme-teek/blob/master/plugins/vitepress-plugin-md-h1/README.md)
* [vitepress-plugin-catalogue](https://github.com/Kele-Bingtang/vitepress-theme-teek/blob/master/plugins/vitepress-plugin-catalogue/README.md)
* [vitepress-plugin-doc-analysis](https://github.com/Kele-Bingtang/vitepress-theme-teek/blob/master/plugins/vitepress-plugin-doc-analysis/README.md)
* [vitepress-plugin-file-content-loader](https://github.com/Kele-Bingtang/vitepress-theme-teek/blob/master/plugins/vitepress-plugin-file-content-loader/README.md)
* [vitepress-plugin-auto-frontmatter](https://github.com/Kele-Bingtang/vitepress-theme-teek/blob/master/plugins/vitepress-plugin-auto-frontmatter/README.md)

这些插件已经上传至 NPM 仓库，具体的使用说明可以前往 NPM 查看使用说明，或者访问 [Github 仓库](https://github.com/Kele-Bingtang/vitepress-theme-teek/tree/master/plugins)，每个插件下都有 `README.md` 文档介绍。

也可以在 Teek 的 [配置项](/config/theme#viteplugins) 中查看这些插件。

## vitepress-plugin-permalink

Teek 使用 `vitepress-plugin-permalink` 来实现永久链接功能。

如果想要禁用该插件，进行如下配置：

```ts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  vitePlugins: {
    permalink: false,
  },
});
```

Teek 默认扫描文档根目录下（`.vitepress` 层级开始）的所有 Markdown 文件，如果希望忽略某些目录，可进行如下配置：

```ts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  vitePlugins: {
    permalinkOption: {
      ignoreList: ["目录名"], // 支持正则表达式
    },
  },
});
```

当希望只扫描指定的目录，可进行如下配置：

```ts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  vitePlugins: {
    permalinkOption: {
      path: "guide", // 基于 .vitepress 目录层级添加，开头不需要有 /
    },
  },
});
```

### 国际化

`vitepress-plugin-permalink` 支持国际化功能，在生成 `permalink` 时，默认会给不同语言文档的 `permalink` 添加语言前缀。

假如存在一个 Markdown 文档 `guide.md`，`frontmatter` 内容如下：

```yaml
---
title: guide
permalink: /guide
---
```

国际化目录结构如下；

```
docs/
├─ es/
│  ├─ guide.md
├─ guide.md
```

那么在生成 `permalink` 时，`es` 语言下的 `guide.md` 的 `permalink` 为 `/es/guide`。

### 配置项

`permalinkOption` 的详细配置项请看 [Permalink 配置项](https://github.com/Kele-Bingtang/vitepress-theme-teek/blob/master/plugins/vitepress-plugin-permalink/src/types.ts)。

```ts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  vitePlugins: {
    permalinkOption: {},
  },
});
```

## vitepress-plugin-sidebar-resolve

Teek 使用 `vitepress-plugin-sidebar-resolve` 来实现自动生成侧边栏功能。

在 [结构化目录 - 特殊目录](/guide/directory-structure#特殊目录) 和 [结构化目录 - 文档风](/guide/directory-structure#文档风) 中已经介绍了该插件部分使用方式以及注意事项，如果想要禁用该插件，进行如下配置：

```ts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  vitePlugins: {
    sidebar: false,
  },
});
```

### 国际化

在国际化的环境下，当把 root 语言（默认语言）的 Markdown 文件放到某个路径下时，`vitepress-plugin-sidebar-resolve` 无法感知到，因此请使用 `localeRootDir` 配置项告诉它，在 [国际化特殊场景](/guide/i18n#给-root-语言添加目录) 有说明。

### 配置项

`sidebarOption` 的详细配置项请看 [SideBar 配置项](https://github.com/Kele-Bingtang/vitepress-theme-teek/blob/master/plugins/vitepress-plugin-sidebar-resolve/src/types.ts)。

```ts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  vitePlugins: {
    sidebarOption: {},
  },
});
```

### 侧边栏初始化格式

配置项 `initItems` 和 `initItemsText` 会影响侧边栏的生成格式，举个例子说明：

假设根目录下有目录名为 `guide`：

* 当 `initItems` 为 true，则最终结果为 `sidebar: { "/guide": { items: [], collapsed }}`
  * 当 `initItemsText` 为 true，则最终结果为 `sidebar: { "/guide": { text: "guide", items: [], collapsed }}`
  * 当 `initItemsText` 为 false，则最终结果为 `sidebar: { "/guide": { items: [] }}`
* 当 `initItems` 为 false，则最终结果为 `sidebar: { "/guide": [] }`

### 侧边栏新增图标&#x20;

如果希望侧边栏标题前新增图标，可以在 `frontmatter.title` 配置：

```yaml
---
title: <i class='iconfont icon-teek'></i> 我是标题
---
```

或者单独使用 `frontmatter.sidebarPrefix` 或 `frontmatter.sidebarSuffix` 配置， 插件会将图标并添加到标题前/后

```yaml
---
sidebarPrefix: <i class='iconfont icon-teek'></i>
sidebarSuffix: <i class='iconfont icon-teek'></i>
---
```

如果使用的是 `iconfont` 图标，每次使用都要加 ` <i class='iconfont icon-{xxx}'></i>` 比较麻烦，因此插件提供了 `prefixTransform` 和 `suffixTransform` 配置项，可以对所有的 `sidebarPrefix` 和 `sidebarSuffix` 进行二次处理，如：

```typescript
import { defineConfig } from "vitepress";
import Sidebar from "vitepress-plugin-sidebar-resolve";

export default defineConfig({
  vite: {
    plugins: [
      Sidebar({
        prefixTransform: prefix => {
          // 判断是否为 HTML 标签，如果是则直接返回
          const htmlTagRegex = /^<([a-zA-Z][a-zA-Z0-9]*)\b[^>]*>/;
          if (htmlTagRegex.test(prefix)) return prefix;

          return `<i class="iconfont icon-${prefix}"></i>`;
        },
      }),
    ],
  },
});
```

此时在 `frontmatter.sidebarPrefix` 配置如下即可生效：

```yaml
---
sidebarPrefix: teek
---
```

上面演示的是如何在标题前缀添加图标，后缀操作同理。

## vitepress-plugin-md-h1

Teek 使用 `vitepress-plugin-md-h1` 来给文章页生成一级标题（假如 Markdown 文档没有设置过一级标题）。

一级标题获取顺序：`frontmatter.title` > 文件名

::: tip
只在页面加载 Markdown 内容时生成一级标题，并不会真正修改 Markdown 文档内容。
:::

假设一个文档 `install.md` 的 `frontmatter` 内容如下：

```yaml
---
title: guide
---
```

在页面访问该文档时，页面自动生成一级标题 `guide`，当把 `frontmatter.title` 去掉后，页面的一级标题变为 `install`。

如果想要禁用该插件，进行如下配置：

```ts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  vitePlugins: {
    mdH1: false,
  },
});
```

### 配置项

`mdH1Option` 的详细配置项请看 [MdH1Option 配置项](https://github.com/Kele-Bingtang/vitepress-theme-teek/blob/master/plugins/vitepress-plugin-md-h1/src/types.ts)。

```ts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  vitePlugins: {
    mdH1Option: {},
  },
});
```

## vitepress-plugin-catalogue

`vitepress-plugin-catalogue` 插件会将所有 `frontmatter.catalogue` 为 true 的文档信息挂载到 `themeConfig.catalogues` 中，Teek 使用该数据来生成目录页。

该插件与 Teek 强绑定，无法像上面的组件一样，通过 `vitePlugins.catalogue = false` 来禁用。

如果想要禁用该插件，只能通过禁用 Teek 主题来实现，配置如下：

```ts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  teekTheme: false, // 禁用 Teek 主题
});
```

如果某个 markdown 文档不想被纳入目录里，则：

```yaml
---
inCatalogue: false
---
```

### 配置项

`catalogueOption` 的详细配置项请看 [Catalogue 配置项](https://github.com/Kele-Bingtang/vitepress-theme-teek/blob/master/plugins/vitepress-plugin-catalogue/src/types.ts)。

```ts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  vitePlugins: {
    catalogueOption: {},
  },
});
```

## vitepress-plugin-doc-analysis

Teek 使用 `vitepress-plugin-doc-analysis` 来实现站点信息和文章页信息功能。

`vitepress-plugin-doc-analysis` 在 VitePress 启动后，扫描所有的 Markdown 文档，然后统计文章数量，文章字数等，最终将分析后的数据挂载到 `themeConfig.docAnalysisInfo` 中。

在首页看到的站点信息、文章页看到的文章字数、预计阅读时间等，都是使用 `themeConfig.docAnalysisInfo` 的数据来构成。

如果不希望某个 Markdown 文档被插件分析，请在该文档 `frontmatter` 配置：

```yaml
---
docAnalysis: false
---
```

如果想要禁用该插件，进行如下配置：

```ts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  vitePlugins: {
    docAnalysis: false,
  },
});
```

关于文章的预计阅读时间的计算，该插件默认 1 分钟内阅读的中文字数为 300，1 分钟内阅读的英文字数为 160，如果认为不合理，可以对其进行修改：

```ts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  vitePlugins: {
    docAnalysisOption: {
      cn: 400,
      en: 200,
    },
  },
});
```

### 国际化

当处于国际化环境下，插件将不同语言的数据挂载到 `locales.[lang].themeConfig.docAnalysisInfo`。

### 配置项

`docAnalysisOption` 的详细配置项请看 [DocAnalysis 配置项](https://github.com/Kele-Bingtang/vitepress-theme-teek/blob/master/plugins/vitepress-plugin-doc-analysis/src/types.ts)。

```ts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  vitePlugins: {
    docAnalysisOption: {},
  },
});
```

## vitepress-plugin-file-content-loader

Teek 使用 `vitepress-plugin-file-content-loader` 来构建首页的文章列表和归档页数据，并挂载到 `themeConfig.posts` 中。

该插件本质上将 VitePress 官网的 [构建时数据加载](https://vitepress.dev/zh/guide/data-loading) 功能转为插件，因此具体的介绍说明请前往 VitePress 官网阅读。

::: info 为什么设计为插件？
构建时数据加载功能是在访问网站的时候开始执行，如果使用该功能扫描了大量的 Markdown 文档，那么会导致第一次进入页面卡顿，因此基于该功能实现了插件，在项目启动过程完成数据的加载。
:::

该插件与 Teek 强绑定，如果想要禁用该插件，只能通过禁用 Teek 主题来实现，配置如下：

```ts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  teekTheme: false, // 禁用 Teek 主题
});
```

### 国际化

当处于国际化环境下，插件将不同语言的数据挂载到 `themeConfig.posts.locales.[lang]` 下。

### 配置项

```ts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  vitePlugins: {
    fileContentLoaderIgnore: {}, // fileContentLoader 插件扫描 markdown 文档时，指定忽略路径，格式为 glob 表达式，如 test/**
  },
});
```

## vitepress-plugin-auto-frontmatter

Teek 使用 `vitepress-plugin-auto-frontmatter` 自动给 Markdown 文档添加 `frontmatter`。

该插件会直接修改 Markdown 文档的 `frontmatter`，因此为了安全性考虑，默认是关闭的，如果希望开启，进行如下配置：

```ts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  vitePlugins: {
    autoFrontmatter: true,
  },
});
```

如果开启了该插件，那么 Teek 将会对所有 Markdown 文档的 `frontmatter` 添加如下格式：

```yaml
---
title: getting
date: 2025-03-03 00:45:16
permalink: /pages/eb8f2f
categories:
  - guide
---
```

* `title` 为文章的标题
* `date` 为文章的创建时间
* `permalink` 为文章的永久链接，采用随机数确保唯一
* `categories` 为文章的分类，根据目录层级获取

::: tip

Teek 则不会修改已经存在的数据，判断的规则是比较 key，不比较 value。

:::

如果需要拓展自定义 `frontmatter`，让 Teek 在生成 `frontmatter` 的时候额外添加其他数据，请使用 `transform` 配置项，具体使用请看 [vitepress-plugin-auto-frontmatter](https://github.com/Kele-Bingtang/vitepress-theme-teek/blob/master/plugins/vitepress-plugin-auto-frontmatter/README.md) 的 `Example 2` 和 `Example 3`。

### 配置项

`autoFrontmatterOption` 的详细配置项请看 [AutoFrontmatter 配置项](https://github.com/Kele-Bingtang/vitepress-theme-teek/blob/master/plugins/vitepress-plugin-auto-frontmatter/src/types.ts)。

Teek 在 `vitepress-plugin-auto-frontmatter` 的配置项基础上额外添加两个配置项：

* permalinkPrefix：自动生成 `permalink` 的固定前缀，如 `pages`、`pages/demo`，默认为 `pages`
* categories：是否自动生成 `categories`

```ts
import { defineTeekConfig } from "vitepress-theme-teek/config";

const teekConfig = defineTeekConfig({
  vitePlugins: {
    autoFrontmatterOption: {
      permalinkPrefix: "pages", // 自动生成 permalink 的固定前缀，如 pages、pages/demo，默认为 pages
      categories: true, // 是否自动生成 categories
      // ...
    },
  },
});
```

---

---
url: /15.主题开发/80.Vite 插件.md
---

# Vite 插件

VitePress 处于 Vite 环境下，因此天然支持 Vite 插件。

Teek 有过一个想法，那就是将所有功能完全插件化，通过 `NPM` 下载各个插件来合并成主题：

* 目录页插件
* 归档页插件
* 文章信息插件
* Footer 插件
* ...

这完全是可行的，每个插件都是独立的，支持任何 VitePress 项目。

但是目前没有太多精力去实现这个计划，您可以通过 Teek 的按需引入功能（等价于下载插件），来加载自己需要的功能。

在了解 Vite 插件实现之前，建议您先去 [Vite 官方文档](https://cn.vite.dev) 了解什么是 Vite。

下面介绍在 VitePress 中自定义 Vite 插件的场景。

## Vite 插件基础模板

首先介绍 Vite 插件的基础模板：

```ts
import type { Plugin } from "vite";

interface Options {
  // ...
}

export default function VitePluginVitePressTemplate(option: Options = {}): Plugin {
  return {
    name: "vite-plugin-vitepress-template",
    // ...
  };
}
```

Vite 插件本质是一个函数，需要返回一个对象，对象的各个 Key 就是 Vite 提供的钩子，比如 `transform`、`config` 等，我们需要识别这些钩子分别执行了哪部分逻辑，这样才能针对性的实现自己的功能。

Vite 提供了哪些钩子请看官网 [插件 API](https://cn.vite.dev/guide/api-plugin.html#config)。

## 扫描项目文件

如果您使用了 Teek 主题，那么在项目启动时，终端会打印：

```sh
Injected Sidebar Data Successfully. 注入侧边栏数据成功!
Injected Permalinks Data Successfully. 注入永久链接数据成功!
Injected DocAnalysisInfo Data Successfully. 注入文档分析数据成功!
Injected Catalogues Data Successfully. 注入目录页数据成功!
Injected posts Data Successfully. 注入 posts 数据成功!
```

每一行都是一个 Vite 插件输出的内容，这些插件都是去扫描项目的 Markdown 文件，然后生成数据并注入到 VitePress 的 `themeConfig` 中。

扫描项目文件的目的有如下场景：

* 生成侧边栏：根据 Markdown 文件路径生成侧边栏数据
* 解析 Markdown 文档的 `frontmatter` 来生成文章信息，或给 Markdown 文件自动添加 `frontmatter`
* 解析 Markdown 文档的内容，生成站点信息功能（总字数、文章字数、阅读时长等）
* ...

这里需要用到 Vite 提供的 `config` 钩子，在解析 VitePress 配置前会调用该钩子，因此我们在这个钩子里执行扫描项目文件的逻辑，最后将数据注入到 VitePress 的 `themeConfig` 中。

```ts
import type { Plugin } from "vite";

interface Options {
  // ...
}

export default function VitePluginVitePressDemo(option: Options = {}): Plugin & { name: string } {
  return {
    name: "vitepress-plugin-demo",
    config(config: any) {
      // 获取 themeConfig 配置
      const {
        site: { themeConfig = {} },
        srcDir,
      } = config.vitepress;

      // 使用 node API 扫描项目文件，项目文件的根路径为 srcDir
      const data = scanProjectFiles(srcDir);

      themeConfig.demo = data;
    },
  };
}

const scanProjectFiles = (srcDir: string) => {};
```

这里就不详细介绍 `scanProjectFiles` 的逻辑，您可以阅读 Teek 的 Vite 插件源码来了解具体实现。

## 加载功能组件

开头说的可以将各个功能完全插件化，就是利用插件来往 VitePress 的插槽中插入组件。

Vite 提供的 `load`、`transform`、`resolveId` 等钩子，是在访问某个资源的时候被调用，比如在浏览器访问某个页面时，我们可以通过这些钩子拦截到页面的代码，然后进行内容加工再返回给浏览器渲染。

因此当进入 VitePress 页面时，我们可以拦截 VitePress 的 `Layout` 组件，然后将自己实现的组件插入到插槽中，最后返回给浏览器渲染。

VitePress 提供了哪些插槽请看 [布局插槽](https://vitepress.dev/zh/guide/extending-default-theme#layout-slots)。

比如自定义一个组件插入到 `Layout` 的 `layout-top` 插槽中。

::: code-group

```ts [index.ts]
const isESM = () => {
  return typeof __filename === "undefined" || typeof __dirname === "undefined";
};

const getDirname = () => {
  return isESM() ? dirname(fileURLToPath(import.meta.url)) : __dirname;
};

// 插件名
const componentName = "MyComponent";
const componentFile = `${componentName}.vue`;
const aliasComponentFile = `${getDirname()}/components/${componentFile}`;
const virtualModuleId = "virtual:my-component-option";
const resolvedVirtualModuleId = `\0${virtualModuleId}`;

export function VitePluginVitePressMyNotFound(
  option: {
    notFoundDelayLoad?: number;
  } = {}
): Plugin & { name: string } {
  return {
    name: "vite-plugin-vitepress-my-not-found",
    config() {
      return {
        resolve: {
          alias: {
            [`./${componentFile}`]: aliasComponentFile,
          },
        },
      };
    },
    resolveId(id: string) {
      if (id === virtualModuleId) return resolvedVirtualModuleId;
    },
    load(id: string) {
      // 使用虚拟模块将 option 传入组件里
      if (id === resolvedVirtualModuleId) return `export default ${JSON.stringify(option)}`;

      // 在 Layout.vue 插槽插入自定义组件
      if (id.endsWith("vitepress/dist/client/theme-default/Layout.vue")) {
        // 读取原始的 Vue 文件内容
        const code = readFileSync(id, "utf-8");

        // 插入自定义组件
        const slotName = "layout-top";
        const slotPosition = `<slot name="${slotName}" />`;
        const setupPosition = '<script setup lang="ts">';

        return code
          .replace(
            slotPosition,
            `<${componentName}><template #${slotName}>${slotPosition}</template></${componentName}>`
          )
          .replace(setupPosition, `${setupPosition}\nimport ${componentName} from './${componentFile}'`);
      }
    },
  };
}
```

```vue [MyComponent.vue]
<script setup lang="ts" name="MyComponent">
// @ts-ignore
import option from "virtual:my-component-option";

const { label = "myComponent" } = { ...option };
</script>

<template>
  <div>{{ label }}</div>
</template>
```

:::

插件通过虚拟模块将 `option` 配置传入到 `virtual:my-component-option` 中，因此可以在组件里引入。虚拟模块的内容请看 Vite 官网 [虚拟模块相关说明](https://cn.vite.dev/guide/api-plugin.html#virtual-modules-convention)

上面 `index.ts` 给出的代码模板具有通用性，你只需要：

* 将 `const componentName = "MyComponent";` 改为要插入的组件名
* 将 `const slotName = "layout-top";` 改为要插入的插槽名

::: tip 为什么不用 `transform` 钩子？
`transform` 钩子返回的资源内容已经过 rollup 编译过，不再是源内容，因此无法找到插槽位置，一个解决方案是使用 `load` 钩子。
:::

## unbuild 构建

unbuild 是一个用于构建库和工具的现代构建工具，由 `UnJS` 团队开发和维护。它旨在简化构建过程，提供高效的打包和构建功能，特别适用于构建 JavaScript 和 TypeScript 项目。

Teek 使用 unbuild 构建 VitePress 插件，这里仅介绍 unbuild 的 `entries` 配置项，其他 unbuild 的配置项请看 [unbuild 文档](https://unbuild.unjs.io/guide/configuration)。

::: warning
如果插件在 node 环境下运行，需要构建为 `js` 相关文件，如果在 `client` 环境下运行，则可以保留 `ts`、`vue` 等文件。

VitePress 的 `.vitepress/config.mts` 在 `node` 环境运行，因此 `config.mts` 文件引入的第三方依赖必须是 `js` 相关文件，在 `.vitepress/theme/index.ts` 文件则可以引入 `ts`、`vue` 等不需要构建的文件。
:::

### 入口文件

如果插件仅只有一个入口文件 `index.ts`，则 unbuild 的配置文件内容如下所示：

```ts
// unbuild.config.ts
import { defineBuildConfig } from "unbuild";

export default defineBuildConfig({
  entries: ["src/index"],
  // ...
});
```

等于：

```ts
// unbuild.config.ts
import { defineBuildConfig } from "unbuild";

export default defineBuildConfig({
  entries: [{ builder: "rollup", input: "src/index", outDir: "dist" }],
  // ...
});
```

后者比较灵活，可以指定输出的位置 `outDir`。

::: tip
如果您觉得 `input` 或 `outDir` 的 `src/index`、`dist` 不易于阅读，可以改成 `./src/index` 和 `./dist`。
:::

### Vue 组件

如果插件有 vue 组件，则 unbuild 的配置文件内容如下所示：

```ts
// unbuild.config.ts
import { defineBuildConfig } from "unbuild";

export default defineBuildConfig({
  entries: [
    { builder: "mkdist", input: "src/components", outDir: "dist/components", pattern: ["**/*.vue"], loaders: ["vue"] },
  ],
  // ...
});
```

`mkdist` 是一个用于构建 Vue 组件的 unbuild 插件，它将 Vue 组件转换为 CommonJS 和 ESM 格式，并支持 TypeScript，它会保留源目录解构。

因此可以不使用 `outDir` 选项，`outDir` 默认为 `dist`，因此它会自动将 components 目录下的文件复制到 dist 目录下。

### mkdist 构建多个类型文件

如果插件需要构建多个类型文件，则 unbuild 的配置文件内容如下所示：

```ts
// unbuild.config.ts
import { defineBuildConfig } from "unbuild";

export default defineBuildConfig({
  entries: [
    { builder: "mkdist", input: "src", outDir: "dist", pattern: ["**/*.ts"], format: "cjs", loaders: ["js"] },
    { builder: "mkdist", input: "src", outDir: "dist", pattern: ["**/*.ts"], format: "esm", loaders: ["js"] },
    { builder: "mkdist", input: "src", outDir: "dist", pattern: ["**/*.css"], loaders: ["postcss"] },
  ],
  // ...
});
```

### 静态目录

如果插件有一个静态文件目录 `assets` 需要复制到输出目录下，则 unbuild 的配置文件内容如下所示：

```ts
// unbuild.config.ts
import { defineBuildConfig } from "unbuild";

export default defineBuildConfig({
  entries: [{ builder: "copy", input: "src/assets", outDir: "./dist/assets" }],
  // ...
});
```

使用 `copy` 功能，`input` 只能是目录。

### 使用 rollup 插件

如果你需要一些额外的 `rollup` 插件打包，则 unbuild 的配置文件内容如下所示：

```ts
// unbuild.config.ts
import { defineBuildConfig } from "unbuild";
import RollupPlugin from "rollup-plugin";

export default defineBuildConfig({
  entries: [{ builder: "rollup", input: "src", outDir: "dist" }],
  hooks: {
    "rollup:options": (_, options) => {
      if (Array.isArray(options.plugins)) options.plugins.push(RollupPlugin);
    },
  },
  // ...
});
```

`hooks` 是 unbuild 的一个高级配置项，unbuild 会在指定的阶段调用 `hooks` 中的钩子，和 Vite 插件的钩子函数一样。

比如你希望在构建成功后，将一些文件 `copy` 到输出目录中，则可以使用 `hooks` 的 `buildEnd` 钩子，并安装 `fs-extra` 工具实现 `copy`。

```ts
import { defineBuildConfig } from "unbuild";
import { copy } from "fs-extra";

export default defineBuildConfig({
  entries: ["src/index"],
  hooks: {
    "build:done": async () => {
      await copy("src/xx.d.ts", "dist/xx.d.ts");
    },
  },
});
```

### externals

`externals` 是一个数组，用于指定不需要构建的依赖包，它将直接从外部引入，而不是构建到输出目录中。

当你使用了第三方依赖 如 vue、vite 等，需要将这些依赖添加到 `externals` 中，否则它们将被构建到输出目录中，导致依赖非常大。

```ts
// unbuild.config.ts
import { defineBuildConfig } from "unbuild";
import RollupPlugin from "rollup-plugin";

export default defineBuildConfig({
  externals: ["vue", "vite"],
  // ...
});
```

其他项目使用您的插件时，如何确保这些在 `externals` 被排除的依赖正确安装呢？毕竟没有这些依赖，插件将无法运行。

您可以在 `package.json` 的 `dependencies` 中添加这些依赖，这些第三方依赖就会跟随你的插件一起安装到项目里。

::: tip
`devDependencies` 是开发依赖，不会随着插件一起安装到项目里，因此需要您斟酌哪些第三方依赖是运行必须的，则放到 `dependencies` 里，哪些是开发时必须的，则放到 `devDependencies` 里。
:::

---

---
url: /daily/博客文档/VitPress/0_Vitpress博客搭建.md
---

# VitPress博客搭建

> 官方文档：<https://vitepress.dev/zh/>
>
> 快速上手教程：<https://vitepress.yiov.top/>

## VitePress 是什么？

VitePress 是一个[静态站点生成器](https://en.wikipedia.org/wiki/Static_site_generator) (SSG)，专为构建快速、以内容为中心的站点而设计。简而言之，VitePress 获取用 Markdown 编写的内容，对其应用主题，并生成可以轻松部署到任何地方的静态 HTML 页面。

## VitePress 与VuePres区别

## 快速开始

### 环境准备

```
nodejs version >= 18
```

### 安装

然后我们安装vitepress

```sh
npm add -D vitepress@next
# alpha版本
# npm i -D vitepress@2.0.0-alpha.6
```

### 初始化向导

VitePress 附带一个命令行设置向导，构建一个基本项目。安装后，通过运行以下命令启动向导：

```sh
npx vitepress init
```

回答几个简单的问题：

```sh
┌  Welcome to VitePress!
│
◇  Where should VitePress initialize the config?
│  ./docs
│
◇  Where should VitePress look for your markdown files?
│  ./docs
│
◇  Site title:
│  My Awesome Project
│
◇  Site description:
│  A VitePress Site
│
◇  Theme:
│  Default Theme
│
◇  Use TypeScript for config and theme files?
│  Yes
│
◇  Add VitePress npm scripts to package.json?
│  Yes
│
◇  Add a prefix for VitePress npm scripts?
│  Yes
│
◇  Prefix for VitePress npm scripts:
│  docs
│
└  Done! Now run pnpm run docs:dev and start writing.
```

### 文件结构

```markdown
.
├─ docs
│  ├─ .vitepress
│  │  └─ config.js
│  ├─ api-examples.md
│  ├─ markdown-examples.md
│  └─ index.md
└─ package.json
```

`docs` 目录作为 VitePress 站点的项目**根目录**。`.vitepress` 目录是 VitePress 配置文件、开发服务器缓存、构建输出和可选主题自定义代码的位置。

默认情况下，VitePress 将其开发服务器缓存存储在 `.vitepress/cache` 中，并将生产构建输出存储在 `.vitepress/dist` 中。如果使用 Git，应该将它们添加到 `.gitignore` 文件中。

[(3 封私信 / 80 条消息) 十分钟教会你如何使用VitePress搭建及部署个人博客站点 - 知乎](https://zhuanlan.zhihu.com/p/551291839)

[像编写文档一样轻松构建你的官网！-VitePress保姆级教程\_vitepress模板-CSDN博客](https://blog.csdn.net/qq_44793507/article/details/142521250)

从VuePress迁移至VitePress：https://notes.fe-mm.com/daily-notes/issue-37

主题扩展：https://vitepress.mosong.cc/extend/

---

---
url: /daily/博客文档/VitPress/5_Vue组件.md
---

# Vue组件

## 简介

常说的 SFC 组件，即 `Single file component` ，也就是我们的vue组件

组件是将HTML、CSS以及JavaScript封装成了一个 `*.vue` 文件

分别是：`<script>`、`<template>`、`<style>`

::: tip 说明

* JavaScript 对应：`<script>`
* HTML 对应：`<template>`
* CSS 对应：`<style>`

:::

## 安装

::: tip 说明

已安装过的无视，按 `CTRL+C` 退出开发预览模式后安装

:::

```sh
npm i vue
```

## 使用

在 `theme` 目录中 创建 `components`文件夹，然后创建 `Mycomponent.vue`

```markdown{5,6}
docs
├─ .vitepress
│  └─ config.mts
│  └─ theme
│  │   ├─ components
│  │   │   └─ Mycomponent.vue
│  │   └─ index.ts
└─ index.md
```

然后将下面代码粘贴在 `Mycomponent.vue` 中

```vue
<script setup>
// 这里是JavaScript
</script>

<template>
<!-- 这里是HTML -->
</template>

<style>
/* 这里是CSS */
</style>
```

然后，在 `theme\index.ts` 中注册全局组件

```markdown{7}
docs
├─ .vitepress
│  └─ config.mts
│  └─ theme
│  │   ├─ components
│  │   │   └─ Mycomponent.vue
│  │   └─ index.ts
└─ index.md
```

```css
/* .vitepress/theme/index.ts */
import Mycomponent from "./components/Mycomponent.vue"  // [!code focus:1]

export default {
  extends: DefaultTheme,
  enhanceApp({app}) {  // [!code focus:4]
    // 注册全局组件
    app.component('Mycomponent' , Mycomponent)
  }
}
```

## 演示

### 不蒜子

使用前请安装 [浏览量的插件：不蒜子](https://vitepress.yiov.top/plugin.html#浏览量) ，想好看自己研究一下吧

现在仅做一个简单的封装示例，在 `theme/components` 文件夹中创建 `DataPanel.vue` 组件

```markdown{6}
docs
├─ .vitepress
│  └─ config.mts
│  └─ theme
│  │   ├─ components
│  │   │   └─ DataPanel.vue
│  │   └─ index.ts
└─ index.md
```

在 `DataPanel.vue` 填入如下代码，保存

::: tip 说明

代码参考自 [ChoDocs](https://chodocs.cn/) 的早期页面，现已下架

作者使用了 [unocss](https://unocss.dev/) ，我就直接生扒下来了，凑合用吧

代码中使用了 [动态的emoji表情](https://www.emojiall.com/zh-hans/image-emoji-platform/telegram/animation)，可自行替换

:::

```vue
<script setup lang="ts">
</script>

<template>
  <div class="panel">
    <div class="container">
      <section class="grid">
        <span class="text">
          本站总访问量 <span id="busuanzi_value_site_pv" class="font-bold">--</span> 次
        </span>
        <img src="/heart.gif" alt="heart" width="50" height="50" />
        <span class="text">
          本站访客数 <span id="busuanzi_value_site_uv" class="font-bold">--</span> 人次
        </span>
      </section>
    </div>
  </div>
</template>

<style scoped>
.panel {
  margin-top: 12px;
  margin-bottom: 8px;
}

.container {
  background-color: var(--vp-c-bg-soft);
  border-radius: 8px;
  width: 100%;
  min-height: 32px;
  max-width: 1152px;
  margin-left: auto;
  margin-right: auto;
}

.grid {
  font-weight: 500;
  padding-top: 12px;
  padding-bottom: 12px;
  padding-left: 12px;
  padding-right: 12px;
  justify-items: center;
  align-items: center;
  grid-template-columns: repeat(3, minmax(0, 1fr));
  display: grid;
}

.text {
  font-size: .875rem;
  line-height: 1.25rem;
}
</style>
```

然后，在 `index.ts` 中注册全局组件

```js
/* .vitepress/theme/index.ts */
import DefaultTheme from 'vitepress/theme'
import DataPanel from "./components/DataPanel.vue"

export default {
  extends: DefaultTheme,
  enhanceApp({app}) { 
    // 注册全局组件
    app.component('DataPanel' , DataPanel)
  }
}
```

最后回到首页，插入组件看效果

```markdown
<!-- index.md -->
<DataPanel />
```

---

---
url: /daily/博客文档/Vuepress/0_Vuepress博客搭建.md
---

# Vuepress博客搭建

> 官方文档：<https://vuepress.vuejs.org/zh/>

# Vuepress搭建

`Vuepress` 是由 `Vue` 支持的静态网站生成工具，因为 `Vue` 上手起来很简单，所以 `Vuepress` 使用起来也不难。如果想快速搭建一个静态的博客网站来简单记录记录笔记或者文章，用 `Vuepress` 是个不错的选择，因为它对新手很友好。

其他一些类似的博客工具

* docsify
* docusaurus
* NotionNext

## 准备开发环境

1. git（版本管理）
2. nodejs（核心，推荐14.x，新版本启动时需要增加一些配置选项）
3. yarn（npm包管理的替代）
4. vscode/WebStorm（编译器）

## 徽章

## 自定义样式

> <http://www.taodudu.cc/news/show-3334923.html?action=onClick>
>
> <https://bbs.csdn.net/topics/607761494>
>
> <https://blog.csdn.net/qq_41327483/article/details/119103300>
>
> <https://juejin.cn/post/7242181894116573245#heading-6>
>
> <https://blog.csdn.net/weixin_42029738/article/details/125833297>
>
> <https://blog.csdn.net/howareyou2104/article/details/107412555>

新建 `.vuepress/styles/palette.styl` 文件增加样式

```css
// 侧边栏样式
// 左侧侧边栏标题
.sidebar > .sidebar-links > li > a.sidebar-link {
  font-size: 1.5em !important;
  margin-left: -1em;
}

// 右侧文章标题导航栏
a.sidebar-link {
  font-size: 10px !important;
  line-height: 0.5 !important;
}

// 左侧边栏标题字体大小样式
.sidebar-heading span {
  font-size: 1.2em;
  font-weight: bold;
}

.sidebar-heading.open span {
  font-weight: bold;
}

// 左侧边栏展开文章的字体大小
a.sidebar-link.active {
  color: #070808 !important;
  font-size: 14px !important;
  background: #c3d4b742 !important;
}

.sidebar-sub-headers a.sidebar-link {
  margin: 0 1rem 0 1rem !important;
}

// 分组的透明度修改，未生效
.sidebar-group.is-sub-group > .sidebar-heading:not(.clickable) {
  opacity: 0.5;
}

// 去除左上角标题，作者，标签等
.page .page-title {
  display: none;
}
```

## 让Google搜索到GitHub上的个人博客

<https://blog.csdn.net/weixin_44058333/article/details/100165245>

## 添加 iconfont 图标

iconfont官网下载图标：[https://www.iconfont.cn](https://www.iconfont.cn/)

方法1：

1. 首先，在iconfont网站上创建或下载需要的图标。
2. 下载完成后，会得到一份包含iconfont.css、iconfont.eot、iconfont.svg、iconfont.ttf、iconfont.woff等文件的压缩包。
3. 将这些文件放入的VuePress项目的.vuepress/public目录下。
4. 在.vuepress/config.js文件中的head属性中引入iconfont.css文件。例如：

```javascript
module.exports = {
  head: [
    ['link', { rel: 'stylesheet', href: '/iconfont.css' }]
  ],
  // 其他配置...
}
```

1. 然后，就可以在你的Markdown文件或Vue组件中使用这些图标了。例如：

```javascript
<i class="iconfont icon-example"></i>
```

注意，icon-example应该替换为图标的类名。可以在iconfont.css文件中找到这些类名。

方法2：（未成功）

参考：https://juejin.cn/post/7242181894116573245#heading-6

打包报错window is not defined：https://blog.csdn.net/Miss\_Liang/article/details/99843061

根据参考链接中的方法，直接在 `.vuepress` 下新建 `enhanceApp.js`，style目录下存放在 iconfont 官网下载的图标。一直无法成功，代码如下：

```javascript
//enhanceApp.js
//import './styles/font_20csbaofexh/iconfont.css'
//import './styles/font_cs5v8kb16mu/iconfont.css'
//const pluginsConf1 = require("./styles/font_20csbaofexh/iconfont.js");
//const pluginsConf2 = require("./styles/font_cs5v8kb16mu/iconfont.js");

module.export = {
    mounted() {
        import('./styles/font_20csbaofexh/iconfont.js').then(icon => {
        })
        import('./styles/font_cs5v8kb16mu/iconfont.js').then(icon => {
        })
    }
}
```

## 添加评论

参考：https://valine.js.org/

1. 注册 LeanCloud ，获取APP ID 和 APP Key
2. config.js 中增加插件配置

## 添加复制显示版权信息

参考：https://www.jianshu.com/p/0082676af581

## 插件

### markdown-it-disable-url-encode（中文识别）

1. 安装

```shell
npm i markdown-it-disable-url-encode
```

2. 引入

```shell
module.exports = {
  // .....
  markdown: {
    // ......
    extendMarkdown: md => {
      md.use(require("markdown-it-disable-url-encode"));
    }
  }
};
```

### vuepress-plugin-auto-sidebar（自动生成侧边栏 vuepress⭐）

> 项目地址：<https://github.com/shanyuhai123/vuepress-plugin-auto-sidebar>
>
> 项目文档：<https://shanyuhai123.github.io/vuepress-plugin-auto-sidebar/zh/>

缺点：左侧侧边栏只能识别到同一级目录下的文件，无法识别到子集

1. 安装

```shell
// VuePress v1
npm i vuepress-plugin-auto-sidebar -D

// VuePress v2
# vuepress v2 alpha(vuepress v2 仍处于测试阶段)
npm i vuepress-plugin-auto-sidebar@alpha -D
```

2. 使用

```shell
// VuePress v1
// 在 .vuepress/config.js 中配置插件
// edit .vuepress/config.js file
module.exports = {
  plugins: [
    ["vuepress-plugin-auto-sidebar", {}]
  ]
}


// VuePress v2：
// VuePress v2 不再支持插件修改 sidebar，所以你需要自行引入生成的 sidebar.js 文件。
// VuePress v2 no longer supports plugins to modify sidebar, so you need to import the generated sidebar.js file yourself.

const sidebarConf = require('./sidebar')

module.exports = {
  plugins: [
    ["vuepress-plugin-auto-sidebar", {}]
  ],
  themeConfig: {
    sidebar: sidebarConf
  }
}
```

3. 生成

```shell
# 插件扩展了 vuepress cli 来帮助快速生成简单的导航栏，package.json 中增加启动项
# vuepress v2 不支持
vuepress nav docs
```

### 代码实现自动生成侧边栏

> 1. 参考（vuepress-theme-reco）：<https://blog.csdn.net/weixin_44113868/article/details/118343085>
> 2. 参考：<https://blog.csdn.net/weixin_42068913/article/details/116207129>
> 3. 参考⭐：https://blog.csdn.net/qq\_44402184/article/details/133671540

使用参考3中的，并对其进行改造，使其支持子集目录

```javascript
// 侧边栏识别工具js
const fs = require('fs');
const path = require('path');

/**
 * 读取指定目录下的所有.md文件，按照文件名从大到小排列
 * @param relativePath 相对路径
 * @returns {string[]|*[]} 文件名数组
 */
function findMdFiles(relativePath) {
  const directoryPath = path.join(process.cwd(), relativePath);

  let mdFiles = [];
  console.log("日志1：" + directoryPath);
  try {
    const files = fs.readdirSync(directoryPath);

    files.forEach((file) => {
      const filePath = path.join(directoryPath, file);
      const stats = fs.statSync(filePath);

      if (stats.isDirectory()) {
        // mdFiles = mdFiles.concat(findMdFiles(filePath));
        const relativeFilePath = path.relative(process.cwd(), filePath);
        mdFiles = mdFiles.concat(findMdFiles(relativeFilePath));
      } else if (file.endsWith('.md') && file !== 'README.md') {
        //mdFiles.push(path.parse(file).name);
        //mdFiles.push(filePath);
        //path.relative(process.cwd(), filePath) 来获取从 relativePath 开始的路径加上 .md 文件名。
        // 修改了排序函数，使其使用 path.basename(a) 和 path.basename(b) 来获取文件名，然后再进行比较
        const relativeFilePath = path.relative(process.cwd(), filePath).replace(/\\/g, '/');
        const htmlFilePath1 = relativeFilePath.replace('.md', '');
        mdFiles.push("/" + htmlFilePath1);
      }
    });

    // 按照从大到小排序
    mdFiles.sort((a, b) => {
      const aNum = parseInt(path.basename(a).slice(1));
      const bNum = parseInt(path.basename(b).slice(1));
      return bNum - aNum;
    });
    console.log("日志2：" + mdFiles);
    return mdFiles;
  } catch (error) {
    console.error(`Error reading directory ${directoryPath}: ${error}`);
        return [];
    }
}

module.exports = {
    findMdFiles
};
```

使用时引用即可

```javascript
const path = require('./path.mts');
...
...
'/Java/java开发技巧/': [
        {
            title: '函数式编程',   // 必要的
            collapsable: false, // 可选的, 默认值是 true,
            sidebarDepth: 3,    // 可选的, 默认值是 1
            children: path.findMdFiles('/Java/java开发技巧/函数式编程/')
        },
  ]
```

### vuepress-theme-sidebar（自动生成导航栏 vuepress2.x默认主题可用）

项目地址：https://github.com/dingshaohua-cn/vuepress-theme-sidebar

### plugin-register-components（.vue文件识别）

参考：https://www.cnblogs.com/wangdashi/p/16308107.html

### vuepress-plugin-bgm-player（音乐播放器 vuepress-reco）

参考：https://github.com/vuepress-reco/vuepress-plugin-bgm-player

### 多页面生成（同一个md生成多个html页面）

参考：https://www.cnblogs.com/dingshaohua/p/15386262.html

### 添加侧边栏访问地图（vdoing主题可用）

参考：[https://wiki.eryajf.net/pages/76f813/#\_1-%E6%95%88%E6%9E%9C](https://wiki.eryajf.net/pages/76f813/#_1-效果)

### 集成element-ui

参考：https://cloud.tencent.com/developer/article/1700029

### 基于Algolia实现网站全文搜索

参考：https://zhuanlan.zhihu.com/p/549263050?utm\_id=0

### 数学公式支持

**markdown-it-texmath**

**markdown-it-katex**

**markdown-it-mathjax3**（当前使用）

**@mdit/plugin-katex**

> <https://blog.csdn.net/Flyingheart1991/article/details/126067149>
>
> <https://blog.csdn.net/m0_63748493/article/details/132354410>

## 问题

1、项目打包报错：Note: The code generator has deoptimised the styling of D:.. as it exceeds the max of 500KB

> 参考：<https://blog.csdn.net/zora_55/article/details/128797544>

在项目的根目录下创建.banelrc文件，内容为

```bash
{
   "compact": false,
   "presets": ["env", "stage-0"],
   "plugins": ["transform-runtime"]
}
```

2、Error: error:0308010C:digital envelope routines::unsupported

> 参考：<https://blog.csdn.net/zjjxxh/article/details/127173968>

命令增加：set NODE\_OPTIONS=--openssl-legacy-provider
或
降低nodejs版本

3、vite打包报错：块的大小超过限制，Some chunks are larger than 500kb after minification

# Vuepress-reco 1.0 主题

> 官方文档：<http://v1.vuepress-reco.recoluan.com/views/1.x/>

<http://zpj80231.gitee.io/znote/other/project.html>

<https://www.bookbook.cc/>

# Vuepress-reco 2.0 主题

> 官方文档：<https://vuepress-theme-reco.recoluan.com/>

# vdoing 主题

> 主题地址：<https://github.com/xugaoyi/vuepress-theme-vdoing>
>
> 示例博客：<https://m.sofineday.com>

# 示例博客

<https://zpj80231.gitee.io/znote/>

<https://www.bookbook.cc/>

<https://www.xk857.com/>

<https://www.pdai.tech/>

# 参考资料

\[1]. <https://www.bilibili.com/video/BV1vb411m7NY>

\[2]. <https://blog.csdn.net/qq_19978927/article/details/108039032>

\[3]. <https://blog.csdn.net/weixin_45732455/article/details/129940312>

\[4]. <https://segmentfault.com/a/1190000041285750>

---

---
url: /daily/博客文档/Vuepress/1_VuepressReco博客搭建.md
---

# VuepressReco博客搭建

> [vuepress-reco](https://theme-reco.vuejs.press/)

### 二、搭建流程

#### 1、环境准备

```
node version >= 20
# 安装脚手架工具
npm install @vuepress-reco/theme-cli -g
```

#### 2、使用脚手架创建项目

```
npx @vuepress-reco/theme-cli init
```

执行之后会出现以下内容，依次输入

```sh
D:\Program Files\JetBrains\java_project\my_project\ibooks>npx @vuepress-reco/theme-cli init
? Whether to create a new directory? Yes # 是否创建目录 输入 Y
? What's the name of new directory? xu_vuepress_reco2_blog # 项目目录名称
? What's the title of your project? xu_vuepress_reco2_blog # 标题（如果准备创建2.x版本，此项无效，可不填写）
? What's the description of your project? xu_vuepress_reco2_blog by vuepress-reco 2.x# 描述（如果准备创建2.x版本，此项无效，可不填写）
? What's the author's name? xxl # 作者（如果准备创建2.x版本，此项无效，可不填写）
? What style do you want your home page to be?(The 2.x version is the alpha version) 2.x # 选择2.x
√ [1/1] Load file from git

Load successful, enjoy it!

# Inter your blog
$ cd xu_vuepress_reco2_blog
# Install package
$ yarn & npm install
```

选择之后稍作等待项目就创建成功了，使用`WebStorm`或`vscode`打开该项目，执行`npm install`安装依赖，安装完成之后运行`npm run dev`，打开控制台输出的访问链接即可看到页面效果。

[vuepress-reco搭建与部署指南 - 夏慕槿苏 - 博客园](https://www.cnblogs.com/jinsulive/p/18771812)

[使用VuePress-Reco快速搭建博客(保姆级)-CSDN博客](https://blog.csdn.net/m0_58724783/article/details/140558578)

---

---
url: /StableDiffusion/WebUI/0_WebUI介绍.md
---

# WebUI介绍

WebUI是一款基于AI模型（如Stable Diffusion）的用户界面工具，允许你轻松生成图片、设计艺术作品、甚至实现多样化的创意应用。（重点！它是开源的，免费的！！！）

> 项目主页：<https://github.com/webui-dev/webui>

## Stable Diffusion WebUI安装

### 第一种方式：自主安装部署

目前，市面上基于Stable Diffusion制作的实用程序中，最受欢迎的是一个由一位越南开发者[Automatic1111](https://github.com/AUTOMATIC1111)制作的[Stable Diffusion WebUI（SD Web UI）](https://github.com/AUTOMATIC1111/stable-diffusion-webui)，提供了非常可视化的参数调节与对海量扩展应用的支持。

#### Windows安装方式

* **第一步：** 下载前置软件应用：**Python、Git、对应显卡驱动**，按视频指引完成前置安装；

  **Python官网下载（3.10.6版本）：**

  <https://www.python.org/downloads>

  **Git官网下载（最新版）：**

  <https://git-scm.com/download>

  **显卡驱动（按自己的显卡下载并更新到最新版）：**

  英伟达显卡驱动：<https://www.nvidia.cn/geforce/geforce-experience>

  AMD显卡驱动：<https://www.amd.com/zh-hans/support>

* **第二步：从代码仓库Git Clone（克隆）WebUI的本体包**；

  找一个比较空的文件夹，选中上方文件路径栏，删除原有内容，输入“cmd“后回车，**打开命令提示符**（命令行）。

  **输入如下代码地址敲回车**，从官方地址或者镜像地址克隆项目到你的电脑上：

  官方GitHub地址：

  ```jsx
  git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git
  ```

  如果连不上，使用镜像地址：

  ```jsx
  git clone https://gitee.com/nenly/stable-diffusion-webui.git
  ```

* **第三步**：下载一个大模型（Checkpoint），手动放置在根目录下的模型文件夹内；

  大模型格式为.ckpt或.safetensor，放置地址为上面的文件夹（根目录）内的/models/stable-diffusion文件夹内；

* **第四步：**双击运行**webui-user.bat**文件，自动下载部分依赖并等待安装完成；

  安装预计**总占用空间3~4G**左右，一般在30分钟内可完成。

* **第五步：** 等待安装完成，看到 **“Running on Local URL”** 一类的字样后，复制其后的链接在浏览器中打开，即可进入WebUI；

  默认链接是<http://127.0.0.1:7860/>，如不显示，可以尝试输入这个地址到浏览器中打开

* **第六步（可选）**：安装汉化文件、部分基础扩展和进阶扩展：

  ⛔ **建议安装：** 中文本地化补丁（汉化），图库浏览器，Tag Autocompletion（提示词补全）；

* 以后每次运行程序：按第四步中所示方式，双击运行webui-user.bat文件即可。

### 第二种方式：使用整合包

**“整合包”** 一般指开发者对Automatic1111制作的Stable Diffusion WebUI进行打包并使其程序化的一种方式。使用整合包，一般可以省去一些自主配置环境依赖、下载必要模型的功夫。如果你打算使用整合包，以下是一些我推荐的整合包：

---

---
url: /常用框架/SpringAIAlibaba/Agent Framework/13_Workflow工作流.md
---

# Workflow 工作流

---

---
url: /Java/架构设计/分布式/05.分布式任务调度/1_XXL-JOB介绍.md
---

# XXL-JOB介绍

> 文档：<https://www.xuxueli.com/xxl-job/>
>
> 源码地址：<https://gitee.com/xuxueli0323/xxl-job>

XXL-JOB有调度端（Admin）和执行器组成，Admin端是单独部署的主要负责调度，执行器就是各个SpringBoot工程用来执行业务功能。

## 数据库配置

首先创建数据库`xxl_job`，然后导入[SQL文件](https://gitee.com/xuxueli0323/xxl-job/blob/master/doc/db/tables_xxl_job.sql)。

## JAR包部署

下载源码后，打包部署`xxl-job-admin`及`xxl-job-executor-samples`。

部署后通过 `http://ip:8080/xxl-job-admin/`访问。

## Docker部署

```shell
docker pull xuxueli/xxl-job-admin:2.3.1

docker run -d -e PARAMS="--spring.datasource.url=jdbc:mysql://数据库ip地址:3306/xxl_job?Unicode=true&characterEncoding=UTF-8&autoReconnect=true&serverTimezone=Asia/Shanghai --spring.datasource.username=数据库账号 --spring.datasource.password=数据库密码 --xxl.job.accessToken=自己设置accesToken"  -p 8080:8080 -v  /mydata/docker/xxl-job-data:/data/applogs --name xxl-job-admin  --restart=always xuxueli/xxl-job-admin:2.3.1
```

部署后通过 `http://ip:8080/xxl-job-admin/`访问。

---

---
url: /Java/架构设计/分布式/05.分布式任务调度/2_XXL-JOB执行器配置.md
---

# XXL-JOB执行器配置

执行器配置简单来说就是在SpringBoot项目中配置XXL-JOB。

## 1、引入依赖

```xml
<dependency>
    <groupId>com.xuxueli</groupId>
    <artifactId>xxl-job-core</artifactId>
    <version>2.3.1</version>
</dependency>
```

## 2、application.yml配置文件

```yaml
xxl:
  job:
    admin:
      addresses: http://localhost:8080/xxl-job-admin
      accessToken: xxl666
    executor:
      appname: xxl-job-executor-sample # 给你的执行器起个名字
```

### 3、创建配置文件

```java
@Slf4j
@Component
public class XxlJobConfig {

    @Value("${xxl.job.admin.addresses}")
    private String adminAddresses;
    @Value("${xxl.job.admin.accessToken}")
    private String accessToken;
    @Value("${xxl.job.executor.appname}")
    private String appname;

    @Bean
    public XxlJobSpringExecutor xxlJobExecutor() {
        log.info(">>>>>>>>>>> xxl-job config init.");
        XxlJobSpringExecutor xxlJobSpringExecutor = new XxlJobSpringExecutor();
        xxlJobSpringExecutor.setAdminAddresses(adminAddresses);
        xxlJobSpringExecutor.setAppname(appname);
        xxlJobSpringExecutor.setAccessToken(accessToken);
        return xxlJobSpringExecutor;
    }

}
```
