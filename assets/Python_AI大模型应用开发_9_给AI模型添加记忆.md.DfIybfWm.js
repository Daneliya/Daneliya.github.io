import{_ as a,c as i,o as n,aN as e}from"./chunks/framework.CMIDsdwV.js";const c=JSON.parse('{"title":"给AI模型添加记忆","description":"","frontmatter":{"title":"给AI模型添加记忆","tags":["Python"],"categories":["Python"]},"headers":[],"relativePath":"Python/AI大模型应用开发/9_给AI模型添加记忆.md","filePath":"Python/AI大模型应用开发/9_给AI模型添加记忆.md","lastUpdated":1767444225000}'),l={name:"Python/AI大模型应用开发/9_给AI模型添加记忆.md"};function t(r,s,h,o,p,d){return n(),i("div",null,[...s[0]||(s[0]=[e(`<div style="display:none;" hidden="true" aria-hidden="true">Are you an LLM? You can read better optimized documentation at /Python\\AI大模型应用开发\\9_给AI模型添加记忆.md for this page in Markdown format</div><h1 id="给ai模型添加记忆" tabindex="-1">给AI模型添加记忆 <a class="header-anchor" href="#给ai模型添加记忆" aria-label="Permalink to &quot;给AI模型添加记忆&quot;">​</a></h1><h2 id="一、memory—让ai模型不再忘掉对话" tabindex="-1">一、Memory—让AI模型不再忘掉对话 <a class="header-anchor" href="#一、memory—让ai模型不再忘掉对话" aria-label="Permalink to &quot;一、Memory—让AI模型不再忘掉对话&quot;">​</a></h2><h3 id="_1、核心问题-模型缺乏上下文记忆" tabindex="-1">1、核心问题：模型缺乏上下文记忆 <a class="header-anchor" href="#_1、核心问题-模型缺乏上下文记忆" aria-label="Permalink to &quot;1、核心问题：模型缺乏上下文记忆&quot;">​</a></h3><p>AI 模型本身不具备 “上文记忆” 能力，若仅单次传递当前提示（如先问 “李白是谁”，再问 “他是哪国人”），模型会因缺失前序对话信息而无法正确响应。</p><p><strong>解决方案</strong>：将历史对话记录存入 “消息列表”，每轮对话时将 “历史消息 + 当前提示” 一并传给模型，让模型基于完整上下文生成回应。</p><h3 id="_2、关键工具-conversationbuffermemory-对话缓冲记忆" tabindex="-1">2、关键工具：ConversationBufferMemory（对话缓冲记忆） <a class="header-anchor" href="#_2、关键工具-conversationbuffermemory-对话缓冲记忆" aria-label="Permalink to &quot;2、关键工具：ConversationBufferMemory（对话缓冲记忆）&quot;">​</a></h3><p>LangChain 的<code>memory</code>模块提供<code>ConversationBufferMemory</code>类，专门用于储存和管理历史对话，核心功能如下：</p><h4 id="_2-1-初始化记忆实例" tabindex="-1">2.1. 初始化记忆实例 <a class="header-anchor" href="#_2-1-初始化记忆实例" aria-label="Permalink to &quot;2.1. 初始化记忆实例&quot;">​</a></h4><p>需设置<code>return_messages=True</code>：确保储存的历史对话以 “消息对象列表” 形式存在（而非字符串），便于后续拼接进提示。</p><p>示例：</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> langchain.memory </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> ConversationBufferMemory</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">memory </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> ConversationBufferMemory(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">return_messages</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><h4 id="_2-2-查看记忆内容" tabindex="-1">2.2. 查看记忆内容 <a class="header-anchor" href="#_2-2-查看记忆内容" aria-label="Permalink to &quot;2.2. 查看记忆内容&quot;">​</a></h4><p>调用<code>load_memory_variables</code>方法（传入空字典），可查看当前记忆中储存的历史对话，初始时<code>history</code>对应空列表：</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">memory.load_memory_variables({})  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 输出：{&quot;history&quot;: []}</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="_2-3-储存历史对话" tabindex="-1">2.3. 储存历史对话 <a class="header-anchor" href="#_2-3-储存历史对话" aria-label="Permalink to &quot;2.3. 储存历史对话&quot;">​</a></h4><p>通过<code>save_context</code>方法手动储存单轮对话（用户输入 + AI 输出），参数为两个字典（分别对应用户和 AI 的内容）：</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 储存“用户问李白是谁”和“AI的回应”</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">memory.save_context(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    {</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;input&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;李白是谁？&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">},</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    {</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;output&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;李白是唐代著名诗人，被誉为‘诗仙’。&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>再次查看记忆时，<code>history</code>列表会包含该轮对话的消息对象。</p><h3 id="_3、构建带记忆的提示模板" tabindex="-1">3、构建带记忆的提示模板 <a class="header-anchor" href="#_3、构建带记忆的提示模板" aria-label="Permalink to &quot;3、构建带记忆的提示模板&quot;">​</a></h3><p>需在提示模板中预留 “历史对话” 的位置，确保历史消息能拼接在当前提示前（系统消息需放在最前面），关键依赖<code>MessagesPlaceholder</code>：</p><h4 id="_3-1-用-messagesplaceholder-占位历史消息" tabindex="-1">3.1. 用 MessagesPlaceholder 占位历史消息 <a class="header-anchor" href="#_3-1-用-messagesplaceholder-占位历史消息" aria-label="Permalink to &quot;3.1. 用 MessagesPlaceholder 占位历史消息&quot;">​</a></h4><p><code>langchain.prompts</code>模块的<code>MessagesPlaceholder</code>类，用于在提示模板中为 “历史消息列表” 占位，需指定<code>variable_name=&quot;history&quot;</code>（与记忆中储存历史的键名一致）。</p><h4 id="_3-2-完整提示模板结构" tabindex="-1">3.2. 完整提示模板结构 <a class="header-anchor" href="#_3-2-完整提示模板结构" aria-label="Permalink to &quot;3.2. 完整提示模板结构&quot;">​</a></h4><p>示例（系统消息 + 历史消息占位 + 当前用户提示）：</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> langchain.prompts </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> ChatPromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate, SystemMessagePromptTemplate</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">prompt </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> ChatPromptTemplate.from_messages([</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    SystemMessagePromptTemplate.from_template(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;你是一个知识问答助手，基于上下文回答问题。&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">),</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    MessagesPlaceholder(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">variable_name</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;history&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">),  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 历史消息占位</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    HumanMessagePromptTemplate.from_template(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{user_input}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 当前用户输入</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">])</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><h3 id="_4、构建带记忆的对话链-手动实现" tabindex="-1">4、构建带记忆的对话链（手动实现） <a class="header-anchor" href="#_4、构建带记忆的对话链-手动实现" aria-label="Permalink to &quot;4、构建带记忆的对话链（手动实现）&quot;">​</a></h3><h4 id="_4-1-组件串联逻辑" tabindex="-1">4.1. 组件串联逻辑 <a class="header-anchor" href="#_4-1-组件串联逻辑" aria-label="Permalink to &quot;4.1. 组件串联逻辑&quot;">​</a></h4><ol><li>从<code>memory</code>中加载历史对话（<code>history</code>列表）。</li><li>将 “历史对话 + 当前用户输入” 传入提示模板，生成完整提示。</li><li>调用模型生成回应。</li><li>用<code>memory.save_context</code>储存本轮 “用户输入 + AI 回应”，更新历史记忆。</li></ol><h4 id="_4-2-示例流程" tabindex="-1">4.2. 示例流程 <a class="header-anchor" href="#_4-2-示例流程" aria-label="Permalink to &quot;4.2. 示例流程&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 1. 加载历史对话</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">history </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> memory.load_memory_variables({})[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;history&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 2. 生成完整提示（历史+当前输入）</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">user_input </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;他是哪国人？&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">prompt_value </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> prompt.invoke({</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;history&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: history, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;user_input&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: user_input})</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 3. 模型生成回应（假设model为已定义的聊天模型）</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">response </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.invoke(prompt_value)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 4. 储存本轮对话到记忆</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">memory.save_context({</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;input&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: user_input}, {</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;output&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: response.content})</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><p>手动实现需反复调用 “加载记忆 - 生成提示 - 储存记忆”，流程较繁琐。LangChain 已提供<strong>现成的带记忆对话链</strong>（无需手动管理记忆）。</p><h2 id="二、conversationchain—开箱即用的带记忆对话链" tabindex="-1">二、ConversationChain—开箱即用的带记忆对话链 <a class="header-anchor" href="#二、conversationchain—开箱即用的带记忆对话链" aria-label="Permalink to &quot;二、ConversationChain—开箱即用的带记忆对话链&quot;">​</a></h2><h3 id="_1、conversationchain-简化带记忆的对话实现" tabindex="-1">1、ConversationChain：简化带记忆的对话实现 <a class="header-anchor" href="#_1、conversationchain-简化带记忆的对话实现" aria-label="Permalink to &quot;1、ConversationChain：简化带记忆的对话实现&quot;">​</a></h3><p>LangChain 的<code>chains</code>模块提供<code>ConversationChain</code>（对话链），是专门用于实现 “带上下文记忆对话” 的现成工具。它已封装好 “记忆加载、提示拼接、模型调用、记忆更新” 的完整流程，无需手动管理历史对话，大幅简化代码。</p><h3 id="_2、核心使用步骤" tabindex="-1">2、核心使用步骤 <a class="header-anchor" href="#_2、核心使用步骤" aria-label="Permalink to &quot;2、核心使用步骤&quot;">​</a></h3><h4 id="_2-1-准备依赖组件" tabindex="-1">2.1. 准备依赖组件 <a class="header-anchor" href="#_2-1-准备依赖组件" aria-label="Permalink to &quot;2.1. 准备依赖组件&quot;">​</a></h4><p>需提前创建两个核心组件，作为<code>ConversationChain</code>的参数：</p><ul><li><strong>聊天模型（如 ChatModel）</strong>：用于生成对话回应。</li><li><strong>记忆实例（如 ConversationBufferMemory）</strong>：用于储存历史对话，需确保<code>return_messages=True</code>（储存为消息列表）。</li></ul><p>示例：</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> langchain.chat_models </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> ChatOpenAI</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> langchain.memory </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> ConversationBufferMemory</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 1. 创建模型</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> ChatOpenAI()</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 2. 创建记忆（自动储存历史对话）</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">memory </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> ConversationBufferMemory(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">return_messages</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><h4 id="_2-2-初始化-conversationchain" tabindex="-1">2.2. 初始化 ConversationChain <a class="header-anchor" href="#_2-2-初始化-conversationchain" aria-label="Permalink to &quot;2.2. 初始化 ConversationChain&quot;">​</a></h4><p>通过<code>ConversationChain</code>类，传入<code>llm</code>（模型）和<code>memory</code>（记忆）参数，即可完成对话链构建：</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> langchain.chains </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> ConversationChain</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 构建对话链</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">conversation_chain </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> ConversationChain(</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    llm</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">model,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    memory</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">memory</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><h4 id="_2-3-调用对话链实现多轮对话" tabindex="-1">2.3. 调用对话链实现多轮对话 <a class="header-anchor" href="#_2-3-调用对话链实现多轮对话" aria-label="Permalink to &quot;2.3. 调用对话链实现多轮对话&quot;">​</a></h4><p>通过<code>invoke</code>方法与链交互，参数为含<code>input</code>键的字典（<code>input</code>对应当前用户提示）：</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 第一轮对话</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">response1 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> conversation_chain.invoke({</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;input&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;李白是谁？&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">})</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(response1[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;response&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">])  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 输出AI对“李白是谁”的回应</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 第二轮对话（基于上下文）</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">response2 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> conversation_chain.invoke({</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;input&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;他是哪国人？&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">})</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(response2[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;response&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">])  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 输出AI基于“李白”上下文的回应（如“中国人，唐代人”）</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><h3 id="_3、conversationchain-的核心优势" tabindex="-1">3、ConversationChain 的核心优势 <a class="header-anchor" href="#_3、conversationchain-的核心优势" aria-label="Permalink to &quot;3、ConversationChain 的核心优势&quot;">​</a></h3><p>相比手动构建带记忆的链，<code>ConversationChain</code>自动完成以下操作，无需手动干预：</p><ol><li><strong>自动加载历史记忆</strong>：调用时无需手动调用<code>load_memory_variables</code>，链会自动从<code>memory</code>中读取历史对话。</li><li><strong>自动更新记忆</strong>：每轮对话后，无需手动调用<code>save_context</code>，链会自动将 “当前用户输入 + AI 回应” 存入<code>memory</code>。</li><li><strong>简化调用逻辑</strong>：仅需传递当前用户提示（<code>input</code>），即可实现带上下文的连续对话。</li></ol><h3 id="_4、自定义提示模板-可选" tabindex="-1">4、自定义提示模板（可选） <a class="header-anchor" href="#_4、自定义提示模板-可选" aria-label="Permalink to &quot;4、自定义提示模板（可选）&quot;">​</a></h3><p><code>ConversationChain</code>支持通过<code>prompt</code>参数自定义提示模板（如设定 AI 人设），但需注意<strong>变量名必须匹配链的预期</strong>：</p><ul><li>表示 “用户输入” 的变量名：必须为<code>input</code>。</li><li>表示 “历史对话” 的变量名：必须为<code>history</code>。</li></ul><p>示例（设定 “脾气暴躁的助手” 人设）：</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> langchain.prompts </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> ChatPromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate, SystemMessagePromptTemplate</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 自定义提示模板</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">custom_prompt </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> ChatPromptTemplate.from_messages([</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    SystemMessagePromptTemplate.from_template(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;你是一个脾气暴躁、喜欢阴阳怪气的助手，回答简洁且带讽刺感。&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">),</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    MessagesPlaceholder(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">variable_name</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;history&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">),  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 历史对话变量名：history</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    HumanMessagePromptTemplate.from_template(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{input}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 用户输入变量名：input</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">])</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 用自定义模板初始化对话链</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">conversation_chain </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> ConversationChain(</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    llm</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">model,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    memory</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">memory,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    prompt</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">custom_prompt</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br></div></div><h2 id="三、memory—记忆咋还有不同类型" tabindex="-1">三、Memory—记忆咋还有不同类型？ <a class="header-anchor" href="#三、memory—记忆咋还有不同类型" aria-label="Permalink to &quot;三、Memory—记忆咋还有不同类型？&quot;">​</a></h2><p>在 LangChain 中，对话记忆（Memory）是实现多轮上下文对话的核心组件，不同记忆类型的设计目标、储存逻辑和适用场景差异显著。除了基础的<code>ConversationBufferMemory</code>，还有 4 种常用记忆类型，以下从<strong>核心原理、关键参数、优劣势、适用场景</strong>四个维度展开整理：</p><h3 id="_1、基础型记忆-conversationbuffermemory-对话缓冲记忆" tabindex="-1">1、基础型记忆：ConversationBufferMemory（对话缓冲记忆） <a class="header-anchor" href="#_1、基础型记忆-conversationbuffermemory-对话缓冲记忆" aria-label="Permalink to &quot;1、基础型记忆：ConversationBufferMemory（对话缓冲记忆）&quot;">​</a></h3><h4 id="核心原理" tabindex="-1">核心原理 <a class="header-anchor" href="#核心原理" aria-label="Permalink to &quot;核心原理&quot;">​</a></h4><p>最基础的记忆类型，<strong>完整储存所有历史对话消息</strong>（人类消息 + AI 消息），不做任何截断或总结，每轮对话后直接追加新消息到记忆列表中。</p><h4 id="关键特点" tabindex="-1">关键特点 <a class="header-anchor" href="#关键特点" aria-label="Permalink to &quot;关键特点&quot;">​</a></h4><ul><li>无额外参数，初始化时仅需指定<code>return_messages=True</code>（确保储存为消息对象列表，而非字符串）。</li><li>优点：<strong>无信息丢失</strong>，储存逻辑简单直接，适合对话轮数少、消息短的场景。</li><li>缺点： <ol><li>随对话轮数增加，历史消息列表持续变长，消耗 Token 量线性上升；</li><li>一旦消息总 Token 数超过模型上下文窗口上限，需手动截断，否则无法传入模型。</li></ol></li></ul><h4 id="适用场景" tabindex="-1">适用场景 <a class="header-anchor" href="#适用场景" aria-label="Permalink to &quot;适用场景&quot;">​</a></h4><ul><li>短对话场景（如 3-5 轮内）；</li><li>对对话细节完整性要求极高，不允许任何信息丢失的场景（如精准问答、指令复现）。</li></ul><h3 id="_2、窗口型记忆-conversationbufferwindowmemory-对话缓冲窗口记忆" tabindex="-1">2、窗口型记忆：ConversationBufferWindowMemory（对话缓冲窗口记忆） <a class="header-anchor" href="#_2、窗口型记忆-conversationbufferwindowmemory-对话缓冲窗口记忆" aria-label="Permalink to &quot;2、窗口型记忆：ConversationBufferWindowMemory（对话缓冲窗口记忆）&quot;">​</a></h3><h4 id="核心原理-1" tabindex="-1">核心原理 <a class="header-anchor" href="#核心原理-1" aria-label="Permalink to &quot;核心原理&quot;">​</a></h4><p>在<code>ConversationBufferMemory</code>基础上增加 “窗口限制”，<strong>仅储存最近 K 轮对话</strong>（“一轮对话” 指 “人类提问 + AI 回应” 的完整交互，而非单条消息），超过 K 轮的早期对话直接丢弃。</p><h4 id="关键参数" tabindex="-1">关键参数 <a class="header-anchor" href="#关键参数" aria-label="Permalink to &quot;关键参数&quot;">​</a></h4><ul><li><code>k</code>：窗口尺寸（必填），表示最多保留的对话轮数（如<code>k=2</code>即保留最近 2 轮对话）。</li></ul><h4 id="优劣势" tabindex="-1">优劣势 <a class="header-anchor" href="#优劣势" aria-label="Permalink to &quot;优劣势&quot;">​</a></h4><ul><li>优点： <ol><li>主动控制历史消息长度，避免 Token 消耗过快；</li><li>无需手动截断，可长期维持对话（只要 K 值合理），实现 “轻量化上下文”。</li></ol></li><li>缺点：<strong>超过 K 轮的早期信息直接丢失</strong>，若后续对话需引用更早内容，模型会 “失忆”（如<code>k=1</code>时，模型仅记得上一轮对话，更早的信息无法调用）。</li></ul><h4 id="适用场景-1" tabindex="-1">适用场景 <a class="header-anchor" href="#适用场景-1" aria-label="Permalink to &quot;适用场景&quot;">​</a></h4><ul><li>对话轮数较多，但仅需参考近期上下文的场景（如日常闲聊、短期任务交互）；</li><li>模型上下文窗口较小，需严格控制 Token 消耗的场景。</li></ul><h3 id="_3、总结型记忆-conversationsummarymemory-对话总结记忆" tabindex="-1">3、总结型记忆：ConversationSummaryMemory（对话总结记忆） <a class="header-anchor" href="#_3、总结型记忆-conversationsummarymemory-对话总结记忆" aria-label="Permalink to &quot;3、总结型记忆：ConversationSummaryMemory（对话总结记忆）&quot;">​</a></h3><h4 id="核心原理-2" tabindex="-1">核心原理 <a class="header-anchor" href="#核心原理-2" aria-label="Permalink to &quot;核心原理&quot;">​</a></h4><p>不直接储存原始历史消息，而是<strong>通过大模型对历史对话进行总结</strong>，仅储存总结后的文本；每轮新对话后，会将新消息融入原有总结，更新为更完整的总结内容。</p><h4 id="关键参数-1" tabindex="-1">关键参数 <a class="header-anchor" href="#关键参数-1" aria-label="Permalink to &quot;关键参数&quot;">​</a></h4><ul><li><code>llm</code>：用于生成总结的大模型（必填），需传入 LangChain 支持的 LLM / 聊天模型实例（如<code>ChatOpenAI</code>）。</li></ul><h4 id="优劣势-1" tabindex="-1">优劣势 <a class="header-anchor" href="#优劣势-1" aria-label="Permalink to &quot;优劣势&quot;">​</a></h4><ul><li>优点： <ol><li>总结文本通常比原始消息更短，大幅降低 Token 消耗，延缓达到上下文窗口上限的时间；</li><li>不直接丢弃早期信息，通过总结保留核心逻辑（如长期对话中的关键需求、任务目标）。</li></ol></li><li>缺点： <ol><li>总结过程依赖大模型，<strong>额外消耗 Token</strong>（生成总结本身需调用模型）；</li><li>总结可能丢失细节（如具体数值、语气、小众信息），适合 “抓核心” 而非 “记细节” 的场景。</li></ol></li></ul><h4 id="适用场景-2" tabindex="-1">适用场景 <a class="header-anchor" href="#适用场景-2" aria-label="Permalink to &quot;适用场景&quot;">​</a></h4><ul><li>长对话场景（如 10 轮以上），需保留整体逻辑但无需细节的交互（如项目需求沟通、问题分析）；</li><li>对 Token 消耗敏感，且可接受少量细节丢失的场景。</li></ul><h3 id="_4、混合总结型记忆-conversationsummarybufferedmemory-对话总结缓冲记忆" tabindex="-1">4、混合总结型记忆：ConversationSummaryBufferedMemory（对话总结缓冲记忆） <a class="header-anchor" href="#_4、混合总结型记忆-conversationsummarybufferedmemory-对话总结缓冲记忆" aria-label="Permalink to &quot;4、混合总结型记忆：ConversationSummaryBufferedMemory（对话总结缓冲记忆）&quot;">​</a></h3><h4 id="核心原理-3" tabindex="-1">核心原理 <a class="header-anchor" href="#核心原理-3" aria-label="Permalink to &quot;核心原理&quot;">​</a></h4><p>结合<code>ConversationBufferMemory</code>（原始储存）和<code>ConversationSummaryMemory</code>（总结储存）的逻辑：</p><ol><li>当储存的原始消息 Token 数未超过上限时，<strong>直接保留原始消息</strong>（保证近期细节不丢失）；</li><li>当 Token 数达到上限后，<strong>对更早的消息进行总结</strong>，近期的消息仍保留原始内容（仅压缩远期信息，保留近期细节）。</li></ol><h4 id="关键参数-2" tabindex="-1">关键参数 <a class="header-anchor" href="#关键参数-2" aria-label="Permalink to &quot;关键参数&quot;">​</a></h4><ul><li><code>llm</code>：用于生成总结的大模型（必填）；</li><li><code>max_token_limit</code>：储存消息的 Token 上限（必填），超过该值时触发远期消息总结。</li></ul><h4 id="优劣势-2" tabindex="-1">优劣势 <a class="header-anchor" href="#优劣势-2" aria-label="Permalink to &quot;优劣势&quot;">​</a></h4><ul><li>优点： <ol><li>兼顾 “细节保留” 和 “Token 控制”：近期消息用原始内容（记细节），远期消息用总结（省 Token）；</li><li>不直接丢弃任何信息，通过总结压缩远期内容，核心逻辑不丢失。</li></ol></li><li>缺点： <ol><li>总结过程仍需额外消耗 Token；</li><li>逻辑较复杂，初始化需配置模型和 Token 上限，对参数设置有一定要求。</li></ol></li></ul><h4 id="适用场景-3" tabindex="-1">适用场景 <a class="header-anchor" href="#适用场景-3" aria-label="Permalink to &quot;适用场景&quot;">​</a></h4><ul><li>中长对话场景（如 5-15 轮），既需保留近期细节，又需控制整体 Token 消耗（如客户服务、复杂任务协作）；</li><li>对对话细节精度有要求，但可接受远期信息简化的场景。</li></ul><h3 id="_5、token-限制型记忆-conversationtokenbufferedmemory-对话-token-缓冲记忆" tabindex="-1">5、Token 限制型记忆：ConversationTokenBufferedMemory（对话 Token 缓冲记忆） <a class="header-anchor" href="#_5、token-限制型记忆-conversationtokenbufferedmemory-对话-token-缓冲记忆" aria-label="Permalink to &quot;5、Token 限制型记忆：ConversationTokenBufferedMemory（对话 Token 缓冲记忆）&quot;">​</a></h3><h4 id="核心原理-4" tabindex="-1">核心原理 <a class="header-anchor" href="#核心原理-4" aria-label="Permalink to &quot;核心原理&quot;">​</a></h4><p>与<code>ConversationBufferWindowMemory</code>类似，均直接储存原始消息，但<strong>以 “Token 数” 为限制条件</strong>：当储存的所有消息总 Token 数超过设定上限时，自动丢弃最早的消息，直到总 Token 数低于上限。</p><h4 id="关键参数-3" tabindex="-1">关键参数 <a class="header-anchor" href="#关键参数-3" aria-label="Permalink to &quot;关键参数&quot;">​</a></h4><ul><li><code>max_token_limit</code>：储存消息的 Token 上限（必填），需参考模型上下文窗口大小设置（如模型窗口为 4096Token，可设<code>max_token_limit=3000</code>，预留空间给新提示）。</li></ul><h4 id="优劣势-3" tabindex="-1">优劣势 <a class="header-anchor" href="#优劣势-3" aria-label="Permalink to &quot;优劣势&quot;">​</a></h4><ul><li>优点： <ol><li>直接对齐模型的 “Token 窗口” 概念，无需估算对话轮数，更精准控制 Token 消耗；</li><li>保留原始消息，不丢失细节（只要未被丢弃），适合对细节敏感的场景。</li></ol></li><li>缺点：<strong>超过 Token 上限的早期消息直接丢弃</strong>，若远期信息重要则会 “失忆”；且需提前了解模型 Token 窗口大小，参数设置依赖模型特性。</li></ul><h4 id="适用场景-4" tabindex="-1">适用场景 <a class="header-anchor" href="#适用场景-4" aria-label="Permalink to &quot;适用场景&quot;">​</a></h4><ul><li>消息长度差异大（如部分消息长、部分消息短），需按 Token 精准控制的场景；</li><li>对消息细节要求高，且模型上下文窗口明确的场景（如技术文档问答、代码协作）。</li></ul><h3 id="_6、5-种记忆类型对比总表" tabindex="-1">6、5 种记忆类型对比总表 <a class="header-anchor" href="#_6、5-种记忆类型对比总表" aria-label="Permalink to &quot;6、5 种记忆类型对比总表&quot;">​</a></h3><table tabindex="0"><thead><tr><th>记忆类型</th><th>核心逻辑</th><th>关键参数</th><th>优点</th><th>缺点</th><th>适用场景</th></tr></thead><tbody><tr><td>ConversationBufferMemory</td><td>完整储存所有原始消息</td><td>-</td><td>无信息丢失，逻辑简单</td><td>Token 消耗快，易超窗口</td><td>短对话、需完整细节</td></tr><tr><td>ConversationBufferWindowMemory</td><td>储存最近 K 轮原始消息</td><td><code>k</code>（轮数）</td><td>轻量化，控制 Token</td><td>超 K 轮信息丢失</td><td>多轮对话、仅需近期上下文</td></tr><tr><td>ConversationSummaryMemory</td><td>储存历史对话总结</td><td><code>llm</code>（总结模型）</td><td>省 Token，保留核心逻辑</td><td>丢细节，额外 Token 消耗</td><td>长对话、需核心逻辑</td></tr><tr><td>ConversationSummaryBufferedMemory</td><td>近期原始 + 远期总结（超 Token 上限触发）</td><td><code>llm</code>、<code>max_token_limit</code></td><td>保近期细节 + 省 Token</td><td>逻辑复杂，额外 Token 消耗</td><td>中长对话、需细节 + 控 Token</td></tr><tr><td>ConversationTokenBufferedMemory</td><td>储存原始消息（超 Token 上限丢最早）</td><td><code>max_token_limit</code></td><td>精准控 Token，保细节</td><td>超上限信息丢失</td><td>消息长度不均、需细节</td></tr></tbody></table><p>通过选择适配场景的记忆类型，可在 “上下文完整性”“Token 消耗”“细节保留” 三者间找到平衡，实现高效、稳定的多轮对话。</p>`,104)])])}const u=a(l,[["render",t]]);export{c as __pageData,u as default};
