import{_ as a,c as r,o as l,aN as e}from"./chunks/framework.CMIDsdwV.js";const p=JSON.parse('{"title":"AI大模型科普","titleTemplate":"AI大模型科普","description":"","frontmatter":{"title":"AI大模型科普","titleTemplate":"AI大模型科普","deep":false,"tags":["Python"],"categories":["Python"]},"headers":[],"relativePath":"Python/AI大模型应用开发/1_AI大模型科普.md","filePath":"Python/AI大模型应用开发/1_AI大模型科普.md","lastUpdated":1767444225000}'),n={name:"Python/AI大模型应用开发/1_AI大模型科普.md"};function o(i,t,s,d,h,u){return l(),r("div",null,[...t[0]||(t[0]=[e(`<div style="display:none;" hidden="true" aria-hidden="true">Are you an LLM? You can read better optimized documentation at /Python\\AI大模型应用开发\\1_AI大模型科普.md for this page in Markdown format</div><h1 id="ai大模型科普" tabindex="-1">AI大模型科普 <a class="header-anchor" href="#ai大模型科普" aria-label="Permalink to &quot;AI大模型科普&quot;">​</a></h1><h2 id="一、啥是-aigc-及一系列ai技术词" tabindex="-1">一、啥是“AIGC”及一系列AI技术词 <a class="header-anchor" href="#一、啥是-aigc-及一系列ai技术词" aria-label="Permalink to &quot;一、啥是“AIGC”及一系列AI技术词&quot;">​</a></h2><h3 id="_1、什么是-aigc" tabindex="-1">1、什么是 AIGC？ <a class="header-anchor" href="#_1、什么是-aigc" aria-label="Permalink to &quot;1、什么是 AIGC？&quot;">​</a></h3><p><strong>AIGC</strong> 是 <strong>AI-Generated Content</strong> 的缩写，意为“<strong>人工智能生成内容</strong>”。</p><ul><li>它指的是由 AI 自动生成的文字、图片、音频、视频、代码等内容。</li><li>例如： <ul><li>用 ChatGPT 写文章 ✍️</li><li>用 Midjourney 生成图像 🖼️</li><li>用 GitHub Copilot 写代码 💻</li><li>用 Sora 生成视频 🎥</li></ul></li><li>这些都属于 <strong>AIGC 的范畴</strong>。</li></ul><blockquote><p>🔍 小知识：虽然“AIGC”在中国更流行，但在国际上更常用的是 <strong>Generative AI（生成式 AI）</strong>。两者本质相同，但语境略有差异。</p></blockquote><h3 id="_2、aigc-与-生成式-ai-的关系" tabindex="-1">2、AIGC 与 生成式 AI 的关系 <a class="header-anchor" href="#_2、aigc-与-生成式-ai-的关系" aria-label="Permalink to &quot;2、AIGC 与 生成式 AI 的关系&quot;">​</a></h3><table tabindex="0"><thead><tr><th>术语</th><th>含义</th><th>关系</th></tr></thead><tbody><tr><td><strong>生成式 AI (Generative AI)</strong></td><td>能够“创造”新内容的 AI 技术</td><td>是“工具”或“能力”</td></tr><tr><td><strong>AIGC</strong></td><td>由生成式 AI 创造出的内容本身</td><td>是“产物”或“结果”</td></tr></tbody></table><p>✅ 所以：</p><blockquote><p><strong>生成式 AI → 生成 → AIGC</strong></p></blockquote><p>👉 比如：<strong>ChatGPT、Midjourney、Stable Diffusion</strong> 都是生成式 AI 模型，它们输出的内容就是 AIGC。</p><h3 id="_3、ai-的大框架-从人工智能到大模型" tabindex="-1">3、AI 的大框架：从人工智能到大模型 <a class="header-anchor" href="#_3、ai-的大框架-从人工智能到大模型" aria-label="Permalink to &quot;3、AI 的大框架：从人工智能到大模型&quot;">​</a></h3><p>为了理清这些概念，我们需要从宏观角度理解 AI 的层级结构。</p><h4 id="📊-ai-的-家族树-结构" tabindex="-1">📊 AI 的“家族树”结构 <a class="header-anchor" href="#📊-ai-的-家族树-结构" aria-label="Permalink to &quot;📊 AI 的“家族树”结构&quot;">​</a></h4><div class="language- vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>                    人工智能 (AI)</span></span>
<span class="line"><span>                         ↓</span></span>
<span class="line"><span>                   机器学习 (ML)</span></span>
<span class="line"><span>                  ↙       ↓        ↘</span></span>
<span class="line"><span>            监督学习    无监督学习    强化学习</span></span>
<span class="line"><span>                         ↓</span></span>
<span class="line"><span>                    深度学习 (DL)</span></span>
<span class="line"><span>                  ↙              ↘</span></span>
<span class="line"><span>          生成式 AI           大语言模型 (LLM)</span></span>
<span class="line"><span>                ↘               ↙</span></span>
<span class="line"><span>                 AIGC（AI生成内容）</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>下面我们逐层解析：</p><hr><h4 id="_1-什么是人工智能-ai" tabindex="-1">1. 什么是人工智能（AI）？ <a class="header-anchor" href="#_1-什么是人工智能-ai" aria-label="Permalink to &quot;1. 什么是人工智能（AI）？&quot;">​</a></h4><ul><li><strong>定义</strong>：让机器模拟人类智能行为的技术，如理解语言、识别图像、推理决策等。</li><li>自 1956 年达特茅斯会议确立为独立学科以来，历经多次“寒冬”与“爆发”。</li></ul><hr><h4 id="_2-什么是机器学习-machine-learning-ml" tabindex="-1">2. 什么是机器学习（Machine Learning, ML）？ <a class="header-anchor" href="#_2-什么是机器学习-machine-learning-ml" aria-label="Permalink to &quot;2. 什么是机器学习（Machine Learning, ML）？&quot;">​</a></h4><ul><li><strong>核心思想</strong>：不靠人工编写规则，而是让计算机通过数据“自己学习”规律。</li><li>❌ 传统方式（非机器学习）： <ul><li>“如果图片有红色 → 是玫瑰；有橙色 → 是向日葵” → 明确编程逻辑。</li></ul></li><li>✅ 机器学习方式： <ul><li>给大量带标签的花的照片（输入 + 正确答案），让模型自己找规律，预测新图是什么花。</li></ul></li></ul><p>机器学习三大范式：</p><table tabindex="0"><thead><tr><th>类型</th><th>特点</th><th>应用举例</th></tr></thead><tbody><tr><td><strong>监督学习</strong></td><td>数据带“标签”（正确答案）</td><td>图像分类、房价预测</td></tr><tr><td><strong>无监督学习</strong></td><td>数据无标签，模型自主发现模式</td><td>新闻聚类、用户分群</td></tr><tr><td><strong>强化学习</strong></td><td>通过“奖励/惩罚”反馈学习最优策略</td><td>游戏 AI、机器人控制</td></tr></tbody></table><blockquote><p>🐶 类比：就像训练小狗，做对了给零食，做错了不给，逐渐学会听话。</p></blockquote><hr><h4 id="_3-什么是深度学习-deep-learning" tabindex="-1">3. 什么是深度学习（Deep Learning）？ <a class="header-anchor" href="#_3-什么是深度学习-deep-learning" aria-label="Permalink to &quot;3. 什么是深度学习（Deep Learning）？&quot;">​</a></h4><ul><li>是机器学习的一种方法，核心是使用<strong>人工神经网络</strong>（模仿人脑结构）。</li><li>“深度”指网络有很多层（输入层 → 多个隐藏层 → 输出层）。</li><li>每一层提取更复杂的特征： <ul><li>第一层：边缘</li><li>第二层：形状</li><li>第三层：器官（如眼睛、耳朵）</li><li>最终：判断是否是“猫”</li></ul></li></ul><blockquote><p>✅ 深度学习可应用于监督、无监督、强化学习。</p></blockquote><hr><h4 id="_4-什么是生成式-ai" tabindex="-1">4. 什么是生成式 AI？ <a class="header-anchor" href="#_4-什么是生成式-ai" aria-label="Permalink to &quot;4. 什么是生成式 AI？&quot;">​</a></h4><ul><li>是深度学习的一个重要应用方向。</li><li>目标：<strong>学习已有数据的模式，生成全新的、类似的内容</strong>。</li><li>常见形式： <ul><li>文本生成（如 GPT）</li><li>图像生成（如扩散模型 Diffusion）</li><li>音频合成（如语音克隆）</li><li>视频生成（如 Sora）</li></ul></li></ul><blockquote><p>⚠️ 注意：<strong>不是所有生成式 AI 都是大模型</strong>。例如图像生成的扩散模型就不是大语言模型。</p></blockquote><ol start="5"><li>什么是大语言模型（LLM, Large Language Model）？</li></ol><ul><li>是深度学习在自然语言处理领域的巅峰应用。</li><li>“大”体现在： <ul><li>参数量巨大（数十亿到万亿级）</li><li>训练数据海量（整个互联网文本）</li></ul></li><li>能力强大： <ul><li>理解上下文</li><li>生成流畅文本</li><li>回答问题、写诗、编程等</li></ul></li></ul><p>常见 LLM 示例：</p><table tabindex="0"><thead><tr><th>模型</th><th>国家/公司</th><th>特点</th></tr></thead><tbody><tr><td>GPT 系列（如 GPT-4）</td><td>OpenAI（美国）</td><td>强大的文本生成能力</td></tr><tr><td>ChatGLM（智谱AI）</td><td>中国</td><td>中文优化</td></tr><tr><td>Qwen（通义千问）</td><td>阿里云</td><td>多模态、开源</td></tr><tr><td>ERNIE Bot（文心一言）</td><td>百度</td><td>结合搜索优势</td></tr></tbody></table><blockquote><p>❓ 争议点：<strong>所有大语言模型都是生成式 AI 吗？</strong></p><ul><li>多数是（如 GPT），但也有例外：</li><li>例如 Google 的 <strong>BERT</strong>：擅长理解语言（用于搜索排序、情感分析），但<strong>不擅长生成连贯长文本</strong>，因此有人认为它不算“生成式 AI”。</li></ul></blockquote><h3 id="_4、一句话总结" tabindex="-1">4、一句话总结 <a class="header-anchor" href="#_4、一句话总结" aria-label="Permalink to &quot;4、一句话总结&quot;">​</a></h3><blockquote><p><strong>AIGC 是结果，生成式 AI 是能力，大模型是工具，深度学习是方法，机器学习是路径，人工智能是目标。</strong></p></blockquote><h2 id="二、啥是大语言模型-llm" tabindex="-1">二、啥是大语言模型（LLM） <a class="header-anchor" href="#二、啥是大语言模型-llm" aria-label="Permalink to &quot;二、啥是大语言模型（LLM）&quot;">​</a></h2><h3 id="_1、大语言模型的-出圈-时刻" tabindex="-1">1、大语言模型的“出圈”时刻 <a class="header-anchor" href="#_1、大语言模型的-出圈-时刻" aria-label="Permalink to &quot;1、大语言模型的“出圈”时刻&quot;">​</a></h3><ul><li><strong>2022年11月30日</strong>，OpenAI 发布 <strong>ChatGPT</strong>。</li><li>它成为互联网历史上<strong>最快突破1亿用户</strong>的产品，引爆全球对 AI 的关注。</li><li>一夜之间，各类 AI 聊天助手如雨后春笋般涌现。</li><li>而这一切的核心技术基础，就是——<strong>大语言模型（Large Language Model, LLM）</strong>。</li></ul><h3 id="_2、什么是大语言模型-llm" tabindex="-1">2、什么是大语言模型（LLM）？ <a class="header-anchor" href="#_2、什么是大语言模型-llm" aria-label="Permalink to &quot;2、什么是大语言模型（LLM）？&quot;">​</a></h3><h4 id="✅-定义" tabindex="-1">✅ 定义： <a class="header-anchor" href="#✅-定义" aria-label="Permalink to &quot;✅ 定义：&quot;">​</a></h4><p><strong>大语言模型</strong>（LLM），全称 <em>Large Language Model</em>，是一种基于<strong>深度学习</strong>的自然语言处理模型，能够理解并生成人类语言。</p><h4 id="🔧-核心能力" tabindex="-1">🔧 核心能力： <a class="header-anchor" href="#🔧-核心能力" aria-label="Permalink to &quot;🔧 核心能力：&quot;">​</a></h4><p>给它一段文本输入，它可以完成多种任务，例如：</p><ul><li>文本生成（写文章、写诗、写代码）</li><li>内容总结</li><li>情感分析</li><li>语言翻译</li><li>分类与改写</li></ul><blockquote><p>🌐 它不是“一个工具”，而是一个“通才型 AI 大脑”。</p></blockquote><hr><h3 id="_3、-大-在哪里-——参数与数据的双重爆炸" tabindex="-1">3、“大”在哪里？——参数与数据的双重爆炸 <a class="header-anchor" href="#_3、-大-在哪里-——参数与数据的双重爆炸" aria-label="Permalink to &quot;3、“大”在哪里？——参数与数据的双重爆炸&quot;">​</a></h3><p>很多人以为“大”只是数据多，其实不然。<strong>“大”主要体现在两个方面</strong>：</p><table tabindex="0"><thead><tr><th>维度</th><th>说明</th></tr></thead><tbody><tr><td><strong>1. 海量训练数据</strong></td><td>使用整个互联网规模的文本：<br>• 书籍、新闻、论文<br>• Wikipedia、社交媒体帖子等<br>让模型“读万卷书”，理解语言规律</td></tr><tr><td><strong>2. 巨量参数</strong></td><td>参数是模型在训练中“学到的知识”<br>决定了模型如何响应输入<br>参数越多，模型越灵活、越强大</td></tr></tbody></table><h4 id="📈-参数增长趋势-以-gpt-系列为例" tabindex="-1">📈 参数增长趋势（以 GPT 系列为例）： <a class="header-anchor" href="#📈-参数增长趋势-以-gpt-系列为例" aria-label="Permalink to &quot;📈 参数增长趋势（以 GPT 系列为例）：&quot;">​</a></h4><table tabindex="0"><thead><tr><th>模型</th><th>参数数量</th><th>相当于什么？</th></tr></thead><tbody><tr><td>GPT-1（2018-06）</td><td>1.17 亿</td><td>初级语言模型</td></tr><tr><td>GPT-2（2019-02）</td><td>15 亿</td><td>能写简单文章</td></tr><tr><td>GPT-3（2020-05）</td><td><strong>1750 亿</strong></td><td>超大规模，接近人类语言理解能力</td></tr><tr><td>ChatGPT（2020-11）</td><td><strong>1750 亿</strong></td><td></td></tr><tr><td>GPT-4（2023-03）</td><td><strong>1.76 万亿</strong></td><td></td></tr><tr><td>GPT-5（2025-08）</td><td><strong>17.5万亿</strong></td><td></td></tr></tbody></table><blockquote><p>🍞 类比理解： 就像做蛋糕，小模型只能调“面粉、糖、蛋”；大模型还能调“奶油、牛奶、苏打粉、可可粉、温度、时间”……变量越多，越能做出复杂美味的蛋糕，甚至创造新口味！</p></blockquote><hr><h3 id="_4、大模型-vs-小模型-通才-vs-专才" tabindex="-1">4、大模型 vs 小模型：通才 vs 专才 <a class="header-anchor" href="#_4、大模型-vs-小模型-通才-vs-专才" aria-label="Permalink to &quot;4、大模型 vs 小模型：通才 vs 专才&quot;">​</a></h3><table tabindex="0"><thead><tr><th>类型</th><th>特点</th><th>举例</th></tr></thead><tbody><tr><td><strong>小模型</strong></td><td>针对单一任务训练<br>如：情感分类、命名实体识别</td><td>训练一个模型只做“判断评论是好评还是差评”</td></tr><tr><td><strong>大模型</strong></td><td>一个模型搞定多种任务<br>无需重新训练，通过“提示”即可切换功能</td><td>同一个模型：<br>→ 写文章<br>→ 改写句子<br>→ 回答问题<br>→ 写代码</td></tr></tbody></table><blockquote><p>✅ 大模型的优势：<strong>泛化能力强、部署成本低、适应性广</strong></p></blockquote><hr><h3 id="_5、技术里程碑-transformer-架构的诞生" tabindex="-1">5、技术里程碑：Transformer 架构的诞生 <a class="header-anchor" href="#_5、技术里程碑-transformer-架构的诞生" aria-label="Permalink to &quot;5、技术里程碑：Transformer 架构的诞生&quot;">​</a></h3><p>虽然 ChatGPT 是2022年“出圈”的，但它的技术根源要追溯到 <strong>2017年</strong>。</p><h4 id="📄-2017年6月-谷歌发布划时代论文" tabindex="-1">📄 2017年6月：谷歌发布划时代论文 <a class="header-anchor" href="#📄-2017年6月-谷歌发布划时代论文" aria-label="Permalink to &quot;📄 2017年6月：谷歌发布划时代论文&quot;">​</a></h4><blockquote><p><strong><a href="https://arxiv.org/pdf/1706.03762" target="_blank" rel="noreferrer">《Attention is All You Need》</a></strong></p></blockquote><p>这篇论文提出了 <strong>Transformer 架构</strong>，彻底改变了自然语言处理的发展方向。</p><h4 id="🔁-此前主流-rnn-循环神经网络" tabindex="-1">🔁 此前主流：RNN（循环神经网络） <a class="header-anchor" href="#🔁-此前主流-rnn-循环神经网络" aria-label="Permalink to &quot;🔁 此前主流：RNN（循环神经网络）&quot;">​</a></h4><ul><li>问题： <ul><li><strong>逐字顺序处理</strong>：必须等前一个词处理完才能处理下一个 → 速度慢</li><li><strong>长距离依赖难捕捉</strong>：比如句子开头的“猫”和结尾的“它”之间的关系容易丢失</li></ul></li><li>改进版：LSTM（长短期记忆网络），但依然无法根本解决效率问题</li></ul><h4 id="🚀-transformer-的突破性创新" tabindex="-1">🚀 Transformer 的突破性创新 <a class="header-anchor" href="#🚀-transformer-的突破性创新" aria-label="Permalink to &quot;🚀 Transformer 的突破性创新&quot;">​</a></h4><h5 id="_1-自注意力机制-self-attention" tabindex="-1">1. <strong>自注意力机制（Self-Attention）</strong> <a class="header-anchor" href="#_1-自注意力机制-self-attention" aria-label="Permalink to &quot;1. **自注意力机制（Self-Attention）**&quot;">​</a></h5><ul><li>核心思想：<strong>每个词在处理时，都会“关注”句子中所有其他词</strong></li><li>模型会为每个词分配一个“注意力权重”，表示它与其他词的相关性</li><li>权重是在训练中自动学习得到的</li></ul><blockquote><p>🎯 举例： 句子：“The cat was hungry, so it ate the food.”</p><ul><li>当处理 “it” 时，模型会发现它和 “cat” 的关联更强，而不是离得更近的 “hungry”</li><li>即使相隔很远，也能准确理解指代关系</li></ul></blockquote><p>✅ 优势：<strong>精准捕捉长距离语义依赖</strong></p><h5 id="_2-位置编码-positional-encoding" tabindex="-1">2. <strong>位置编码（Positional Encoding）</strong> <a class="header-anchor" href="#_2-位置编码-positional-encoding" aria-label="Permalink to &quot;2. **位置编码（Positional Encoding）**&quot;">​</a></h5><ul><li>问题：语言中“顺序”很重要（“你打我” ≠ “我打你”）</li><li>RNN 天然按顺序处理，但 Transformer 是<strong>并行处理所有词</strong></li><li>解决方案：给每个词加上“位置信息”的数字编码（位置向量）</li></ul><blockquote><p>🧮 输入 = 词向量 + 位置向量 → 模型既知道“词是什么”，也知道“词在哪儿”</p></blockquote><p>✅ 优势：<strong>支持并行计算，大幅提升训练速度</strong></p><hr><h3 id="_6、transformer-为何如此重要" tabindex="-1">6、Transformer 为何如此重要？ <a class="header-anchor" href="#_6、transformer-为何如此重要" aria-label="Permalink to &quot;6、Transformer 为何如此重要？&quot;">​</a></h3><table tabindex="0"><thead><tr><th>优势</th><th>说明</th></tr></thead><tbody><tr><td>✅ 并行计算</td><td>所有词同时处理，不再串行等待 → <strong>训练速度快几十倍</strong></td></tr><tr><td>✅ 长距离依赖</td><td>自注意力机制完美解决“遗忘远距离信息”问题</td></tr><tr><td>✅ 可扩展性强</td><td>支持训练超大规模模型（百亿、千亿参数）</td></tr><tr><td>✅ 成为行业标准</td><td>几乎所有现代大模型都基于 Transformer 或其变体</td></tr></tbody></table><blockquote><p>🌟 正是因为 Transformer 的出现，才使得 GPT、BERT、ChatGLM、通义千问等大模型成为可能。</p></blockquote><hr><h3 id="_7、gpt-名字的秘密" tabindex="-1">7、GPT 名字的秘密 <a class="header-anchor" href="#_7、gpt-名字的秘密" aria-label="Permalink to &quot;7、GPT 名字的秘密&quot;">​</a></h3><p><strong>GPT = Generative Pre-trained Transformer</strong></p><table tabindex="0"><thead><tr><th>缩写</th><th>含义</th><th>说明</th></tr></thead><tbody><tr><td><strong>G</strong>enerative</td><td>生成式</td><td>能生成新文本（而非仅分类）</td></tr><tr><td><strong>P</strong>re-trained</td><td>预训练</td><td>先在海量文本上自学语言规律</td></tr><tr><td><strong>T</strong>ransformer</td><td>架构</td><td>基于 Transformer 网络结构</td></tr></tbody></table><blockquote><p>🔍 所以，“GPT” 三个字母就揭示了它的核心技术路线。</p></blockquote><hr><h3 id="_8、常见大语言模型应用" tabindex="-1">8、常见大语言模型应用 <a class="header-anchor" href="#_8、常见大语言模型应用" aria-label="Permalink to &quot;8、常见大语言模型应用&quot;">​</a></h3><p>你日常使用的这些 AI 工具，背后都是大模型驱动：</p><table tabindex="0"><thead><tr><th>应用产品</th><th>所用大模型</th><th>国家/公司</th></tr></thead><tbody><tr><td>ChatGPT</td><td>GPT 系列</td><td>OpenAI（美国）</td></tr><tr><td>文心一言</td><td>ERNIE Bot</td><td>百度（中国）</td></tr><tr><td>通义千问</td><td>Qwen</td><td>阿里云（中国）</td></tr><tr><td>ChatGLM</td><td>GLM 系列</td><td>智谱AI（中国）</td></tr><tr><td>Claude</td><td>Claude 系列</td><td>Anthropic（美国）</td></tr></tbody></table><h2 id="三、ai聊天助手背后的黑科技" tabindex="-1">三、AI聊天助手背后的黑科技 <a class="header-anchor" href="#三、ai聊天助手背后的黑科技" aria-label="Permalink to &quot;三、AI聊天助手背后的黑科技&quot;">​</a></h2><p>一个常见的说法是，像GPT这样的生成式大模型通过“预测下一个最可能出现的词”来生成文本，类似于搜索引擎的自动补全。但这个过程背后是如何实现的？关键在于 <strong>Transformer 架构</strong>。</p><hr><h3 id="_1、transformer-大模型的基石" tabindex="-1">1、Transformer：大模型的基石 <a class="header-anchor" href="#_1、transformer-大模型的基石" aria-label="Permalink to &quot;1、Transformer：大模型的基石&quot;">​</a></h3><p>自2017年论文《Attention is All You Need》提出 <strong>Transformer</strong> 架构以来，它几乎统一了自然语言处理领域。无论是OpenAI的GPT、清华的JLM，还是百度的ERNIE，其核心都离不开Transformer。</p><p>Transformer由两个主要部分组成：</p><ul><li><strong>编码器（Encoder）</strong></li><li><strong>解码器（Decoder）</strong></li></ul><hr><h3 id="_2、输入处理流程" tabindex="-1">2、输入处理流程 <a class="header-anchor" href="#_2、输入处理流程" aria-label="Permalink to &quot;2、输入处理流程&quot;">​</a></h3><ol><li><strong>Token化（Tokenization）</strong><ul><li>输入文本被拆分为基本单位——<strong>Token</strong>。</li><li>Token可以是一个单词、子词或汉字。</li><li>每个Token被映射为一个整数ID（Token ID），因为计算机只能处理数字。</li></ul></li><li><strong>词嵌入（Embedding）</strong><ul><li>每个Token ID通过嵌入层转换为一个<strong>向量</strong>（一串数字）。</li><li>向量能表达更丰富的语义和语法信息，比如“男人”与“国王”、“女人”与“女王”之间的类比关系可以在向量空间中体现。</li><li>相似含义的词在向量空间中距离更近。</li></ul></li><li><strong>位置编码（Positional Encoding）</strong><ul><li>由于Transformer本身不感知顺序，需要加入位置信息。</li><li>将表示词序的位置向量与词向量相加，使模型能理解词语的先后顺序。</li></ul></li></ol><hr><h3 id="_3、编码器-encoder-的作用" tabindex="-1">3、编码器（Encoder）的作用 <a class="header-anchor" href="#_3、编码器-encoder-的作用" aria-label="Permalink to &quot;3、编码器（Encoder）的作用&quot;">​</a></h3><p>编码器的任务是将输入文本转化为一种<strong>抽象的向量表示</strong>，包含词汇、语法、语义和上下文信息。</p><p>核心机制是 <strong>自注意力机制（Self-Attention）</strong>：</p><ul><li>模型在处理每个词时，会关注句子中所有其他词。</li><li>计算每对词之间的相关性，赋予不同“注意力权重”。</li><li>例如，“it”指代“animal”还是“street”，模型会根据上下文判断并加强相关词的权重。</li></ul><p><strong>多头自注意力（Multi-Head Attention）</strong>：</p><ul><li>多个并行的自注意力模块，各自关注不同特征（如语法、情感、实体等）。</li><li>提升模型对复杂语言结构的理解能力。</li></ul><p>后续还有**前馈神经网络（Feed-Forward Network）**进一步处理信息。</p><p>编码器通常<strong>多层堆叠</strong>，逐层提取更高层次的语言特征。</p><hr><h3 id="_4、解码器-decoder-的作用" tabindex="-1">4、解码器（Decoder）的作用 <a class="header-anchor" href="#_4、解码器-decoder-的作用" aria-label="Permalink to &quot;4、解码器（Decoder）的作用&quot;">​</a></h3><p>解码器负责<strong>逐个生成输出文本</strong>，是生成式模型的核心。</p><ol><li><strong>输入</strong>： <ul><li>来自编码器的输入文本抽象表示。</li><li>已生成的部分输出（保持连贯性）。</li><li>初始时输入一个“开始”标记（Start Token）。</li></ul></li><li><strong>嵌入 + 位置编码</strong>： <ul><li>与编码器相同，先将输入Token转为向量并加入位置信息。</li></ul></li><li><strong>带掩码的多头自注意力（Masked Multi-Head Attention）</strong>： <ul><li>只关注当前词及其之前的词，屏蔽后续词。</li><li>确保生成过程遵循时间顺序，不“偷看”未来内容。</li></ul></li><li><strong>编码器-解码器注意力（Encoder-Decoder Attention）</strong>： <ul><li>将编码器的输出与解码器的状态关联，确保生成内容与原始输入相关。</li></ul></li><li><strong>前馈神经网络</strong>： <ul><li>进一步增强表达能力。</li></ul></li></ol><p>解码器也<strong>多层堆叠</strong>，提升生成质量。</p><hr><h3 id="_5、输出生成" tabindex="-1">5、输出生成 <a class="header-anchor" href="#_5、输出生成" aria-label="Permalink to &quot;5、输出生成&quot;">​</a></h3><p>解码器最终通过两个层生成结果：</p><ul><li><strong>线性层（Linear Layer）</strong>：将向量映射到词汇表大小的维度。</li><li><strong>Softmax层</strong>：输出每个Token的概率分布。</li></ul><p>模型选择<strong>概率最高的Token</strong>作为下一个输出，重复此过程，直到生成“结束标记”（End Token）。</p><blockquote><p>注意：模型并不知道输出是否真实，只是基于统计规律“猜测”，因此可能出现“<strong>幻觉</strong>”（一本正经胡说八道）。</p></blockquote><hr><h3 id="_6、transformer的三大变体" tabindex="-1">6、Transformer的三大变体 <a class="header-anchor" href="#_6、transformer的三大变体" aria-label="Permalink to &quot;6、Transformer的三大变体&quot;">​</a></h3><ol><li><strong>仅编码器模型（Encoder-only）</strong><ul><li>如 <strong>BERT</strong></li><li>擅长理解任务：填空、情感分析、命名实体识别等。</li></ul></li><li><strong>仅解码器模型（Decoder-only）</strong><ul><li>如 <strong>GPT系列</strong></li><li>擅长生成任务：文本生成、对话、写作等。</li></ul></li><li><strong>编码器-解码器模型（Encoder-Decoder）</strong><ul><li>如 <strong>T5、BART</strong></li><li>擅长序列到序列任务：翻译、摘要、问答等。</li></ul></li></ol><h2 id="四、如何3步训练出一个ai聊天助手" tabindex="-1">四、如何3步训练出一个AI聊天助手 <a class="header-anchor" href="#四、如何3步训练出一个ai聊天助手" aria-label="Permalink to &quot;四、如何3步训练出一个AI聊天助手&quot;">​</a></h2><p>三步法概述</p><h3 id="_1、无监督预训练" tabindex="-1">1、无监督预训练 <a class="header-anchor" href="#_1、无监督预训练" aria-label="Permalink to &quot;1、无监督预训练&quot;">​</a></h3><ul><li>通过大量文本进行无监督学习，构建基础模型。 <ul><li>利用互联网上的各种文本资源（如书籍、新闻文章、科学论文等）作为训练数据。</li><li>模型从中学习语言的语法和语义规则，了解表达结构和模式。</li></ul></li><li>理解token的概念及其在模型中的作用。 <ul><li>Token是模型处理文本的基本单位，短词可能是一个token，长词或中文字符则可能被拆分为多个token。</li></ul></li><li>预训练过程的技术细节与挑战。 <ul><li>预训练耗时费力且成本高昂，但最终得到一个能够预测下一个token的基础模型。</li></ul></li></ul><h3 id="_2、监督微调" tabindex="-1">2、监督微调 <a class="header-anchor" href="#_2、监督微调" aria-label="Permalink to &quot;2、监督微调&quot;">​</a></h3><ul><li>使用高质量的人类对话数据对基础模型进行微调。 <ul><li>微调使模型更加适应特定任务，比如回答问题的能力。</li></ul></li><li>微调过程中的数据规模与训练时长。 <ul><li>相较于预训练，微调所需的训练数据规模更小，训练时长也更短。</li></ul></li><li>监督微调（SFT）的结果及其改进点。 <ul><li>经过SFT后，模型能更好地对问题作出回应，但仍需进一步优化以提升性能。</li></ul></li></ul><h3 id="_3、强化学习与奖励模型" tabindex="-1">3、强化学习与奖励模型 <a class="header-anchor" href="#_3、强化学习与奖励模型" aria-label="Permalink to &quot;3、强化学习与奖励模型&quot;">​</a></h3><ul><li>利用人类评估员对回答质量进行评分。 <ul><li>基于评分数据训练出一个奖励模型，该模型用于预测回答的质量评分。</li></ul></li><li>强化学习的过程与原理。 <ul><li>强化学习类似于训练小狗，模型根据反馈调整策略，最大化奖励或最小化损失。</li></ul></li><li>强化学习的应用与效果。 <ul><li>强化学习帮助模型不断优化其生成策略，提高回答的质量。</li></ul></li></ul>`,133)])])}const b=a(n,[["render",o]]);export{p as __pageData,b as default};
